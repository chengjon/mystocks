# æ•°æ®åº“ç®¡ç†å™¨ä½¿ç”¨ç¤ºä¾‹

æœ¬æ–‡æ¡£æä¾›æ•°æ®åº“ç®¡ç†å™¨æ¨¡å—çš„è¯¦ç»†ä½¿ç”¨ç¤ºä¾‹ï¼ŒåŒ…æ‹¬å¤šæ•°æ®åº“é…ç½®ã€ç›‘æ§ç³»ç»Ÿä½¿ç”¨å’Œå®é™…åœºæ™¯åº”ç”¨ã€‚

## ğŸ“‹ ç›®å½•

1. [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
2. [åŸºç¡€ä½¿ç”¨ç¤ºä¾‹](#åŸºç¡€ä½¿ç”¨ç¤ºä¾‹)
3. [Redisçƒ­æ•°æ®å›ºåŒ–ç¤ºä¾‹](#redisçƒ­æ•°æ®å›ºåŒ–ç¤ºä¾‹) â­ **æ–°å¢**
4. [å¼ºåˆ¶æ›´æ–°å®æ—¶æ•°æ®](#å¼ºåˆ¶æ›´æ–°å®æ—¶æ•°æ®) â­ **æ–°å¢**
5. [å¤šæ•°æ®åº“ç®¡ç†](#å¤šæ•°æ®åº“ç®¡ç†)
6. [ç›‘æ§ç³»ç»Ÿä½¿ç”¨](#ç›‘æ§ç³»ç»Ÿä½¿ç”¨)
7. [DataFrameè½¬SQL](#dataframeè½¬sql)
8. [TDengineæ—¶åºæ•°æ®åº“](#tdengineæ—¶åºæ•°æ®åº“)
9. [Jupyterç¯å¢ƒä½¿ç”¨](#jupyterç¯å¢ƒä½¿ç”¨)
10. [å®‰å…¨é…ç½®](#å®‰å…¨é…ç½®)
11. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
12. [ä¸v2.0ç³»ç»Ÿé›†æˆ](#ä¸v20ç³»ç»Ÿé›†æˆ)

## ğŸ”§ ç¯å¢ƒé…ç½®

### æ•°æ®åº“ç¯å¢ƒå‡†å¤‡

#### 1. MySQL/MariaDBé…ç½®

```bash
# å®‰è£…MySQLé©±åŠ¨
pip install pymysql

# .envæ–‡ä»¶é…ç½®
MYSQL_HOST=192.168.123.104
MYSQL_PORT=3306
MYSQL_USER=root
MYSQL_PASSWORD=your_password
MYSQL_DATABASE=test_db

# MariaDBé…ç½®ï¼ˆå¯é€‰ï¼‰
MARIADB_HOST=192.168.123.104
MARIADB_PORT=3307
MARIADB_USER=root
MARIADB_PASSWORD=your_password
```

#### 2. PostgreSQLé…ç½®

```bash
# å®‰è£…PostgreSQLé©±åŠ¨
pip install psycopg2-binary

# .envæ–‡ä»¶é…ç½®
POSTGRESQL_HOST=192.168.123.104
POSTGRESQL_PORT=5433
POSTGRESQL_USER=postgres
POSTGRESQL_PASSWORD=your_password
POSTGRESQL_DATABASE=quant_research
```

#### 3. TDengineé…ç½®

```bash
# å®‰è£…TDengineé©±åŠ¨
pip install taospy

# .envæ–‡ä»¶é…ç½®
TDENGINE_HOST=192.168.123.104
TDENGINE_PORT=6030
TDENGINE_USER=root
TDENGINE_PASSWORD=taosdata
TDENGINE_DATABASE=market_data
```

#### 4. Redisé…ç½®

```bash
# å®‰è£…Redisé©±åŠ¨
pip install redis

# .envæ–‡ä»¶é…ç½®
REDIS_HOST=192.168.123.104
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0
```

### ç›‘æ§æ•°æ®åº“é…ç½®

```bash
# ç›‘æ§æ•°æ®åº“URLï¼ˆç‹¬ç«‹äºä¸šåŠ¡æ•°æ®åº“ï¼‰
MONITOR_DB_URL=mysql+pymysql://root:password@192.168.123.104:3306/db_monitor
```

## ğŸš€ åŸºç¡€ä½¿ç”¨ç¤ºä¾‹

### 1. æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–

```python
from db_manager.database_manager import DatabaseTableManager, DatabaseType
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def basic_manager_example():
    """æ•°æ®åº“ç®¡ç†å™¨åŸºç¡€ä½¿ç”¨ç¤ºä¾‹"""
    
    print("=== æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ– ===")
    
    # åˆ›å»ºç®¡ç†å™¨å®ä¾‹
    manager = DatabaseTableManager()
    
    # æ£€æŸ¥å„æ•°æ®åº“çš„å¯ç”¨æ€§
    databases = [
        (DatabaseType.MYSQL, "MySQL"),
        (DatabaseType.POSTGRESQL, "PostgreSQL"),
        (DatabaseType.TDENGINE, "TDengine"),
        (DatabaseType.REDIS, "Redis"),
        (DatabaseType.MARIADB, "MariaDB")
    ]
    
    print("æ•°æ®åº“å¯ç”¨æ€§æ£€æŸ¥:")
    for db_type, db_name in databases:
        try:
            connection = manager.get_connection(db_type, "test_db")
            if connection:
                print(f"  âœ… {db_name}: å¯ç”¨")
                connection.close()
            else:
                print(f"  âŒ {db_name}: è¿æ¥å¤±è´¥")
        except Exception as e:
            print(f"  âŒ {db_name}: {str(e)[:50]}...")
    
    return manager

# è¿è¡Œç¤ºä¾‹
manager = basic_manager_example()
```

### 2. è¡¨ç»“æ„åˆ›å»ºç¤ºä¾‹

```python
def table_creation_example():
    """è¡¨ç»“æ„åˆ›å»ºç¤ºä¾‹"""
    
    manager = DatabaseTableManager()
    
    # 1. ç®€å•è¡¨ç»“æ„å®šä¹‰
    simple_table_config = {
        'table_name': 'stock_basic_info',
        'database_name': 'test_db',
        'database_type': 'MySQL',
        'columns': [
            {
                'name': 'symbol',
                'type': 'VARCHAR',
                'length': 10,
                'nullable': False,
                'comment': 'è‚¡ç¥¨ä»£ç '
            },
            {
                'name': 'name',
                'type': 'VARCHAR',
                'length': 100,
                'nullable': False,
                'comment': 'è‚¡ç¥¨åç§°'
            },
            {
                'name': 'exchange',
                'type': 'VARCHAR',
                'length': 10,
                'nullable': False,
                'comment': 'äº¤æ˜“æ‰€'
            },
            {
                'name': 'industry',
                'type': 'VARCHAR',
                'length': 50,
                'comment': 'æ‰€å±è¡Œä¸š'
            },
            {
                'name': 'list_date',
                'type': 'DATE',
                'comment': 'ä¸Šå¸‚æ—¥æœŸ'
            },
            {
                'name': 'created_at',
                'type': 'TIMESTAMP',
                'nullable': False,
                'default': 'CURRENT_TIMESTAMP',
                'comment': 'åˆ›å»ºæ—¶é—´'
            }
        ],
        'primary_key': ['symbol'],
        'indexes': [
            {'name': 'idx_exchange', 'columns': ['exchange']},
            {'name': 'idx_industry', 'columns': ['industry']}
        ]
    }
    
    print("=== åˆ›å»ºè‚¡ç¥¨åŸºæœ¬ä¿¡æ¯è¡¨ ===")
    try:
        # ç”ŸæˆDDL
        ddl = manager.generate_ddl(simple_table_config)
        print(f"ç”Ÿæˆçš„DDL:")
        print(ddl)
        
        # åˆ›å»ºè¡¨
        success = manager.create_table_from_config(simple_table_config)
        print(f"è¡¨åˆ›å»ºç»“æœ: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
        
    except Exception as e:
        print(f"è¡¨åˆ›å»ºå¤±è´¥: {e}")
    
    # 2. å¤æ‚è¡¨ç»“æ„å®šä¹‰ï¼ˆæ—¥çº¿æ•°æ®è¡¨ï¼‰
    kline_table_config = {
        'table_name': 'stock_daily_kline',
        'database_name': 'test_db',
        'database_type': 'MySQL',
        'columns': [
            {'name': 'id', 'type': 'BIGINT', 'nullable': False, 'auto_increment': True},
            {'name': 'symbol', 'type': 'VARCHAR', 'length': 10, 'nullable': False},
            {'name': 'trade_date', 'type': 'DATE', 'nullable': False},
            {'name': 'open', 'type': 'DECIMAL', 'precision': 10, 'scale': 2},
            {'name': 'high', 'type': 'DECIMAL', 'precision': 10, 'scale': 2},
            {'name': 'low', 'type': 'DECIMAL', 'precision': 10, 'scale': 2},
            {'name': 'close', 'type': 'DECIMAL', 'precision': 10, 'scale': 2, 'nullable': False},
            {'name': 'volume', 'type': 'BIGINT', 'default': 0},
            {'name': 'amount', 'type': 'DECIMAL', 'precision': 15, 'scale': 2, 'default': 0},
            {'name': 'adj_factor', 'type': 'DECIMAL', 'precision': 10, 'scale': 6, 'default': 1},
            {'name': 'pre_close', 'type': 'DECIMAL', 'precision': 10, 'scale': 2},
            {'name': 'change_amount', 'type': 'DECIMAL', 'precision': 10, 'scale': 2},
            {'name': 'change_pct', 'type': 'DECIMAL', 'precision': 8, 'scale': 4},
            {'name': 'turnover_rate', 'type': 'DECIMAL', 'precision': 8, 'scale': 4},
            {'name': 'updated_at', 'type': 'TIMESTAMP', 'default': 'CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP'}
        ],
        'primary_key': ['id'],
        'unique_keys': [['symbol', 'trade_date']],
        'indexes': [
            {'name': 'idx_symbol_date', 'columns': ['symbol', 'trade_date']},
            {'name': 'idx_trade_date', 'columns': ['trade_date']},
            {'name': 'idx_symbol', 'columns': ['symbol']}
        ],
        'engine': 'InnoDB',
        'charset': 'utf8mb4'
    }
    
    print("\n=== åˆ›å»ºè‚¡ç¥¨æ—¥çº¿æ•°æ®è¡¨ ===")
    try:
        ddl = manager.generate_ddl(kline_table_config)
        print(f"æ—¥çº¿æ•°æ®è¡¨DDL:")
        print(ddl)
        
        success = manager.create_table_from_config(kline_table_config)
        print(f"æ—¥çº¿æ•°æ®è¡¨åˆ›å»ºç»“æœ: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
        
    except Exception as e:
        print(f"æ—¥çº¿æ•°æ®è¡¨åˆ›å»ºå¤±è´¥: {e}")

table_creation_example()
```

## ğŸ”¥ Redisçƒ­æ•°æ®å›ºåŒ–ç¤ºä¾‹

### 1. åŸºç¡€å›ºåŒ–ä½¿ç”¨

```python
from db_manager.redis_data_fixation import RedisDataFixationManager, FixationStrategy
import pandas as pd

def basic_fixation_example():
    """Redisæ•°æ®å›ºåŒ–åŸºç¡€ç¤ºä¾‹"""
    
    print("=== Redisçƒ­æ•°æ®å›ºåŒ–ç¤ºä¾‹ ===")
    
    # åˆ›å»ºå›ºåŒ–ç®¡ç†å™¨
    fixation_manager = RedisDataFixationManager()
    
    # æ¨¡æ‹Ÿå®æ—¶æ•°æ®
    sample_data = pd.DataFrame({
        'symbol': ['000001.SZ', '000002.SZ', '600000.SH'],
        'price': [10.50, 8.20, 12.80],
        'volume': [1000000, 800000, 1200000],
        'amount': [10500000, 6560000, 15360000],
        'timestamp': pd.Timestamp.now()
    })
    
    # ç«‹å³å›ºåŒ–æ•°æ®
    print("ğŸ“Š ç«‹å³å›ºåŒ–æ•°æ®åˆ°æ°¸ä¹…å­˜å‚¨...")
    fixation_results = fixation_manager.fixate_redis_data_immediate(sample_data)
    
    for storage_type, success in fixation_results.items():
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        print(f"   {storage_type}: {status}")
    
    # è·å–å›ºåŒ–ç»Ÿè®¡
    stats = fixation_manager.get_fixation_statistics()
    print(f"\nğŸ“ˆ å›ºåŒ–ç»Ÿè®¡: {stats}")

basic_fixation_example()
```

### 2. ä¸åŒå›ºåŒ–ç­–ç•¥é…ç½®

```python
def fixation_strategy_example():
    """ä¸åŒå›ºåŒ–ç­–ç•¥çš„é…ç½®ç¤ºä¾‹"""
    
    print("=== å›ºåŒ–ç­–ç•¥é…ç½®ç¤ºä¾‹ ===")
    
    # ç­–ç•¥1: ç«‹å³å›ºåŒ–ï¼ˆé€‚åˆé‡è¦æ•°æ®ï¼‰
    immediate_config = {
        'fixation_strategy': FixationStrategy.IMMEDIATE,
        'backup_to_tick_data': True,
        'backup_to_daily_kline': False
    }
    
    # ç­–ç•¥2: è¿‡æœŸå‰å›ºåŒ–ï¼ˆæ¨èç­–ç•¥ï¼‰
    before_expire_config = {
        'fixation_strategy': FixationStrategy.BEFORE_EXPIRE,
        'fixation_interval_seconds': 240,  # 4åˆ†é’Ÿ
        'backup_to_tick_data': True,
        'backup_to_daily_kline': True
    }
    
    # ç­–ç•¥3: å®šæ—¶å›ºåŒ–ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
    scheduled_config = {
        'fixation_strategy': FixationStrategy.SCHEDULED,
        'fixation_interval_seconds': 300,  # 5åˆ†é’Ÿ
        'batch_size': 1000
    }
    
    strategies = [
        ("ç«‹å³å›ºåŒ–", immediate_config),
        ("è¿‡æœŸå‰å›ºåŒ–", before_expire_config), 
        ("å®šæ—¶å›ºåŒ–", scheduled_config)
    ]
    
    for strategy_name, config in strategies:
        print(f"\nğŸ“‹ {strategy_name}ç­–ç•¥:")
        for key, value in config.items():
            print(f"   {key}: {value}")

fixation_strategy_example()
```

### 3. å¤šé‡å¤‡ä»½é…ç½®

```python
def multi_backup_example():
    """å¤šé‡å¤‡ä»½é…ç½®ç¤ºä¾‹"""
    
    print("=== å¤šé‡å¤‡ä»½é…ç½®ç¤ºä¾‹ ===")
    
    # é…ç½®æ–‡ä»¶å†…å®¹ç¤ºä¾‹
    fixation_config = """
    # Rediså›ºåŒ–é…ç½® - å¤šé‡å¤‡ä»½ç­–ç•¥
    FIXATION_STRATEGY=before_expire
    FIXATION_INTERVAL_SECONDS=240
    
    # ä¸»è¦å¤‡ä»½ï¼šTDengineæ—¶åºå­˜å‚¨ï¼ˆæ¨èï¼‰
    BACKUP_TO_TICK_DATA=true
    
    # æ¬¡è¦å¤‡ä»½ï¼šPostgreSQLåˆ†æå­˜å‚¨ï¼ˆå¯é€‰ï¼‰
    BACKUP_TO_DAILY_KLINE=true
    
    # åº”æ€¥å¤‡ä»½ï¼šæ–‡ä»¶ç³»ç»Ÿï¼ˆå…œåº•æ–¹æ¡ˆï¼‰
    BACKUP_TO_FILE_SYSTEM=true
    BACKUP_DIRECTORY=./backup/redis_fixation
    
    # æ€§èƒ½é…ç½®
    ENABLE_COMPRESSION=true
    BATCH_SIZE=1000
    MAX_RETRY_ATTEMPTS=3
    """
    
    print("ğŸ“„ æ¨èçš„å›ºåŒ–é…ç½®:")
    print(fixation_config)
    
    # å­˜å‚¨ç›®æ ‡è¯´æ˜
    storage_targets = {
        "TDengine": {
            "ç”¨é€”": "æ—¶åºåˆ†æã€å†å²å›æµ‹",
            "ä¼˜ç‚¹": "é«˜æ€§èƒ½æ—¶åºæŸ¥è¯¢ã€æ•°æ®å‹ç¼©",
            "é€‚ç”¨": "Tickçº§æ•°æ®ã€æŠ€æœ¯æŒ‡æ ‡è®¡ç®—"
        },
        "PostgreSQL": {
            "ç”¨é€”": "èšåˆåˆ†æã€æŠ¥è¡¨ç»Ÿè®¡", 
            "ä¼˜ç‚¹": "SQLåˆ†æã€å¤æ‚æŸ¥è¯¢",
            "é€‚ç”¨": "æ—¥çº¿æ•°æ®ã€è´¢åŠ¡åˆ†æ"
        },
        "æ–‡ä»¶ç³»ç»Ÿ": {
            "ç”¨é€”": "åº”æ€¥å¤‡ä»½ã€æ•°æ®è¿ç§»",
            "ä¼˜ç‚¹": "ç®€å•å¯é ã€æ˜“äºæ¢å¤",
            "é€‚ç”¨": "ç¾éš¾æ¢å¤ã€ç³»ç»Ÿè¿ç§»"
        }
    }
    
    print("\nğŸ—„ï¸ å­˜å‚¨ç›®æ ‡è¯´æ˜:")
    for target, info in storage_targets.items():
        print(f"\nğŸ“Š {target}:")
        for key, value in info.items():
            print(f"   {key}: {value}")

multi_backup_example()
```

## âš¡ å¼ºåˆ¶æ›´æ–°å®æ—¶æ•°æ®

### 1. åŸºç¡€å¼ºåˆ¶æ›´æ–°

```python
def force_update_basic_example():
    """å¼ºåˆ¶æ›´æ–°åŸºç¡€ç¤ºä¾‹"""
    
    print("=== å¼ºåˆ¶æ›´æ–°å®æ—¶æ•°æ®ç¤ºä¾‹ ===")
    
    from db_manager.redis_data_fixation import RedisDataFixationManager
    
    # åˆ›å»ºå›ºåŒ–ç®¡ç†å™¨
    fixation_manager = RedisDataFixationManager()
    
    # å¼ºåˆ¶æ›´æ–°æ²ªæ·±å¸‚åœºæ•°æ®
    print("ğŸ”„ æ‰§è¡Œå¼ºåˆ¶æ›´æ–°...")
    update_result = fixation_manager.force_update_realtime_data(
        market_symbol="hs",
        bypass_cache=True
    )
    
    # æ˜¾ç¤ºæ›´æ–°ç»“æœ
    if update_result['success']:
        print("âœ… å¼ºåˆ¶æ›´æ–°æˆåŠŸ!")
        print(f"ğŸ“Š æ›´æ–°æ—¶é—´: {update_result['update_time']}")
        print(f"ğŸ“ˆ æ•°æ®æ¡æ•°: {update_result['data_count']}")
        print(f"ğŸ“‹ æ•°æ®åˆ—: {update_result['data_columns']}")
    else:
        print(f"âŒ å¼ºåˆ¶æ›´æ–°å¤±è´¥: {update_result.get('error', 'æœªçŸ¥é”™è¯¯')}")

force_update_basic_example()
```

### 2. å‘½ä»¤è¡Œå¼ºåˆ¶æ›´æ–°

```bash
# æ–¹å¼1: é€šè¿‡ç»Ÿä¸€å¯åŠ¨å™¨
python run_realtime_market_saver.py --force-update

# æ–¹å¼2: ç›´æ¥è¿è¡Œæ ¸å¿ƒç¨‹åº
python db_manager/save_realtime_market_data.py --force-update

# æ–¹å¼3: ç»„åˆä½¿ç”¨ï¼ˆå¼ºåˆ¶æ›´æ–°+è‡ªåŠ¨å›ºåŒ–ï¼‰
python db_manager/save_realtime_market_data.py --force-update --enable-fixation
```

### 3. ä¸åŒå¸‚åœºçš„å¼ºåˆ¶æ›´æ–°

```python
def multi_market_force_update():
    """å¤šå¸‚åœºå¼ºåˆ¶æ›´æ–°ç¤ºä¾‹"""
    
    print("=== å¤šå¸‚åœºå¼ºåˆ¶æ›´æ–°ç¤ºä¾‹ ===")
    
    from db_manager.redis_data_fixation import RedisDataFixationManager
    
    fixation_manager = RedisDataFixationManager()
    
    # æ”¯æŒçš„å¸‚åœºä»£ç 
    markets = {
        "hs": "æ²ªæ·±å¸‚åœº",
        "sh": "ä¸Šæµ·å¸‚åœº", 
        "sz": "æ·±åœ³å¸‚åœº"
    }
    
    results = {}
    
    for market_code, market_name in markets.items():
        print(f"\nğŸ”„ å¼ºåˆ¶æ›´æ–°{market_name} ({market_code})...")
        
        result = fixation_manager.force_update_realtime_data(
            market_symbol=market_code,
            bypass_cache=True
        )
        
        results[market_code] = result
        
        if result['success']:
            print(f"âœ… {market_name}æ›´æ–°æˆåŠŸ: {result['data_count']} æ¡æ•°æ®")
        else:
            print(f"âŒ {market_name}æ›´æ–°å¤±è´¥: {result.get('error')}")
    
    # æ±‡æ€»ç»“æœ
    print("\nğŸ“Š æ›´æ–°ç»“æœæ±‡æ€»:")
    total_success = sum(1 for r in results.values() if r['success'])
    total_markets = len(results)
    print(f"æˆåŠŸç‡: {total_success}/{total_markets} ({total_success/total_markets*100:.1f}%)")

multi_market_force_update()
```

### 4. å¼ºåˆ¶æ›´æ–°çš„æ—¶æœºé€‰æ‹©

```python
def update_timing_guide():
    """å¼ºåˆ¶æ›´æ–°æ—¶æœºæŒ‡å—"""
    
    print("=== å¼ºåˆ¶æ›´æ–°æ—¶æœºæŒ‡å— ===")
    
    timing_scenarios = {
        "äº¤æ˜“æ—¶é—´å¼€å§‹": {
            "æ—¶æœº": "09:30",
            "åŸå› ": "è·å–å¼€ç›˜åçš„æœ€æ–°æ•°æ®",
            "å‘½ä»¤": "python db_manager/save_realtime_market_data.py --force-update"
        },
        "åˆé—´ä¼‘å¸‚å": {
            "æ—¶æœº": "13:00",
            "åŸå› ": "è·å–ä¸Šåˆäº¤æ˜“çš„å®Œæ•´æ•°æ®",
            "å‘½ä»¤": "python db_manager/save_realtime_market_data.py --force-update --enable-fixation"
        },
        "æ”¶ç›˜å": {
            "æ—¶æœº": "15:30",
            "åŸå› ": "è·å–å…¨å¤©äº¤æ˜“çš„æœ€ç»ˆæ•°æ®",
            "å‘½ä»¤": "python db_manager/save_realtime_market_data.py --force-update --enable-fixation"
        },
        "æ•°æ®å¼‚å¸¸æ—¶": {
            "æ—¶æœº": "å‘ç°æ•°æ®å¼‚å¸¸æ—¶",
            "åŸå› ": "è·³è¿‡å¯èƒ½æœ‰é—®é¢˜çš„ç¼“å­˜æ•°æ®",
            "å‘½ä»¤": "python db_manager/save_realtime_market_data.py --force-update"
        },
        "ç³»ç»Ÿé‡å¯å": {
            "æ—¶æœº": "Redisé‡å¯æˆ–æ¸…ç©ºå",
            "åŸå› ": "é‡æ–°å¡«å……ç¼“å­˜æ•°æ®",
            "å‘½ä»¤": "python db_manager/save_realtime_market_data.py --force-update"
        }
    }
    
    print("ğŸ“… æ¨èçš„å¼ºåˆ¶æ›´æ–°æ—¶æœº:")
    for scenario, info in timing_scenarios.items():
        print(f"\nğŸ•’ {scenario}:")
        for key, value in info.items():
            print(f"   {key}: {value}")

update_timing_guide()
```

## ğŸ—„ï¸ å¤šæ•°æ®åº“ç®¡ç†

### 1. ä¸åŒæ•°æ®åº“çš„DDLç”Ÿæˆ

```python
def multi_database_ddl_example():
    """å¤šæ•°æ®åº“DDLç”Ÿæˆç¤ºä¾‹"""
    
    manager = DatabaseTableManager()
    
    # åŸºç¡€è¡¨ç»“æ„å®šä¹‰
    base_config = {
        'table_name': 'stock_quotes',
        'columns': [
            {'name': 'symbol', 'type': 'VARCHAR', 'length': 10, 'nullable': False},
            {'name': 'timestamp', 'type': 'TIMESTAMP', 'nullable': False},
            {'name': 'price', 'type': 'DECIMAL', 'precision': 10, 'scale': 2},
            {'name': 'volume', 'type': 'BIGINT'}
        ],
        'primary_key': ['symbol', 'timestamp']
    }
    
    # ä¸åŒæ•°æ®åº“çš„é…ç½®
    database_configs = [
        ('MySQL', 'mysql_db'),
        ('PostgreSQL', 'postgres_db'),
        ('TDengine', 'tdengine_db')
    ]
    
    print("=== å¤šæ•°æ®åº“DDLç”Ÿæˆå¯¹æ¯” ===")
    
    for db_type, db_name in database_configs:
        config = base_config.copy()
        config['database_type'] = db_type
        config['database_name'] = db_name
        
        try:
            ddl = manager.generate_ddl(config)
            print(f"\n{db_type} DDL:")
            print("-" * 50)
            print(ddl)
            
        except Exception as e:
            print(f"\n{db_type} DDLç”Ÿæˆå¤±è´¥: {e}")

multi_database_ddl_example()
```

### 2. TDengineè¶…çº§è¡¨åˆ›å»º

```python
def tdengine_supertable_example():
    """TDengineè¶…çº§è¡¨åˆ›å»ºç¤ºä¾‹"""
    
    manager = DatabaseTableManager()
    
    # TDengineè¶…çº§è¡¨é…ç½®
    supertable_config = {
        'table_name': 'stock_tick_data',
        'database_name': 'market_data',
        'database_type': 'TDengine',
        'is_super_table': True,  # æ ‡è®°ä¸ºè¶…çº§è¡¨
        'columns': [
            # æ—¶é—´æˆ³åˆ—ï¼ˆå¿…é¡»æ˜¯ç¬¬ä¸€åˆ—ï¼‰
            {'name': 'ts', 'type': 'TIMESTAMP', 'nullable': False, 'comment': 'æ—¶é—´æˆ³'},
            # æ•°æ®åˆ—
            {'name': 'price', 'type': 'FLOAT', 'nullable': False, 'comment': 'æˆäº¤ä»·æ ¼'},
            {'name': 'volume', 'type': 'INT', 'nullable': False, 'comment': 'æˆäº¤é‡'},
            {'name': 'amount', 'type': 'DOUBLE', 'comment': 'æˆäº¤é‡‘é¢'},
            {'name': 'buy_order_id', 'type': 'BIGINT', 'comment': 'ä¹°å•å·'},
            {'name': 'sell_order_id', 'type': 'BIGINT', 'comment': 'å–å•å·'}
        ],
        'tags': [
            # æ ‡ç­¾åˆ—
            {'name': 'symbol', 'type': 'VARCHAR', 'length': 20, 'comment': 'è‚¡ç¥¨ä»£ç '},
            {'name': 'exchange', 'type': 'VARCHAR', 'length': 10, 'comment': 'äº¤æ˜“æ‰€'},
            {'name': 'market_type', 'type': 'VARCHAR', 'length': 10, 'comment': 'å¸‚åœºç±»å‹'}
        ]
    }
    
    print("=== TDengineè¶…çº§è¡¨åˆ›å»ºç¤ºä¾‹ ===")
    
    try:
        # ç”ŸæˆTDengineè¶…çº§è¡¨DDL
        ddl = manager.generate_ddl(supertable_config)
        print("TDengineè¶…çº§è¡¨DDL:")
        print(ddl)
        
        # åˆ›å»ºè¶…çº§è¡¨
        success = manager.create_table_from_config(supertable_config)
        print(f"è¶…çº§è¡¨åˆ›å»ºç»“æœ: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
        
        # åˆ›å»ºå­è¡¨ç¤ºä¾‹
        subtable_config = {
            'table_name': 'stock_tick_000001',
            'parent_table': 'stock_tick_data',
            'database_name': 'market_data',
            'database_type': 'TDengine',
            'tag_values': {
                'symbol': '000001',
                'exchange': 'SZ',
                'market_type': 'MAIN'
            }
        }
        
        subtable_ddl = manager.generate_subtable_ddl(subtable_config)
        print(f"\nå­è¡¨DDLç¤ºä¾‹:")
        print(subtable_ddl)
        
    except Exception as e:
        print(f"TDengineè¶…çº§è¡¨æ“ä½œå¤±è´¥: {e}")

# æ³¨æ„ï¼šéœ€è¦å®‰è£…TDengineå®¢æˆ·ç«¯æ‰èƒ½æ‰§è¡Œ
# tdengine_supertable_example()
```

## ğŸ“Š ç›‘æ§ç³»ç»Ÿä½¿ç”¨

### 1. ç›‘æ§æ•°æ®åº“åˆå§‹åŒ–

```python
from db_manager.init_db_monitor import init_monitoring_database

def monitoring_system_example():
    """ç›‘æ§ç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹"""
    
    print("=== ç›‘æ§ç³»ç»Ÿåˆå§‹åŒ– ===")
    
    # 1. é¦–æ¬¡åˆå§‹åŒ–ï¼ˆä¿ç•™å·²æœ‰æ•°æ®ï¼‰
    print("1. é¦–æ¬¡åˆå§‹åŒ–ç›‘æ§ç³»ç»Ÿ...")
    success = init_monitoring_database(drop_existing=False)
    
    if success:
        print("âœ… ç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        print("å·²åˆ›å»ºç›‘æ§è¡¨:")
        print("  - table_creation_log: è¡¨åˆ›å»ºæ—¥å¿—")
        print("  - table_operation_log: è¡¨æ“ä½œæ—¥å¿—")
        print("  - table_validation_log: è¡¨éªŒè¯æ—¥å¿—")
        print("  - column_definition_log: åˆ—å®šä¹‰æ—¥å¿—")
    else:
        print("âŒ ç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
        return False
    
    # 2. å¼ºåˆ¶é‡å»ºï¼ˆåˆ é™¤å·²æœ‰è¡¨ï¼‰
    print("\n2. å¼ºåˆ¶é‡å»ºç›‘æ§è¡¨...")
    rebuild_success = init_monitoring_database(drop_existing=True)
    print(f"é‡å»ºç»“æœ: {'æˆåŠŸ' if rebuild_success else 'å¤±è´¥'}")
    
    return success

# è¿è¡Œç›‘æ§ç³»ç»Ÿç¤ºä¾‹
monitoring_system_example()
```

### 2. ç›‘æ§æ—¥å¿—æŸ¥è¯¢

```python
def monitoring_logs_example():
    """ç›‘æ§æ—¥å¿—æŸ¥è¯¢ç¤ºä¾‹"""
    
    from db_manager.database_manager import DatabaseTableManager, DatabaseType
    import pandas as pd
    
    manager = DatabaseTableManager()
    
    print("=== ç›‘æ§æ—¥å¿—æŸ¥è¯¢ç¤ºä¾‹ ===")
    
    try:
        # è¿æ¥ç›‘æ§æ•°æ®åº“
        connection = manager.get_connection(DatabaseType.MYSQL, "db_monitor")
        
        if not connection:
            print("âŒ æ— æ³•è¿æ¥ç›‘æ§æ•°æ®åº“")
            return
        
        # 1. æŸ¥è¯¢è¡¨åˆ›å»ºæ—¥å¿—
        print("1. æœ€è¿‘çš„è¡¨åˆ›å»ºæ—¥å¿—:")
        creation_logs_query = \"\"\"
        SELECT table_name, database_type, created_at, 
               operation_status, error_message
        FROM table_creation_log 
        ORDER BY created_at DESC 
        LIMIT 10
        \"\"\"
        
        creation_logs = pd.read_sql(creation_logs_query, connection)
        if not creation_logs.empty:
            print(creation_logs.to_string(index=False))
        else:
            print("  æš‚æ— è¡¨åˆ›å»ºæ—¥å¿—")
        
        # 2. æŸ¥è¯¢è¡¨æ“ä½œæ—¥å¿—
        print("\n2. æœ€è¿‘çš„è¡¨æ“ä½œæ—¥å¿—:")
        operation_logs_query = \"\"\"
        SELECT operation_id, table_name, operation_type, 
               operation_status, operation_time
        FROM table_operation_log 
        ORDER BY operation_time DESC 
        LIMIT 10
        \"\"\"
        
        operation_logs = pd.read_sql(operation_logs_query, connection)
        if not operation_logs.empty:
            print(operation_logs.to_string(index=False))
        else:
            print("  æš‚æ— è¡¨æ“ä½œæ—¥å¿—")
        
        # 3. ç»Ÿè®¡ä¿¡æ¯
        print("\n3. ç›‘æ§ç»Ÿè®¡ä¿¡æ¯:")
        
        stats_queries = {
            "æ€»è¡¨åˆ›å»ºæ•°": "SELECT COUNT(*) as count FROM table_creation_log",
            "æˆåŠŸåˆ›å»ºæ•°": "SELECT COUNT(*) as count FROM table_creation_log WHERE operation_status = 'success'",
            "å¤±è´¥åˆ›å»ºæ•°": "SELECT COUNT(*) as count FROM table_creation_log WHERE operation_status = 'failed'",
            "æ€»æ“ä½œæ•°": "SELECT COUNT(*) as count FROM table_operation_log"
        }
        
        for stat_name, query in stats_queries.items():
            result = pd.read_sql(query, connection)
            count = result.iloc[0]['count'] if not result.empty else 0
            print(f"  {stat_name}: {count}")
        
        connection.close()
        
    except Exception as e:
        print(f"âŒ ç›‘æ§æ—¥å¿—æŸ¥è¯¢å¤±è´¥: {e}")

monitoring_logs_example()
```

## ğŸ“ DataFrameè½¬SQL

### 1. åŸºæœ¬è½¬æ¢ç¤ºä¾‹

```python
from db_manager.df2sql import DataFrameToSQL
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

def dataframe_to_sql_example():
    """DataFrameè½¬SQLç¤ºä¾‹"""
    
    print("=== DataFrameåˆ°SQLè½¬æ¢ç¤ºä¾‹ ===")
    
    # 1. åˆ›å»ºç¤ºä¾‹æ•°æ®
    print("1. åˆ›å»ºç¤ºä¾‹è‚¡ç¥¨æ•°æ®...")
    
    # ç”Ÿæˆç¤ºä¾‹è‚¡ç¥¨æ—¥çº¿æ•°æ®
    symbols = ['000001', '600000', '000002']
    dates = pd.date_range(start='2024-01-01', end='2024-01-31', freq='D')
    
    data_records = []
    for symbol in symbols:
        base_price = np.random.uniform(10, 50)
        for date in dates:
            # æ¨¡æ‹Ÿä»·æ ¼æ³¢åŠ¨
            price_change = np.random.normal(0, 0.02)
            close_price = base_price * (1 + price_change)
            
            record = {
                'symbol': symbol,
                'trade_date': date.date(),
                'open': round(close_price * (1 + np.random.uniform(-0.01, 0.01)), 2),
                'high': round(close_price * (1 + np.random.uniform(0, 0.03)), 2),
                'low': round(close_price * (1 + np.random.uniform(-0.03, 0)), 2),
                'close': round(close_price, 2),
                'volume': np.random.randint(1000000, 50000000),
                'amount': round(close_price * np.random.randint(1000000, 50000000), 2),
                'turnover_rate': round(np.random.uniform(0.1, 5.0), 2)
            }
            data_records.append(record)
            base_price = close_price  # ä»·æ ¼å»¶ç»­
    
    stock_df = pd.DataFrame(data_records)
    print(f"ç”Ÿæˆæ•°æ®: {len(stock_df)} æ¡è®°å½•")
    print(stock_df.head())
    
    # 2. è½¬æ¢ä¸ºSQL
    print("\n2. è½¬æ¢ä¸ºSQLè¡¨ç»“æ„...")
    
    df_to_sql = DataFrameToSQL()
    
    # è‡ªåŠ¨åˆ†æDataFrameå¹¶ç”Ÿæˆè¡¨ç»“æ„
    table_schema = df_to_sql.analyze_dataframe(
        stock_df, 
        table_name='auto_stock_daily',
        database_type='MySQL'
    )
    
    print("è‡ªåŠ¨ç”Ÿæˆçš„è¡¨ç»“æ„:")
    for col in table_schema['columns']:
        print(f"  {col['name']}: {col['type']}")
    
    # 3. ç”ŸæˆDDL
    ddl = df_to_sql.generate_ddl(table_schema)
    print("\nç”Ÿæˆçš„DDL:")
    print(ddl)
    
    # 4. ç”ŸæˆINSERTè¯­å¥
    print("\n4. ç”ŸæˆINSERTè¯­å¥ç¤ºä¾‹...")
    insert_sql = df_to_sql.generate_insert_sql(
        stock_df.head(3), 
        'auto_stock_daily'
    )
    print("INSERTè¯­å¥ç¤ºä¾‹:")
    print(insert_sql)
    
    # 5. æ‰¹é‡æ’å…¥ä¼˜åŒ–
    print("\n5. æ‰¹é‡æ’å…¥ä¼˜åŒ–...")
    batch_inserts = df_to_sql.generate_batch_insert_sql(
        stock_df, 
        'auto_stock_daily',
        batch_size=1000
    )
    
    print(f"ç”Ÿæˆäº† {len(batch_inserts)} ä¸ªæ‰¹æ¬¡çš„INSERTè¯­å¥")
    print(f"ç¬¬ä¸€ä¸ªæ‰¹æ¬¡åŒ…å« {batch_inserts[0].count('VALUES') if batch_inserts else 0} æ¡è®°å½•")

dataframe_to_sql_example()
```

### 2. å¤æ‚æ•°æ®ç±»å‹å¤„ç†

```python
def complex_datatype_example():
    """å¤æ‚æ•°æ®ç±»å‹å¤„ç†ç¤ºä¾‹"""
    
    print("=== å¤æ‚æ•°æ®ç±»å‹å¤„ç†ç¤ºä¾‹ ===")
    
    # åˆ›å»ºåŒ…å«å¤æ‚æ•°æ®ç±»å‹çš„DataFrame
    complex_data = pd.DataFrame({
        'id': range(1, 6),
        'name': ['è‚¡ç¥¨A', 'è‚¡ç¥¨B', 'è‚¡ç¥¨C', 'è‚¡ç¥¨D', 'è‚¡ç¥¨E'],
        'price': [12.34, 56.78, 90.12, 34.56, 78.90],
        'volume': [1000000, 2000000, 1500000, 800000, 1200000],
        'is_active': [True, True, False, True, False],
        'created_at': pd.Timestamp.now(),
        'metadata': [{'sector': 'tech'}, {'sector': 'finance'}, {}, {'sector': 'energy'}, None],
        'tags': [['Aè‚¡', 'å¤§ç›˜'], ['æ¸¯è‚¡'], ['åˆ›ä¸šæ¿', 'å°ç›˜'], ['ä¸»æ¿'], []],
        'description': ['è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æè¿°æ–‡æœ¬' * 10, 'ç®€çŸ­æè¿°', None, '', 'ä¸­ç­‰é•¿åº¦çš„æè¿°æ–‡æœ¬']
    })
    
    print("å¤æ‚æ•°æ®ç¤ºä¾‹:")
    print(complex_data.info())
    print(complex_data.head())
    
    df_to_sql = DataFrameToSQL()
    
    # åˆ†æå¤æ‚æ•°æ®ç±»å‹
    print("\n=== æ•°æ®ç±»å‹åˆ†æ ===")
    
    # è‡ªå®šä¹‰ç±»å‹æ˜ å°„é…ç½®
    type_mapping_config = {
        'string_length_analysis': True,      # åˆ†æå­—ç¬¦ä¸²é•¿åº¦
        'json_detection': True,              # æ£€æµ‹JSONæ ¼å¼æ•°æ®
        'array_handling': 'json',            # æ•°ç»„å¤„ç†æ–¹å¼: 'json' æˆ– 'text'
        'boolean_as_tinyint': True,          # å¸ƒå°”å€¼è½¬æ¢ä¸ºTINYINT
        'decimal_precision': (10, 2),        # å°æ•°ç²¾åº¦é…ç½®
        'text_threshold': 255                # TEXTç±»å‹é˜ˆå€¼
    }
    
    table_schema = df_to_sql.analyze_dataframe(
        complex_data,
        table_name='complex_stock_data',
        database_type='MySQL',
        config=type_mapping_config
    )
    
    print("åˆ†æåçš„è¡¨ç»“æ„:")
    for col in table_schema['columns']:
        col_info = f"  {col['name']}: {col['type']}"
        if 'length' in col:
            col_info += f"({col['length']})"
        if 'nullable' in col:
            col_info += f" {'NULL' if col['nullable'] else 'NOT NULL'}"
        if 'comment' in col:
            col_info += f" -- {col['comment']}"
        print(col_info)
    
    # ç”Ÿæˆä¼˜åŒ–åçš„DDL
    ddl = df_to_sql.generate_ddl(table_schema)
    print("\nä¼˜åŒ–åçš„DDL:")
    print(ddl)

complex_datatype_example()
```

## âš¡ TDengineæ—¶åºæ•°æ®åº“

### 1. TDengineè¿æ¥å’ŒåŸºæœ¬æ“ä½œ

```python
def tdengine_basic_example():
    """TDengineåŸºæœ¬æ“ä½œç¤ºä¾‹"""
    
    from db_manager.database_manager import DatabaseTableManager, DatabaseType
    
    manager = DatabaseTableManager()
    
    print("=== TDengineåŸºæœ¬æ“ä½œç¤ºä¾‹ ===")
    
    try:
        # 1. æµ‹è¯•TDengineè¿æ¥
        print("1. æµ‹è¯•TDengineè¿æ¥...")
        connection = manager.get_connection(DatabaseType.TDENGINE, "market_data")
        
        if not connection:
            print("âŒ TDengineè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥:")
            print("  - TDengineæœåŠ¡æ˜¯å¦å¯åŠ¨")
            print("  - taospyåº“æ˜¯å¦å®‰è£…: pip install taospy")
            print("  - ç¯å¢ƒå˜é‡é…ç½®æ˜¯å¦æ­£ç¡®")
            return
        
        print("âœ… TDengineè¿æ¥æˆåŠŸ")
        
        # 2. åˆ›å»ºæ•°æ®åº“ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        print("\n2. åˆ›å»ºTDengineæ•°æ®åº“...")
        create_db_sql = \"\"\"
        CREATE DATABASE IF NOT EXISTS market_data 
        PRECISION 'ms' 
        KEEP 365 
        DAYS 1 
        BLOCKS 6 
        CACHE 64 
        MAXROWS 4096 
        MINROWS 100 
        COMP 2 
        WAL 1 
        FSYNC 3000 
        REPLICA 1
        \"\"\"
        
        cursor = connection.cursor()
        cursor.execute(create_db_sql)
        print("âœ… æ•°æ®åº“åˆ›å»º/éªŒè¯æˆåŠŸ")
        
        # 3. ä½¿ç”¨æ•°æ®åº“
        cursor.execute("USE market_data")
        
        # 4. åˆ›å»ºè¶…çº§è¡¨
        print("\n3. åˆ›å»ºè¶…çº§è¡¨...")
        create_stable_sql = \"\"\"
        CREATE STABLE IF NOT EXISTS stock_tick (
            ts TIMESTAMP,
            price FLOAT,
            volume INT,
            amount DOUBLE
        ) TAGS (
            symbol VARCHAR(20),
            exchange VARCHAR(10)
        )
        \"\"\"
        
        cursor.execute(create_stable_sql)
        print("âœ… è¶…çº§è¡¨åˆ›å»ºæˆåŠŸ")
        
        # 5. åˆ›å»ºå­è¡¨
        print("\n4. åˆ›å»ºå­è¡¨...")
        create_table_sql = \"\"\"
        CREATE TABLE IF NOT EXISTS stock_tick_000001 
        USING stock_tick 
        TAGS ('000001', 'SZ')
        \"\"\"
        
        cursor.execute(create_table_sql)
        print("âœ… å­è¡¨åˆ›å»ºæˆåŠŸ")
        
        # 6. æ’å…¥ç¤ºä¾‹æ•°æ®
        print("\n5. æ’å…¥ç¤ºä¾‹æ•°æ®...")
        
        import time
        current_time = int(time.time() * 1000)  # æ¯«ç§’æ—¶é—´æˆ³
        
        insert_sql = f\"\"\"
        INSERT INTO stock_tick_000001 VALUES 
        ({current_time}, 12.34, 1000, 12340.0),
        ({current_time + 1000}, 12.35, 1500, 18525.0),
        ({current_time + 2000}, 12.33, 800, 9864.0)
        \"\"\"
        
        cursor.execute(insert_sql)
        print("âœ… æ•°æ®æ’å…¥æˆåŠŸ")
        
        # 7. æŸ¥è¯¢æ•°æ®
        print("\n6. æŸ¥è¯¢æ•°æ®...")
        query_sql = \"\"\"
        SELECT ts, price, volume, amount 
        FROM stock_tick_000001 
        ORDER BY ts DESC 
        LIMIT 5
        \"\"\"
        
        cursor.execute(query_sql)
        results = cursor.fetchall()
        
        print("æŸ¥è¯¢ç»“æœ:")
        for row in results:
            print(f"  æ—¶é—´: {row[0]}, ä»·æ ¼: {row[1]}, æˆäº¤é‡: {row[2]}, æˆäº¤é¢: {row[3]}")
        
        cursor.close()
        connection.close()
        
    except Exception as e:
        print(f"âŒ TDengineæ“ä½œå¤±è´¥: {e}")
        print("è¯·ç¡®ä¿:")
        print("  1. TDengineæœåŠ¡å·²å¯åŠ¨")
        print("  2. Pythonç¯å¢ƒå·²å®‰è£…taospy: pip install taospy")
        print("  3. ç¯å¢ƒå˜é‡é…ç½®æ­£ç¡®")

# æ³¨æ„ï¼šéœ€è¦TDengineç¯å¢ƒæ‰èƒ½è¿è¡Œ
# tdengine_basic_example()
```

### 2. æ—¶åºæ•°æ®æ‰¹é‡å¤„ç†

```python
def tdengine_batch_processing_example():
    """TDengineæ—¶åºæ•°æ®æ‰¹é‡å¤„ç†ç¤ºä¾‹"""
    
    import pandas as pd
    import numpy as np
    from datetime import datetime, timedelta
    
    print("=== TDengineæ‰¹é‡æ•°æ®å¤„ç†ç¤ºä¾‹ ===")
    
    # 1. ç”Ÿæˆå¤§é‡æ—¶åºæ•°æ®
    print("1. ç”Ÿæˆæ—¶åºæ•°æ®...")
    
    symbols = ['000001', '000002', '600000', '600036']
    start_time = datetime.now() - timedelta(days=1)
    
    # ç”Ÿæˆæ¯åˆ†é’Ÿçš„æ•°æ®ç‚¹
    time_range = pd.date_range(
        start=start_time,
        end=datetime.now(),
        freq='1min'
    )
    
    all_data = []
    for symbol in symbols:
        base_price = np.random.uniform(10, 50)
        
        for timestamp in time_range:
            # æ¨¡æ‹Ÿä»·æ ¼éšæœºæ¸¸èµ°
            price_change = np.random.normal(0, 0.001)
            base_price *= (1 + price_change)
            
            record = {
                'symbol': symbol,
                'exchange': 'SZ' if symbol.startswith('000') else 'SH',
                'timestamp': timestamp,
                'price': round(base_price, 2),
                'volume': np.random.randint(100, 10000),
                'amount': round(base_price * np.random.randint(100, 10000), 2)
            }
            all_data.append(record)
    
    tick_df = pd.DataFrame(all_data)
    print(f"ç”Ÿæˆæ•°æ®: {len(tick_df)} æ¡è®°å½•")
    print(f"æ—¶é—´èŒƒå›´: {tick_df['timestamp'].min()} åˆ° {tick_df['timestamp'].max()}")
    print(f"è‚¡ç¥¨æ•°é‡: {tick_df['symbol'].nunique()}")
    
    # 2. è½¬æ¢ä¸ºTDengineæ ¼å¼
    print("\n2. è½¬æ¢ä¸ºTDengineæ‰¹é‡æ’å…¥æ ¼å¼...")
    
    def generate_tdengine_batch_sql(df, batch_size=1000):
        \"\"\"ç”ŸæˆTDengineæ‰¹é‡æ’å…¥SQL\"\"\"
        
        batches = []
        
        # æŒ‰symbolåˆ†ç»„å¤„ç†
        for symbol, group in df.groupby('symbol'):
            exchange = group.iloc[0]['exchange']
            table_name = f"stock_tick_{symbol}"
            
            # ç”Ÿæˆå­è¡¨åˆ›å»ºSQL
            create_table_sql = f\"\"\"
            CREATE TABLE IF NOT EXISTS {table_name} 
            USING stock_tick 
            TAGS ('{symbol}', '{exchange}')
            \"\"\"
            batches.append(('CREATE_TABLE', create_table_sql))
            
            # åˆ†æ‰¹ç”Ÿæˆæ’å…¥SQL
            for i in range(0, len(group), batch_size):
                batch_data = group.iloc[i:i+batch_size]
                
                values = []
                for _, row in batch_data.iterrows():
                    ts = int(row['timestamp'].timestamp() * 1000)
                    values.append(f"({ts}, {row['price']}, {row['volume']}, {row['amount']})")
                
                insert_sql = f\"\"\"
                INSERT INTO {table_name} VALUES 
                {', '.join(values)}
                \"\"\"
                
                batches.append(('INSERT', insert_sql))
        
        return batches
    
    batch_sqls = generate_tdengine_batch_sql(tick_df, batch_size=500)
    print(f"ç”Ÿæˆæ‰¹é‡SQL: {len(batch_sqls)} æ¡è¯­å¥")
    
    # 3. æ˜¾ç¤ºSQLç¤ºä¾‹
    print("\n3. SQLç¤ºä¾‹:")
    
    create_count = sum(1 for batch_type, _ in batch_sqls if batch_type == 'CREATE_TABLE')
    insert_count = sum(1 for batch_type, _ in batch_sqls if batch_type == 'INSERT')
    
    print(f"  åˆ›å»ºè¡¨è¯­å¥: {create_count} æ¡")
    print(f"  æ’å…¥è¯­å¥: {insert_count} æ¡")
    
    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªåˆ›å»ºè¡¨è¯­å¥
    for batch_type, sql in batch_sqls:
        if batch_type == 'CREATE_TABLE':
            print(f"\nåˆ›å»ºè¡¨SQLç¤ºä¾‹:")
            print(sql)
            break
    
    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ’å…¥è¯­å¥ï¼ˆæˆªå–ï¼‰
    for batch_type, sql in batch_sqls:
        if batch_type == 'INSERT':
            print(f"\næ’å…¥SQLç¤ºä¾‹ï¼ˆå‰200å­—ç¬¦ï¼‰:")
            print(sql[:200] + "...")
            break
    
    print("\nâœ… æ‰¹é‡æ•°æ®å¤„ç†å®Œæˆ")
    print("æ³¨æ„: å®é™…æ‰§è¡Œéœ€è¦TDengineç¯å¢ƒ")

tdengine_batch_processing_example()
```

## ğŸ““ Jupyterç¯å¢ƒä½¿ç”¨

### 1. Jupyter Notebookä¸­çš„åŸºæœ¬ä½¿ç”¨

```python
# Jupyter Cell 1: ç¯å¢ƒæ£€æŸ¥å’Œå¯¼å…¥
import os
import sys
import pandas as pd

# æ£€æŸ¥å½“å‰å·¥ä½œç›®å½•
print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")

# ç¡®ä¿åœ¨æ­£ç¡®çš„é¡¹ç›®ç›®å½•
# os.chdir('/path/to/your/mystocks/project')

# å¯¼å…¥å¿…è¦æ¨¡å—
try:
    from db_manager.database_manager import DatabaseTableManager, DatabaseType
    from db_manager.init_db_monitor import init_monitoring_database
    print("âœ… æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿åœ¨æ­£ç¡®çš„é¡¹ç›®ç›®å½•ä¸­è¿è¡Œ")
```

```python
# Jupyter Cell 2: åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
print("=== Jupyterç¯å¢ƒä¸­çš„æ•°æ®åº“ç®¡ç†å™¨ä½¿ç”¨ ===")

# åˆ›å»ºç®¡ç†å™¨ï¼ˆé¿å…ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼‰
manager = DatabaseTableManager()

# æ£€æŸ¥æ•°æ®åº“è¿æ¥
databases_to_test = [
    (DatabaseType.MYSQL, "MySQL"),
    (DatabaseType.POSTGRESQL, "PostgreSQL"), 
    (DatabaseType.REDIS, "Redis")
]

print("æ•°æ®åº“è¿æ¥æµ‹è¯•:")
for db_type, db_name in databases_to_test:
    try:
        conn = manager.get_connection(db_type, "test_db")
        if conn:
            print(f"  âœ… {db_name}: è¿æ¥æˆåŠŸ")
            conn.close()
        else:
            print(f"  âŒ {db_name}: è¿æ¥å¤±è´¥")
    except Exception as e:
        print(f"  âŒ {db_name}: {str(e)[:50]}...")
```

```python
# Jupyter Cell 3: ç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–ï¼ˆæ¨èæ–¹å¼ï¼‰
print("=== åœ¨Jupyterä¸­åˆå§‹åŒ–ç›‘æ§ç³»ç»Ÿ ===")

# ç›´æ¥è°ƒç”¨APIå‡½æ•°ï¼Œé¿å…argparseå†²çª
try:
    success = init_monitoring_database(drop_existing=False)
    
    if success:
        print("âœ… ç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        print("åˆ›å»ºçš„ç›‘æ§è¡¨:")
        print("  - table_creation_log")
        print("  - table_operation_log") 
        print("  - table_validation_log")
        print("  - column_definition_log")
    else:
        print("âŒ ç›‘æ§ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
        
except Exception as e:
    print(f"âŒ åˆå§‹åŒ–è¿‡ç¨‹å‡ºé”™: {e}")
```

```python
# Jupyter Cell 4: è¡¨åˆ›å»ºç¤ºä¾‹
print("=== åœ¨Jupyterä¸­åˆ›å»ºè¡¨ ===")

# å®šä¹‰è¡¨ç»“æ„
table_config = {
    'table_name': 'jupyter_test_table',
    'database_name': 'test_db',
    'database_type': 'MySQL',
    'columns': [
        {'name': 'id', 'type': 'INT', 'nullable': False, 'auto_increment': True},
        {'name': 'name', 'type': 'VARCHAR', 'length': 100, 'nullable': False},
        {'name': 'value', 'type': 'DECIMAL', 'precision': 10, 'scale': 2},
        {'name': 'created_at', 'type': 'TIMESTAMP', 'default': 'CURRENT_TIMESTAMP'}
    ],
    'primary_key': ['id']
}

try:
    # ç”ŸæˆDDL
    ddl = manager.generate_ddl(table_config)
    print("ç”Ÿæˆçš„DDL:")
    print(ddl)
    
    # åˆ›å»ºè¡¨
    success = manager.create_table_from_config(table_config)
    print(f"\nè¡¨åˆ›å»ºç»“æœ: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
    
except Exception as e:
    print(f"âŒ è¡¨åˆ›å»ºå¤±è´¥: {e}")
```

### 2. Jupyterä¸­çš„å¸¸è§é—®é¢˜è§£å†³

```python
# Jupyter Cell: å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ

def jupyter_troubleshooting():
    """Jupyterç¯å¢ƒæ•…éšœæ’é™¤"""
    
    print("=== Jupyterç¯å¢ƒæ•…éšœæ’é™¤ ===")
    
    # 1. å·¥ä½œç›®å½•æ£€æŸ¥
    current_dir = os.getcwd()
    print(f"1. å½“å‰å·¥ä½œç›®å½•: {current_dir}")
    
    # æ£€æŸ¥æ˜¯å¦åœ¨æ­£ç¡®çš„é¡¹ç›®ç›®å½•
    required_dirs = ['db_manager', 'adapters', 'interfaces']
    missing_dirs = [d for d in required_dirs if not os.path.exists(d)]
    
    if missing_dirs:
        print(f"âŒ ç¼ºå°‘å¿…è¦ç›®å½•: {missing_dirs}")
        print("è¯·ä½¿ç”¨ os.chdir() åˆ‡æ¢åˆ°æ­£ç¡®çš„é¡¹ç›®æ ¹ç›®å½•")
    else:
        print("âœ… ç›®å½•ç»“æ„æ­£ç¡®")
    
    # 2. ç¯å¢ƒå˜é‡æ£€æŸ¥
    print("\n2. ç¯å¢ƒå˜é‡æ£€æŸ¥:")
    env_vars = [
        'MYSQL_HOST', 'MYSQL_USER', 'MYSQL_PASSWORD',
        'POSTGRESQL_HOST', 'REDIS_HOST', 'MONITOR_DB_URL'
    ]
    
    for var in env_vars:
        value = os.getenv(var)
        if value:
            # éšè—æ•æ„Ÿä¿¡æ¯
            if 'PASSWORD' in var or 'URL' in var:
                display_value = value[:5] + '*' * (len(value) - 5)
            else:
                display_value = value
            print(f"  âœ… {var}: {display_value}")
        else:
            print(f"  âŒ {var}: æœªè®¾ç½®")
    
    # 3. å¯¼å…¥æ£€æŸ¥
    print("\n3. ä¾èµ–åŒ…æ£€æŸ¥:")
    packages = ['pandas', 'numpy', 'pymysql', 'psycopg2', 'redis', 'taospy']
    
    for package in packages:
        try:
            __import__(package)
            print(f"  âœ… {package}: å·²å®‰è£…")
        except ImportError:
            print(f"  âŒ {package}: æœªå®‰è£…")
    
    # 4. æä¾›è§£å†³æ–¹æ¡ˆ
    print("\n4. å¸¸è§è§£å†³æ–¹æ¡ˆ:")
    print("  - å·¥ä½œç›®å½•é—®é¢˜: os.chdir('/path/to/mystocks')")
    print("  - ç¯å¢ƒå˜é‡é—®é¢˜: æ£€æŸ¥.envæ–‡ä»¶æ˜¯å¦å­˜åœ¨å¹¶æ­£ç¡®é…ç½®")
    print("  - åŒ…ä¾èµ–é—®é¢˜: pip install package_name")
    print("  - å†…æ ¸é‡å¯: Kernel -> Restart & Clear Output")

jupyter_troubleshooting()
```

## ğŸ”’ å®‰å…¨é…ç½®

### 1. å®‰å…¨æ£€æŸ¥è„šæœ¬ä½¿ç”¨

```python
from db_manager.security_check import SecurityChecker

def security_configuration_example():
    """å®‰å…¨é…ç½®ç¤ºä¾‹"""
    
    print("=== æ•°æ®åº“å®‰å…¨é…ç½®æ£€æŸ¥ ===")
    
    # åˆ›å»ºå®‰å…¨æ£€æŸ¥å™¨
    checker = SecurityChecker()
    
    # 1. æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®
    print("1. ç¯å¢ƒå˜é‡å®‰å…¨æ£€æŸ¥:")
    env_check = checker.check_environment_variables()
    
    for check_item, result in env_check.items():
        status = "âœ…" if result['passed'] else "âŒ"
        print(f"  {status} {check_item}: {result['message']}")
    
    # 2. æ£€æŸ¥æ•°æ®åº“è¿æ¥å®‰å…¨æ€§
    print("\n2. æ•°æ®åº“è¿æ¥å®‰å…¨æ£€æŸ¥:")
    
    # æ£€æŸ¥MySQLè¿æ¥
    mysql_security = checker.check_mysql_security()
    print(f"  MySQLå®‰å…¨æ€§: {'é€šè¿‡' if mysql_security['secure'] else 'éœ€è¦æ”¹è¿›'}")
    
    for issue in mysql_security.get('issues', []):
        print(f"    âš ï¸ {issue}")
    
    # 3. æ£€æŸ¥ä»£ç ä¸­çš„æ•æ„Ÿä¿¡æ¯
    print("\n3. ä»£ç å®‰å…¨æ‰«æ:")
    
    # æ‰«ææŒ‡å®šç›®å½•
    scan_results = checker.scan_code_for_secrets(['db_manager/', 'adapters/'])
    
    if scan_results['secrets_found']:
        print(f"  âŒ å‘ç° {len(scan_results['secrets'])} ä¸ªæ½œåœ¨çš„æ•æ„Ÿä¿¡æ¯")
        for secret in scan_results['secrets'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"    - {secret['file']}:{secret['line']} - {secret['type']}")
    else:
        print("  âœ… æœªå‘ç°æ˜æ˜¾çš„æ•æ„Ÿä¿¡æ¯æ³„éœ²")
    
    # 4. ç”Ÿæˆå®‰å…¨å»ºè®®
    print("\n4. å®‰å…¨å»ºè®®:")
    recommendations = checker.get_security_recommendations()
    
    for category, suggestions in recommendations.items():
        print(f"  {category}:")
        for suggestion in suggestions:
            print(f"    - {suggestion}")

# è¿è¡Œå®‰å…¨æ£€æŸ¥ç¤ºä¾‹
security_configuration_example()
```

### 2. å®‰å…¨çš„ç¯å¢ƒå˜é‡é…ç½®

```python
def secure_environment_setup():
    """å®‰å…¨çš„ç¯å¢ƒå˜é‡é…ç½®ç¤ºä¾‹"""
    
    print("=== å®‰å…¨çš„ç¯å¢ƒå˜é‡é…ç½® ===")
    
    # 1. æ¨èçš„.envæ–‡ä»¶æ ¼å¼
    recommended_env = \"\"\"
# MySQL/MariaDBé…ç½®
MYSQL_HOST=192.168.123.104
MYSQL_PORT=3306
MYSQL_USER=mystocks_user          # ä½¿ç”¨ä¸“ç”¨ç”¨æˆ·ï¼Œä¸è¦ç”¨root
MYSQL_PASSWORD=complex_password_123  # ä½¿ç”¨å¤æ‚å¯†ç 
MYSQL_DATABASE=mystocks_db

# PostgreSQLé…ç½®
POSTGRESQL_HOST=192.168.123.104
POSTGRESQL_PORT=5433
POSTGRESQL_USER=mystocks_pg_user
POSTGRESQL_PASSWORD=another_complex_password_456
POSTGRESQL_DATABASE=quant_research

# Redisé…ç½®ï¼ˆå¦‚æœæœ‰å¯†ç ï¼‰
REDIS_HOST=192.168.123.104
REDIS_PORT=6379
REDIS_PASSWORD=redis_password_789   # Rediså¯†ç 
REDIS_DB=0

# TDengineé…ç½®
TDENGINE_HOST=192.168.123.104
TDENGINE_PORT=6030
TDENGINE_USER=mystocks_td_user
TDENGINE_PASSWORD=td_password_abc
TDENGINE_DATABASE=market_data

# ç›‘æ§æ•°æ®åº“ï¼ˆç‹¬ç«‹é…ç½®ï¼‰
MONITOR_DB_URL=mysql+pymysql://monitor_user:monitor_pass@192.168.123.104:3306/db_monitor

# å¯é€‰ï¼šåŠ å¯†å¯†é’¥ï¼ˆç”¨äºæ•æ„Ÿæ•°æ®åŠ å¯†ï¼‰
ENCRYPTION_KEY=your_32_byte_encryption_key_here

# æ—¥å¿—é…ç½®
LOG_LEVEL=INFO
LOG_FILE=mystocks_system.log
\"\"\"
    
    print("æ¨èçš„.envæ–‡ä»¶æ ¼å¼:")
    print(recommended_env)
    
    # 2. å®‰å…¨æœ€ä½³å®è·µ
    print("\n=== å®‰å…¨æœ€ä½³å®è·µ ===")
    best_practices = [
        "1. æ•°æ®åº“ç”¨æˆ·æƒé™æœ€å°åŒ–åŸåˆ™",
        "   - ä¸ºåº”ç”¨ç¨‹åºåˆ›å»ºä¸“ç”¨æ•°æ®åº“ç”¨æˆ·",
        "   - åªæˆäºˆå¿…è¦çš„æƒé™ï¼ˆSELECT, INSERT, UPDATE, DELETEï¼‰",
        "   - é¿å…ä½¿ç”¨rootæˆ–è¶…çº§ç”¨æˆ·è´¦æˆ·",
        "",
        "2. å¯†ç å®‰å…¨ç­–ç•¥",
        "   - ä½¿ç”¨å¤æ‚å¯†ç ï¼ˆå­—æ¯+æ•°å­—+ç‰¹æ®Šå­—ç¬¦ï¼‰",
        "   - å®šæœŸæ›´æ¢å¯†ç ",
        "   - ä¸åŒæœåŠ¡ä½¿ç”¨ä¸åŒå¯†ç ",
        "",
        "3. ç½‘ç»œå®‰å…¨é…ç½®",
        "   - ä½¿ç”¨å†…ç½‘IPåœ°å€",
        "   - é…ç½®é˜²ç«å¢™è§„åˆ™",
        "   - å¯ç”¨SSL/TLSè¿æ¥ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰",
        "",
        "4. æ–‡ä»¶æƒé™ç®¡ç†",
        "   - .envæ–‡ä»¶è®¾ç½®ä¸ºåªè¯»æƒé™",
        "   - ä¸è¦å°†.envæ–‡ä»¶æäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶",
        "   - ä½¿ç”¨.gitignoreå¿½ç•¥æ•æ„Ÿæ–‡ä»¶",
        "",
        "5. ç›‘æ§å’Œå®¡è®¡",
        "   - å¯ç”¨æ•°æ®åº“è¿æ¥æ—¥å¿—",
        "   - å®šæœŸæ£€æŸ¥å¼‚å¸¸è®¿é—®",
        "   - ä½¿ç”¨ç›‘æ§æ•°æ®åº“è®°å½•æ‰€æœ‰æ“ä½œ"
    ]
    
    for practice in best_practices:
        print(practice)
    
    # 3. æ•°æ®åº“ç”¨æˆ·åˆ›å»ºç¤ºä¾‹
    print("\n=== æ•°æ®åº“ç”¨æˆ·åˆ›å»ºç¤ºä¾‹ ===")
    
    mysql_user_sql = \"\"\"
-- MySQLç”¨æˆ·åˆ›å»ºå’Œæƒé™è®¾ç½®
CREATE USER 'mystocks_user'@'%' IDENTIFIED BY 'complex_password_123';

-- æˆäºˆç‰¹å®šæ•°æ®åº“æƒé™
GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, INDEX, ALTER 
ON mystocks_db.* TO 'mystocks_user'@'%';

-- åˆ·æ–°æƒé™
FLUSH PRIVILEGES;
\"\"\"
    
    postgresql_user_sql = \"\"\"
-- PostgreSQLç”¨æˆ·åˆ›å»ºå’Œæƒé™è®¾ç½®
CREATE USER mystocks_pg_user WITH PASSWORD 'another_complex_password_456';

-- åˆ›å»ºæ•°æ®åº“å¹¶è®¾ç½®æ‰€æœ‰è€…
CREATE DATABASE quant_research OWNER mystocks_pg_user;

-- æˆäºˆæ•°æ®åº“æƒé™
GRANT ALL PRIVILEGES ON DATABASE quant_research TO mystocks_pg_user;
\"\"\"
    
    print("MySQLç”¨æˆ·åˆ›å»ºSQL:")
    print(mysql_user_sql)
    print("PostgreSQLç”¨æˆ·åˆ›å»ºSQL:")
    print(postgresql_user_sql)

secure_environment_setup()
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### 1. å¸¸è§é—®é¢˜è¯Šæ–­

```python
def troubleshooting_guide():
    """æ•…éšœæ’é™¤æŒ‡å—"""
    
    print("=== æ•°æ®åº“ç®¡ç†å™¨æ•…éšœæ’é™¤æŒ‡å— ===")
    
    # 1. è¿æ¥é—®é¢˜è¯Šæ–­
    print("1. æ•°æ®åº“è¿æ¥é—®é¢˜è¯Šæ–­:")
    
    from db_manager.database_manager import DatabaseTableManager, DatabaseType
    
    manager = DatabaseTableManager()
    
    # è¯Šæ–­å„ç§æ•°æ®åº“è¿æ¥
    databases = [
        (DatabaseType.MYSQL, "MySQL", "MYSQL_HOST"),
        (DatabaseType.POSTGRESQL, "PostgreSQL", "POSTGRESQL_HOST"),
        (DatabaseType.REDIS, "Redis", "REDIS_HOST"),
        (DatabaseType.TDENGINE, "TDengine", "TDENGINE_HOST")
    ]
    
    for db_type, db_name, env_var in databases:
        print(f"\n  æ£€æŸ¥ {db_name}:")
        
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        host = os.getenv(env_var)
        if not host:
            print(f"    âŒ ç¯å¢ƒå˜é‡ {env_var} æœªè®¾ç½®")
            continue
        else:
            print(f"    âœ… ä¸»æœºåœ°å€: {host}")
        
        # å°è¯•è¿æ¥
        try:
            connection = manager.get_connection(db_type, "test_db")
            if connection:
                print(f"    âœ… è¿æ¥æˆåŠŸ")
                connection.close()
            else:
                print(f"    âŒ è¿æ¥å¤±è´¥")
        except Exception as e:
            print(f"    âŒ è¿æ¥é”™è¯¯: {str(e)[:100]}...")
            
            # æä¾›å…·ä½“çš„è§£å†³å»ºè®®
            if "Connection refused" in str(e):
                print(f"      ğŸ’¡ å»ºè®®: æ£€æŸ¥{db_name}æœåŠ¡æ˜¯å¦å¯åŠ¨")
            elif "Access denied" in str(e):
                print(f"      ğŸ’¡ å»ºè®®: æ£€æŸ¥ç”¨æˆ·åå’Œå¯†ç æ˜¯å¦æ­£ç¡®")
            elif "Unknown database" in str(e):
                print(f"      ğŸ’¡ å»ºè®®: æ£€æŸ¥æ•°æ®åº“åç§°æ˜¯å¦å­˜åœ¨")
            elif "timed out" in str(e):
                print(f"      ğŸ’¡ å»ºè®®: æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé˜²ç«å¢™è®¾ç½®")
    
    # 2. ä¾èµ–åŒ…é—®é¢˜è¯Šæ–­
    print("\n2. ä¾èµ–åŒ…é—®é¢˜è¯Šæ–­:")
    
    required_packages = {
        'pandas': 'æ•°æ®å¤„ç†æ ¸å¿ƒåº“',
        'numpy': 'æ•°å€¼è®¡ç®—åº“',
        'pyyaml': 'YAMLé…ç½®æ–‡ä»¶è§£æ',
        'sqlalchemy': 'æ•°æ®åº“ORMæ¡†æ¶',
        'pymysql': 'MySQLæ•°æ®åº“é©±åŠ¨',
        'psycopg2': 'PostgreSQLæ•°æ®åº“é©±åŠ¨',
        'redis': 'Rediså®¢æˆ·ç«¯',
        'taospy': 'TDengineå®¢æˆ·ç«¯'
    }
    
    for package, description in required_packages.items():
        try:
            __import__(package)
            print(f"  âœ… {package}: å·²å®‰è£… - {description}")
        except ImportError:
            print(f"  âŒ {package}: æœªå®‰è£… - {description}")
            print(f"      ğŸ’¡ å®‰è£…å‘½ä»¤: pip install {package}")
    
    # 3. é…ç½®æ–‡ä»¶é—®é¢˜è¯Šæ–­
    print("\n3. é…ç½®æ–‡ä»¶é—®é¢˜è¯Šæ–­:")
    
    # æ£€æŸ¥.envæ–‡ä»¶
    env_file_path = ".env"
    if os.path.exists(env_file_path):
        print(f"  âœ… .envæ–‡ä»¶å­˜åœ¨: {env_file_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æƒé™
        import stat
        file_mode = oct(stat.S_IMODE(os.lstat(env_file_path).st_mode))
        print(f"  ğŸ“‹ æ–‡ä»¶æƒé™: {file_mode}")
        
        if file_mode == '0o644' or file_mode == '0o600':
            print(f"    âœ… æ–‡ä»¶æƒé™å®‰å…¨")
        else:
            print(f"    âš ï¸ å»ºè®®è®¾ç½®æ›´ä¸¥æ ¼çš„æ–‡ä»¶æƒé™")
        
    else:
        print(f"  âŒ .envæ–‡ä»¶ä¸å­˜åœ¨: {env_file_path}")
        print(f"      ğŸ’¡ è¯·åˆ›å»º.envæ–‡ä»¶å¹¶é…ç½®æ•°æ®åº“è¿æ¥ä¿¡æ¯")
    
    # 4. æ—¥å¿—æ–‡ä»¶æ£€æŸ¥
    print("\n4. æ—¥å¿—æ–‡ä»¶æ£€æŸ¥:")
    
    log_directories = ['logs/', 'db_manager/logs/']
    for log_dir in log_directories:
        if os.path.exists(log_dir):
            print(f"  âœ… æ—¥å¿—ç›®å½•å­˜åœ¨: {log_dir}")
            
            # åˆ—å‡ºæœ€è¿‘çš„æ—¥å¿—æ–‡ä»¶
            import glob
            log_files = glob.glob(os.path.join(log_dir, "*.log"))
            recent_logs = sorted(log_files, key=os.path.getmtime, reverse=True)[:3]
            
            for log_file in recent_logs:
                mtime = os.path.getmtime(log_file)
                import datetime
                mtime_str = datetime.datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M:%S')
                print(f"    ğŸ“„ {log_file} (ä¿®æ”¹æ—¶é—´: {mtime_str})")
        else:
            print(f"  âŒ æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {log_dir}")

troubleshooting_guide()
```

### 2. æ€§èƒ½è°ƒä¼˜å»ºè®®

```python
def performance_tuning_guide():
    """æ€§èƒ½è°ƒä¼˜æŒ‡å—"""
    
    print("=== æ•°æ®åº“æ€§èƒ½è°ƒä¼˜æŒ‡å— ===")
    
    # 1. è¿æ¥æ± é…ç½®å»ºè®®
    print("1. è¿æ¥æ± é…ç½®ä¼˜åŒ–:")
    
    connection_pool_config = {
        "MySQL": {
            "pool_size": 10,           # è¿æ¥æ± å¤§å°
            "max_overflow": 20,        # æœ€å¤§æº¢å‡ºè¿æ¥æ•°
            "pool_timeout": 30,        # è¿æ¥è¶…æ—¶æ—¶é—´
            "pool_recycle": 3600,      # è¿æ¥å›æ”¶æ—¶é—´(ç§’)
            "pool_pre_ping": True      # è¿æ¥å‰pingæµ‹è¯•
        },
        "PostgreSQL": {
            "pool_size": 8,
            "max_overflow": 15,
            "pool_timeout": 30,
            "pool_recycle": 7200
        },
        "Redis": {
            "connection_pool_size": 15,
            "retry_on_timeout": True,
            "socket_timeout": 5
        }
    }
    
    for db_type, config in connection_pool_config.items():
        print(f"\n  {db_type} è¿æ¥æ± é…ç½®:")
        for param, value in config.items():
            print(f"    {param}: {value}")
    
    # 2. æ‰¹é‡æ“ä½œä¼˜åŒ–
    print("\n2. æ‰¹é‡æ“ä½œä¼˜åŒ–å»ºè®®:")
    
    batch_optimization_tips = [
        "ğŸ“Š æ•°æ®æ’å…¥ä¼˜åŒ–:",
        "  - ä½¿ç”¨æ‰¹é‡INSERTè¯­å¥ï¼Œæ¯æ‰¹1000-5000æ¡è®°å½•",
        "  - å…³é—­è‡ªåŠ¨æäº¤ï¼Œæ‰‹åŠ¨æ§åˆ¶äº‹åŠ¡",
        "  - ä½¿ç”¨LOAD DATA INFILE (MySQL) æˆ– COPY (PostgreSQL)",
        "",
        "ğŸ” æŸ¥è¯¢ä¼˜åŒ–:",
        "  - ä¸ºå¸¸ç”¨æŸ¥è¯¢å­—æ®µåˆ›å»ºç´¢å¼•",
        "  - ä½¿ç”¨LIMITé™åˆ¶è¿”å›ç»“æœæ•°é‡",
        "  - é¿å…SELECT * ï¼ŒåªæŸ¥è¯¢éœ€è¦çš„å­—æ®µ",
        "  - ä½¿ç”¨WHEREæ¡ä»¶è¿‡æ»¤æ•°æ®",
        "",
        "âš¡ TDengineä¼˜åŒ–:",
        "  - ä½¿ç”¨è¶…çº§è¡¨ç»“æ„å­˜å‚¨æ—¶åºæ•°æ®",
        "  - åˆç†è®¾ç½®æ ‡ç­¾åˆ—ï¼Œé¿å…é«˜åŸºæ•°æ ‡ç­¾",
        "  - æ‰¹é‡å†™å…¥æ•°æ®ï¼Œå‡å°‘ç½‘ç»œå¼€é”€",
        "",
        "ğŸ’¾ Redisä¼˜åŒ–:",
        "  - è®¾ç½®åˆç†çš„è¿‡æœŸæ—¶é—´ï¼Œé¿å…å†…å­˜æ³„æ¼",
        "  - ä½¿ç”¨pipelineå‡å°‘ç½‘ç»œå¾€è¿”",
        "  - é€‰æ‹©åˆé€‚çš„æ•°æ®ç»“æ„(Hash, List, Set)"
    ]
    
    for tip in batch_optimization_tips:
        print(tip)
    
    # 3. ç›‘æ§æŒ‡æ ‡å»ºè®®
    print("\n3. æ€§èƒ½ç›‘æ§æŒ‡æ ‡:")
    
    monitoring_metrics = {
        "æ•°æ®åº“è¿æ¥": [
            "æ´»è·ƒè¿æ¥æ•°",
            "è¿æ¥æ± ä½¿ç”¨ç‡",
            "è¿æ¥å»ºç«‹æ—¶é—´",
            "è¿æ¥è¶…æ—¶æ¬¡æ•°"
        ],
        "æŸ¥è¯¢æ€§èƒ½": [
            "å¹³å‡æŸ¥è¯¢æ—¶é—´",
            "æ…¢æŸ¥è¯¢æ•°é‡",
            "æŸ¥è¯¢é”™è¯¯ç‡",
            "å¹¶å‘æŸ¥è¯¢æ•°"
        ],
        "ç³»ç»Ÿèµ„æº": [
            "CPUä½¿ç”¨ç‡",
            "å†…å­˜ä½¿ç”¨ç‡",
            "ç£ç›˜I/O",
            "ç½‘ç»œå»¶è¿Ÿ"
        ]
    }
    
    for category, metrics in monitoring_metrics.items():
        print(f"\n  {category}:")
        for metric in metrics:
            print(f"    - {metric}")
    
    # 4. å®é™…ä¼˜åŒ–ç¤ºä¾‹
    print("\n4. å®é™…ä¼˜åŒ–ä»£ç ç¤ºä¾‹:")
    
    optimization_code = \"\"\"
# æ‰¹é‡æ’å…¥ä¼˜åŒ–ç¤ºä¾‹
def optimized_batch_insert(data_df, table_name, batch_size=1000):
    \"\"\"ä¼˜åŒ–çš„æ‰¹é‡æ’å…¥æ–¹æ³•\"\"\"
    
    manager = DatabaseTableManager()
    connection = manager.get_connection(DatabaseType.MYSQL, "test_db")
    
    try:
        # å…³é—­è‡ªåŠ¨æäº¤
        connection.autocommit = False
        cursor = connection.cursor()
        
        # åˆ†æ‰¹å¤„ç†æ•°æ®
        for i in range(0, len(data_df), batch_size):
            batch_data = data_df.iloc[i:i+batch_size]
            
            # ç”Ÿæˆæ‰¹é‡æ’å…¥SQL
            values = []
            for _, row in batch_data.iterrows():
                value_str = "(" + ",".join([f"'{v}'" for v in row.values]) + ")"
                values.append(value_str)
            
            sql = f"INSERT INTO {table_name} VALUES {','.join(values)}"
            cursor.execute(sql)
            
            print(f"æ’å…¥æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch_data)} æ¡è®°å½•")
        
        # æäº¤äº‹åŠ¡
        connection.commit()
        print("æ‰¹é‡æ’å…¥å®Œæˆ")
        
    except Exception as e:
        connection.rollback()
        print(f"æ’å…¥å¤±è´¥ï¼Œå·²å›æ»š: {e}")
    finally:
        cursor.close()
        connection.close()

# è¿æ¥æ± é…ç½®ç¤ºä¾‹
def configure_connection_pool():
    \"\"\"é…ç½®è¿æ¥æ± \"\"\"
    from sqlalchemy import create_engine
    from sqlalchemy.pool import QueuePool
    
    database_url = "mysql+pymysql://user:password@localhost/dbname"
    
    engine = create_engine(
        database_url,
        poolclass=QueuePool,
        pool_size=10,
        max_overflow=20,
        pool_timeout=30,
        pool_recycle=3600,
        pool_pre_ping=True
    )
    
    return engine
\"\"\"
    
    print(optimization_code)

performance_tuning_guide()
```

## ğŸ”— ä¸v2.0ç³»ç»Ÿé›†æˆ

### å®Œæ•´é›†æˆç¤ºä¾‹

```python
def complete_v2_integration_example():
    """å®Œæ•´çš„v2.0ç³»ç»Ÿé›†æˆç¤ºä¾‹"""
    
    print("=== db_managerä¸MyStocks v2.0å®Œæ•´é›†æˆ ===")
    
    # 1. åˆå§‹åŒ–v2.0ç³»ç»Ÿ
    try:
        from unified_manager import MyStocksUnifiedManager
        from core import DataClassification
        
        print("1. åˆå§‹åŒ–MyStocks v2.0ç³»ç»Ÿ...")
        v2_manager = MyStocksUnifiedManager()
        init_result = v2_manager.initialize_system()
        
        if not init_result['config_loaded']:
            print("âŒ v2.0ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return
        
        print("âœ… v2.0ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        
    except ImportError:
        print("âŒ MyStocks v2.0æ¨¡å—æœªæ‰¾åˆ°ï¼Œè¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…")
        return
    
    # 2. åˆå§‹åŒ–db_manager
    print("\n2. åˆå§‹åŒ–db_manager...")
    
    db_manager = DatabaseTableManager()
    
    # åˆå§‹åŒ–ç›‘æ§æ•°æ®åº“
    monitor_success = init_monitoring_database(drop_existing=False)
    print(f"ç›‘æ§æ•°æ®åº“åˆå§‹åŒ–: {'æˆåŠŸ' if monitor_success else 'å¤±è´¥'}")
    
    # 3. éªŒè¯æ•°æ®åº“è¿æ¥
    print("\n3. éªŒè¯æ•°æ®åº“è¿æ¥...")
    
    database_status = {}
    test_databases = [
        (DatabaseType.MYSQL, "MySQL"),
        (DatabaseType.POSTGRESQL, "PostgreSQL"),
        (DatabaseType.REDIS, "Redis"),
        (DatabaseType.TDENGINE, "TDengine")
    ]
    
    for db_type, db_name in test_databases:
        try:
            conn = db_manager.get_connection(db_type, "test_db")
            database_status[db_name] = conn is not None
            if conn:
                conn.close()
        except:
            database_status[db_name] = False
        
        status = "âœ…" if database_status[db_name] else "âŒ"
        print(f"  {status} {db_name}: {'å¯ç”¨' if database_status[db_name] else 'ä¸å¯ç”¨'}")
    
    # 4. åˆ›å»ºæµ‹è¯•è¡¨ï¼ˆé€šè¿‡v2.0ç³»ç»Ÿï¼‰
    print("\n4. é€šè¿‡v2.0ç³»ç»Ÿåˆ›å»ºæµ‹è¯•è¡¨...")
    
    # ä½¿ç”¨v2.0çš„é…ç½®é©±åŠ¨è¡¨ç®¡ç†
    test_table_config = {
        'table_name': 'integration_test_stock_data',
        'database_name': 'test_db',
        'database_type': 'MySQL',
        'classification': DataClassification.DAILY_KLINE.value,
        'columns': [
            {'name': 'symbol', 'type': 'VARCHAR', 'length': 10, 'nullable': False},
            {'name': 'trade_date', 'type': 'DATE', 'nullable': False},
            {'name': 'close', 'type': 'DECIMAL', 'precision': 10, 'scale': 2},
            {'name': 'volume', 'type': 'BIGINT'},
            {'name': 'created_at', 'type': 'TIMESTAMP', 'default': 'CURRENT_TIMESTAMP'}
        ],
        'primary_key': ['symbol', 'trade_date']
    }
    
    try:
        # ä½¿ç”¨db_manageråˆ›å»ºè¡¨
        table_success = db_manager.create_table_from_config(test_table_config)
        print(f"æµ‹è¯•è¡¨åˆ›å»º: {'æˆåŠŸ' if table_success else 'å¤±è´¥'}")
        
        if table_success:
            # é€šè¿‡v2.0ç³»ç»ŸéªŒè¯è¡¨å­˜åœ¨
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šéªŒè¯é€»è¾‘
            print("âœ… è¡¨åˆ›å»ºæˆåŠŸï¼Œå·²è¢«v2.0ç³»ç»Ÿè¯†åˆ«")
        
    except Exception as e:
        print(f"âŒ è¡¨åˆ›å»ºå¤±è´¥: {e}")
    
    # 5. æ•°æ®æµæµ‹è¯•
    print("\n5. æ•°æ®æµæµ‹è¯•...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    import pandas as pd
    import numpy as np
    from datetime import datetime, timedelta
    
    test_data = pd.DataFrame({
        'symbol': ['000001', '600000', '000002'] * 5,
        'trade_date': [datetime.now().date() - timedelta(days=i) for i in range(15)],
        'close': np.random.uniform(10, 50, 15),
        'volume': np.random.randint(1000000, 10000000, 15)
    })
    
    try:
        # é€šè¿‡v2.0ç³»ç»Ÿä¿å­˜æ•°æ®ï¼ˆè‡ªåŠ¨è·¯ç”±åˆ°PostgreSQLï¼‰
        save_success = v2_manager.save_data_by_classification(
            test_data, 
            DataClassification.DAILY_KLINE
        )
        print(f"æ•°æ®ä¿å­˜æµ‹è¯•: {'æˆåŠŸ' if save_success else 'å¤±è´¥'}")
        
        if save_success:
            # é€šè¿‡v2.0ç³»ç»ŸæŸ¥è¯¢æ•°æ®
            loaded_data = v2_manager.load_data_by_classification(
                DataClassification.DAILY_KLINE,
                filters={'symbol': '000001'},
                limit=5
            )
            print(f"æ•°æ®æŸ¥è¯¢æµ‹è¯•: {'æˆåŠŸ' if not loaded_data.empty else 'å¤±è´¥'}")
            print(f"æŸ¥è¯¢åˆ° {len(loaded_data)} æ¡è®°å½•")
        
    except Exception as e:
        print(f"âŒ æ•°æ®æµæµ‹è¯•å¤±è´¥: {e}")
    
    # 6. ç›‘æ§ç³»ç»ŸéªŒè¯
    print("\n6. ç›‘æ§ç³»ç»ŸéªŒè¯...")
    
    try:
        # è·å–ç³»ç»ŸçŠ¶æ€
        system_status = v2_manager.get_system_status()
        
        monitoring = system_status.get('monitoring', {})
        op_stats = monitoring.get('operation_statistics', {})
        
        print(f"æ€»æ“ä½œæ•°: {op_stats.get('total_operations', 0)}")
        print(f"æˆåŠŸæ“ä½œ: {op_stats.get('successful_operations', 0)}")
        print(f"å¤±è´¥æ“ä½œ: {op_stats.get('failed_operations', 0)}")
        
        # æ£€æŸ¥ç›‘æ§è¡¨æ˜¯å¦æœ‰æ•°æ®
        if database_status.get('MySQL', False):
            monitor_conn = db_manager.get_connection(DatabaseType.MYSQL, "db_monitor")
            if monitor_conn:
                cursor = monitor_conn.cursor()
                cursor.execute("SELECT COUNT(*) FROM table_operation_log")
                log_count = cursor.fetchone()[0]
                print(f"ç›‘æ§æ—¥å¿—è®°å½•æ•°: {log_count}")
                cursor.close()
                monitor_conn.close()
        
    except Exception as e:
        print(f"âŒ ç›‘æ§ç³»ç»ŸéªŒè¯å¤±è´¥: {e}")
    
    # 7. é›†æˆæ€»ç»“
    print("\n=== é›†æˆæµ‹è¯•æ€»ç»“ ===")
    
    integration_results = {
        "v2.0ç³»ç»Ÿåˆå§‹åŒ–": init_result['config_loaded'],
        "ç›‘æ§æ•°æ®åº“": monitor_success,
        "MySQLè¿æ¥": database_status.get('MySQL', False),
        "PostgreSQLè¿æ¥": database_status.get('PostgreSQL', False),
        "Redisè¿æ¥": database_status.get('Redis', False),
        "TDengineè¿æ¥": database_status.get('TDengine', False),
        "è¡¨åˆ›å»ºåŠŸèƒ½": table_success if 'table_success' in locals() else False,
        "æ•°æ®æµæµ‹è¯•": save_success if 'save_success' in locals() else False
    }
    
    for component, status in integration_results.items():
        result = "âœ… é€šè¿‡" if status else "âŒ å¤±è´¥"
        print(f"  {component}: {result}")
    
    success_rate = sum(integration_results.values()) / len(integration_results)
    print(f"\næ•´ä½“é›†æˆæˆåŠŸç‡: {success_rate:.1%}")
    
    if success_rate >= 0.8:
        print("ğŸ‰ é›†æˆæµ‹è¯•åŸºæœ¬æˆåŠŸï¼ç³»ç»Ÿå¯ä»¥æ­£å¸¸ä½¿ç”¨")
    else:
        print("âš ï¸ é›†æˆæµ‹è¯•å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥å¤±è´¥çš„ç»„ä»¶")

# è¿è¡Œå®Œæ•´é›†æˆæµ‹è¯•
complete_v2_integration_example()
```

## ğŸ¯ æœ€ä½³å®è·µæ€»ç»“

### åŸºç¡€å®è·µ
1. **ç¯å¢ƒé…ç½®**: ç¡®ä¿æ‰€æœ‰æ•°æ®åº“æœåŠ¡æ­£å¸¸è¿è¡Œï¼Œç¯å¢ƒå˜é‡æ­£ç¡®é…ç½®
2. **å®‰å…¨ç®¡ç†**: ä½¿ç”¨ä¸“ç”¨æ•°æ®åº“ç”¨æˆ·ï¼Œè®¾ç½®å¤æ‚å¯†ç ï¼Œå¯ç”¨ç›‘æ§
3. **æ€§èƒ½ä¼˜åŒ–**: ä½¿ç”¨è¿æ¥æ± ï¼Œæ‰¹é‡æ“ä½œï¼Œåˆç†è®¾ç½®ç´¢å¼•
4. **é”™è¯¯å¤„ç†**: å®ç°å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œé‡è¯•æœºåˆ¶
5. **ç›‘æ§ç»´æŠ¤**: å¯ç”¨ç›‘æ§æ•°æ®åº“ï¼Œå®šæœŸæ£€æŸ¥ç³»ç»ŸçŠ¶æ€

### Redisçƒ­æ•°æ®å›ºåŒ–æœ€ä½³å®è·µ â­

#### 1. å›ºåŒ–ç­–ç•¥é€‰æ‹©
```bash
# ç”Ÿäº§ç¯å¢ƒæ¨èé…ç½®
FIXATION_STRATEGY=before_expire
FIXATION_INTERVAL_SECONDS=240    # 4åˆ†é’Ÿï¼ˆRedis 5åˆ†é’Ÿè¿‡æœŸçš„80%ï¼‰
BACKUP_TO_TICK_DATA=true        # ä¸»è¦å¤‡ä»½åˆ°TDengine
BACKUP_TO_DAILY_KLINE=false     # å¯é€‰å¤‡ä»½åˆ°PostgreSQL
BACKUP_TO_FILE_SYSTEM=true      # åº”æ€¥å¤‡ä»½åˆ°æ–‡ä»¶ç³»ç»Ÿ
```

#### 2. å¼ºåˆ¶æ›´æ–°æ—¶æœº
- **äº¤æ˜“å¼€å§‹å‰**: 09:25 æ¸…ç†ç¼“å­˜ï¼Œè·å–å¼€ç›˜æ•°æ®
- **åˆé—´ä¼‘å¸‚**: 12:00 å›ºåŒ–ä¸Šåˆæ•°æ®
- **æ”¶ç›˜å**: 15:05 å›ºåŒ–å…¨å¤©æ•°æ®
- **å¼‚å¸¸æƒ…å†µ**: å‘ç°æ•°æ®å¼‚å¸¸æ—¶ç«‹å³å¼ºåˆ¶æ›´æ–°

#### 3. ç›‘æ§æŒ‡æ ‡
- å›ºåŒ–æˆåŠŸç‡ > 95%
- æ•°æ®å»¶è¿Ÿ < 30ç§’
- å­˜å‚¨ä½¿ç”¨ç‡ < 80%
- é”™è¯¯ç‡ < 1%

#### 4. è¿ç»´å‘½ä»¤
```bash
# æ—¥å¸¸ç›‘æ§
python db_manager/validate_mystocks_architecture.py

# å¼ºåˆ¶æ›´æ–°
python db_manager/save_realtime_market_data.py --force-update

# å¯ç”¨å›ºåŒ–
python db_manager/save_realtime_market_data.py --enable-fixation

# ç»¼åˆè¿è¡Œ
python run_realtime_market_saver.py --validate
```

### æ•…éšœæ¢å¤é¢„æ¡ˆ

#### Redisæ•°æ®ä¸¢å¤±æ¢å¤
1. **ä»TDengineæ¢å¤**: æŸ¥è¯¢æœ€è¿‘çš„Tickæ•°æ®é‡æ–°åŠ è½½åˆ°Redis
2. **ä»æ–‡ä»¶ç³»ç»Ÿæ¢å¤**: ä½¿ç”¨å¤‡ä»½CSVæ–‡ä»¶æ¢å¤æ•°æ®
3. **å¼ºåˆ¶æ›´æ–°**: è·³è¿‡ç¼“å­˜é‡æ–°è·å–æœ€æ–°æ•°æ®

#### å›ºåŒ–å¤±è´¥å¤„ç†
1. **æ£€æŸ¥å­˜å‚¨ç©ºé—´**: ç¡®ä¿ç›®æ ‡æ•°æ®åº“æœ‰è¶³å¤Ÿç©ºé—´
2. **éªŒè¯ç½‘ç»œè¿æ¥**: æ£€æŸ¥åˆ°å„æ•°æ®åº“çš„ç½‘ç»œè¿é€šæ€§
3. **é‡è¯•æœºåˆ¶**: ç³»ç»Ÿè‡ªåŠ¨é‡è¯•æœ€å¤š3æ¬¡
4. **é™çº§å¤„ç†**: è‡ªåŠ¨åˆ‡æ¢åˆ°æ–‡ä»¶ç³»ç»Ÿå¤‡ä»½

### æ€§èƒ½è°ƒä¼˜å»ºè®®

#### Redisä¼˜åŒ–
- è®¾ç½®åˆé€‚çš„å†…å­˜æ·˜æ±°ç­–ç•¥: `maxmemory-policy allkeys-lru`
- å¯ç”¨AOFæŒä¹…åŒ–: `appendonly yes`
- è°ƒæ•´è¿‡æœŸæ£€æŸ¥é¢‘ç‡: `hz 10`

#### TDengineä¼˜åŒ–  
- æŒ‰symbolå’Œæ—¶é—´åˆ†åŒºå­˜å‚¨
- è®¾ç½®åˆé€‚çš„æ•°æ®å‹ç¼©æ¯”
- å®šæœŸæ¸…ç†è¿‡æœŸæ•°æ®

#### PostgreSQLä¼˜åŒ–
- åˆ›å»ºå¤åˆç´¢å¼•: `(symbol, trade_date)`
- å¯ç”¨å¹¶è¡ŒæŸ¥è¯¢: `max_parallel_workers = 4`
- å®šæœŸVACUUMå’ŒANALYZE

é€šè¿‡ä»¥ä¸Šç¤ºä¾‹ï¼Œæ‚¨å¯ä»¥å……åˆ†åˆ©ç”¨æ•°æ®åº“ç®¡ç†å™¨æ¨¡å—çš„å¼ºå¤§åŠŸèƒ½ï¼Œæ„å»ºç¨³å®šé«˜æ•ˆçš„æ•°æ®åº“ç®¡ç†ç³»ç»Ÿã€‚