# CLI-5 ä»»åŠ¡åˆ†é…ï¼šPhase 6 GPUåŠ é€Ÿç›‘æ§ä»ªè¡¨æ¿

**åˆ†é…æ—¶é—´**: 2025-12-29
**é¢„è®¡å·¥ä½œé‡**: 8-10 å·¥ä½œæ—¥
**ä¼˜å…ˆçº§**: Round 1 - ä¸CLI-1å¹¶è¡Œ
**ä¾èµ–**: æ—  (GPUåç«¯å·²åœ¨Phase 6.4å®Œæˆ)
**Worktreeè·¯å¾„**: `/opt/claude/mystocks_phase6_monitoring`
**åˆ†æ”¯**: `phase6-gpu-monitoring`

---

## ğŸ“‹ ä»»åŠ¡æ¦‚è§ˆ

### æ ¸å¿ƒç›®æ ‡
ä¸º**å·²å®ç°çš„GPUåŠ é€Ÿå¼•æ“** (Phase 6.4å®Œæˆ, 68.58xæ€§èƒ½æå‡) æ„å»º**ä¸“ä¸šçº§ç›‘æ§ä»ªè¡¨æ¿**,æä¾›å®æ—¶GPUçŠ¶æ€ã€æ€§èƒ½æŒ‡æ ‡ã€åŠ é€Ÿæ¯”åˆ†æå’Œæ™ºèƒ½ä¼˜åŒ–å»ºè®®ã€‚

### èƒŒæ™¯ä¿¡æ¯
**GPUåŠ é€Ÿå¼•æ“ç°çŠ¶** (Phase 6.4å·²å®Œæˆ):
- âœ… çŸ©é˜µè¿ç®—åŠ é€Ÿ: **187.35x** (æœ€å¤§306.62x)
- âœ… å†…å­˜æ“ä½œåŠ é€Ÿ: **82.53x** (æœ€å¤§372.72x)
- âœ… å³°å€¼æ€§èƒ½: **662.52 GFLOPS**
- âœ… é•¿æœŸç¨³å®šæ€§: 83.3%æˆåŠŸç‡, 100%å¹¶å‘å®‰å…¨
- âœ… HALå±‚æ¶æ„: 4å±‚æŠ½è±¡,ç­–ç•¥éš”ç¦»,æ•…éšœå®¹ç¾
- âœ… å†…å­˜ç®¡ç†: æ™ºèƒ½å†…å­˜æ± ,100%å‘½ä¸­ç‡

**ç›‘æ§éœ€æ±‚**:
- å®æ—¶GPUçŠ¶æ€ (åˆ©ç”¨ç‡ã€æ˜¾å­˜ã€æ¸©åº¦ã€åŠŸè€—)
- æ€§èƒ½æŒ‡æ ‡è¿½è¸ª (GFLOPSã€åŠ é€Ÿæ¯”ã€ååé‡)
- å†å²æ•°æ®åˆ†æ (è¶‹åŠ¿å›¾ã€æ€§èƒ½æŠ¥å‘Š)
- æ™ºèƒ½ä¼˜åŒ–å»ºè®® (åŸºäºç›‘æ§æ•°æ®)

### å…³é”®äº¤ä»˜ç‰©
1. **GPUçŠ¶æ€ç›‘æ§ç»„ä»¶**: å®æ—¶æ˜¾ç¤ºGPUç¡¬ä»¶çŠ¶æ€
2. **æ€§èƒ½ä»ªè¡¨æ¿**: åŠ é€Ÿæ¯”ã€GFLOPSã€ååé‡å¯è§†åŒ–
3. **å†å²æ•°æ®åˆ†æ**: é•¿æœŸæ€§èƒ½è¶‹åŠ¿å’ŒæŠ¥å‘Šç”Ÿæˆ
4. **ä¼˜åŒ–å»ºè®®å¼•æ“**: AIé©±åŠ¨çš„æ€§èƒ½ä¼˜åŒ–å»ºè®®
5. **å‘Šè­¦ç³»ç»Ÿ**: GPUå¼‚å¸¸è‡ªåŠ¨å‘Šè­¦

### æŠ€æœ¯æ ˆ
- **åç«¯**: FastAPI (GPUç›‘æ§API), psutil, pynvml (NVIDIA Management Library)
- **å‰ç«¯**: Vue 3 + TypeScript, ECharts (æ€§èƒ½å›¾è¡¨)
- **å®æ—¶é€šä¿¡**: Server-Sent Events (SSE)
- **æ•°æ®å­˜å‚¨**: PostgreSQL (å†å²æ•°æ®), Redis (å®æ—¶ç¼“å­˜)

---

## ğŸ¯ åˆ†é˜¶æ®µä»»åŠ¡åˆ—è¡¨

### **é˜¶æ®µ1: GPUç›‘æ§åç«¯ (Day 1-3)**

#### T5.1 GPUç¡¬ä»¶ç›‘æ§æœåŠ¡
**ç›®æ ‡**: å®æ—¶é‡‡é›†GPUç¡¬ä»¶çŠ¶æ€æ•°æ®

**å…³é”®å®ç°**:
```python
import pynvml
from typing import Dict, List, Optional
from pydantic import BaseModel
from datetime import datetime

class GPUMetrics(BaseModel):
    """GPUç›‘æ§æŒ‡æ ‡"""
    device_id: int
    device_name: str
    timestamp: datetime

    # æ ¸å¿ƒæŒ‡æ ‡
    gpu_utilization: float      # GPUåˆ©ç”¨ç‡ (%)
    memory_used: int            # å·²ä½¿ç”¨æ˜¾å­˜ (MB)
    memory_total: int           # æ€»æ˜¾å­˜ (MB)
    memory_utilization: float   # æ˜¾å­˜åˆ©ç”¨ç‡ (%)
    temperature: float          # æ¸©åº¦ (Â°C)
    power_usage: float          # åŠŸè€— (W)
    power_limit: float          # åŠŸè€—ä¸Šé™ (W)

    # æ€§èƒ½æŒ‡æ ‡
    sm_clock: int               # SMæ—¶é’Ÿé¢‘ç‡ (MHz)
    memory_clock: int           # æ˜¾å­˜æ—¶é’Ÿé¢‘ç‡ (MHz)
    pcie_throughput_tx: float   # PCIeå‘é€ååé‡ (MB/s)
    pcie_throughput_rx: float   # PCIeæ¥æ”¶ååé‡ (MB/s)

class GPUMonitoringService:
    """GPUç›‘æ§æœåŠ¡"""

    def __init__(self):
        # åˆå§‹åŒ–NVMLåº“
        pynvml.nvmlInit()
        self.device_count = pynvml.nvmlDeviceGetCount()
        self.handles = [
            pynvml.nvmlDeviceGetHandleByIndex(i)
            for i in range(self.device_count)
        ]

    def get_metrics(self, device_id: int = 0) -> GPUMetrics:
        """è·å–å•ä¸ªGPUçš„å®æ—¶æŒ‡æ ‡"""
        handle = self.handles[device_id]

        # åŸºæœ¬ä¿¡æ¯
        name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')

        # åˆ©ç”¨ç‡
        util = pynvml.nvmlDeviceGetUtilizationRates(handle)
        gpu_util = util.gpu
        memory_util = util.memory

        # æ˜¾å­˜
        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
        mem_used = mem_info.used // (1024 ** 2)  # è½¬MB
        mem_total = mem_info.total // (1024 ** 2)

        # æ¸©åº¦
        temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)

        # åŠŸè€—
        power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # è½¬W
        power_limit = pynvml.nvmlDeviceGetPowerManagementLimit(handle) / 1000.0

        # æ—¶é’Ÿé¢‘ç‡
        sm_clock = pynvml.nvmlDeviceGetClockInfo(handle, pynvml.NVML_CLOCK_SM)
        mem_clock = pynvml.nvmlDeviceGetClockInfo(handle, pynvml.NVML_CLOCK_MEM)

        # PCIeååé‡
        pcie_tx = pynvml.nvmlDeviceGetPcieThroughput(handle, pynvml.NVML_PCIE_UTIL_TX_BYTES) / 1024  # KB/s â†’ MB/s
        pcie_rx = pynvml.nvmlDeviceGetPcieThroughput(handle, pynvml.NVML_PCIE_UTIL_RX_BYTES) / 1024

        return GPUMetrics(
            device_id=device_id,
            device_name=name,
            timestamp=datetime.now(),
            gpu_utilization=float(gpu_util),
            memory_used=int(mem_used),
            memory_total=int(mem_total),
            memory_utilization=float(memory_util),
            temperature=float(temp),
            power_usage=float(power),
            power_limit=float(power_limit),
            sm_clock=int(sm_clock),
            memory_clock=int(mem_clock),
            pcie_throughput_tx=float(pcie_tx),
            pcie_throughput_rx=float(pcie_rx)
        )

    def get_all_metrics(self) -> List[GPUMetrics]:
        """è·å–æ‰€æœ‰GPUçš„æŒ‡æ ‡"""
        return [self.get_metrics(i) for i in range(self.device_count)]

    def get_process_info(self, device_id: int = 0) -> List[Dict]:
        """è·å–GPUä¸Šè¿è¡Œçš„è¿›ç¨‹ä¿¡æ¯"""
        handle = self.handles[device_id]
        processes = pynvml.nvmlDeviceGetComputeRunningProcesses(handle)

        result = []
        for proc in processes:
            try:
                import psutil
                p = psutil.Process(proc.pid)
                result.append({
                    "pid": proc.pid,
                    "process_name": p.name(),
                    "memory_used_mb": proc.usedGpuMemory // (1024 ** 2),
                    "cmdline": " ".join(p.cmdline()[:3])  # æˆªå–å‰3ä¸ªå‚æ•°
                })
            except psutil.NoSuchProcess:
                pass

        return result

    def __del__(self):
        """æ¸…ç†NVML"""
        try:
            pynvml.nvmlShutdown()
        except:
            pass
```

**APIç«¯ç‚¹**:
```python
from fastapi import APIRouter
from typing import List

router = APIRouter(prefix="/api/gpu", tags=["GPUç›‘æ§"])

gpu_monitor = GPUMonitoringService()

@router.get("/metrics", response_model=List[GPUMetrics])
async def get_gpu_metrics():
    """è·å–æ‰€æœ‰GPUçš„å®æ—¶æŒ‡æ ‡"""
    return gpu_monitor.get_all_metrics()

@router.get("/metrics/{device_id}", response_model=GPUMetrics)
async def get_gpu_metrics_by_id(device_id: int):
    """è·å–æŒ‡å®šGPUçš„å®æ—¶æŒ‡æ ‡"""
    return gpu_monitor.get_metrics(device_id)

@router.get("/processes/{device_id}")
async def get_gpu_processes(device_id: int):
    """è·å–GPUä¸Šè¿è¡Œçš„è¿›ç¨‹"""
    return gpu_monitor.get_process_info(device_id)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æˆåŠŸé‡‡é›†GPUåˆ©ç”¨ç‡ã€æ˜¾å­˜ã€æ¸©åº¦ã€åŠŸè€—
- [ ] è¿›ç¨‹ä¿¡æ¯å‡†ç¡®æ˜¾ç¤º
- [ ] APIå“åº”æ—¶é—´ < 100ms

**é¢„ä¼°æ—¶é—´**: 1å¤©

---

#### T5.2 æ€§èƒ½æŒ‡æ ‡é‡‡é›† (GFLOPS/åŠ é€Ÿæ¯”)
**ç›®æ ‡**: é‡‡é›†GPUåŠ é€Ÿå¼•æ“çš„æ€§èƒ½æŒ‡æ ‡

**å…³é”®å®ç°**:
```python
from typing import Dict, Optional
from datetime import datetime, timedelta
import asyncio

class PerformanceMetrics(BaseModel):
    """æ€§èƒ½æŒ‡æ ‡"""
    timestamp: datetime

    # çŸ©é˜µè¿ç®—æ€§èƒ½
    matrix_gflops: float            # çŸ©é˜µè¿ç®—GFLOPS
    matrix_speedup: float           # çŸ©é˜µè¿ç®—åŠ é€Ÿæ¯”
    matrix_throughput: float        # çŸ©é˜µè¿ç®—ååé‡ (ops/s)

    # å†…å­˜æ“ä½œæ€§èƒ½
    memory_bandwidth_gbs: float     # å†…å­˜å¸¦å®½ (GB/s)
    memory_speedup: float           # å†…å­˜æ“ä½œåŠ é€Ÿæ¯”
    memory_throughput: float        # å†…å­˜æ“ä½œååé‡ (ops/s)

    # ç»¼åˆæŒ‡æ ‡
    overall_speedup: float          # ç»¼åˆåŠ é€Ÿæ¯”
    cache_hit_rate: float           # ç¼“å­˜å‘½ä¸­ç‡ (%)
    success_rate: float             # ä»»åŠ¡æˆåŠŸç‡ (%)

class PerformanceCollector:
    """æ€§èƒ½æŒ‡æ ‡é‡‡é›†å™¨"""

    def __init__(self):
        self.recent_benchmarks = []  # æœ€è¿‘100æ¬¡åŸºå‡†æµ‹è¯•ç»“æœ
        self.cache_stats = {"hits": 0, "misses": 0}

    async def collect_performance_metrics(self) -> PerformanceMetrics:
        """é‡‡é›†å½“å‰æ€§èƒ½æŒ‡æ ‡"""

        # è¿è¡Œè½»é‡çº§åŸºå‡†æµ‹è¯• (æ¯æ¬¡é‡‡é›†æ—¶æ‰§è¡Œä¸€æ¬¡)
        benchmark_result = await self._run_lightweight_benchmark()

        # è®¡ç®—åŠ é€Ÿæ¯”
        matrix_speedup = self._calculate_speedup(
            benchmark_result['gpu_matrix_time'],
            benchmark_result['cpu_matrix_time']
        )

        memory_speedup = self._calculate_speedup(
            benchmark_result['gpu_memory_time'],
            benchmark_result['cpu_memory_time']
        )

        overall_speedup = (matrix_speedup + memory_speedup) / 2

        # è®¡ç®—GFLOPS
        matrix_gflops = self._calculate_gflops(
            benchmark_result['matrix_ops'],
            benchmark_result['gpu_matrix_time']
        )

        # è®¡ç®—å†…å­˜å¸¦å®½
        memory_bandwidth = self._calculate_bandwidth(
            benchmark_result['memory_bytes'],
            benchmark_result['gpu_memory_time']
        )

        # ç¼“å­˜å‘½ä¸­ç‡
        cache_hit_rate = self._calculate_cache_hit_rate()

        # æˆåŠŸç‡ (æœ€è¿‘100æ¬¡ä»»åŠ¡)
        success_rate = self._calculate_success_rate()

        return PerformanceMetrics(
            timestamp=datetime.now(),
            matrix_gflops=matrix_gflops,
            matrix_speedup=matrix_speedup,
            matrix_throughput=benchmark_result['matrix_throughput'],
            memory_bandwidth_gbs=memory_bandwidth,
            memory_speedup=memory_speedup,
            memory_throughput=benchmark_result['memory_throughput'],
            overall_speedup=overall_speedup,
            cache_hit_rate=cache_hit_rate,
            success_rate=success_rate
        )

    async def _run_lightweight_benchmark(self) -> Dict:
        """è¿è¡Œè½»é‡çº§åŸºå‡†æµ‹è¯• (512x512çŸ©é˜µä¹˜æ³•)"""
        import cupy as cp
        import numpy as np
        import time

        # çŸ©é˜µå¤§å°
        N = 512

        # GPUåŸºå‡†æµ‹è¯•
        A_gpu = cp.random.rand(N, N, dtype=cp.float32)
        B_gpu = cp.random.rand(N, N, dtype=cp.float32)

        start = time.perf_counter()
        C_gpu = cp.matmul(A_gpu, B_gpu)
        cp.cuda.Device().synchronize()
        gpu_matrix_time = time.perf_counter() - start

        # CPUåŸºå‡†æµ‹è¯•
        A_cpu = np.random.rand(N, N).astype(np.float32)
        B_cpu = np.random.rand(N, N).astype(np.float32)

        start = time.perf_counter()
        C_cpu = np.matmul(A_cpu, B_cpu)
        cpu_matrix_time = time.perf_counter() - start

        # è®¡ç®—FLOPS (çŸ©é˜µä¹˜æ³•: 2*N^3 FLOPs)
        matrix_ops = 2 * (N ** 3)
        matrix_throughput = 1.0 / gpu_matrix_time  # æ¯ç§’çŸ©é˜µè¿ç®—æ¬¡æ•°

        # å†…å­˜æ“ä½œåŸºå‡†æµ‹è¯•
        memory_bytes = N * N * 4  # float32 = 4 bytes

        start = time.perf_counter()
        D_gpu = cp.copy(A_gpu)
        cp.cuda.Device().synchronize()
        gpu_memory_time = time.perf_counter() - start

        start = time.perf_counter()
        D_cpu = np.copy(A_cpu)
        cpu_memory_time = time.perf_counter() - start

        memory_throughput = 1.0 / gpu_memory_time

        return {
            'gpu_matrix_time': gpu_matrix_time,
            'cpu_matrix_time': cpu_matrix_time,
            'gpu_memory_time': gpu_memory_time,
            'cpu_memory_time': cpu_memory_time,
            'matrix_ops': matrix_ops,
            'memory_bytes': memory_bytes,
            'matrix_throughput': matrix_throughput,
            'memory_throughput': memory_throughput
        }

    def _calculate_speedup(self, gpu_time: float, cpu_time: float) -> float:
        """è®¡ç®—åŠ é€Ÿæ¯”"""
        if gpu_time == 0:
            return 0.0
        return cpu_time / gpu_time

    def _calculate_gflops(self, ops: int, time_sec: float) -> float:
        """è®¡ç®—GFLOPS"""
        if time_sec == 0:
            return 0.0
        flops = ops / time_sec
        return flops / 1e9  # è½¬GFLOPS

    def _calculate_bandwidth(self, bytes_transferred: int, time_sec: float) -> float:
        """è®¡ç®—å†…å­˜å¸¦å®½ (GB/s)"""
        if time_sec == 0:
            return 0.0
        bytes_per_sec = bytes_transferred / time_sec
        return bytes_per_sec / 1e9  # è½¬GB/s

    def _calculate_cache_hit_rate(self) -> float:
        """è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡"""
        total = self.cache_stats["hits"] + self.cache_stats["misses"]
        if total == 0:
            return 0.0
        return (self.cache_stats["hits"] / total) * 100

    def _calculate_success_rate(self) -> float:
        """è®¡ç®—ä»»åŠ¡æˆåŠŸç‡ (æœ€è¿‘100æ¬¡)"""
        if not self.recent_benchmarks:
            return 0.0

        successful = sum(1 for b in self.recent_benchmarks if b['success'])
        return (successful / len(self.recent_benchmarks)) * 100

    def record_benchmark(self, result: Dict):
        """è®°å½•åŸºå‡†æµ‹è¯•ç»“æœ"""
        self.recent_benchmarks.append(result)
        if len(self.recent_benchmarks) > 100:
            self.recent_benchmarks.pop(0)

    def update_cache_stats(self, hit: bool):
        """æ›´æ–°ç¼“å­˜ç»Ÿè®¡"""
        if hit:
            self.cache_stats["hits"] += 1
        else:
            self.cache_stats["misses"] += 1
```

**APIç«¯ç‚¹**:
```python
perf_collector = PerformanceCollector()

@router.get("/performance", response_model=PerformanceMetrics)
async def get_performance_metrics():
    """è·å–å½“å‰æ€§èƒ½æŒ‡æ ‡"""
    return await perf_collector.collect_performance_metrics()
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æˆåŠŸé‡‡é›†GFLOPSã€åŠ é€Ÿæ¯”ã€ååé‡
- [ ] ç¼“å­˜å‘½ä¸­ç‡è®¡ç®—å‡†ç¡®
- [ ] è½»é‡çº§åŸºå‡†æµ‹è¯•å®Œæˆæ—¶é—´ < 500ms

**é¢„ä¼°æ—¶é—´**: 1å¤©

---

#### T5.3 å†å²æ•°æ®æŒä¹…åŒ–å’ŒæŸ¥è¯¢
**ç›®æ ‡**: å°†ç›‘æ§æ•°æ®æŒä¹…åŒ–åˆ°PostgreSQL,æ”¯æŒå†å²æŸ¥è¯¢

**æ•°æ®åº“Schema**:
```sql
-- GPUç›‘æ§å†å²æ•°æ®è¡¨
CREATE TABLE IF NOT EXISTS gpu_monitoring_history (
    id SERIAL PRIMARY KEY,
    device_id INT NOT NULL,
    timestamp TIMESTAMP NOT NULL,

    -- GPUç¡¬ä»¶æŒ‡æ ‡
    gpu_utilization FLOAT,
    memory_used INT,
    memory_total INT,
    memory_utilization FLOAT,
    temperature FLOAT,
    power_usage FLOAT,
    sm_clock INT,
    memory_clock INT,

    -- æ€§èƒ½æŒ‡æ ‡
    matrix_gflops FLOAT,
    matrix_speedup FLOAT,
    memory_bandwidth_gbs FLOAT,
    overall_speedup FLOAT,
    cache_hit_rate FLOAT,
    success_rate FLOAT,

    created_at TIMESTAMP DEFAULT NOW()
);

-- ç´¢å¼•ä¼˜åŒ–
CREATE INDEX idx_gpu_monitoring_device_time ON gpu_monitoring_history(device_id, timestamp DESC);
CREATE INDEX idx_gpu_monitoring_timestamp ON gpu_monitoring_history(timestamp DESC);

-- æ€§èƒ½äº‹ä»¶è¡¨ (å¼‚å¸¸äº‹ä»¶è®°å½•)
CREATE TABLE IF NOT EXISTS gpu_performance_events (
    id SERIAL PRIMARY KEY,
    device_id INT NOT NULL,
    timestamp TIMESTAMP NOT NULL,
    event_type VARCHAR(50),  -- 'high_temp', 'low_utilization', 'memory_leak', 'performance_drop'
    severity VARCHAR(20),    -- 'info', 'warning', 'critical'
    message TEXT,
    metadata JSONB,
    resolved BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT NOW()
);
```

**å†å²æ•°æ®æœåŠ¡**:
```python
from sqlalchemy import create_engine, Column, Integer, Float, DateTime, String, Boolean, Text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from typing import List, Optional
from datetime import datetime, timedelta

Base = declarative_base()

class GPUMonitoringHistory(Base):
    __tablename__ = "gpu_monitoring_history"

    id = Column(Integer, primary_key=True)
    device_id = Column(Integer, nullable=False)
    timestamp = Column(DateTime, nullable=False)
    gpu_utilization = Column(Float)
    memory_used = Column(Integer)
    memory_total = Column(Integer)
    temperature = Column(Float)
    power_usage = Column(Float)
    matrix_gflops = Column(Float)
    overall_speedup = Column(Float)
    cache_hit_rate = Column(Float)

class GPUPerformanceEvent(Base):
    __tablename__ = "gpu_performance_events"

    id = Column(Integer, primary_key=True)
    device_id = Column(Integer, nullable=False)
    timestamp = Column(DateTime, nullable=False)
    event_type = Column(String(50))
    severity = Column(String(20))
    message = Column(Text)
    resolved = Column(Boolean, default=False)

class HistoryDataService:
    """å†å²æ•°æ®æœåŠ¡"""

    def __init__(self, db_url: str):
        self.engine = create_engine(db_url)
        Base.metadata.create_all(self.engine)
        Session = sessionmaker(bind=self.engine)
        self.session = Session()

    def save_metrics(self, gpu_metrics: GPUMetrics, perf_metrics: PerformanceMetrics):
        """ä¿å­˜ç›‘æ§æŒ‡æ ‡"""
        record = GPUMonitoringHistory(
            device_id=gpu_metrics.device_id,
            timestamp=gpu_metrics.timestamp,
            gpu_utilization=gpu_metrics.gpu_utilization,
            memory_used=gpu_metrics.memory_used,
            memory_total=gpu_metrics.memory_total,
            temperature=gpu_metrics.temperature,
            power_usage=gpu_metrics.power_usage,
            matrix_gflops=perf_metrics.matrix_gflops,
            overall_speedup=perf_metrics.overall_speedup,
            cache_hit_rate=perf_metrics.cache_hit_rate
        )
        self.session.add(record)
        self.session.commit()

    def query_history(
        self,
        device_id: int,
        start_time: datetime,
        end_time: datetime
    ) -> List[GPUMonitoringHistory]:
        """æŸ¥è¯¢å†å²æ•°æ®"""
        return self.session.query(GPUMonitoringHistory).filter(
            GPUMonitoringHistory.device_id == device_id,
            GPUMonitoringHistory.timestamp >= start_time,
            GPUMonitoringHistory.timestamp <= end_time
        ).order_by(GPUMonitoringHistory.timestamp.desc()).all()

    def get_aggregated_stats(
        self,
        device_id: int,
        hours: int = 24
    ) -> Dict:
        """è·å–èšåˆç»Ÿè®¡ (æœ€è¿‘Nå°æ—¶)"""
        start_time = datetime.now() - timedelta(hours=hours)

        from sqlalchemy import func

        stats = self.session.query(
            func.avg(GPUMonitoringHistory.gpu_utilization).label('avg_utilization'),
            func.max(GPUMonitoringHistory.gpu_utilization).label('max_utilization'),
            func.avg(GPUMonitoringHistory.temperature).label('avg_temperature'),
            func.max(GPUMonitoringHistory.temperature).label('max_temperature'),
            func.avg(GPUMonitoringHistory.matrix_gflops).label('avg_gflops'),
            func.max(GPUMonitoringHistory.matrix_gflops).label('peak_gflops'),
            func.avg(GPUMonitoringHistory.overall_speedup).label('avg_speedup')
        ).filter(
            GPUMonitoringHistory.device_id == device_id,
            GPUMonitoringHistory.timestamp >= start_time
        ).first()

        return {
            "avg_utilization": float(stats.avg_utilization or 0),
            "max_utilization": float(stats.max_utilization or 0),
            "avg_temperature": float(stats.avg_temperature or 0),
            "max_temperature": float(stats.max_temperature or 0),
            "avg_gflops": float(stats.avg_gflops or 0),
            "peak_gflops": float(stats.peak_gflops or 0),
            "avg_speedup": float(stats.avg_speedup or 0)
        }

    def log_event(self, event: GPUPerformanceEvent):
        """è®°å½•æ€§èƒ½äº‹ä»¶"""
        self.session.add(event)
        self.session.commit()
```

**APIç«¯ç‚¹**:
```python
history_service = HistoryDataService(db_url="postgresql://user:pass@localhost/mystocks")

@router.get("/history/{device_id}")
async def get_gpu_history(
    device_id: int,
    hours: int = 24
):
    """è·å–å†å²æ•°æ®"""
    end_time = datetime.now()
    start_time = end_time - timedelta(hours=hours)
    return history_service.query_history(device_id, start_time, end_time)

@router.get("/stats/{device_id}")
async def get_aggregated_stats(device_id: int, hours: int = 24):
    """è·å–èšåˆç»Ÿè®¡"""
    return history_service.get_aggregated_stats(device_id, hours)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ•°æ®æˆåŠŸæŒä¹…åŒ–åˆ°PostgreSQL
- [ ] å†å²æŸ¥è¯¢é€Ÿåº¦ < 500ms (24å°æ—¶æ•°æ®)
- [ ] èšåˆç»Ÿè®¡è®¡ç®—å‡†ç¡®

**é¢„ä¼°æ—¶é—´**: 1å¤©

---

### **é˜¶æ®µ2: å‰ç«¯ä»ªè¡¨æ¿ (Day 4-6)**

#### T5.4 GPUçŠ¶æ€å¡ç‰‡ç»„ä»¶
**ç›®æ ‡**: å®æ—¶æ˜¾ç¤ºGPUç¡¬ä»¶çŠ¶æ€

**æ ¸å¿ƒç»„ä»¶**:
```typescript
// web/frontend/src/components/GPUMonitoring/GPUStatusCard.vue
<template>
  <el-card class="gpu-status-card">
    <template #header>
      <div class="card-header">
        <span>GPU {{ deviceId }}: {{ deviceName }}</span>
        <el-tag :type="getStatusTagType(status)" size="small">
          {{ status }}
        </el-tag>
      </div>
    </template>

    <div class="metrics-grid">
      <!-- GPUåˆ©ç”¨ç‡ -->
      <div class="metric-item">
        <div class="metric-label">GPUåˆ©ç”¨ç‡</div>
        <el-progress
          type="dashboard"
          :percentage="metrics.gpu_utilization"
          :color="getUtilizationColor(metrics.gpu_utilization)"
        >
          <template #default="{ percentage }">
            <span class="percentage-value">{{ percentage }}%</span>
          </template>
        </el-progress>
      </div>

      <!-- æ˜¾å­˜ä½¿ç”¨ -->
      <div class="metric-item">
        <div class="metric-label">æ˜¾å­˜ä½¿ç”¨</div>
        <el-progress
          type="dashboard"
          :percentage="metrics.memory_utilization"
          :color="getMemoryColor(metrics.memory_utilization)"
        >
          <template #default>
            <span class="percentage-value">
              {{ formatMemory(metrics.memory_used) }} / {{ formatMemory(metrics.memory_total) }}
            </span>
          </template>
        </el-progress>
      </div>

      <!-- æ¸©åº¦ -->
      <div class="metric-item">
        <div class="metric-label">æ¸©åº¦</div>
        <div class="metric-value" :class="getTemperatureClass(metrics.temperature)">
          {{ metrics.temperature.toFixed(1) }}Â°C
        </div>
        <el-progress
          :percentage="(metrics.temperature / 100) * 100"
          :show-text="false"
          :color="getTemperatureColor(metrics.temperature)"
        />
      </div>

      <!-- åŠŸè€— -->
      <div class="metric-item">
        <div class="metric-label">åŠŸè€—</div>
        <div class="metric-value">
          {{ metrics.power_usage.toFixed(1) }} W / {{ metrics.power_limit.toFixed(0) }} W
        </div>
        <el-progress
          :percentage="(metrics.power_usage / metrics.power_limit) * 100"
          :show-text="false"
        />
      </div>

      <!-- æ—¶é’Ÿé¢‘ç‡ -->
      <div class="metric-item">
        <div class="metric-label">SMé¢‘ç‡</div>
        <div class="metric-value">{{ metrics.sm_clock }} MHz</div>
      </div>

      <div class="metric-item">
        <div class="metric-label">æ˜¾å­˜é¢‘ç‡</div>
        <div class="metric-value">{{ metrics.memory_clock }} MHz</div>
      </div>

      <!-- PCIeååé‡ -->
      <div class="metric-item">
        <div class="metric-label">PCIeååé‡</div>
        <div class="metric-value">
          â†‘ {{ metrics.pcie_throughput_tx.toFixed(2) }} MB/s<br>
          â†“ {{ metrics.pcie_throughput_rx.toFixed(2) }} MB/s
        </div>
      </div>
    </div>

    <!-- è¿è¡Œè¿›ç¨‹ -->
    <el-divider />
    <div class="processes-section">
      <div class="section-title">è¿è¡Œè¿›ç¨‹ ({{ processes.length }})</div>
      <el-table :data="processes" size="small" max-height="200">
        <el-table-column prop="pid" label="PID" width="80" />
        <el-table-column prop="process_name" label="è¿›ç¨‹å" width="150" />
        <el-table-column label="æ˜¾å­˜å ç”¨" width="120">
          <template #default="{ row }">
            {{ formatMemory(row.memory_used_mb) }}
          </template>
        </el-table-column>
        <el-table-column prop="cmdline" label="å‘½ä»¤è¡Œ" show-overflow-tooltip />
      </el-table>
    </div>
  </el-card>
</template>

<script setup lang="ts">
import { ref, onMounted, onUnmounted } from 'vue';
import axios from 'axios';

const props = defineProps<{
  deviceId: number;
}>();

const deviceName = ref('');
const status = ref('æ­£å¸¸');
const metrics = ref({
  gpu_utilization: 0,
  memory_used: 0,
  memory_total: 0,
  memory_utilization: 0,
  temperature: 0,
  power_usage: 0,
  power_limit: 0,
  sm_clock: 0,
  memory_clock: 0,
  pcie_throughput_tx: 0,
  pcie_throughput_rx: 0
});
const processes = ref([]);

let updateInterval: number;

const fetchMetrics = async () => {
  try {
    const [metricsResp, processesResp] = await Promise.all([
      axios.get(`/api/gpu/metrics/${props.deviceId}`),
      axios.get(`/api/gpu/processes/${props.deviceId}`)
    ]);

    deviceName.value = metricsResp.data.device_name;
    metrics.value = metricsResp.data;
    processes.value = processesResp.data;

    // åˆ¤æ–­çŠ¶æ€
    if (metrics.value.temperature > 85) {
      status.value = 'é«˜æ¸©';
    } else if (metrics.value.gpu_utilization > 95) {
      status.value = 'ç¹å¿™';
    } else if (metrics.value.gpu_utilization < 10) {
      status.value = 'ç©ºé—²';
    } else {
      status.value = 'æ­£å¸¸';
    }
  } catch (error) {
    console.error('è·å–GPUæŒ‡æ ‡å¤±è´¥:', error);
  }
};

const getUtilizationColor = (value: number) => {
  if (value < 30) return '#909399';  // ç°è‰² (ç©ºé—²)
  if (value < 70) return '#67C23A';  // ç»¿è‰² (æ­£å¸¸)
  if (value < 90) return '#E6A23C';  // æ©™è‰² (ç¹å¿™)
  return '#F56C6C';  // çº¢è‰² (æ»¡è½½)
};

const getMemoryColor = (value: number) => {
  if (value < 60) return '#67C23A';
  if (value < 80) return '#E6A23C';
  return '#F56C6C';
};

const getTemperatureColor = (temp: number) => {
  if (temp < 60) return '#67C23A';
  if (temp < 80) return '#E6A23C';
  return '#F56C6C';
};

const getTemperatureClass = (temp: number) => {
  if (temp > 85) return 'temp-critical';
  if (temp > 75) return 'temp-warning';
  return 'temp-normal';
};

const getStatusTagType = (status: string) => {
  const map: Record<string, any> = {
    'æ­£å¸¸': 'success',
    'ç¹å¿™': 'warning',
    'é«˜æ¸©': 'danger',
    'ç©ºé—²': 'info'
  };
  return map[status] || 'info';
};

const formatMemory = (mb: number) => {
  if (mb >= 1024) {
    return `${(mb / 1024).toFixed(2)} GB`;
  }
  return `${mb.toFixed(0)} MB`;
};

onMounted(() => {
  fetchMetrics();
  updateInterval = window.setInterval(fetchMetrics, 2000);  // æ¯2ç§’æ›´æ–°
});

onUnmounted(() => {
  clearInterval(updateInterval);
});
</script>

<style scoped>
.gpu-status-card {
  margin-bottom: 16px;
}

.card-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
}

.metrics-grid {
  display: grid;
  grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
  gap: 16px;
}

.metric-item {
  text-align: center;
}

.metric-label {
  font-size: 12px;
  color: #909399;
  margin-bottom: 8px;
}

.metric-value {
  font-size: 18px;
  font-weight: bold;
  margin: 8px 0;
}

.temp-critical {
  color: #F56C6C;
}

.temp-warning {
  color: #E6A23C;
}

.temp-normal {
  color: #67C23A;
}

.processes-section {
  margin-top: 16px;
}

.section-title {
  font-size: 14px;
  font-weight: bold;
  margin-bottom: 8px;
}
</style>
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] GPUçŠ¶æ€å®æ—¶æ›´æ–° (2ç§’åˆ·æ–°)
- [ ] è¿›åº¦æ¡é¢œè‰²æ ¹æ®é˜ˆå€¼å˜åŒ–
- [ ] è¿›ç¨‹ä¿¡æ¯æ­£ç¡®æ˜¾ç¤º

**é¢„ä¼°æ—¶é—´**: 1å¤©

---

#### T5.5 æ€§èƒ½å›¾è¡¨ç»„ä»¶ (GFLOPS/åŠ é€Ÿæ¯”è¶‹åŠ¿)
**ç›®æ ‡**: å¯è§†åŒ–æ€§èƒ½è¶‹åŠ¿

**æ ¸å¿ƒç»„ä»¶**:
```typescript
// web/frontend/src/components/GPUMonitoring/PerformanceChart.vue
<template>
  <el-card>
    <template #header>
      <div class="header">
        <span>æ€§èƒ½è¶‹åŠ¿</span>
        <el-radio-group v-model="timeRange" size="small" @change="fetchData">
          <el-radio-button label="1h">1å°æ—¶</el-radio-button>
          <el-radio-button label="6h">6å°æ—¶</el-radio-button>
          <el-radio-button label="24h">24å°æ—¶</el-radio-button>
        </el-radio-group>
      </div>
    </template>

    <div ref="chartRef" style="width: 100%; height: 400px;"></div>
  </el-card>
</template>

<script setup lang="ts">
import { ref, onMounted, watch } from 'vue';
import * as echarts from 'echarts';
import axios from 'axios';

const timeRange = ref('1h');
const chartRef = ref<HTMLElement>();
let chartInstance: echarts.ECharts;

const fetchData = async () => {
  const hours = parseInt(timeRange.value);
  const response = await axios.get(`/api/gpu/history/0?hours=${hours}`);
  const data = response.data;

  // æå–æ—¶é—´åºåˆ—
  const timestamps = data.map((d: any) => new Date(d.timestamp).toLocaleTimeString());
  const gflops = data.map((d: any) => d.matrix_gflops);
  const speedup = data.map((d: any) => d.overall_speedup);
  const temperature = data.map((d: any) => d.temperature);
  const utilization = data.map((d: any) => d.gpu_utilization);

  const option = {
    title: {
      text: 'GPUæ€§èƒ½è¶‹åŠ¿'
    },
    tooltip: {
      trigger: 'axis'
    },
    legend: {
      data: ['GFLOPS', 'åŠ é€Ÿæ¯”', 'æ¸©åº¦', 'GPUåˆ©ç”¨ç‡']
    },
    grid: {
      left: '3%',
      right: '4%',
      bottom: '3%',
      containLabel: true
    },
    xAxis: {
      type: 'category',
      boundaryGap: false,
      data: timestamps
    },
    yAxis: [
      {
        type: 'value',
        name: 'GFLOPS / åŠ é€Ÿæ¯”',
        position: 'left'
      },
      {
        type: 'value',
        name: 'æ¸©åº¦ / åˆ©ç”¨ç‡',
        position: 'right'
      }
    ],
    series: [
      {
        name: 'GFLOPS',
        type: 'line',
        smooth: true,
        data: gflops,
        yAxisIndex: 0,
        itemStyle: { color: '#5470C6' }
      },
      {
        name: 'åŠ é€Ÿæ¯”',
        type: 'line',
        smooth: true,
        data: speedup,
        yAxisIndex: 0,
        itemStyle: { color: '#91CC75' }
      },
      {
        name: 'æ¸©åº¦',
        type: 'line',
        smooth: true,
        data: temperature,
        yAxisIndex: 1,
        itemStyle: { color: '#EE6666' }
      },
      {
        name: 'GPUåˆ©ç”¨ç‡',
        type: 'line',
        smooth: true,
        data: utilization,
        yAxisIndex: 1,
        itemStyle: { color: '#FAC858' }
      }
    ]
  };

  chartInstance.setOption(option);
};

onMounted(() => {
  chartInstance = echarts.init(chartRef.value!);
  fetchData();
});

watch(timeRange, fetchData);
</script>
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] å›¾è¡¨å®æ—¶æ›´æ–°
- [ ] æ”¯æŒ1h/6h/24hæ—¶é—´èŒƒå›´åˆ‡æ¢
- [ ] å››æ¡æ›²çº¿æ­£å¸¸æ˜¾ç¤º

**é¢„ä¼°æ—¶é—´**: 1å¤©

---

#### T5.6 æ™ºèƒ½ä¼˜åŒ–å»ºè®®ç»„ä»¶
**ç›®æ ‡**: åŸºäºç›‘æ§æ•°æ®ç”Ÿæˆä¼˜åŒ–å»ºè®®

**å…³é”®å®ç°**:
```python
from typing import List
from pydantic import BaseModel

class OptimizationRecommendation(BaseModel):
    """ä¼˜åŒ–å»ºè®®"""
    title: str
    category: str  # 'performance', 'temperature', 'memory', 'efficiency'
    severity: str  # 'info', 'warning', 'critical'
    description: str
    expected_improvement: str
    action_steps: List[str]

class OptimizationAdvisor:
    """ä¼˜åŒ–å»ºè®®å¼•æ“"""

    def analyze_and_recommend(
        self,
        gpu_metrics: GPUMetrics,
        perf_metrics: PerformanceMetrics,
        stats_24h: Dict
    ) -> List[OptimizationRecommendation]:
        """åˆ†æå¹¶ç”Ÿæˆä¼˜åŒ–å»ºè®®"""

        recommendations = []

        # è§„åˆ™1: GPUåˆ©ç”¨ç‡è¿‡ä½
        if stats_24h['avg_utilization'] < 30:
            recommendations.append(OptimizationRecommendation(
                title="GPUåˆ©ç”¨ç‡è¿‡ä½",
                category="efficiency",
                severity="warning",
                description=f"è¿‡å»24å°æ—¶å¹³å‡GPUåˆ©ç”¨ç‡ä»…{stats_24h['avg_utilization']:.1f}%,å­˜åœ¨èµ„æºæµªè´¹",
                expected_improvement="æå‡åˆ©ç”¨ç‡å¯é™ä½æ¯GFLOPæˆæœ¬",
                action_steps=[
                    "å¢åŠ æ‰¹å¤„ç†å¤§å° (batch_size)",
                    "å¹¶è¡Œæ‰§è¡Œå¤šä¸ªå›æµ‹ä»»åŠ¡",
                    "æ£€æŸ¥æ˜¯å¦æœ‰CPUç“¶é¢ˆé™åˆ¶GPUæ€§èƒ½"
                ]
            ))

        # è§„åˆ™2: æ¸©åº¦è¿‡é«˜
        if stats_24h['max_temperature'] > 85:
            recommendations.append(OptimizationRecommendation(
                title="æ¸©åº¦è¿‡é«˜è­¦å‘Š",
                category="temperature",
                severity="critical",
                description=f"GPUæœ€é«˜æ¸©åº¦è¾¾åˆ°{stats_24h['max_temperature']:.1f}Â°C,å¯èƒ½å½±å“æ€§èƒ½å’Œå¯¿å‘½",
                expected_improvement="é™æ¸©å¯æå‡3-5%æ€§èƒ½å¹¶å»¶é•¿ç¡¬ä»¶å¯¿å‘½",
                action_steps=[
                    "æ£€æŸ¥æœºç®±é£æ‰‡è¿è¡ŒçŠ¶æ€",
                    "æ¸…ç†GPUæ•£çƒ­å™¨ç°å°˜",
                    "é™ä½GPUåŠŸè€—é™åˆ¶ (power_limit)",
                    "è€ƒè™‘å¢åŠ æœºç®±æ•£çƒ­é£æ‰‡"
                ]
            ))

        # è§„åˆ™3: æ˜¾å­˜åˆ©ç”¨ç‡ä½
        if gpu_metrics.memory_utilization < 50:
            recommendations.append(OptimizationRecommendation(
                title="æ˜¾å­˜åˆ©ç”¨ç‡è¾ƒä½",
                category="memory",
                severity="info",
                description=f"å½“å‰æ˜¾å­˜åˆ©ç”¨ç‡{gpu_metrics.memory_utilization:.1f}%,å¯å¢åŠ æ•°æ®æ‰¹å¤„ç†å¤§å°",
                expected_improvement="æå‡æ˜¾å­˜åˆ©ç”¨ç‡å¯æé«˜10-20%ååé‡",
                action_steps=[
                    "å¢åŠ batch_size (å½“å‰å¯èƒ½åå°)",
                    "å‡å°‘å†…å­˜æ± é¢„ç•™ç©ºé—´",
                    "é¢„åŠ è½½æ›´å¤šæ•°æ®åˆ°æ˜¾å­˜"
                ]
            ))

        # è§„åˆ™4: åŠ é€Ÿæ¯”ä½äºé¢„æœŸ
        if perf_metrics.overall_speedup < 50:
            recommendations.append(OptimizationRecommendation(
                title="åŠ é€Ÿæ¯”ä½äºé¢„æœŸ",
                category="performance",
                severity="warning",
                description=f"å½“å‰ç»¼åˆåŠ é€Ÿæ¯”{perf_metrics.overall_speedup:.2f}x,è¿œä½äºç›®æ ‡68.58x",
                expected_improvement="ä¼˜åŒ–ç®—æ³•å¯è¾¾åˆ°ç›®æ ‡åŠ é€Ÿæ¯”",
                action_steps=[
                    "æ£€æŸ¥æ˜¯å¦ä½¿ç”¨Strassenç®—æ³• (O(n^2.807))",
                    "å¯ç”¨CUDAæµå¹¶è¡Œ",
                    "ä½¿ç”¨åˆ†å—çŸ©é˜µä¹˜æ³•ä¼˜åŒ–å¤§çŸ©é˜µ",
                    "æ£€æŸ¥GPUé©±åŠ¨ç‰ˆæœ¬æ˜¯å¦æœ€æ–°"
                ]
            ))

        # è§„åˆ™5: ç¼“å­˜å‘½ä¸­ç‡ä½
        if perf_metrics.cache_hit_rate < 80:
            recommendations.append(OptimizationRecommendation(
                title="ç¼“å­˜å‘½ä¸­ç‡åä½",
                category="performance",
                severity="info",
                description=f"å†…å­˜æ± ç¼“å­˜å‘½ä¸­ç‡{perf_metrics.cache_hit_rate:.1f}%,å­˜åœ¨ä¼˜åŒ–ç©ºé—´",
                expected_improvement="æå‡ç¼“å­˜å‘½ä¸­ç‡å¯å‡å°‘30%å†…å­˜åˆ†é…å¼€é”€",
                action_steps=[
                    "å¢åŠ å†…å­˜æ± å¤§å°",
                    "ä¼˜åŒ–å†…å­˜å—é‡ç”¨ç­–ç•¥",
                    "é¢„åˆ†é…å¸¸ç”¨å°ºå¯¸å†…å­˜å—"
                ]
            ))

        return recommendations

# APIç«¯ç‚¹
advisor = OptimizationAdvisor()

@router.get("/recommendations", response_model=List[OptimizationRecommendation])
async def get_optimization_recommendations(device_id: int = 0):
    """è·å–ä¼˜åŒ–å»ºè®®"""
    gpu_metrics = gpu_monitor.get_metrics(device_id)
    perf_metrics = await perf_collector.collect_performance_metrics()
    stats_24h = history_service.get_aggregated_stats(device_id, hours=24)

    return advisor.analyze_and_recommend(gpu_metrics, perf_metrics, stats_24h)
```

**å‰ç«¯ç»„ä»¶**:
```typescript
// web/frontend/src/components/GPUMonitoring/OptimizationPanel.vue
<template>
  <el-card>
    <template #header>
      <span>æ™ºèƒ½ä¼˜åŒ–å»ºè®®</span>
      <el-button size="small" @click="fetchRecommendations">åˆ·æ–°</el-button>
    </template>

    <el-alert
      v-for="rec in recommendations"
      :key="rec.title"
      :title="rec.title"
      :type="getSeverityType(rec.severity)"
      :closable="false"
      class="recommendation-alert"
    >
      <template #default>
        <p><strong>é—®é¢˜æè¿°:</strong> {{ rec.description }}</p>
        <p><strong>é¢„æœŸæ”¹å–„:</strong> {{ rec.expected_improvement }}</p>
        <div class="action-steps">
          <strong>å»ºè®®æ“ä½œ:</strong>
          <ol>
            <li v-for="step in rec.action_steps" :key="step">{{ step }}</li>
          </ol>
        </div>
      </template>
    </el-alert>

    <el-empty v-if="recommendations.length === 0" description="æš‚æ— ä¼˜åŒ–å»ºè®®,ç³»ç»Ÿè¿è¡Œè‰¯å¥½" />
  </el-card>
</template>

<script setup lang="ts">
import { ref, onMounted } from 'vue';
import axios from 'axios';

const recommendations = ref([]);

const fetchRecommendations = async () => {
  const response = await axios.get('/api/gpu/recommendations?device_id=0');
  recommendations.value = response.data;
};

const getSeverityType = (severity: string) => {
  const map: Record<string, any> = {
    'info': 'info',
    'warning': 'warning',
    'critical': 'error'
  };
  return map[severity] || 'info';
};

onMounted(fetchRecommendations);
</script>
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] ä¼˜åŒ–å»ºè®®å‡†ç¡® (è§„åˆ™è¦†ç›–5å¤§ç±»)
- [ ] å‰ç«¯æ˜¾ç¤ºæ¸…æ™°æ˜“æ‡‚
- [ ] åˆ·æ–°åŠŸèƒ½æ­£å¸¸

**é¢„ä¼°æ—¶é—´**: 1å¤©

---

### **é˜¶æ®µ3: å®æ—¶æ¨é€å’Œå‘Šè­¦ (Day 7-8)**

#### T5.7 SSEå®æ—¶æ¨é€GPUæŒ‡æ ‡
**ç›®æ ‡**: ä½¿ç”¨Server-Sent Eventsæ¨é€å®æ—¶æ•°æ®

**åç«¯å®ç°**:
```python
from fastapi import APIRouter
from fastapi.responses import StreamingResponse
import asyncio
import json

router = APIRouter(prefix="/api/gpu/stream", tags=["GPUå®æ—¶æ¨é€"])

@router.get("/{device_id}")
async def gpu_metrics_stream(device_id: int):
    """GPUæŒ‡æ ‡å®æ—¶æ¨é€ (SSE)"""

    async def event_generator():
        try:
            while True:
                # è·å–æœ€æ–°æŒ‡æ ‡
                gpu_metrics = gpu_monitor.get_metrics(device_id)
                perf_metrics = await perf_collector.collect_performance_metrics()

                # åˆå¹¶æ•°æ®
                data = {
                    **gpu_metrics.dict(),
                    **perf_metrics.dict()
                }

                # æ¨é€SSEäº‹ä»¶
                yield f"data: {json.dumps(data)}\n\n"

                await asyncio.sleep(2)  # æ¯2ç§’æ¨é€ä¸€æ¬¡

        except asyncio.CancelledError:
            pass

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive"
        }
    )
```

**å‰ç«¯æ¥æ”¶**:
```typescript
// web/frontend/src/composables/useGPUStream.ts
import { ref, onMounted, onUnmounted } from 'vue';

export function useGPUStream(deviceId: number) {
  const metrics = ref<any>({});
  let eventSource: EventSource | null = null;

  const connect = () => {
    eventSource = new EventSource(`/api/gpu/stream/${deviceId}`);

    eventSource.onmessage = (event) => {
      metrics.value = JSON.parse(event.data);
    };

    eventSource.onerror = () => {
      console.error('SSEè¿æ¥æ–­å¼€,5ç§’åé‡è¿');
      setTimeout(connect, 5000);
    };
  };

  onMounted(connect);
  onUnmounted(() => eventSource?.close());

  return { metrics };
}
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] SSEè¿æ¥ç¨³å®š (2ç§’åˆ·æ–°)
- [ ] æ–­çº¿è‡ªåŠ¨é‡è¿
- [ ] æ•°æ®å®æ—¶æ›´æ–°å‰ç«¯

**é¢„ä¼°æ—¶é—´**: 1å¤©

---

#### T5.8 GPUå¼‚å¸¸å‘Šè­¦ç³»ç»Ÿ
**ç›®æ ‡**: æ£€æµ‹å¼‚å¸¸å¹¶å‘é€å‘Šè­¦

**å‘Šè­¦è§„åˆ™**:
```python
class GPUAlertRule:
    """GPUå‘Šè­¦è§„åˆ™"""

    @staticmethod
    def check_alerts(gpu_metrics: GPUMetrics, perf_metrics: PerformanceMetrics) -> List[GPUPerformanceEvent]:
        """æ£€æŸ¥æ˜¯å¦è§¦å‘å‘Šè­¦"""
        events = []

        # è§„åˆ™1: é«˜æ¸©å‘Šè­¦
        if gpu_metrics.temperature > 85:
            events.append(GPUPerformanceEvent(
                device_id=gpu_metrics.device_id,
                timestamp=datetime.now(),
                event_type='high_temp',
                severity='critical',
                message=f"GPUæ¸©åº¦è¿‡é«˜: {gpu_metrics.temperature:.1f}Â°C (é˜ˆå€¼: 85Â°C)",
                metadata={"temperature": gpu_metrics.temperature}
            ))

        # è§„åˆ™2: æ˜¾å­˜æ³„æ¼
        if gpu_metrics.memory_utilization > 95:
            events.append(GPUPerformanceEvent(
                device_id=gpu_metrics.device_id,
                timestamp=datetime.now(),
                event_type='memory_leak',
                severity='warning',
                message=f"æ˜¾å­˜ä½¿ç”¨ç‡è¿‡é«˜: {gpu_metrics.memory_utilization:.1f}% (é˜ˆå€¼: 95%)",
                metadata={"memory_utilization": gpu_metrics.memory_utilization}
            ))

        # è§„åˆ™3: æ€§èƒ½ä¸‹é™
        if perf_metrics.overall_speedup < 30:
            events.append(GPUPerformanceEvent(
                device_id=gpu_metrics.device_id,
                timestamp=datetime.now(),
                event_type='performance_drop',
                severity='warning',
                message=f"åŠ é€Ÿæ¯”å¼‚å¸¸ä¸‹é™: {perf_metrics.overall_speedup:.2f}x (é¢„æœŸ: >50x)",
                metadata={"speedup": perf_metrics.overall_speedup}
            ))

        # è§„åˆ™4: ä½åˆ©ç”¨ç‡ (24å°æ—¶å¹³å‡)
        stats_24h = history_service.get_aggregated_stats(gpu_metrics.device_id, 24)
        if stats_24h['avg_utilization'] < 20:
            events.append(GPUPerformanceEvent(
                device_id=gpu_metrics.device_id,
                timestamp=datetime.now(),
                event_type='low_utilization',
                severity='info',
                message=f"GPUåˆ©ç”¨ç‡è¿‡ä½: 24å°æ—¶å¹³å‡{stats_24h['avg_utilization']:.1f}%",
                metadata={"avg_utilization": stats_24h['avg_utilization']}
            ))

        return events

# åå°ä»»åŠ¡: å®šæœŸæ£€æŸ¥å‘Šè­¦
async def alert_checker_loop():
    """å‘Šè­¦æ£€æŸ¥å¾ªç¯ (æ¯30ç§’)"""
    while True:
        try:
            gpu_metrics = gpu_monitor.get_metrics(0)
            perf_metrics = await perf_collector.collect_performance_metrics()

            events = GPUAlertRule.check_alerts(gpu_metrics, perf_metrics)

            for event in events:
                # è®°å½•åˆ°æ•°æ®åº“
                history_service.log_event(event)

                # æ¨é€åˆ°å‰ç«¯ (é€šè¿‡SSE)
                await sse_manager.broadcast({
                    "type": "gpu_alert",
                    "data": event.dict()
                })

        except Exception as e:
            logger.error(f"å‘Šè­¦æ£€æŸ¥å¤±è´¥: {e}")

        await asyncio.sleep(30)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] 4ç§å‘Šè­¦è§„åˆ™æ­£å¸¸è§¦å‘
- [ ] å‘Šè­¦æ¨é€åˆ°å‰ç«¯
- [ ] å‘Šè­¦è®°å½•åˆ°æ•°æ®åº“

**é¢„ä¼°æ—¶é—´**: 1å¤©

---

### **é˜¶æ®µ4: é›†æˆæµ‹è¯•ä¸æ–‡æ¡£ (Day 9-10)**

#### T5.9 ç«¯åˆ°ç«¯æµ‹è¯•
**ç›®æ ‡**: éªŒè¯å®Œæ•´ç›‘æ§æµç¨‹

**æµ‹è¯•ç”¨ä¾‹**:
```python
import pytest

def test_gpu_metrics_api():
    """æµ‹è¯•GPUæŒ‡æ ‡API"""
    response = client.get("/api/gpu/metrics/0")
    assert response.status_code == 200
    data = response.json()
    assert 'gpu_utilization' in data
    assert 'temperature' in data

def test_performance_metrics_api():
    """æµ‹è¯•æ€§èƒ½æŒ‡æ ‡API"""
    response = client.get("/api/gpu/performance")
    assert response.status_code == 200
    data = response.json()
    assert 'matrix_gflops' in data
    assert 'overall_speedup' in data

def test_history_data_persistence():
    """æµ‹è¯•å†å²æ•°æ®æŒä¹…åŒ–"""
    # ä¿å­˜æ•°æ®
    gpu_metrics = gpu_monitor.get_metrics(0)
    perf_metrics = await perf_collector.collect_performance_metrics()
    history_service.save_metrics(gpu_metrics, perf_metrics)

    # æŸ¥è¯¢æ•°æ®
    data = history_service.query_history(0, datetime.now() - timedelta(hours=1), datetime.now())
    assert len(data) > 0

def test_alert_triggering():
    """æµ‹è¯•å‘Šè­¦è§¦å‘"""
    # æ¨¡æ‹Ÿé«˜æ¸©åœºæ™¯
    mock_metrics = GPUMetrics(
        device_id=0,
        device_name="Test GPU",
        timestamp=datetime.now(),
        temperature=90.0,  # é«˜äºé˜ˆå€¼85Â°C
        gpu_utilization=50.0,
        memory_used=8000,
        memory_total=12000,
        memory_utilization=66.7,
        power_usage=250.0,
        power_limit=300.0,
        sm_clock=1500,
        memory_clock=7000,
        pcie_throughput_tx=10.0,
        pcie_throughput_rx=10.0
    )

    events = GPUAlertRule.check_alerts(mock_metrics, perf_metrics)
    assert len(events) > 0
    assert any(e.event_type == 'high_temp' for e in events)
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹é€šè¿‡
- [ ] æµ‹è¯•è¦†ç›–ç‡ > 80%

**é¢„ä¼°æ—¶é—´**: 1å¤©

---

#### T5.10 æ–‡æ¡£å’Œäº¤ä»˜
**ç›®æ ‡**: å®Œæ•´æ–‡æ¡£å’Œéƒ¨ç½²æŒ‡å—

**æ–‡æ¡£æ¸…å•**:
1. `GPU_MONITORING_ARCHITECTURE.md` - æ¶æ„è®¾è®¡æ–‡æ¡£
2. `GPU_MONITORING_API_REFERENCE.md` - APIå‚è€ƒæ–‡æ¡£
3. `GPU_MONITORING_DEPLOYMENT_GUIDE.md` - éƒ¨ç½²æŒ‡å—
4. `README_CLI5.md` - CLI-5å®ŒæˆæŠ¥å‘Š

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ–‡æ¡£å®Œæ•´æ— é—æ¼
- [ ] éƒ¨ç½²æŒ‡å—å¯æ“ä½œæ€§å¼º

**é¢„ä¼°æ—¶é—´**: 1å¤©

---

## ğŸ“Š è¿›åº¦è·Ÿè¸ªä¸éªŒæ”¶

### é‡Œç¨‹ç¢‘æ£€æŸ¥ç‚¹

| é‡Œç¨‹ç¢‘ | æ—¶é—´èŠ‚ç‚¹ | éªŒæ”¶æ ‡å‡† |
|--------|---------|---------|
| M1: GPUç›‘æ§åç«¯å®Œæˆ | Day 3 | APIæ­£å¸¸,æ•°æ®æŒä¹…åŒ–æˆåŠŸ |
| M2: å‰ç«¯ä»ªè¡¨æ¿ä¸Šçº¿ | Day 6 | çŠ¶æ€å¡ç‰‡+å›¾è¡¨æ­£å¸¸æ˜¾ç¤º |
| M3: å®æ—¶æ¨é€å’Œå‘Šè­¦å¯ç”¨ | Day 8 | SSEç¨³å®š,å‘Šè­¦æ­£å¸¸è§¦å‘ |
| M4: é›†æˆæµ‹è¯•é€šè¿‡ | Day 10 | æµ‹è¯•è¦†ç›–ç‡>80%,æ–‡æ¡£å®Œæ•´ |

---

## ğŸ”— ä¾èµ–å…³ç³»

### ä¸Šæ¸¸ä¾èµ–
- **GPUåŠ é€Ÿå¼•æ“ (Phase 6.4)**: âœ… å·²å®Œæˆ (68.58xæ€§èƒ½æå‡)

### ä¸‹æ¸¸å½±å“
- **CLI-4 (AIç­›é€‰)**: æä¾›GPUæ€§èƒ½æ•°æ®ç”¨äºä¼˜åŒ–å»ºè®®
- **CLI-6 (è´¨é‡ä¿è¯)**: éœ€è¦GPUç›‘æ§APIçš„æµ‹è¯•ç”¨ä¾‹

---

## ğŸ“ äº¤ä»˜æ¸…å•

### ä»£ç äº¤ä»˜
- [ ] `src/gpu_monitoring/` - åç«¯GPUç›‘æ§æ¨¡å—
  - `gpu_monitor_service.py` - GPUç¡¬ä»¶ç›‘æ§
  - `performance_collector.py` - æ€§èƒ½æŒ‡æ ‡é‡‡é›†
  - `history_service.py` - å†å²æ•°æ®æœåŠ¡
  - `optimization_advisor.py` - ä¼˜åŒ–å»ºè®®å¼•æ“
- [ ] `web/frontend/src/views/GPUMonitoring/` - å‰ç«¯é¡µé¢
  - `GPUStatusCard.vue` - GPUçŠ¶æ€å¡ç‰‡
  - `PerformanceChart.vue` - æ€§èƒ½å›¾è¡¨
  - `OptimizationPanel.vue` - ä¼˜åŒ–å»ºè®®é¢æ¿
  - `AlertCenter.vue` - å‘Šè­¦ä¸­å¿ƒ

---

## ğŸ¯ æˆåŠŸæ ‡å‡†

### åŠŸèƒ½å®Œæ•´æ€§
- [x] å®æ—¶é‡‡é›†GPUç¡¬ä»¶æŒ‡æ ‡ (æ¯2ç§’åˆ·æ–°)
- [x] æ€§èƒ½æŒ‡æ ‡å‡†ç¡® (GFLOPS/åŠ é€Ÿæ¯”/ååé‡)
- [x] å†å²æ•°æ®æŒä¹…åŒ–å’ŒæŸ¥è¯¢
- [x] æ™ºèƒ½ä¼˜åŒ–å»ºè®®ç”Ÿæˆ
- [x] å¼‚å¸¸å‘Šè­¦åŠæ—¶è§¦å‘

### æ€§èƒ½æŒ‡æ ‡
- [x] æŒ‡æ ‡é‡‡é›†å»¶è¿Ÿ < 100ms
- [x] SSEæ¨é€å»¶è¿Ÿ < 2ç§’
- [x] å†å²æŸ¥è¯¢é€Ÿåº¦ < 500ms (24å°æ—¶æ•°æ®)
- [x] å‰ç«¯å›¾è¡¨æ¸²æŸ“ < 1ç§’

### è´¨é‡æ ‡å‡†
- [x] æµ‹è¯•è¦†ç›–ç‡ > 80%
- [x] ä»£ç Reviewé€šè¿‡
- [x] æ–‡æ¡£å®Œæ•´æ— é—æ¼

---

## å·¥ä½œæµç¨‹ä¸Gitæäº¤è§„èŒƒ

### ğŸ“š å®Œæ•´å·¥ä½œæµç¨‹æŒ‡å—

è¯¦ç»†çš„Worker CLIå·¥ä½œæµç¨‹è¯·å‚è€ƒ:
ğŸ“– **[CLIå·¥ä½œæµç¨‹æŒ‡å—](../../mystocks_spec/docs/guides/multi-cli-tasks/CLI_WORKFLOW_GUIDE.md)**

### âš¡ å¿«é€Ÿå‚è€ƒ

#### æ¯æ—¥å·¥ä½œæµç¨‹

```bash
# 1. æ‹‰å–æœ€æ–°ä»£ç 
cd /opt/claude/mystocks_phase6_monitoring
git pull

# 2. æŸ¥çœ‹ä»Šæ—¥ä»»åŠ¡
vim README.md

# 3. å¼€å‘å®ç°
vim src/gpu_monitoring/gpu_monitor_service.py

# 4. æµ‹è¯•ä»£ç 
pytest tests/test_gpu_monitoring.py -xvs

# 5. ä»£ç è´¨é‡æ£€æŸ¥
ruff check . --fix
black .
pylint src/

# 6. Gitæäº¤
git add .
git commit -m "feat(monitoring): add GPU metrics collection service

- Implement GPUMonitoringService with pynvml wrapper
- Add real-time GPU utilization tracking
- Include temperature and power monitoring

Task: T5.1
Acceptance: [x] GPU metrics [x] Temperature [x] Power usage"

# 7. æ›´æ–°READMEè¿›åº¦
vim README.md
git add README.md
git commit -m "docs(readme): update progress to T+24h"

# 8. æ¨é€åˆ°è¿œç¨‹
git push
```

#### Gitæäº¤æ¶ˆæ¯è§„èŒƒ

```bash
# æ ¼å¼: <type>(<scope>): <subject>

# ç¤ºä¾‹:
git commit -m "feat(advisor): implement optimization recommendation engine

- Analyze GPU utilization patterns
- Generate actionable optimization suggestions
- Include cost-benefit analysis

Task: T5.4
Acceptance: [x] Analysis [x] Recommendations [x] Cost estimation"
```

#### å®Œæˆæ ‡å‡†æ£€æŸ¥æ¸…å•

- [ ] æ‰€æœ‰éªŒæ”¶æ ‡å‡†é€šè¿‡
- [ ] ä»£ç å·²æäº¤åˆ°Gitï¼ˆé¢‘ç¹æäº¤ï¼‰
- [ ] æµ‹è¯•è¦†ç›–ç‡>80%
- [ ] ä»£ç è´¨é‡æ£€æŸ¥é€šè¿‡ï¼ˆPylint>8.0ï¼‰
- [ ] READMEå·²æ›´æ–°ï¼ˆè¿›åº¦+ä»»åŠ¡çŠ¶æ€ï¼‰

#### è¿›åº¦æ›´æ–°æ ¼å¼

```markdown
## è¿›åº¦æ›´æ–°

### T+0h (2025-12-29 15:00)
- âœ… ä»»åŠ¡å¯åŠ¨
- ğŸ“ å½“å‰ä»»åŠ¡: T5.1 GPUç›‘æ§æœåŠ¡å®ç°
- â³ é¢„è®¡å®Œæˆ: 2025-12-30

### T+24h (2025-12-30 15:00)
- âœ… T5.1 GPUç›‘æ§æœåŠ¡å®Œæˆ
  - Gitæäº¤: abc1234
  - éªŒæ”¶æ ‡å‡†: [x] å…¨éƒ¨é€šè¿‡
- ğŸ“ å½“å‰ä»»åŠ¡: T5.2 æ€§èƒ½æ•°æ®æ”¶é›†å™¨
- ğŸš§ é˜»å¡é—®é¢˜: æ— 
```

### ğŸ¯ å…³é”®æ³¨æ„äº‹é¡¹

1. **GPUèµ„æºç›‘æ§**: åˆ©ç”¨ç°æœ‰68.58x GPUåŠ é€ŸåŸºç¡€è®¾æ–½
2. **é¢‘ç¹æäº¤**: æ¯å®Œæˆä¸€ä¸ªæœåŠ¡æ¨¡å—å°±æäº¤
3. **æ€§èƒ½ä¼˜åŒ–**: ç¡®ä¿ç›‘æ§å¼€é”€<5% GPUèµ„æº
4. **åŠæ—¶æ›´æ–°README**: æ¯å¤©è‡³å°‘æ›´æ–°ä¸€æ¬¡è¿›åº¦

### ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ

- ğŸ“– [å®Œæ•´å·¥ä½œæµç¨‹](../../mystocks_spec/docs/guides/multi-cli-tasks/CLI_WORKFLOW_GUIDE.md)
- ğŸ“š [GPUå¼€å‘ç»éªŒ](../../mystocks_spec/docs/api/GPUå¼€å‘ç»éªŒæ€»ç»“.md)
- ğŸš§ é‡åˆ°é˜»å¡: åœ¨READMEä¸­è®°å½•

---

**æœ€åæ›´æ–°**: 2025-12-29
**è´£ä»»äºº**: CLI-5 Worker (Phase 6 GPU Monitoring)
**é¢„è®¡å®Œæˆ**: 2025-01-08 (8-10å·¥ä½œæ—¥)
