# MyStocks API å…¨é¢æ¶æ„è¯„å®¡æŠ¥å‘Š

**è¯„å®¡æ—¥æœŸ**: 2025-12-04
**è¯„å®¡äºº**: Backend Architect (Claude Code)
**é¡¹ç›®çŠ¶æ€**: Phase 4 å®Œæˆï¼Œå‡†å¤‡Realæ•°æ®å¯¹æ¥
**æ–‡æ¡£ç‰ˆæœ¬**: 1.0

---

## ğŸ“Š æ€»ä½“è¯„åˆ†

| ç»´åº¦ | è¯„åˆ† (0-100) | ç­‰çº§ | è¯´æ˜ |
|------|-------------|------|------|
| **æ•´ä½“æ¶æ„è´¨é‡** | **88/100** | **ä¼˜ç§€** | ä¼ä¸šçº§æ¶æ„è®¾è®¡ï¼Œç¬¦åˆä¸»æµæœ€ä½³å®è·µ |
| **å®‰å…¨æ€§** | **92/100** | **å“è¶Š** | ä¼ä¸šçº§å®‰å…¨æ ‡å‡†ï¼Œ13ä¸ªä¸¥é‡æ¼æ´å·²ä¿®å¤ |
| **å¯æ‰©å±•æ€§** | **85/100** | **ä¼˜ç§€** | è‰¯å¥½çš„æ¨¡å—åŒ–å’Œåˆ†å±‚è®¾è®¡ |
| **å¯ç»´æŠ¤æ€§** | **78/100** | **è‰¯å¥½** | å­˜åœ¨æŠ€æœ¯å€ºåŠ¡ï¼Œä½†æ¶æ„æ¸…æ™° |
| **æ€§èƒ½ä¼˜åŒ–** | **82/100** | **è‰¯å¥½** | Phase 3ä¼˜åŒ–å®Œæˆï¼Œæœ‰è¿›ä¸€æ­¥æå‡ç©ºé—´ |
| **æµ‹è¯•è¦†ç›–** | **42/100** | **å¾…æ”¹è¿›** | 49ä¸ªæµ‹è¯•æ–‡ä»¶ï¼Œè¦†ç›–ç‡çº¦6%ï¼Œéœ€å¤§å¹…æå‡ |
| **æ–‡æ¡£å®Œæ•´æ€§** | **90/100** | **å“è¶Š** | è¯¦å°½çš„APIæ–‡æ¡£å’Œæ¶æ„æ–‡æ¡£ |
| **Realæ•°æ®å¯¹æ¥å‡†å¤‡åº¦** | **75/100** | **åŸºæœ¬å°±ç»ª** | æ¶æ„æ”¯æŒï¼Œéœ€æ•°æ®æµæ”¹é€  |

**ç»¼åˆè¯„åˆ†**: **79/100** (è‰¯å¥½ï¼Œæ¥è¿‘ä¼˜ç§€)

---

## ğŸ¯ å…³é”®å‘ç° (Top 5)

### âœ… **ä¼˜åŠ¿**

1. **ä¼ä¸šçº§å®‰å…¨æ¶æ„ (92åˆ†)**
   - Phase 4å®Œæˆåï¼ŒAPIåˆè§„æ€§ä»62%æå‡è‡³97%
   - 13ä¸ªä¸¥é‡å®‰å…¨æ¼æ´å·²ä¿®å¤ï¼ˆbackup_recovery.pyç­‰ï¼‰
   - å¤šå±‚è®¤è¯æˆæƒä½“ç³»å®Œå–„ï¼ˆJWT + RBAC + ç»†ç²’åº¦æƒé™ï¼‰
   - å®Œæ•´çš„å®‰å…¨é˜²æŠ¤æœºåˆ¶ï¼ˆSQLæ³¨å…¥ã€XSSã€CSRFã€å‘½ä»¤æ³¨å…¥ï¼‰

2. **æ¸…æ™°çš„åˆ†å±‚æ¶æ„ (88åˆ†)**
   - FastAPIåº”ç”¨å±‚ â†’ æœåŠ¡å±‚ â†’ æ•°æ®è®¿é—®å±‚ â†’ å­˜å‚¨å±‚
   - 45ä¸ªAPIæ–‡ä»¶ï¼Œ41ä¸ªæœåŠ¡ç±»ï¼Œå®Œæ•´çš„å…³æ³¨ç‚¹åˆ†ç¦»
   - Gatewayå±‚å®ç°ï¼ˆç†”æ–­å™¨ã€é™æµå™¨ã€è¯·æ±‚è·¯ç”±ï¼‰
   - ç»Ÿä¸€å“åº”æ ¼å¼å’Œé”™è¯¯å¤„ç†

3. **ä¼˜ç§€çš„æ–‡æ¡£ä½“ç³» (90åˆ†)**
   - å®Œæ•´çš„OpenAPI 3.1.0è§„èŒƒï¼ˆJSON/YAMLï¼‰
   - Phase 4å®ŒæˆæŠ¥å‘Šã€åˆè§„æ€§æµ‹è¯•æ¡†æ¶ï¼ˆ1,200+ LOCï¼‰
   - APIå¼€å‘æŒ‡å—ã€æ£€æŸ¥æ¸…å•ã€å¿«é€Ÿæ¨¡æ¿
   - 280+ APIç«¯ç‚¹è¯¦ç»†æ–‡æ¡£

4. **æ€§èƒ½ä¼˜åŒ–æ¶æ„ (82åˆ†)**
   - Phase 3æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–ï¼ˆ20-100è¿æ¥ï¼Œ95%å¤ç”¨ç‡ï¼‰
   - WebSocketæ€§èƒ½ä¼˜åŒ–ï¼ˆ2å€å¹¶å‘ï¼Œ50%å†…å­˜å ç”¨ï¼‰
   - æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿï¼ˆfetch_with_cacheï¼ŒTTLç®¡ç†ï¼‰
   - Locustå‹æµ‹æ¡†æ¶ï¼ˆ4ç§åœºæ™¯ï¼Œ5ç§ç”¨æˆ·è§’è‰²ï¼‰

5. **åŒæ•°æ®åº“æ¶æ„ (85åˆ†)**
   - TDengineï¼šé«˜é¢‘æ—¶åºæ•°æ®ï¼ˆtick/åˆ†é’ŸKçº¿ï¼‰
   - PostgreSQL + TimescaleDBï¼šæ‰€æœ‰å…¶ä»–æ•°æ®ç±»å‹
   - ç»Ÿä¸€æ•°æ®è®¿é—®å±‚ï¼ˆMyStocksUnifiedManagerï¼‰
   - Week 3ç®€åŒ–åï¼Œæ¶æ„å¤æ‚åº¦é™ä½50%

### âš ï¸ **éœ€è¦æ”¹è¿›çš„é¢†åŸŸ**

1. **æµ‹è¯•è¦†ç›–ç‡ä¸è¶³ (42åˆ†)**
   - å½“å‰è¦†ç›–ç‡çº¦6%ï¼Œç›®æ ‡80%
   - 49ä¸ªæµ‹è¯•æ–‡ä»¶ï¼Œä½†éƒ¨åˆ†å¤±è´¥
   - ç¼ºä¹ç³»ç»Ÿçš„é›†æˆæµ‹è¯•å’ŒE2Eæµ‹è¯•
   - Repositoryæ¨¡å¼ä½¿ç”¨ä¸è¶³ï¼ˆä»…2ä¸ªæ–‡ä»¶ï¼‰

2. **æŠ€æœ¯å€ºåŠ¡ç§¯ç´¯ (å¾…æ”¹è¿›)**
   - Pylintåˆ†æï¼š215ä¸ªé”™è¯¯ï¼Œ2,606ä¸ªè­¦å‘Š
   - ä»£ç è´¨é‡é—®é¢˜ï¼ˆé‡æ„éœ€æ±‚ï¼š571ï¼Œè§„èŒƒé—®é¢˜ï¼š1,858ï¼‰
   - éƒ¨åˆ†å¤§æ–‡ä»¶éœ€è¦æ¨¡å—åŒ–æ‹†åˆ†
   - Mockæ•°æ®å’ŒRealæ•°æ®æ··åˆï¼Œéœ€æ¸…æ™°åˆ†ç¦»

3. **Realæ•°æ®å¯¹æ¥å‡†å¤‡ (75åˆ†)**
   - å½“å‰å¤§é‡ä½¿ç”¨Mockæ•°æ®
   - æ•°æ®æµæ”¹é€ éœ€æ±‚æ˜ç¡®ä½†æœªå®Œå…¨å®æ–½
   - å¤–éƒ¨æ•°æ®æºé€‚é…å™¨ï¼ˆ7ä¸ªï¼‰å·²å°±ç»ªï¼Œä½†é›†æˆæµ‹è¯•ä¸è¶³
   - éœ€è¦å®Œå–„çš„æ•°æ®éªŒè¯å’Œé”™è¯¯å¤„ç†

4. **ç›‘æ§å’Œå¯è§‚æµ‹æ€§ (å¾…å®Œå–„)**
   - ç›‘æ§æ•°æ®åº“å·²å»ºç«‹ï¼Œä½†æŒ‡æ ‡ä¸å¤Ÿå…¨é¢
   - ç¼ºå°‘APMé›†æˆï¼ˆDataDog/New Relicï¼‰
   - åˆ†å¸ƒå¼è¿½è¸ªæœªå®Œå…¨å®ç°ï¼ˆOpenTelemetryï¼‰
   - å‘Šè­¦ç³»ç»Ÿå·²æœ‰åŸºç¡€ï¼Œä½†è§„åˆ™ä¸å¤Ÿå®Œå–„

5. **éƒ¨ç½²è¿ç»´å‡†å¤‡ (å¾…åŠ å¼º)**
   - Dockeré…ç½®å­˜åœ¨ï¼Œä½†CI/CDæµæ°´çº¿ä¸å®Œæ•´
   - ç¯å¢ƒé…ç½®ç®¡ç†éœ€æ”¹è¿›ï¼ˆ.envä¾èµ–ï¼‰
   - ç¼ºå°‘è“ç»¿éƒ¨ç½²/é‡‘ä¸é›€å‘å¸ƒæœºåˆ¶
   - ç”Ÿäº§ç¯å¢ƒç›‘æ§å’Œæ—¥å¿—èšåˆå¾…åŠ å¼º

---

## ğŸ“ è¯¦ç»†æ¶æ„åˆ†æ

### 1. **APIè®¾è®¡ä¸RESTåŸåˆ™ (85/100)**

#### âœ… **ç¬¦åˆRESTæœ€ä½³å®è·µ**

**èµ„æºå»ºæ¨¡**:
```
/api/market/market-data/fetch       # å¸‚åœºæ•°æ®
/api/data/stocks/{symbol}           # è‚¡ç¥¨ä¿¡æ¯
/api/watchlist/{id}                 # è‡ªé€‰è‚¡ç®¡ç†
/api/strategy/{strategy_id}         # ç­–ç•¥ç®¡ç†
/api/backtest/execute               # å›æµ‹æ‰§è¡Œ
```

**HTTPæ–¹æ³•ä½¿ç”¨**:
- GET: æŸ¥è¯¢æ“ä½œï¼ˆå¹‚ç­‰ï¼‰
- POST: åˆ›å»ºå’Œæ‰§è¡Œæ“ä½œ
- PUT/PATCH: æ›´æ–°æ“ä½œ
- DELETE: åˆ é™¤æ“ä½œ

**çŠ¶æ€ç ä½¿ç”¨**:
```python
# æ ‡å‡†çŠ¶æ€ç æ˜ å°„
200 OK              # æˆåŠŸ
201 Created         # åˆ›å»ºæˆåŠŸ
400 Bad Request     # å®¢æˆ·ç«¯é”™è¯¯
401 Unauthorized    # æœªè®¤è¯
403 Forbidden       # æ— æƒé™
404 Not Found       # èµ„æºä¸å­˜åœ¨
500 Internal Error  # æœåŠ¡å™¨é”™è¯¯
```

**ç»Ÿä¸€å“åº”æ ¼å¼**:
```python
{
    "success": true,
    "data": {...},
    "message": "æ“ä½œæˆåŠŸ",
    "request_id": "uuid",
    "timestamp": "2025-12-04T10:30:00Z"
}
```

#### âš ï¸ **éœ€è¦æ”¹è¿›çš„åœ°æ–¹**

1. **APIç‰ˆæœ¬æ§åˆ¶ä¸ä¸€è‡´**
   - éƒ¨åˆ†ä½¿ç”¨ `/api/v1/`ï¼Œéƒ¨åˆ†ç›´æ¥ `/api/`
   - å»ºè®®ï¼šç»Ÿä¸€ä½¿ç”¨ `/api/v1/` å‰ç¼€

2. **åˆ†é¡µç­–ç•¥ä¸ç»Ÿä¸€**
   - éƒ¨åˆ†ç«¯ç‚¹ä½¿ç”¨offsetåˆ†é¡µ
   - éƒ¨åˆ†ç«¯ç‚¹ä½¿ç”¨æ¸¸æ ‡åˆ†é¡µ
   - å»ºè®®ï¼šæ ‡å‡†åŒ–ä¸ºcursor-basedåˆ†é¡µ

3. **æ‰¹é‡æ“ä½œæ”¯æŒä¸è¶³**
   - ç¼ºå°‘æ‰¹é‡æŸ¥è¯¢ã€æ‰¹é‡æ›´æ–°ç«¯ç‚¹
   - å»ºè®®ï¼šå®ç° `/api/v1/stocks/batch` ç­‰æ‰¹é‡ç«¯ç‚¹

4. **HATEOASæ”¯æŒç¼ºå¤±**
   - å“åº”ä¸­ç¼ºå°‘ç›¸å…³èµ„æºé“¾æ¥
   - å»ºè®®ï¼šæ·»åŠ  `_links` å­—æ®µæä¾›å¯å‘ç°æ€§

#### ğŸ¯ **æ”¹è¿›å»ºè®®**

```python
# å»ºè®®çš„ç»Ÿä¸€APIè®¾è®¡è§„èŒƒ
{
    "apiVersion": "v1",
    "success": true,
    "data": {
        "items": [...],
        "pagination": {
            "cursor": "next_cursor_token",
            "has_more": true,
            "total": 1000
        }
    },
    "_links": {
        "self": "/api/v1/stocks?cursor=abc",
        "next": "/api/v1/stocks?cursor=xyz",
        "related": {
            "indicators": "/api/v1/indicators?symbol=600000"
        }
    },
    "request_id": "uuid",
    "timestamp": "ISO8601"
}
```

---

### 2. **æœåŠ¡åˆ†å±‚æ¶æ„ (88/100)**

#### âœ… **æ¸…æ™°çš„å››å±‚æ¶æ„**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API Layer (FastAPI)                  â”‚
â”‚  - 45ä¸ªAPIæ–‡ä»¶ (auth, market, data, strategyç­‰)        â”‚
â”‚  - 280+ RESTç«¯ç‚¹                                        â”‚
â”‚  - è¯·æ±‚éªŒè¯ã€å“åº”æ ¼å¼åŒ–                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Service Layer (ä¸šåŠ¡é€»è¾‘)                â”‚
â”‚  - 41ä¸ªServiceç±» (MarketDataService, StrategyServiceç­‰) â”‚
â”‚  - ä¸šåŠ¡è§„åˆ™ã€æ•°æ®è½¬æ¢ã€ç¼“å­˜ç®¡ç†                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Data Access Layer (æ•°æ®è®¿é—®)                â”‚
â”‚  - TDengineDataAccess (é«˜é¢‘æ—¶åº)                        â”‚
â”‚  - PostgreSQLDataAccess (å…¶ä»–æ•°æ®)                      â”‚
â”‚  - MyStocksUnifiedManager (ç»Ÿä¸€ç®¡ç†å™¨)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Storage Layer (å­˜å‚¨)                    â”‚
â”‚  - TDengine (tick/åˆ†é’Ÿæ•°æ®)                             â”‚
â”‚  - PostgreSQL + TimescaleDB (æ—¥çº¿/å…ƒæ•°æ®)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### âœ… **Gatewayå±‚å®ç°å®Œæ•´**

**ç†”æ–­å™¨æ¨¡å¼**:
```python
class CircuitBreaker:
    """
    çŠ¶æ€æœº: CLOSED â†’ OPEN â†’ HALF_OPEN â†’ CLOSED
    - å¤±è´¥é˜ˆå€¼: 5æ¬¡
    - æˆåŠŸé˜ˆå€¼: 2æ¬¡ (åŠå¼€çŠ¶æ€)
    - è¶…æ—¶æ—¶é—´: 60ç§’
    """
```

**é™æµå™¨å®ç°**:
```python
class RateLimiter:
    """
    Token Bucketç®—æ³•
    - å®¹é‡: 100 tokens
    - å¡«å……é€Ÿç‡: 10 tokens/ç§’
    - æ—¶é—´çª—å£: 60ç§’
    """
```

**è¯·æ±‚è·¯ç”±**:
- æ™ºèƒ½è·¯ç”±åˆ°ä¸åŒåç«¯æœåŠ¡
- è´Ÿè½½å‡è¡¡æ”¯æŒ
- è¯·æ±‚è½¬æ¢å’Œå¤´éƒ¨æ³¨å…¥

#### âš ï¸ **éœ€è¦æ”¹è¿›çš„åœ°æ–¹**

1. **Repositoryæ¨¡å¼ä½¿ç”¨ä¸è¶³**
   - ä»…2ä¸ªRepositoryæ–‡ä»¶ï¼ˆstrategy, backtestï¼‰
   - å»ºè®®ï¼šæ‰€æœ‰æ•°æ®è®¿é—®éƒ½åº”é€šè¿‡Repository

2. **ä¾èµ–æ³¨å…¥ä¸å¤Ÿç»Ÿä¸€**
   - éƒ¨åˆ†Serviceç›´æ¥å®ä¾‹åŒ–ä¾èµ–
   - å»ºè®®ï¼šä½¿ç”¨FastAPIçš„Dependsè¿›è¡Œä¾èµ–æ³¨å…¥

3. **äº‹åŠ¡ç®¡ç†ç¼ºå¤±**
   - è·¨æœåŠ¡äº‹åŠ¡å¤„ç†ä¸æ˜ç¡®
   - å»ºè®®ï¼šå®ç°Sagaæ¨¡å¼æˆ–ä¸¤é˜¶æ®µæäº¤

#### ğŸ¯ **æ”¹è¿›å»ºè®®**

```python
# å»ºè®®çš„Repositoryæ¨¡å¼
class StockRepository:
    def __init__(self, db: Session = Depends(get_db)):
        self.db = db

    async def find_by_symbol(self, symbol: str) -> Optional[Stock]:
        """é€šè¿‡ä»£ç æŸ¥æ‰¾è‚¡ç¥¨"""
        return self.db.query(Stock).filter(
            Stock.symbol == symbol
        ).first()

    async def save(self, stock: Stock) -> Stock:
        """ä¿å­˜è‚¡ç¥¨ä¿¡æ¯"""
        self.db.add(stock)
        await self.db.commit()
        return stock

# Serviceå±‚ä½¿ç”¨Repository
class StockService:
    def __init__(
        self,
        stock_repo: StockRepository = Depends(),
        cache: CacheService = Depends()
    ):
        self.stock_repo = stock_repo
        self.cache = cache

    async def get_stock_info(self, symbol: str) -> Dict:
        """è·å–è‚¡ç¥¨ä¿¡æ¯ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        # å…ˆæŸ¥ç¼“å­˜
        cached = await self.cache.get(f"stock:{symbol}")
        if cached:
            return cached

        # æŸ¥æ•°æ®åº“
        stock = await self.stock_repo.find_by_symbol(symbol)
        if not stock:
            raise NotFoundError(f"è‚¡ç¥¨ {symbol} ä¸å­˜åœ¨")

        # æ›´æ–°ç¼“å­˜
        await self.cache.set(f"stock:{symbol}", stock, ttl=3600)
        return stock
```

---

### 3. **è®¤è¯ä¸æˆæƒæ¶æ„ (92/100)**

#### âœ… **å¤šå±‚å®‰å…¨é˜²æŠ¤ä½“ç³»**

**è®¤è¯æœºåˆ¶**:
```python
# JWTä»¤ç‰Œè®¤è¯
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="/api/auth/login")

def create_access_token(data: Dict[str, Any]) -> str:
    """
    åˆ›å»ºJWTä»¤ç‰Œ
    - ç®—æ³•: HS256
    - è¿‡æœŸæ—¶é—´: 30åˆ†é’Ÿ (å¯é…ç½®)
    - Payload: username, user_id, role
    """
```

**æˆæƒä½“ç³»**:
```python
# 3çº§è§’è‰²æƒé™
class AccessLevel(Enum):
    PUBLIC = "public"      # å…¬å¼€è®¿é—®
    USER = "user"          # éœ€è¦è®¤è¯
    ADMIN = "admin"        # ç®¡ç†å‘˜æƒé™

# 4çº§å®‰å…¨ç­‰çº§
class SecurityLevel(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"
```

**ç»†ç²’åº¦æƒé™æ§åˆ¶**:
```python
# Phase 4å®ç°çš„å¤šå±‚æƒé™
@router.get("/backup/list")
async def list_backups(
    current_user: User = Depends(get_current_user),
    min_role: str = Security(require_min_role, scopes=["admin"])
):
    """åªæœ‰ç®¡ç†å‘˜å¯ä»¥æŸ¥çœ‹å¤‡ä»½åˆ—è¡¨"""
```

#### âœ… **è¾“å…¥éªŒè¯å’Œé˜²æŠ¤**

**SQLæ³¨å…¥é˜²æŠ¤**:
```python
# ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢
query = text("""
    SELECT * FROM stocks_basic
    WHERE symbol = :symbol
""")
result = session.execute(query, {"symbol": symbol})
```

**XSSé˜²æŠ¤**:
```python
# Pydanticæ¨¡å‹éªŒè¯
class BackupCreateRequest(BaseModel):
    backup_name: str = Field(
        ...,
        min_length=1,
        max_length=100,
        regex=r'^[a-zA-Z0-9_-]+$'  # ä»…å…è®¸å®‰å…¨å­—ç¬¦
    )

    @validator('backup_name')
    def validate_backup_name(cls, v):
        # é˜²æ­¢è·¯å¾„éå†
        if ".." in v or "/" in v or "\\" in v:
            raise ValueError('å¤‡ä»½åç§°ä¸èƒ½åŒ…å«è·¯å¾„å­—ç¬¦')
        return v
```

**å‘½ä»¤æ³¨å…¥é˜²æŠ¤**:
```python
class CommandValidator:
    DANGEROUS_PATTERNS = [
        r'[;&|`$(){}[\]\\]',  # å‘½ä»¤åˆ†éš”ç¬¦
        r'\.\./',             # è·¯å¾„éå†
        r'rm\s+',             # åˆ é™¤å‘½ä»¤
        r'sudo\s+',           # æƒé™æå‡
    ]

    @classmethod
    def is_safe(cls, command: str) -> bool:
        """æ£€æŸ¥å‘½ä»¤æ˜¯å¦å®‰å…¨"""
        for pattern in cls.DANGEROUS_PATTERNS:
            if re.search(pattern, command):
                return False
        return True
```

**é€Ÿç‡é™åˆ¶**:
```python
@router.post("/backup/create")
@limiter.limit("10 per minute")  # æ¯åˆ†é’Ÿæœ€å¤š10æ¬¡
async def create_backup(request: Request, ...):
    """é˜²æ­¢DDoSæ”»å‡»"""
```

#### âš ï¸ **éœ€è¦æ”¹è¿›çš„åœ°æ–¹**

1. **CSRFä¿æŠ¤å·²ç¦ç”¨**
   - main.pyä¸­CSRFä¸­é—´ä»¶è¢«æ³¨é‡Šæ‰
   - å»ºè®®ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒå¯ç”¨CSRFä¿æŠ¤

2. **å¯†ç ç­–ç•¥ä¸å¤Ÿå¼º**
   - ç¼ºå°‘å¯†ç å¤æ‚åº¦è¦æ±‚
   - å»ºè®®ï¼šå¼ºåˆ¶8ä½ä»¥ä¸Šï¼ŒåŒ…å«å¤§å°å†™å­—æ¯ã€æ•°å­—ã€ç‰¹æ®Šå­—ç¬¦

3. **ä¼šè¯ç®¡ç†å¾…å®Œå–„**
   - JWTæ— æ³•ä¸»åŠ¨æ’¤é”€
   - å»ºè®®ï¼šå®ç°Tokené»‘åå•æˆ–ä½¿ç”¨çŸ­æœŸToken + Refresh Token

4. **å®¡è®¡æ—¥å¿—ä¸å¤Ÿå…¨é¢**
   - éƒ¨åˆ†å…³é”®æ“ä½œæœªè®°å½•
   - å»ºè®®ï¼šæ‰€æœ‰ä¿®æ”¹æ“ä½œéƒ½è®°å½•å®¡è®¡æ—¥å¿—

#### ğŸ¯ **æ”¹è¿›å»ºè®®**

```python
# å»ºè®®çš„å®Œæ•´å®‰å…¨æ¶æ„
class SecurityService:
    """ç»Ÿä¸€å®‰å…¨æœåŠ¡"""

    async def authenticate(
        self,
        username: str,
        password: str
    ) -> Optional[User]:
        """è®¤è¯ç”¨æˆ·"""
        # 1. é€Ÿç‡é™åˆ¶æ£€æŸ¥
        if not await self.check_rate_limit(username):
            raise TooManyAttemptsError()

        # 2. æŸ¥æ‰¾ç”¨æˆ·
        user = await self.user_repo.find_by_username(username)
        if not user:
            await self.log_failed_login(username, "user_not_found")
            return None

        # 3. éªŒè¯å¯†ç 
        if not verify_password(password, user.hashed_password):
            await self.log_failed_login(username, "wrong_password")
            await self.increment_failed_attempts(username)
            return None

        # 4. æ£€æŸ¥è´¦æˆ·çŠ¶æ€
        if not user.is_active:
            raise AccountDisabledError()

        # 5. è®°å½•æˆåŠŸç™»å½•
        await self.log_successful_login(user)

        return user

    async def authorize(
        self,
        user: User,
        resource: str,
        action: str
    ) -> bool:
        """æˆæƒæ£€æŸ¥"""
        # 1. æ£€æŸ¥è§’è‰²æƒé™
        if not self.has_role_permission(user.role, resource, action):
            return False

        # 2. æ£€æŸ¥èµ„æºæ‰€æœ‰æƒ
        if not await self.is_resource_owner(user, resource):
            return False

        # 3. è®°å½•æˆæƒå†³ç­–
        await self.log_authorization(user, resource, action, True)

        return True

    async def create_token_pair(self, user: User) -> Dict[str, str]:
        """åˆ›å»ºTokenå¯¹ï¼ˆAccess + Refreshï¼‰"""
        access_token = create_access_token(
            data={"sub": user.username, "user_id": user.id, "role": user.role},
            expires_delta=timedelta(minutes=15)  # çŸ­æœŸAccess Token
        )

        refresh_token = create_refresh_token(
            data={"sub": user.username, "user_id": user.id},
            expires_delta=timedelta(days=7)  # é•¿æœŸRefresh Token
        )

        # å­˜å‚¨Refresh Tokenåˆ°æ•°æ®åº“
        await self.token_repo.save_refresh_token(
            user_id=user.id,
            token=refresh_token,
            expires_at=datetime.utcnow() + timedelta(days=7)
        )

        return {
            "access_token": access_token,
            "refresh_token": refresh_token,
            "token_type": "Bearer",
            "expires_in": 900  # 15åˆ†é’Ÿ
        }
```

---

### 4. **æ•°æ®åº“æ¶æ„ (85/100)**

#### âœ… **åŒæ•°æ®åº“ç­–ç•¥ä¼˜åŠ¿**

**TDengine (æ—¶åºæ•°æ®åº“)**:
```yaml
ä¼˜åŠ¿:
  - 20:1æè‡´å‹ç¼©æ¯”
  - è¶…é«˜å†™å…¥æ€§èƒ½ (1000ä¸‡ç‚¹/ç§’)
  - è‡ªåŠ¨æ•°æ®ä¿ç•™ç­–ç•¥
  - æ—¶é—´èŒƒå›´æŸ¥è¯¢ä¼˜åŒ–

ä½¿ç”¨åœºæ™¯:
  - Tickæ•°æ® (é€ç¬”æˆäº¤)
  - åˆ†é’Ÿçº§Kçº¿æ•°æ®
  - å®æ—¶è¡Œæƒ…æ¨é€

è¡¨ç»“æ„:
  - è¶…çº§è¡¨: tick_data, minute_data
  - å­è¡¨: æŒ‰è‚¡ç¥¨ä»£ç è‡ªåŠ¨åˆ†è¡¨
```

**PostgreSQL + TimescaleDB**:
```yaml
ä¼˜åŠ¿:
  - ACIDäº‹åŠ¡ä¿è¯
  - å¤æ‚JOINæŸ¥è¯¢æ”¯æŒ
  - å…¨æ–‡æœç´¢å’Œé«˜çº§ç´¢å¼•
  - TimescaleDBæ··åˆè¡¨ä¼˜åŒ–æ—¥çº¿æ•°æ®

ä½¿ç”¨åœºæ™¯:
  - æ—¥çº¿Kçº¿æ•°æ® (TimescaleDB hypertable)
  - è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯ (symbols_info)
  - ç­–ç•¥å’Œå›æµ‹ç»“æœ
  - ç”¨æˆ·æ•°æ®å’Œæƒé™
  - å…ƒæ•°æ®å’Œé…ç½®

è¡¨æ•°é‡: 18ä¸ªè¡¨
æ•°æ®é‡: ~299è¡Œ (åŸºç¡€æ•°æ®)
```

#### âœ… **è¿æ¥æ± ä¼˜åŒ–å®Œæˆ**

```python
# Phase 3ä¼˜åŒ–åçš„é…ç½®
engines["postgresql"] = create_engine(
    connection_string,
    pool_size=20,          # æ ¸å¿ƒè¿æ¥: 10 â†’ 20
    max_overflow=40,       # æœ€å¤§æº¢å‡º: 20 â†’ 40
    pool_timeout=30,       # æ–°å¢: è·å–è¶…æ—¶30ç§’
    pool_pre_ping=True,    # è¿æ¥å¥åº·æ£€æŸ¥
    pool_recycle=3600,     # è¿æ¥å›æ”¶1å°æ—¶
    echo_pool=False        # ç”Ÿäº§ç¯å¢ƒå…³é—­æ—¥å¿—
)

# æ€§èƒ½æå‡
- è¿æ¥å¤ç”¨ç‡: 95%
- å¹¶å‘å¤„ç†èƒ½åŠ›: +3900%
- æŸ¥è¯¢å»¶è¿Ÿ: -50%
```

#### âœ… **ç»Ÿä¸€æ•°æ®è®¿é—®å±‚**

```python
class MyStocksUnifiedManager:
    """ç»Ÿä¸€æ•°æ®è®¿é—®å…¥å£"""

    def __init__(self):
        self.tdengine_access = TDengineDataAccess()
        self.postgresql_access = PostgreSQLDataAccess()
        self.monitoring_db = MonitoringDatabase()

    async def save_data_by_classification(
        self,
        data: pd.DataFrame,
        classification: DataClassification
    ):
        """æ ¹æ®æ•°æ®åˆ†ç±»è‡ªåŠ¨è·¯ç”±åˆ°å¯¹åº”æ•°æ®åº“"""
        if classification in [
            DataClassification.HIGH_FREQUENCY_MARKET,
            DataClassification.TICK_DATA
        ]:
            return await self.tdengine_access.save(data)
        else:
            return await self.postgresql_access.save(data)
```

#### âš ï¸ **éœ€è¦æ”¹è¿›çš„åœ°æ–¹**

1. **æ•°æ®åº“è¿ç§»ç®¡ç†ç¼ºå¤±**
   - ç¼ºå°‘Alembicç­‰è¿ç§»å·¥å…·
   - å»ºè®®ï¼šå¼•å…¥Alembicç®¡ç†schemaå˜æ›´

2. **è¯»å†™åˆ†ç¦»æœªå®ç°**
   - æ‰€æœ‰æ“ä½œéƒ½åœ¨ä¸»åº“
   - å»ºè®®ï¼šé…ç½®è¯»å‰¯æœ¬åˆ†æ‹…æŸ¥è¯¢å‹åŠ›

3. **åˆ†åº“åˆ†è¡¨ç­–ç•¥æœªå®šä¹‰**
   - å•è¡¨æ•°æ®é‡å¤§æ—¶æ€§èƒ½ä¸‹é™
   - å»ºè®®ï¼šåˆ¶å®šåˆ†è¡¨ç­–ç•¥ï¼ˆæŒ‰æ—¶é—´/æŒ‰ä»£ç ï¼‰

4. **å¤‡ä»½æ¢å¤æœºåˆ¶å¾…å®Œå–„**
   - backup_recovery.pyå·²æœ‰åŸºç¡€
   - å»ºè®®ï¼šè‡ªåŠ¨åŒ–å¤‡ä»½å’Œæ¢å¤æµ‹è¯•

5. **æ•°æ®éªŒè¯ä¸å¤Ÿä¸¥æ ¼**
   - ç¼ºå°‘æ•°æ®è´¨é‡æ£€æŸ¥
   - å»ºè®®ï¼šå®ç°DataQualityMonitorå…¨é¢æ£€æŸ¥

#### ğŸ¯ **æ”¹è¿›å»ºè®®**

```python
# å»ºè®®çš„æ•°æ®åº“æ¶æ„å¢å¼º

# 1. å¼•å…¥Alembicè¿ç§»
# alembic/env.py
from alembic import context
from sqlalchemy import engine_from_config, pool
from app.models.base import Base

def run_migrations_online():
    """åœ¨çº¿è¿ç§»"""
    connectable = engine_from_config(
        config.get_section(config.config_ini_section),
        prefix='sqlalchemy.',
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=Base.metadata
        )

        with context.begin_transaction():
            context.run_migrations()

# 2. è¯»å†™åˆ†ç¦»é…ç½®
class DatabaseRouter:
    """æ•°æ®åº“è¯»å†™åˆ†ç¦»è·¯ç”±"""

    def __init__(self):
        self.write_engine = create_engine(WRITE_DB_URL)
        self.read_engines = [
            create_engine(READ_REPLICA_1_URL),
            create_engine(READ_REPLICA_2_URL),
        ]
        self.current_read_index = 0

    def get_write_session(self) -> Session:
        """è·å–å†™åº“ä¼šè¯"""
        return Session(bind=self.write_engine)

    def get_read_session(self) -> Session:
        """è·å–è¯»åº“ä¼šè¯ï¼ˆè½®è¯¢ï¼‰"""
        engine = self.read_engines[self.current_read_index]
        self.current_read_index = (
            self.current_read_index + 1
        ) % len(self.read_engines)
        return Session(bind=engine)

# 3. åˆ†è¡¨ç­–ç•¥
class TimeBasedSharding:
    """æ—¶é—´åˆ†è¡¨ç­–ç•¥"""

    @staticmethod
    def get_table_name(
        base_name: str,
        timestamp: datetime
    ) -> str:
        """æ ¹æ®æ—¶é—´æˆ³è·å–è¡¨å"""
        # æŒ‰æœˆåˆ†è¡¨
        return f"{base_name}_{timestamp.strftime('%Y%m')}"

    @staticmethod
    def get_partition_key(
        symbol: str,
        timestamp: datetime
    ) -> int:
        """è®¡ç®—åˆ†åŒºé”®"""
        # æŒ‰è‚¡ç¥¨ä»£ç å“ˆå¸Œåˆ°4ä¸ªåˆ†åŒº
        return hash(symbol) % 4

# 4. è‡ªåŠ¨å¤‡ä»½
class AutoBackupService:
    """è‡ªåŠ¨å¤‡ä»½æœåŠ¡"""

    async def schedule_daily_backup(self):
        """æ¯æ—¥å‡Œæ™¨2ç‚¹è‡ªåŠ¨å¤‡ä»½"""
        scheduler = BackgroundScheduler()
        scheduler.add_job(
            self.create_full_backup,
            trigger='cron',
            hour=2,
            minute=0
        )
        scheduler.start()

    async def create_full_backup(self):
        """åˆ›å»ºå…¨é‡å¤‡ä»½"""
        timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
        backup_name = f"auto_backup_{timestamp}"

        # PostgreSQLå¤‡ä»½
        await self.backup_postgresql(backup_name)

        # TDengineå¤‡ä»½
        await self.backup_tdengine(backup_name)

        # ä¸Šä¼ åˆ°OSS
        await self.upload_to_oss(backup_name)

        # æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™30å¤©ï¼‰
        await self.cleanup_old_backups(days=30)
```

---

### 5. **æ€§èƒ½ä¼˜åŒ–æ¶æ„ (82/100)**

#### âœ… **Phase 3ä¼˜åŒ–æˆæœ**

**æ•°æ®åº“æ€§èƒ½**:
```yaml
è¿æ¥æ± ä¼˜åŒ–:
  - æ ¸å¿ƒè¿æ¥: 20
  - æœ€å¤§è¿æ¥: 60 (20 + 40 overflow)
  - è¿æ¥å¤ç”¨ç‡: 95%
  - æ€§èƒ½æå‡: +3900% å¹¶å‘èƒ½åŠ›

æŸ¥è¯¢æ‰¹å¤„ç†:
  - æ‰¹æ¬¡å¤§å°: 1000è¡Œ
  - ååé‡æå‡: 2å€
  - æŸ¥è¯¢å»¶è¿Ÿé™ä½: 50%

æ…¢æŸ¥è¯¢ç›‘æ§:
  - é˜ˆå€¼: >1ç§’è‡ªåŠ¨å‘Šè­¦
  - è‡ªåŠ¨ç´¢å¼•å»ºè®®
```

**WebSocketæ€§èƒ½**:
```yaml
è¿æ¥æ± ç®¡ç†:
  - æœ€å°è¿æ¥: 10
  - æœ€å¤§è¿æ¥: 1000
  - è‡ªåŠ¨æ¸…ç†: ç©ºé—²30åˆ†é’Ÿ

æ¶ˆæ¯æ‰¹å¤„ç†:
  - å‹ç¼©æ¯”: 10:1
  - å»¶è¿Ÿ: <50ms
  - ååé‡: 2å€æå‡

å†…å­˜ä¼˜åŒ–:
  - 4çº§å‹åŠ›ç›‘æ§
  - è‡ªåŠ¨GCè§¦å‘
  - å†…å­˜å ç”¨: -50%
```

**ç¼“å­˜ç­–ç•¥**:
```python
# æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ
class CacheIntegration:
    """ç»Ÿä¸€ç¼“å­˜é›†æˆ"""

    async def fetch_with_cache(
        self,
        symbol: str,
        data_type: str,
        fetch_fn: Callable,
        ttl_days: int = 1,
        use_cache: bool = True
    ) -> Dict[str, Any]:
        """
        å¸¦ç¼“å­˜çš„æ•°æ®è·å–
        - å…ˆæŸ¥ç¼“å­˜
        - ç¼“å­˜æœªå‘½ä¸­åˆ™è°ƒç”¨fetch_fn
        - è‡ªåŠ¨æ›´æ–°ç¼“å­˜
        """
        if not use_cache:
            return await fetch_fn()

        cache_key = f"{data_type}:{symbol}"
        cached_data = await self.cache.get(cache_key)

        if cached_data and self._is_cache_valid(
            cached_data, ttl_days
        ):
            return {
                "data": cached_data,
                "source": "cache",
                "cached_at": cached_data.get("timestamp")
            }

        # ä»æºè·å–æ•°æ®
        fresh_data = await fetch_fn()

        # æ›´æ–°ç¼“å­˜
        await self.cache.set(
            cache_key,
            fresh_data,
            ttl=ttl_days * 86400
        )

        return {
            "data": fresh_data,
            "source": "source",
            "fetched_at": datetime.utcnow()
        }
```

#### âœ… **å‹æµ‹æ¡†æ¶å®Œæ•´**

```python
# Locustå‹æµ‹è„šæœ¬
class MarketDataUser(HttpUser):
    """å¸‚åœºæ•°æ®ç”¨æˆ·"""
    wait_time = between(1, 3)

    @task(3)
    def fetch_stock_data(self):
        """è·å–è‚¡ç¥¨æ•°æ®ï¼ˆé«˜é¢‘ï¼‰"""
        self.client.get(
            "/api/data/stocks/600000",
            headers=self.headers
        )

    @task(2)
    def search_stocks(self):
        """æœç´¢è‚¡ç¥¨ï¼ˆä¸­é¢‘ï¼‰"""
        self.client.get(
            "/api/stock-search?q=å¹³å®‰",
            headers=self.headers
        )

    @task(1)
    def run_backtest(self):
        """è¿è¡Œå›æµ‹ï¼ˆä½é¢‘ï¼‰"""
        self.client.post(
            "/api/backtest/execute",
            json={"strategy": "ma_cross"},
            headers=self.headers
        )

# 4ç§å‹æµ‹åœºæ™¯
- åŸºå‡†æµ‹è¯•: 100ç”¨æˆ·, 5åˆ†é’Ÿ
- æ­£å¸¸è´Ÿè½½: 500ç”¨æˆ·, 10åˆ†é’Ÿ
- é«˜å³°è´Ÿè½½: 1000ç”¨æˆ·, 10åˆ†é’Ÿ
- å‹åŠ›æµ‹è¯•: 2000ç”¨æˆ·, 15åˆ†é’Ÿ
```

#### âš ï¸ **éœ€è¦æ”¹è¿›çš„åœ°æ–¹**

1. **CDNé›†æˆç¼ºå¤±**
   - é™æ€èµ„æºæœªä½¿ç”¨CDNåŠ é€Ÿ
   - å»ºè®®ï¼šæ¥å…¥CloudFlareæˆ–é˜¿é‡Œäº‘CDN

2. **æŸ¥è¯¢ä¼˜åŒ–å¾…åŠ å¼º**
   - ç¼ºå°‘N+1æŸ¥è¯¢æ£€æµ‹
   - å»ºè®®ï¼šä½¿ç”¨DataLoaderæ¨¡å¼

3. **å¼‚æ­¥å¤„ç†ä¸å¤Ÿå……åˆ†**
   - éƒ¨åˆ†è€—æ—¶æ“ä½œä»æ˜¯åŒæ­¥
   - å»ºè®®ï¼šä½¿ç”¨Celery/RQå¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—

4. **APIå“åº”æ—¶é—´ä¼˜åŒ–ç©ºé—´**
   - éƒ¨åˆ†ç«¯ç‚¹å“åº”æ—¶é—´>1ç§’
   - å»ºè®®ï¼šä¼˜åŒ–æ…¢æŸ¥è¯¢ï¼Œå¢åŠ ç´¢å¼•

5. **æ•°æ®åº“æŸ¥è¯¢ç¼“å­˜å¾…å®Œå–„**
   - æŸ¥è¯¢ç»“æœç¼“å­˜è¦†ç›–ä¸è¶³
   - å»ºè®®ï¼šå®ç°æŸ¥è¯¢ç»“æœç¼“å­˜å±‚

#### ğŸ¯ **æ”¹è¿›å»ºè®®**

```python
# å»ºè®®çš„æ€§èƒ½ä¼˜åŒ–æ¶æ„

# 1. DataLoaderæ¨¡å¼é˜²æ­¢N+1æŸ¥è¯¢
from aiodataloader import DataLoader

class StockLoader(DataLoader):
    """è‚¡ç¥¨ä¿¡æ¯æ‰¹é‡åŠ è½½å™¨"""

    async def batch_load_fn(
        self,
        symbols: List[str]
    ) -> List[Optional[Stock]]:
        """æ‰¹é‡åŠ è½½è‚¡ç¥¨ä¿¡æ¯"""
        stocks = await self.stock_repo.find_by_symbols(symbols)
        stock_map = {s.symbol: s for s in stocks}
        return [stock_map.get(symbol) for symbol in symbols]

# ä½¿ç”¨DataLoader
loader = StockLoader()
stocks = await asyncio.gather(*[
    loader.load(symbol) for symbol in ['600000', '000001', '000002']
])  # å•æ¬¡æ•°æ®åº“æŸ¥è¯¢ï¼Œè€Œä¸æ˜¯3æ¬¡

# 2. æŸ¥è¯¢ç»“æœç¼“å­˜
class QueryResultCache:
    """æŸ¥è¯¢ç»“æœç¼“å­˜"""

    def __init__(self, redis_client: Redis):
        self.redis = redis_client

    async def get_or_execute(
        self,
        cache_key: str,
        query_fn: Callable,
        ttl: int = 3600
    ) -> Any:
        """è·å–æˆ–æ‰§è¡ŒæŸ¥è¯¢"""
        # å°è¯•ä»ç¼“å­˜è·å–
        cached = await self.redis.get(cache_key)
        if cached:
            return json.loads(cached)

        # æ‰§è¡ŒæŸ¥è¯¢
        result = await query_fn()

        # ç¼“å­˜ç»“æœ
        await self.redis.setex(
            cache_key,
            ttl,
            json.dumps(result, default=str)
        )

        return result

# 3. å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—
from celery import Celery

celery_app = Celery(
    'mystocks',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/1'
)

@celery_app.task(bind=True, max_retries=3)
def run_backtest_task(self, strategy_id: str):
    """å¼‚æ­¥æ‰§è¡Œå›æµ‹"""
    try:
        result = backtest_engine.execute(strategy_id)
        return {
            "status": "success",
            "result": result
        }
    except Exception as exc:
        # æŒ‡æ•°é€€é¿é‡è¯•
        raise self.retry(
            exc=exc,
            countdown=2 ** self.request.retries
        )

# APIç«¯ç‚¹ç«‹å³è¿”å›ä»»åŠ¡ID
@router.post("/backtest/execute")
async def execute_backtest(request: BacktestRequest):
    task = run_backtest_task.delay(request.strategy_id)
    return {
        "task_id": task.id,
        "status": "pending",
        "status_url": f"/backtest/status/{task.id}"
    }

# 4. å“åº”å‹ç¼©
from fastapi.middleware.gzip import GZipMiddleware
app.add_middleware(
    GZipMiddleware,
    minimum_size=1000,      # ä»…å‹ç¼©>1KBçš„å“åº”
    compresslevel=5         # å‹ç¼©ç­‰çº§1-9ï¼Œ5ä¸ºå¹³è¡¡
)

# 5. HTTP/2å’ŒServer Push
# é…ç½®uvicornä½¿ç”¨HTTP/2
uvicorn.run(
    "main:app",
    host="0.0.0.0",
    port=8000,
    http='h2'  # å¯ç”¨HTTP/2
)
```

---

### 6. **ç›‘æ§å’Œå¯è§‚æµ‹æ€§ (70/100)**

#### âœ… **å·²å®ç°çš„ç›‘æ§åŠŸèƒ½**

**ç›‘æ§æ•°æ®åº“**:
```python
class MonitoringDatabase:
    """ç‹¬ç«‹ç›‘æ§æ•°æ®åº“"""

    tables = [
        "operations_log",          # æ“ä½œæ—¥å¿—
        "performance_metrics",     # æ€§èƒ½æŒ‡æ ‡
        "data_quality_checks",     # æ•°æ®è´¨é‡
        "alert_history",           # å‘Šè­¦å†å²
        "system_health"            # ç³»ç»Ÿå¥åº·
    ]
```

**æ€§èƒ½ç›‘æ§**:
```python
class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    async def track_query_performance(
        self,
        query_type: str,
        execution_time: float,
        row_count: int
    ):
        """è·Ÿè¸ªæŸ¥è¯¢æ€§èƒ½"""
        if execution_time > 1.0:  # æ…¢æŸ¥è¯¢å‘Šè­¦
            await self.alert_slow_query(
                query_type,
                execution_time
            )
```

**æ•°æ®è´¨é‡ç›‘æ§**:
```python
class DataQualityMonitor:
    """æ•°æ®è´¨é‡ç›‘æ§"""

    checks = [
        "completeness",  # å®Œæ•´æ€§æ£€æŸ¥
        "accuracy",      # å‡†ç¡®æ€§æ£€æŸ¥
        "freshness",     # æ–°é²œåº¦æ£€æŸ¥
        "consistency"    # ä¸€è‡´æ€§æ£€æŸ¥
    ]
```

**å‘Šè­¦ç³»ç»Ÿ**:
```python
class AlertManager:
    """å‘Šè­¦ç®¡ç†å™¨"""

    channels = [
        "email",      # é‚®ä»¶é€šçŸ¥
        "webhook",    # Webhooké€šçŸ¥
        "log"         # æ—¥å¿—è®°å½•
    ]

    severity_levels = [
        "INFO",
        "WARNING",
        "ERROR",
        "CRITICAL"
    ]
```

#### âš ï¸ **éœ€è¦æ”¹è¿›çš„åœ°æ–¹**

1. **APMé›†æˆç¼ºå¤±**
   - ç¼ºå°‘DataDog/New Relicç­‰APMå·¥å…·
   - å»ºè®®ï¼šé›†æˆAPMè¿›è¡Œå…¨é“¾è·¯è¿½è¸ª

2. **åˆ†å¸ƒå¼è¿½è¸ªä¸å®Œæ•´**
   - ç¼ºå°‘OpenTelemetryé›†æˆ
   - å»ºè®®ï¼šå®ç°åˆ†å¸ƒå¼è¿½è¸ªï¼ˆJaeger/Zipkinï¼‰

3. **æ—¥å¿—èšåˆå¾…å®Œå–„**
   - ç¼ºå°‘ELK/Lokiç­‰æ—¥å¿—èšåˆ
   - å»ºè®®ï¼šæ­å»ºé›†ä¸­å¼æ—¥å¿—ç³»ç»Ÿ

4. **æŒ‡æ ‡å¯è§†åŒ–ä¸è¶³**
   - ç¼ºå°‘Grafanaç­‰å¯è§†åŒ–å¹³å°
   - å»ºè®®ï¼šé›†æˆGrafanaä»ªè¡¨æ¿

5. **å‘Šè­¦è§„åˆ™ä¸å¤Ÿå®Œå–„**
   - å‘Šè­¦é˜ˆå€¼éœ€è¦è°ƒä¼˜
   - å»ºè®®ï¼šåŸºäºSLI/SLOå®šä¹‰å‘Šè­¦è§„åˆ™

#### ğŸ¯ **æ”¹è¿›å»ºè®®**

```python
# å»ºè®®çš„å¯è§‚æµ‹æ€§æ¶æ„

# 1. OpenTelemetryé›†æˆ
from opentelemetry import trace, metrics
from opentelemetry.exporter.jaeger import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

# é…ç½®è¿½è¸ªå™¨
trace.set_tracer_provider(TracerProvider())
jaeger_exporter = JaegerExporter(
    agent_host_name="localhost",
    agent_port=6831,
)
trace.get_tracer_provider().add_span_processor(
    BatchSpanProcessor(jaeger_exporter)
)

# ä½¿ç”¨è¿½è¸ªå™¨
tracer = trace.get_tracer(__name__)

@router.get("/api/stocks/{symbol}")
async def get_stock(symbol: str):
    with tracer.start_as_current_span("get_stock") as span:
        span.set_attribute("stock.symbol", symbol)

        # å­Spanï¼šæ•°æ®åº“æŸ¥è¯¢
        with tracer.start_as_current_span("db_query"):
            stock = await stock_repo.find_by_symbol(symbol)

        # å­Spanï¼šç¼“å­˜æ›´æ–°
        with tracer.start_as_current_span("cache_update"):
            await cache.set(f"stock:{symbol}", stock)

        return stock

# 2. PrometheusæŒ‡æ ‡
from prometheus_client import Counter, Histogram, Gauge

# å®šä¹‰æŒ‡æ ‡
http_requests_total = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

http_request_duration_seconds = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

db_connections_active = Gauge(
    'db_connections_active',
    'Active database connections'
)

# ä¸­é—´ä»¶è®°å½•æŒ‡æ ‡
@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    start_time = time.time()

    response = await call_next(request)

    duration = time.time() - start_time

    http_requests_total.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).inc()

    http_request_duration_seconds.labels(
        method=request.method,
        endpoint=request.url.path
    ).observe(duration)

    return response

# 3. ç»“æ„åŒ–æ—¥å¿—
import structlog

logger = structlog.get_logger()

@router.post("/api/orders")
async def create_order(order: Order, user: User = Depends(get_current_user)):
    logger.info(
        "order_created",
        user_id=user.id,
        order_id=order.id,
        symbol=order.symbol,
        quantity=order.quantity,
        order_type=order.type
    )

    try:
        result = await order_service.create(order)
        logger.info("order_executed", order_id=order.id, status="success")
        return result
    except Exception as exc:
        logger.error(
            "order_failed",
            order_id=order.id,
            error=str(exc),
            exc_info=True
        )
        raise

# 4. SLI/SLOå®šä¹‰
class SLI:
    """Service Level Indicators"""

    # å¯ç”¨æ€§: 99.9% (æ¯æœˆæœ€å¤š43åˆ†é’Ÿæ•…éšœ)
    AVAILABILITY_TARGET = 0.999

    # å“åº”æ—¶é—´: P95 < 500ms
    RESPONSE_TIME_P95_TARGET = 0.5

    # é”™è¯¯ç‡: < 0.1%
    ERROR_RATE_TARGET = 0.001

class AlertRule:
    """å‘Šè­¦è§„åˆ™"""

    @staticmethod
    def check_slo_breach():
        """æ£€æŸ¥SLOæ˜¯å¦è¿å"""
        # å¯ç”¨æ€§æ£€æŸ¥
        availability = get_availability_last_hour()
        if availability < SLI.AVAILABILITY_TARGET:
            alert_manager.send_alert(
                severity="CRITICAL",
                message=f"å¯ç”¨æ€§ä½äºSLO: {availability:.2%}",
                runbook_url="https://wiki/runbook/availability"
            )

        # å“åº”æ—¶é—´æ£€æŸ¥
        p95_latency = get_p95_latency_last_hour()
        if p95_latency > SLI.RESPONSE_TIME_P95_TARGET:
            alert_manager.send_alert(
                severity="WARNING",
                message=f"P95å»¶è¿Ÿè¶…è¿‡SLO: {p95_latency:.3f}s",
                runbook_url="https://wiki/runbook/latency"
            )

        # é”™è¯¯ç‡æ£€æŸ¥
        error_rate = get_error_rate_last_hour()
        if error_rate > SLI.ERROR_RATE_TARGET:
            alert_manager.send_alert(
                severity="WARNING",
                message=f"é”™è¯¯ç‡è¶…è¿‡SLO: {error_rate:.2%}",
                runbook_url="https://wiki/runbook/errors"
            )

# 5. Grafanaä»ªè¡¨æ¿é…ç½®
grafana_dashboard = {
    "dashboard": {
        "title": "MyStocks APIç›‘æ§",
        "panels": [
            {
                "title": "è¯·æ±‚é€Ÿç‡",
                "targets": [{
                    "expr": "rate(http_requests_total[5m])"
                }]
            },
            {
                "title": "å“åº”æ—¶é—´P95",
                "targets": [{
                    "expr": "histogram_quantile(0.95, http_request_duration_seconds)"
                }]
            },
            {
                "title": "é”™è¯¯ç‡",
                "targets": [{
                    "expr": "rate(http_requests_total{status=~\"5..\"}[5m])"
                }]
            },
            {
                "title": "æ•°æ®åº“è¿æ¥æ± ",
                "targets": [{
                    "expr": "db_connections_active"
                }]
            }
        ]
    }
}
```

---

### 7. **æµ‹è¯•ç­–ç•¥ (42/100)**

#### âœ… **ç°æœ‰æµ‹è¯•åŸºç¡€**

**æµ‹è¯•æ–‡ä»¶ç»Ÿè®¡**:
```
æ€»æµ‹è¯•æ–‡ä»¶: 49ä¸ª
æµ‹è¯•ç±»å‹:
  - å•å…ƒæµ‹è¯•: test_*.py
  - é›†æˆæµ‹è¯•: test_*_integration.py
  - E2Eæµ‹è¯•: web/frontendä¸‹çš„Playwrightæµ‹è¯•
```

**æµ‹è¯•è¦†ç›–ç‡ç°çŠ¶**:
```yaml
æ€»ä½“è¦†ç›–ç‡: ~6%
ç›®æ ‡è¦†ç›–ç‡: 80%
data_accesså±‚:
  - PostgreSQL: 67%
  - TDengine: 56%
```

**æµ‹è¯•æ¡†æ¶**:
```python
# pytesté…ç½®
pytest.ini:
  testpaths = tests
  python_files = test_*.py
  python_classes = Test*
  python_functions = test_*
```

#### âš ï¸ **ä¸¥é‡ä¸è¶³ä¹‹å¤„**

1. **è¦†ç›–ç‡ä¸¥é‡ä¸è¶³**
   - å½“å‰6%ï¼Œç›®æ ‡80%ï¼Œå·®è·74%
   - æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ç¼ºå°‘æµ‹è¯•

2. **é›†æˆæµ‹è¯•ç¼ºå¤±**
   - æœåŠ¡é—´é›†æˆæµ‹è¯•ä¸è¶³
   - æ•°æ®åº“é›†æˆæµ‹è¯•ä¸å®Œæ•´

3. **å¥‘çº¦æµ‹è¯•ç¼ºå¤±**
   - ç¼ºå°‘APIå¥‘çº¦æµ‹è¯•ï¼ˆPactï¼‰
   - å‰åç«¯é›†æˆå®¹æ˜“å‡ºç°é—®é¢˜

4. **æ€§èƒ½æµ‹è¯•è‡ªåŠ¨åŒ–ä¸è¶³**
   - Locustè„šæœ¬å­˜åœ¨ï¼Œä½†æœªé›†æˆåˆ°CI/CD
   - ç¼ºå°‘æŒç»­æ€§èƒ½ç›‘æ§

5. **æµ‹è¯•ç¯å¢ƒç®¡ç†æ··ä¹±**
   - æµ‹è¯•æ•°æ®ç®¡ç†ä¸è§„èŒƒ
   - æµ‹è¯•ç¯å¢ƒéš”ç¦»ä¸å……åˆ†

#### ğŸ¯ **æ”¹è¿›å»ºè®®**

```python
# å»ºè®®çš„å…¨é¢æµ‹è¯•ç­–ç•¥

# 1. å•å…ƒæµ‹è¯•æ¨¡æ¿
import pytest
from unittest.mock import Mock, patch
from app.services.stock_service import StockService

class TestStockService:
    """StockServiceå•å…ƒæµ‹è¯•"""

    @pytest.fixture
    def stock_service(self):
        """æµ‹è¯•è£…ç½®ï¼šåˆ›å»ºStockServiceå®ä¾‹"""
        mock_repo = Mock()
        mock_cache = Mock()
        return StockService(
            stock_repo=mock_repo,
            cache=mock_cache
        )

    @pytest.mark.asyncio
    async def test_get_stock_info_cache_hit(
        self,
        stock_service
    ):
        """æµ‹è¯•ï¼šç¼“å­˜å‘½ä¸­åœºæ™¯"""
        # Arrange
        symbol = "600000"
        cached_data = {"symbol": symbol, "name": "æµ¦å‘é“¶è¡Œ"}
        stock_service.cache.get.return_value = cached_data

        # Act
        result = await stock_service.get_stock_info(symbol)

        # Assert
        assert result == cached_data
        stock_service.cache.get.assert_called_once_with(
            f"stock:{symbol}"
        )
        stock_service.stock_repo.find_by_symbol.assert_not_called()

    @pytest.mark.asyncio
    async def test_get_stock_info_cache_miss(
        self,
        stock_service
    ):
        """æµ‹è¯•ï¼šç¼“å­˜æœªå‘½ä¸­åœºæ™¯"""
        # Arrange
        symbol = "600000"
        stock_data = {"symbol": symbol, "name": "æµ¦å‘é“¶è¡Œ"}
        stock_service.cache.get.return_value = None
        stock_service.stock_repo.find_by_symbol.return_value = stock_data

        # Act
        result = await stock_service.get_stock_info(symbol)

        # Assert
        assert result == stock_data
        stock_service.stock_repo.find_by_symbol.assert_called_once_with(symbol)
        stock_service.cache.set.assert_called_once()

    @pytest.mark.asyncio
    async def test_get_stock_info_not_found(
        self,
        stock_service
    ):
        """æµ‹è¯•ï¼šè‚¡ç¥¨ä¸å­˜åœ¨åœºæ™¯"""
        # Arrange
        symbol = "999999"
        stock_service.cache.get.return_value = None
        stock_service.stock_repo.find_by_symbol.return_value = None

        # Act & Assert
        with pytest.raises(NotFoundError) as exc_info:
            await stock_service.get_stock_info(symbol)

        assert f"è‚¡ç¥¨ {symbol} ä¸å­˜åœ¨" in str(exc_info.value)

# 2. é›†æˆæµ‹è¯•æ¨¡æ¿
import pytest_asyncio
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from app.core.database import Base

@pytest_asyncio.fixture
async def test_db():
    """æµ‹è¯•æ•°æ®åº“è£…ç½®"""
    # åˆ›å»ºæµ‹è¯•æ•°æ®åº“
    engine = create_engine("sqlite:///:memory:")
    Base.metadata.create_all(engine)

    SessionLocal = sessionmaker(bind=engine)
    session = SessionLocal()

    yield session

    # æ¸…ç†
    session.close()
    Base.metadata.drop_all(engine)

@pytest.mark.integration
@pytest.mark.asyncio
async def test_stock_service_integration(test_db):
    """é›†æˆæµ‹è¯•ï¼šStockServiceä¸æ•°æ®åº“"""
    # Arrange
    stock_repo = StockRepository(db=test_db)
    cache = RealCacheService()
    stock_service = StockService(
        stock_repo=stock_repo,
        cache=cache
    )

    # Act: åˆ›å»ºè‚¡ç¥¨
    stock = await stock_service.create_stock({
        "symbol": "600000",
        "name": "æµ¦å‘é“¶è¡Œ"
    })

    # Assert: éªŒè¯åˆ›å»º
    assert stock.symbol == "600000"

    # Act: æŸ¥è¯¢è‚¡ç¥¨
    found = await stock_service.get_stock_info("600000")

    # Assert: éªŒè¯æŸ¥è¯¢
    assert found["symbol"] == "600000"
    assert found["name"] == "æµ¦å‘é“¶è¡Œ"

# 3. APIå¥‘çº¦æµ‹è¯•
from pact import Consumer, Provider

pact = Consumer('frontend').has_pact_with(
    Provider('mystocks-api')
)

def test_get_stock_contract():
    """å¥‘çº¦æµ‹è¯•ï¼šè·å–è‚¡ç¥¨ä¿¡æ¯"""
    expected = {
        'symbol': '600000',
        'name': 'æµ¦å‘é“¶è¡Œ',
        'price': 10.5
    }

    (pact
     .given('è‚¡ç¥¨600000å­˜åœ¨')
     .upon_receiving('è·å–è‚¡ç¥¨ä¿¡æ¯è¯·æ±‚')
     .with_request('get', '/api/stocks/600000')
     .will_respond_with(200, body=expected))

    with pact:
        result = requests.get(
            pact.uri + '/api/stocks/600000'
        ).json()
        assert result == expected

# 4. E2Eæµ‹è¯•
from playwright.async_api import async_playwright

@pytest.mark.e2e
@pytest.mark.asyncio
async def test_stock_search_workflow():
    """E2Eæµ‹è¯•ï¼šè‚¡ç¥¨æœç´¢å·¥ä½œæµ"""
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()

        # 1. è®¿é—®é¦–é¡µ
        await page.goto('http://localhost:3000')

        # 2. æœç´¢è‚¡ç¥¨
        await page.fill('[data-testid="search-input"]', 'å¹³å®‰')
        await page.click('[data-testid="search-button"]')

        # 3. éªŒè¯æœç´¢ç»“æœ
        await page.wait_for_selector('[data-testid="search-results"]')
        results = await page.query_selector_all(
            '[data-testid="stock-item"]'
        )
        assert len(results) > 0

        # 4. ç‚¹å‡»ç¬¬ä¸€ä¸ªç»“æœ
        await results[0].click()

        # 5. éªŒè¯è¯¦æƒ…é¡µ
        await page.wait_for_selector('[data-testid="stock-detail"]')
        symbol = await page.text_content(
            '[data-testid="stock-symbol"]'
        )
        assert 'å¹³å®‰' in symbol or '600000' in symbol

        await browser.close()

# 5. æ€§èƒ½æµ‹è¯•
import pytest_benchmark

def test_stock_service_performance(benchmark):
    """æ€§èƒ½æµ‹è¯•ï¼šStockServiceå“åº”æ—¶é—´"""
    stock_service = get_stock_service()

    # åŸºå‡†æµ‹è¯•
    result = benchmark(
        stock_service.get_stock_info,
        "600000"
    )

    # æ–­è¨€æ€§èƒ½è¦æ±‚
    assert benchmark.stats['mean'] < 0.1  # å¹³å‡<100ms
    assert benchmark.stats['max'] < 0.5   # æœ€å¤§<500ms

# 6. æµ‹è¯•è¦†ç›–ç‡é…ç½®
# pytest.ini
[pytest]
addopts =
    --cov=app
    --cov-report=html
    --cov-report=term-missing
    --cov-fail-under=80
    --maxfail=1
    --tb=short
```

---

### 8. **Realæ•°æ®å¯¹æ¥å‡†å¤‡åº¦è¯„ä¼° (75/100)**

#### âœ… **å·²å°±ç»ªçš„åŸºç¡€è®¾æ–½**

**æ•°æ®æºé€‚é…å™¨**:
```yaml
å·²å®ç°çš„7ä¸ªé€‚é…å™¨:
  1. AkshareDataSource      # Akshareä¸­å›½å¸‚åœºæ•°æ®
  2. BaostockDataSource     # Baostockå†å²æ•°æ®
  3. FinancialDataSource    # è´¢åŠ¡æŠ¥è¡¨æ•°æ®
  4. TdxDataSource          # é€šè¾¾ä¿¡ç›´è¿
  5. ByapiDataSource        # REST APIæ•°æ®æº
  6. CustomerDataSource     # å®æ—¶è¡Œæƒ…
  7. TushareDataSource      # Tushareä¸“ä¸šæ•°æ®

ç»Ÿä¸€æ¥å£: IDataSource
```

**æ•°æ®è®¿é—®å±‚**:
```python
class MyStocksUnifiedManager:
    """ç»Ÿä¸€æ•°æ®ç®¡ç†å™¨ - æ”¯æŒRealæ•°æ®å¯¹æ¥"""

    async def save_data_by_classification(
        self,
        data: pd.DataFrame,
        classification: DataClassification
    ):
        """
        æ ¹æ®æ•°æ®åˆ†ç±»è‡ªåŠ¨è·¯ç”±
        - é«˜é¢‘æ•°æ® â†’ TDengine
        - å…¶ä»–æ•°æ® â†’ PostgreSQL
        """
```

**åŒæ•°æ®åº“æ¶æ„**:
```yaml
TDengine:
  - å‡†å¤‡æ¥æ”¶tick/åˆ†é’Ÿæ•°æ®
  - è¶…çº§è¡¨å·²å®šä¹‰
  - è‡ªåŠ¨åˆ†è¡¨æœºåˆ¶å°±ç»ª

PostgreSQL + TimescaleDB:
  - æ—¥çº¿æ•°æ®æ··åˆè¡¨å°±ç»ª
  - å‚è€ƒæ•°æ®è¡¨å·²åˆ›å»º
  - äº‹åŠ¡æ”¯æŒå®Œæ•´
```

#### âš ï¸ **éœ€è¦æ”¹é€ çš„éƒ¨åˆ†**

1. **Mockæ•°æ®åˆ†ç¦»ä¸æ¸…æ™°**
   - å½“å‰Mockæ•°æ®å’ŒRealæ•°æ®æ··åˆ
   - å»ºè®®ï¼šæ˜ç¡®åˆ†ç¦»Mockå’ŒRealæ•°æ®æµ

2. **æ•°æ®éªŒè¯æœºåˆ¶å¾…å®Œå–„**
   - ç¼ºå°‘æ•°æ®æ ¼å¼éªŒè¯
   - å»ºè®®ï¼šPydanticæ¨¡å‹éªŒè¯æ‰€æœ‰å¤–éƒ¨æ•°æ®

3. **é”™è¯¯å¤„ç†å¾…åŠ å¼º**
   - å¤–éƒ¨æ•°æ®æºå¤±è´¥å¤„ç†ä¸å¤Ÿå¥å£®
   - å»ºè®®ï¼šå®ç°ç†”æ–­å™¨å’Œé™çº§ç­–ç•¥

4. **æ•°æ®åŒæ­¥æœºåˆ¶å¾…å®Œå–„**
   - ç¼ºå°‘å¢é‡åŒæ­¥æœºåˆ¶
   - å»ºè®®ï¼šå®ç°CDCï¼ˆChange Data Captureï¼‰

5. **æ•°æ®è´¨é‡ç›‘æ§å¾…åŠ å¼º**
   - ç¼ºå°‘å®æ—¶æ•°æ®è´¨é‡æ£€æŸ¥
   - å»ºè®®ï¼šå®ç°DataQualityMonitorå…¨é¢æ£€æŸ¥

#### ğŸ¯ **Realæ•°æ®å¯¹æ¥æ”¹é€ å»ºè®®**

```python
# å»ºè®®çš„Realæ•°æ®å¯¹æ¥æ¶æ„

# 1. æ•°æ®æºå·¥å‚æ¨¡å¼
class DataSourceFactory:
    """æ•°æ®æºå·¥å‚"""

    @staticmethod
    def create_source(
        source_type: str,
        config: Dict[str, Any]
    ) -> IDataSource:
        """åˆ›å»ºæ•°æ®æºå®ä¾‹"""
        if source_type == "mock":
            return MockDataSource(config)
        elif source_type == "akshare":
            return AkshareDataSource(config)
        elif source_type == "tushare":
            return TushareDataSource(config)
        else:
            raise ValueError(f"æœªçŸ¥æ•°æ®æºç±»å‹: {source_type}")

    @staticmethod
    def get_primary_source() -> IDataSource:
        """è·å–ä¸»æ•°æ®æºï¼ˆåŸºäºé…ç½®ï¼‰"""
        source_type = os.getenv("PRIMARY_DATA_SOURCE", "mock")
        return DataSourceFactory.create_source(
            source_type,
            get_source_config(source_type)
        )

# 2. æ•°æ®éªŒè¯å±‚
from pydantic import BaseModel, validator

class OHLCVData(BaseModel):
    """OHLCVæ•°æ®æ¨¡å‹"""
    symbol: str
    timestamp: datetime
    open: Decimal = Field(gt=0)
    high: Decimal = Field(gt=0)
    low: Decimal = Field(gt=0)
    close: Decimal = Field(gt=0)
    volume: int = Field(ge=0)

    @validator('high')
    def high_must_be_highest(cls, v, values):
        """éªŒè¯é«˜ä»·æ˜¯æœ€é«˜ä»·"""
        if 'low' in values and v < values['low']:
            raise ValueError('é«˜ä»·ä¸èƒ½ä½äºä½ä»·')
        if 'open' in values and v < values['open']:
            raise ValueError('é«˜ä»·ä¸èƒ½ä½äºå¼€ç›˜ä»·')
        if 'close' in values and v < values['close']:
            raise ValueError('é«˜ä»·ä¸èƒ½ä½äºæ”¶ç›˜ä»·')
        return v

    @validator('low')
    def low_must_be_lowest(cls, v, values):
        """éªŒè¯ä½ä»·æ˜¯æœ€ä½ä»·"""
        if 'open' in values and v > values['open']:
            raise ValueError('ä½ä»·ä¸èƒ½é«˜äºå¼€ç›˜ä»·')
        if 'close' in values and v > values['close']:
            raise ValueError('ä½ä»·ä¸èƒ½é«˜äºæ”¶ç›˜ä»·')
        return v

class DataValidator:
    """æ•°æ®éªŒè¯å™¨"""

    @staticmethod
    async def validate_ohlcv(
        data: pd.DataFrame
    ) -> Tuple[pd.DataFrame, List[str]]:
        """
        éªŒè¯OHLCVæ•°æ®
        è¿”å›: (æœ‰æ•ˆæ•°æ®, é”™è¯¯åˆ—è¡¨)
        """
        errors = []
        valid_rows = []

        for idx, row in data.iterrows():
            try:
                validated = OHLCVData(**row.to_dict())
                valid_rows.append(validated.dict())
            except ValidationError as e:
                errors.append(f"Row {idx}: {e}")

        valid_df = pd.DataFrame(valid_rows)
        return valid_df, errors

# 3. æ•°æ®åŒæ­¥ç®¡é“
class DataSyncPipeline:
    """æ•°æ®åŒæ­¥ç®¡é“"""

    def __init__(
        self,
        source: IDataSource,
        validator: DataValidator,
        storage: MyStocksUnifiedManager
    ):
        self.source = source
        self.validator = validator
        self.storage = storage

    async def sync_daily_data(self, symbol: str):
        """åŒæ­¥æ—¥çº¿æ•°æ®"""
        try:
            # 1. è·å–æœ€åæ›´æ–°æ—¶é—´
            last_update = await self.storage.get_last_update_time(
                symbol,
                DataClassification.DAILY_MARKET
            )

            # 2. ä»æ•°æ®æºè·å–å¢é‡æ•°æ®
            start_date = (
                last_update + timedelta(days=1)
                if last_update
                else datetime.now() - timedelta(days=365)
            )

            raw_data = await self.source.fetch_ohlcv(
                symbol,
                start_date,
                datetime.now(),
                interval="1d"
            )

            if raw_data.empty:
                logger.info(f"æ²¡æœ‰æ–°æ•°æ®: {symbol}")
                return

            # 3. éªŒè¯æ•°æ®
            valid_data, errors = await self.validator.validate_ohlcv(
                raw_data
            )

            if errors:
                logger.warning(
                    f"æ•°æ®éªŒè¯å‘ç°é”™è¯¯: {symbol}",
                    errors=errors
                )

            # 4. å­˜å‚¨æ•°æ®
            await self.storage.save_data_by_classification(
                valid_data,
                DataClassification.DAILY_MARKET
            )

            # 5. è®°å½•åŒæ­¥æ—¥å¿—
            await self.log_sync_status(
                symbol,
                DataClassification.DAILY_MARKET,
                len(valid_data),
                len(errors)
            )

        except Exception as exc:
            logger.error(
                f"æ•°æ®åŒæ­¥å¤±è´¥: {symbol}",
                error=str(exc),
                exc_info=True
            )
            await self.alert_sync_failure(symbol, exc)

# 4. æ•°æ®æºé™çº§ç­–ç•¥
class DataSourceWithFallback:
    """å¸¦é™çº§çš„æ•°æ®æº"""

    def __init__(
        self,
        primary: IDataSource,
        fallback: IDataSource
    ):
        self.primary = primary
        self.fallback = fallback
        self.circuit_breaker = CircuitBreaker("data_source")

    async def fetch_ohlcv(self, *args, **kwargs):
        """è·å–OHLCVæ•°æ®ï¼ˆå¸¦é™çº§ï¼‰"""
        # å°è¯•ä¸»æ•°æ®æº
        result = self.circuit_breaker.call(
            self.primary.fetch_ohlcv,
            *args,
            **kwargs
        )

        if result.get("success"):
            return result["result"]

        # é™çº§åˆ°å¤‡ç”¨æ•°æ®æº
        logger.warning(
            "ä¸»æ•°æ®æºå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®æº",
            primary=self.primary.__class__.__name__,
            fallback=self.fallback.__class__.__name__
        )

        return await self.fallback.fetch_ohlcv(*args, **kwargs)

# 5. é…ç½®é©±åŠ¨çš„æ•°æ®æºåˆ‡æ¢
# .envé…ç½®
PRIMARY_DATA_SOURCE=akshare    # ç”Ÿäº§ç¯å¢ƒ
FALLBACK_DATA_SOURCE=mock      # é™çº§æº
ENABLE_DATA_VALIDATION=true    # å¯ç”¨éªŒè¯
DATA_SYNC_INTERVAL=1h          # åŒæ­¥é—´éš”

# ä»£ç ä¸­ä½¿ç”¨
data_source = DataSourceWithFallback(
    primary=DataSourceFactory.get_primary_source(),
    fallback=DataSourceFactory.create_source("mock", {})
)

# APIç«¯ç‚¹æ— éœ€ä¿®æ”¹ï¼Œè‡ªåŠ¨åˆ‡æ¢
@router.get("/api/stocks/{symbol}/daily")
async def get_daily_data(symbol: str):
    """è·å–æ—¥çº¿æ•°æ®ï¼ˆè‡ªåŠ¨ä½¿ç”¨é…ç½®çš„æ•°æ®æºï¼‰"""
    data = await data_source.fetch_ohlcv(
        symbol,
        start_date="2024-01-01",
        end_date="2024-12-31",
        interval="1d"
    )
    return {"data": data}
```

---

## ğŸ“‹ å…·ä½“æ”¹è¿›å»ºè®®ï¼ˆä¼˜å…ˆçº§æ’åºï¼‰

### ğŸ”´ **P0 - ç«‹å³å¤„ç†ï¼ˆ1-2å‘¨ï¼‰**

1. **å¯ç”¨CSRFä¿æŠ¤**
   - é£é™©: SEVERE
   - å½±å“: å®‰å…¨æ¼æ´
   - å·¥ä½œé‡: 1å¤©
   - æ–¹æ¡ˆ: å–æ¶ˆmain.pyä¸­CSRFä¸­é—´ä»¶æ³¨é‡Š

2. **æå‡æµ‹è¯•è¦†ç›–ç‡åˆ°30%**
   - ç°çŠ¶: 6%
   - ç›®æ ‡: 30%
   - å·¥ä½œé‡: 1å‘¨
   - æ–¹æ¡ˆ: ä¼˜å…ˆæµ‹è¯•æ ¸å¿ƒæœåŠ¡å±‚

3. **å®ç°æ•°æ®éªŒè¯å±‚**
   - é£é™©: æ•°æ®è´¨é‡é—®é¢˜
   - å½±å“: Realæ•°æ®å¯¹æ¥å¤±è´¥
   - å·¥ä½œé‡: 3å¤©
   - æ–¹æ¡ˆ: Pydanticæ¨¡å‹éªŒè¯æ‰€æœ‰å¤–éƒ¨æ•°æ®

4. **å®Œå–„é”™è¯¯å¤„ç†å’Œé™çº§**
   - é£é™©: æœåŠ¡ä¸ç¨³å®š
   - å½±å“: ç”Ÿäº§ç¯å¢ƒå¯ç”¨æ€§
   - å·¥ä½œé‡: 3å¤©
   - æ–¹æ¡ˆ: ç†”æ–­å™¨ + é™çº§ç­–ç•¥

### ğŸŸ  **P1 - é«˜ä¼˜å…ˆçº§ï¼ˆ2-4å‘¨ï¼‰**

5. **å¼•å…¥Alembicæ•°æ®åº“è¿ç§»**
   - å½±å“: schemaå˜æ›´é£é™©
   - å·¥ä½œé‡: 2å¤©
   - æ–¹æ¡ˆ: é…ç½®Alembic + ç¼–å†™åˆå§‹è¿ç§»

6. **å®ç°Repositoryæ¨¡å¼å…¨è¦†ç›–**
   - ç°çŠ¶: ä»…2ä¸ªRepository
   - ç›®æ ‡: æ‰€æœ‰æ•°æ®è®¿é—®éƒ½é€šè¿‡Repository
   - å·¥ä½œé‡: 5å¤©
   - æ–¹æ¡ˆ: é‡æ„æ•°æ®è®¿é—®å±‚

7. **é›†æˆOpenTelemetryåˆ†å¸ƒå¼è¿½è¸ª**
   - å½±å“: å¯è§‚æµ‹æ€§
   - å·¥ä½œé‡: 3å¤©
   - æ–¹æ¡ˆ: Jaeger + OpenTelemetry

8. **å®ç°è‡ªåŠ¨åŒ–å¤‡ä»½**
   - å½±å“: æ•°æ®å®‰å…¨
   - å·¥ä½œé‡: 2å¤©
   - æ–¹æ¡ˆ: å®šæ—¶ä»»åŠ¡ + OSSå­˜å‚¨

9. **APIç‰ˆæœ¬æ§åˆ¶æ ‡å‡†åŒ–**
   - å½±å“: APIå…¼å®¹æ€§
   - å·¥ä½œé‡: 2å¤©
   - æ–¹æ¡ˆ: ç»Ÿä¸€ä½¿ç”¨ `/api/v1/` å‰ç¼€

### ğŸŸ¡ **P2 - ä¸­ä¼˜å…ˆçº§ï¼ˆ1-2ä¸ªæœˆï¼‰**

10. **é›†æˆAPMå·¥å…·**
    - å·¥å…·: DataDog/New Relic
    - å·¥ä½œé‡: 3å¤©
    - æ•ˆæœ: å…¨é“¾è·¯æ€§èƒ½ç›‘æ§

11. **å®ç°è¯»å†™åˆ†ç¦»**
    - å½±å“: æ€§èƒ½æå‡
    - å·¥ä½œé‡: 5å¤©
    - æ–¹æ¡ˆ: ä¸»åº“å†™ + è¯»å‰¯æœ¬

12. **æå‡æµ‹è¯•è¦†ç›–ç‡åˆ°80%**
    - ç°çŠ¶: 30%ï¼ˆP0å®Œæˆåï¼‰
    - ç›®æ ‡: 80%
    - å·¥ä½œé‡: 2å‘¨
    - æ–¹æ¡ˆ: å•å…ƒæµ‹è¯• + é›†æˆæµ‹è¯• + E2Eæµ‹è¯•

13. **å®ç°APIå¥‘çº¦æµ‹è¯•**
    - å·¥å…·: Pact
    - å·¥ä½œé‡: 3å¤©
    - æ•ˆæœ: å‰åç«¯é›†æˆé—®é¢˜æ—©å‘ç°

14. **é…ç½®CI/CDæµæ°´çº¿**
    - å¹³å°: GitHub Actions/GitLab CI
    - å·¥ä½œé‡: 5å¤©
    - æ•ˆæœ: è‡ªåŠ¨åŒ–æµ‹è¯• + éƒ¨ç½²

### ğŸŸ¢ **P3 - ä½ä¼˜å…ˆçº§ï¼ˆ2-3ä¸ªæœˆï¼‰**

15. **å®ç°åˆ†åº“åˆ†è¡¨**
    - è§¦å‘æ¡ä»¶: å•è¡¨æ•°æ®é‡>1000ä¸‡
    - å·¥ä½œé‡: 1å‘¨
    - æ–¹æ¡ˆ: æŒ‰æ—¶é—´åˆ†è¡¨

16. **CDNé›†æˆ**
    - å½±å“: é™æ€èµ„æºåŠ é€Ÿ
    - å·¥ä½œé‡: 2å¤©
    - æ–¹æ¡ˆ: CloudFlare/é˜¿é‡Œäº‘CDN

17. **å®ç°è“ç»¿éƒ¨ç½²**
    - å½±å“: é›¶åœæœºéƒ¨ç½²
    - å·¥ä½œé‡: 3å¤©
    - æ–¹æ¡ˆ: K8s + Helm

18. **GraphQL APIæ”¯æŒ**
    - å½±å“: çµæ´»æŸ¥è¯¢
    - å·¥ä½œé‡: 1å‘¨
    - æ–¹æ¡ˆ: Strawberry GraphQL

---

## ğŸ“… Realæ•°æ®å¯¹æ¥å®æ–½è·¯çº¿å›¾

### **Phase 1: åŸºç¡€å‡†å¤‡ï¼ˆWeek 1-2ï¼‰**

#### Week 1: æ•°æ®éªŒè¯å’Œé”™è¯¯å¤„ç†
```yaml
ä»»åŠ¡:
  1. å®ç°Pydanticæ•°æ®éªŒè¯æ¨¡å‹
     - OHLCVData
     - StockInfo
     - FinancialData
     å·¥ä½œé‡: 2å¤©

  2. å®ç°DataValidator
     - æ ¼å¼éªŒè¯
     - å®Œæ•´æ€§æ£€æŸ¥
     - å¼‚å¸¸å€¼æ£€æµ‹
     å·¥ä½œé‡: 2å¤©

  3. å¢å¼ºé”™è¯¯å¤„ç†
     - ç†”æ–­å™¨é›†æˆ
     - é™çº§ç­–ç•¥
     - é‡è¯•æœºåˆ¶
     å·¥ä½œé‡: 1å¤©

éªŒæ”¶æ ‡å‡†:
  - æ‰€æœ‰å¤–éƒ¨æ•°æ®éƒ½ç»è¿‡PydanticéªŒè¯
  - éªŒè¯å¤±è´¥æœ‰è¯¦ç»†é”™è¯¯ä¿¡æ¯
  - ç†”æ–­å™¨æ­£å¸¸å·¥ä½œ
```

#### Week 2: æ•°æ®æºæ”¹é€ 
```yaml
ä»»åŠ¡:
  1. å®ç°DataSourceFactory
     - æ”¯æŒMock/Realæ•°æ®æºåˆ‡æ¢
     - é…ç½®é©±åŠ¨
     - ä¾èµ–æ³¨å…¥
     å·¥ä½œé‡: 2å¤©

  2. å®ç°DataSourceWithFallback
     - ä¸»å¤‡æ•°æ®æº
     - è‡ªåŠ¨é™çº§
     - å¥åº·æ£€æŸ¥
     å·¥ä½œé‡: 2å¤©

  3. ç¯å¢ƒå˜é‡é…ç½®
     - PRIMARY_DATA_SOURCE
     - FALLBACK_DATA_SOURCE
     - ENABLE_DATA_VALIDATION
     å·¥ä½œé‡: 1å¤©

éªŒæ”¶æ ‡å‡†:
  - å¯é€šè¿‡ç¯å¢ƒå˜é‡åˆ‡æ¢æ•°æ®æº
  - ä¸»æ•°æ®æºå¤±è´¥æ—¶è‡ªåŠ¨é™çº§
  - æ‰€æœ‰æ•°æ®æºé€šè¿‡ç»Ÿä¸€æ¥å£è®¿é—®
```

### **Phase 2: æ•°æ®åŒæ­¥ï¼ˆWeek 3-4ï¼‰**

#### Week 3: å¢é‡åŒæ­¥æœºåˆ¶
```yaml
ä»»åŠ¡:
  1. å®ç°DataSyncPipeline
     - å¢é‡æ•°æ®è·å–
     - æ•°æ®éªŒè¯
     - å­˜å‚¨å†™å…¥
     å·¥ä½œé‡: 3å¤©

  2. åŒæ­¥çŠ¶æ€ç®¡ç†
     - æœ€åæ›´æ–°æ—¶é—´è¿½è¸ª
     - åŒæ­¥æ—¥å¿—è®°å½•
     - å¤±è´¥é‡è¯•
     å·¥ä½œé‡: 2å¤©

éªŒæ”¶æ ‡å‡†:
  - æ—¥çº¿æ•°æ®å¯å¢é‡åŒæ­¥
  - åŒæ­¥å¤±è´¥æœ‰é‡è¯•æœºåˆ¶
  - åŒæ­¥çŠ¶æ€å¯æŸ¥è¯¢
```

#### Week 4: å®æ—¶æ•°æ®å¯¹æ¥
```yaml
ä»»åŠ¡:
  1. å®æ—¶æ•°æ®æµå¤„ç†
     - WebSocket/SSEæ¥æ”¶
     - æ•°æ®ç¼“å†²
     - æ‰¹é‡å†™å…¥
     å·¥ä½œé‡: 3å¤©

  2. æ•°æ®è´¨é‡ç›‘æ§
     - å®æ—¶è´¨é‡æ£€æŸ¥
     - å¼‚å¸¸å‘Šè­¦
     - æ•°æ®ä¿®å¤
     å·¥ä½œé‡: 2å¤©

éªŒæ”¶æ ‡å‡†:
  - å®æ—¶æ•°æ®å»¶è¿Ÿ<1ç§’
  - æ•°æ®è´¨é‡ç›‘æ§è¿è¡Œ
  - å¼‚å¸¸æ•°æ®æœ‰å‘Šè­¦
```

### **Phase 3: éªŒè¯å’Œä¼˜åŒ–ï¼ˆWeek 5-6ï¼‰**

#### Week 5: é›†æˆæµ‹è¯•
```yaml
ä»»åŠ¡:
  1. ç¼–å†™é›†æˆæµ‹è¯•
     - æ•°æ®æºé›†æˆæµ‹è¯•
     - æ•°æ®åŒæ­¥æµ‹è¯•
     - æ•°æ®éªŒè¯æµ‹è¯•
     å·¥ä½œé‡: 3å¤©

  2. æ€§èƒ½æµ‹è¯•
     - æ•°æ®åŒæ­¥æ€§èƒ½
     - æŸ¥è¯¢æ€§èƒ½
     - å¹¶å‘æµ‹è¯•
     å·¥ä½œé‡: 2å¤©

éªŒæ”¶æ ‡å‡†:
  - é›†æˆæµ‹è¯•è¦†ç›–ç‡>80%
  - æ€§èƒ½æ»¡è¶³SLAè¦æ±‚
  - å¹¶å‘1000ç”¨æˆ·æ— é—®é¢˜
```

#### Week 6: ç”Ÿäº§ç¯å¢ƒå‡†å¤‡
```yaml
ä»»åŠ¡:
  1. ç›‘æ§å’Œå‘Šè­¦
     - æ•°æ®æºå¥åº·ç›‘æ§
     - åŒæ­¥çŠ¶æ€ç›‘æ§
     - æ•°æ®è´¨é‡ç›‘æ§
     å·¥ä½œé‡: 2å¤©

  2. æ–‡æ¡£å®Œå–„
     - æ•°æ®æºé…ç½®æ–‡æ¡£
     - è¿ç»´æ‰‹å†Œ
     - æ•…éšœæ’æŸ¥æŒ‡å—
     å·¥ä½œé‡: 2å¤©

  3. ç°åº¦å‘å¸ƒ
     - 1% â†’ 10% â†’ 50% â†’ 100%
     - ç›‘æ§æŒ‡æ ‡
     - å›æ»šé¢„æ¡ˆ
     å·¥ä½œé‡: 1å¤©

éªŒæ”¶æ ‡å‡†:
  - ç›‘æ§ä»ªè¡¨æ¿å®Œæ•´
  - æ–‡æ¡£é½å…¨
  - ç°åº¦å‘å¸ƒæˆåŠŸ
```

### **Phase 4: å…¨é‡ä¸Šçº¿ï¼ˆWeek 7-8ï¼‰**

#### Week 7: å…¨é‡åˆ‡æ¢
```yaml
ä»»åŠ¡:
  1. Mockæ•°æ®æºé€æ­¥ä¸‹çº¿
     - åœç”¨Mockæ•°æ®
     - éªŒè¯Realæ•°æ®
     - æ¸…ç†Mockä»£ç 
     å·¥ä½œé‡: 3å¤©

  2. æ€§èƒ½è°ƒä¼˜
     - æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–
     - ç¼“å­˜ç­–ç•¥è°ƒæ•´
     - æŸ¥è¯¢ä¼˜åŒ–
     å·¥ä½œé‡: 2å¤©

éªŒæ”¶æ ‡å‡†:
  - 100% Realæ•°æ®
  - æ€§èƒ½æ»¡è¶³SLA
  - Mockä»£ç æ¸…ç†å®Œæˆ
```

#### Week 8: ç¨³å®šæ€§ä¿éšœ
```yaml
ä»»åŠ¡:
  1. 7x24å°æ—¶ç›‘æ§
     - æ•°æ®æºå¯ç”¨æ€§
     - ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
     - é”™è¯¯ç‡ç›‘æ§
     å·¥ä½œé‡: æŒç»­

  2. é—®é¢˜ä¿®å¤
     - å¿«é€Ÿå“åº”
     - æ ¹å› åˆ†æ
     - é¢„é˜²æªæ–½
     å·¥ä½œé‡: æŒ‰éœ€

  3. æ€»ç»“ä¼˜åŒ–
     - ç»éªŒæ€»ç»“
     - æµç¨‹ä¼˜åŒ–
     - æ–‡æ¡£æ›´æ–°
     å·¥ä½œé‡: 2å¤©

éªŒæ”¶æ ‡å‡†:
  - å¯ç”¨æ€§>99.9%
  - é”™è¯¯ç‡<0.1%
  - é—®é¢˜å“åº”æ—¶é—´<30åˆ†é’Ÿ
```

---

## ğŸ¯ é£é™©è¯„ä¼°å’Œç¼“è§£ç­–ç•¥

### **é«˜é£é™©é¡¹**

#### 1. **Realæ•°æ®è´¨é‡é—®é¢˜**

**é£é™©æè¿°**:
- å¤–éƒ¨æ•°æ®æºæ•°æ®è´¨é‡å‚å·®ä¸é½
- ç¼ºå¤±æ•°æ®ã€å¼‚å¸¸å€¼ã€æ ¼å¼é”™è¯¯
- å½±å“ç³»ç»Ÿç¨³å®šæ€§å’Œæ•°æ®å‡†ç¡®æ€§

**å½±å“èŒƒå›´**:
- å¸‚åœºæ•°æ®æœåŠ¡
- æŠ€æœ¯åˆ†æè®¡ç®—
- å›æµ‹ç³»ç»Ÿå‡†ç¡®æ€§

**ç¼“è§£æªæ–½**:
```python
# 1. å¤šå±‚æ•°æ®éªŒè¯
class DataQualityPipeline:
    """æ•°æ®è´¨é‡ç®¡é“"""

    async def process(self, data: pd.DataFrame):
        # ç¬¬ä¸€å±‚ï¼šæ ¼å¼éªŒè¯
        validated = await self.validate_format(data)

        # ç¬¬äºŒå±‚ï¼šå®Œæ•´æ€§æ£€æŸ¥
        complete = await self.check_completeness(validated)

        # ç¬¬ä¸‰å±‚ï¼šå¼‚å¸¸å€¼æ£€æµ‹
        cleaned = await self.detect_outliers(complete)

        # ç¬¬å››å±‚ï¼šä¸šåŠ¡è§„åˆ™éªŒè¯
        final = await self.validate_business_rules(cleaned)

        return final

# 2. æ•°æ®æºå¯¹æ¯”éªŒè¯
async def cross_validate_data(
    symbol: str,
    date: datetime
) -> pd.DataFrame:
    """å¤šæ•°æ®æºäº¤å‰éªŒè¯"""
    # ä»3ä¸ªæ•°æ®æºè·å–æ•°æ®
    data_akshare = await akshare_source.fetch(symbol, date)
    data_tushare = await tushare_source.fetch(symbol, date)
    data_baostock = await baostock_source.fetch(symbol, date)

    # å¯¹æ¯”éªŒè¯
    if not data_equals(data_akshare, data_tushare, tolerance=0.01):
        logger.warning(f"æ•°æ®æºä¸ä¸€è‡´: {symbol} {date}")
        # ä½¿ç”¨æŠ•ç¥¨æœºåˆ¶å†³å®šæœ€ç»ˆæ•°æ®
        final_data = voting_algorithm([
            data_akshare,
            data_tushare,
            data_baostock
        ])
    else:
        final_data = data_akshare

    return final_data
```

**ç›‘æ§æŒ‡æ ‡**:
- æ•°æ®å®Œæ•´æ€§: >99%
- æ•°æ®å‡†ç¡®æ€§: >99.9%
- å¼‚å¸¸å€¼æ¯”ä¾‹: <0.1%

#### 2. **æ•°æ®æºAPIé™æµ**

**é£é™©æè¿°**:
- Akshare/Tushareç­‰æ•°æ®æºæœ‰APIè°ƒç”¨é™åˆ¶
- é¢‘ç¹è¯·æ±‚å¯èƒ½è¢«é™æµæˆ–å°ç¦
- å½±å“æ•°æ®è·å–å’Œç³»ç»ŸåŠŸèƒ½

**å½±å“èŒƒå›´**:
- å®æ—¶è¡Œæƒ…æ›´æ–°
- å†å²æ•°æ®åŒæ­¥
- ç”¨æˆ·æŸ¥è¯¢å“åº”

**ç¼“è§£æªæ–½**:
```python
# 1. æ™ºèƒ½é™æµå™¨
class AdaptiveRateLimiter:
    """è‡ªé€‚åº”é™æµå™¨"""

    def __init__(self):
        self.limits = {
            'akshare': {'rate': 200, 'period': 60},     # 200æ¬¡/åˆ†é’Ÿ
            'tushare': {'rate': 500, 'period': 60},     # 500æ¬¡/åˆ†é’Ÿï¼ˆä»˜è´¹ï¼‰
            'baostock': {'rate': 100, 'period': 60}     # 100æ¬¡/åˆ†é’Ÿ
        }
        self.buckets = {}

    async def acquire(
        self,
        source: str,
        tokens: int = 1
    ) -> bool:
        """è·å–ä»¤ç‰Œï¼ˆè‡ªé€‚åº”è°ƒæ•´ï¼‰"""
        limit = self.limits.get(source)
        if not limit:
            return True

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨
        allowed = await self.token_bucket_check(
            source,
            limit['rate'],
            limit['period'],
            tokens
        )

        if not allowed:
            # é™ä½é™æµé€Ÿç‡ï¼ˆè‡ªé€‚åº”ï¼‰
            self.adjust_limit(source, decrease=True)
            logger.warning(f"è§¦å‘é™æµ: {source}")

        return allowed

    def adjust_limit(
        self,
        source: str,
        decrease: bool = False
    ):
        """åŠ¨æ€è°ƒæ•´é™æµå‚æ•°"""
        if decrease:
            # é™ä½20%
            self.limits[source]['rate'] = int(
                self.limits[source]['rate'] * 0.8
            )
        else:
            # æ¢å¤10%
            self.limits[source]['rate'] = int(
                self.limits[source]['rate'] * 1.1
            )

# 2. è¯·æ±‚åˆå¹¶å’Œæ‰¹å¤„ç†
class RequestBatcher:
    """è¯·æ±‚æ‰¹å¤„ç†å™¨"""

    async def batch_fetch(
        self,
        symbols: List[str],
        source: IDataSource
    ) -> Dict[str, pd.DataFrame]:
        """æ‰¹é‡è·å–æ•°æ®ï¼ˆå‡å°‘APIè°ƒç”¨ï¼‰"""
        # å°†å¤šä¸ªè¯·æ±‚åˆå¹¶ä¸ºä¸€ä¸ªæ‰¹é‡è¯·æ±‚
        batch_data = await source.batch_fetch(symbols)

        # æ‹†åˆ†ç»“æœ
        result = {}
        for symbol in symbols:
            result[symbol] = batch_data[
                batch_data['symbol'] == symbol
            ]

        return result

# 3. å¤šçº§ç¼“å­˜
class MultiLevelCache:
    """å¤šçº§ç¼“å­˜ç­–ç•¥"""

    def __init__(self):
        self.memory_cache = {}  # L1: å†…å­˜ç¼“å­˜
        self.redis_cache = redis_client  # L2: Redisç¼“å­˜
        self.db_cache = None  # L3: æ•°æ®åº“ç¼“å­˜

    async def get(
        self,
        key: str
    ) -> Optional[Any]:
        """å¤šçº§ç¼“å­˜æŸ¥è¯¢"""
        # L1: å†…å­˜
        if key in self.memory_cache:
            return self.memory_cache[key]

        # L2: Redis
        cached = await self.redis_cache.get(key)
        if cached:
            # å›å¡«L1
            self.memory_cache[key] = cached
            return cached

        # L3: æ•°æ®åº“
        db_data = await self.db_cache.get(key)
        if db_data:
            # å›å¡«L2å’ŒL1
            await self.redis_cache.set(key, db_data)
            self.memory_cache[key] = db_data
            return db_data

        return None
```

**ç›‘æ§æŒ‡æ ‡**:
- APIè°ƒç”¨é€Ÿç‡
- é™æµè§¦å‘æ¬¡æ•°
- ç¼“å­˜å‘½ä¸­ç‡

#### 3. **æ•°æ®åº“æ€§èƒ½ç“¶é¢ˆ**

**é£é™©æè¿°**:
- Realæ•°æ®é‡è¿œå¤§äºMockæ•°æ®
- å†™å…¥å‹åŠ›å¢åŠ 
- æŸ¥è¯¢æ€§èƒ½ä¸‹é™

**å½±å“èŒƒå›´**:
- æ•°æ®å†™å…¥å»¶è¿Ÿ
- æŸ¥è¯¢å“åº”æ—¶é—´
- ç³»ç»Ÿæ•´ä½“æ€§èƒ½

**ç¼“è§£æªæ–½**:
```python
# 1. åˆ†æ‰¹å†™å…¥
class BatchWriter:
    """æ‰¹é‡å†™å…¥å™¨"""

    async def write_batch(
        self,
        data: pd.DataFrame,
        batch_size: int = 1000
    ):
        """åˆ†æ‰¹å†™å…¥æ•°æ®åº“"""
        total_rows = len(data)
        for i in range(0, total_rows, batch_size):
            batch = data.iloc[i:i+batch_size]
            await self.db.bulk_insert(batch)

            # è¿›åº¦æ—¥å¿—
            logger.info(
                f"å†™å…¥è¿›åº¦: {i+len(batch)}/{total_rows}"
            )

            # çŸ­æš‚ä¼‘çœ ï¼Œé¿å…æ•°æ®åº“è¿‡è½½
            await asyncio.sleep(0.1)

# 2. å¼‚æ­¥å†™å…¥é˜Ÿåˆ—
class AsyncWriteQueue:
    """å¼‚æ­¥å†™å…¥é˜Ÿåˆ—"""

    def __init__(self):
        self.queue = asyncio.Queue(maxsize=10000)
        self.worker_task = None

    async def start_worker(self):
        """å¯åŠ¨åå°å†™å…¥å·¥ä½œçº¿ç¨‹"""
        self.worker_task = asyncio.create_task(
            self._write_worker()
        )

    async def _write_worker(self):
        """åå°å†™å…¥çº¿ç¨‹"""
        while True:
            # æ‰¹é‡ä»é˜Ÿåˆ—è·å–æ•°æ®
            batch = []
            for _ in range(1000):
                try:
                    item = await asyncio.wait_for(
                        self.queue.get(),
                        timeout=1.0
                    )
                    batch.append(item)
                except asyncio.TimeoutError:
                    break

            if batch:
                # æ‰¹é‡å†™å…¥
                await self.db.bulk_insert(batch)

    async def enqueue(self, data):
        """å…¥é˜Ÿæ•°æ®ï¼ˆéé˜»å¡ï¼‰"""
        await self.queue.put(data)

# 3. è¯»å†™åˆ†ç¦»
class DatabaseRouter:
    """æ•°æ®åº“è¯»å†™è·¯ç”±"""

    async def execute_write(self, query):
        """å†™æ“ä½œè·¯ç”±åˆ°ä¸»åº“"""
        return await self.master_db.execute(query)

    async def execute_read(self, query):
        """è¯»æ“ä½œè·¯ç”±åˆ°ä»åº“ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰"""
        replica = self.get_next_replica()
        return await replica.execute(query)
```

**ç›‘æ§æŒ‡æ ‡**:
- å†™å…¥TPS
- æŸ¥è¯¢å»¶è¿ŸP95
- æ•°æ®åº“è¿æ¥æ•°

---

## ğŸ“ æ€»ç»“

### **æ•´ä½“è¯„ä»·**

MyStocks APIæ¶æ„åœ¨**ä¼ä¸šçº§å®‰å…¨ã€åˆ†å±‚è®¾è®¡ã€åŒæ•°æ®åº“ç­–ç•¥ã€æ€§èƒ½ä¼˜åŒ–**æ–¹é¢è¡¨ç°ä¼˜ç§€ï¼Œè¾¾åˆ°äº†**79åˆ†ï¼ˆè‰¯å¥½ï¼Œæ¥è¿‘ä¼˜ç§€ï¼‰**çš„ç»¼åˆæ°´å¹³ã€‚Phase 4çš„å®‰å…¨ä¼˜åŒ–å°†åˆè§„æ€§ä»62%æå‡è‡³97%ï¼Œæ˜¯ä¸€ä¸ªæ˜¾è‘—çš„é‡Œç¨‹ç¢‘ã€‚

### **æ ¸å¿ƒä¼˜åŠ¿**

1. **ä¼ä¸šçº§å®‰å…¨æ¶æ„** (92/100)
2. **æ¸…æ™°çš„åˆ†å±‚è®¾è®¡** (88/100)
3. **å®Œæ•´çš„æ–‡æ¡£ä½“ç³»** (90/100)
4. **åŒæ•°æ®åº“ä¼˜åŒ–æ¶æ„** (85/100)
5. **æ€§èƒ½ä¼˜åŒ–æˆæœ** (82/100)

### **ä¸»è¦æŒ‘æˆ˜**

1. **æµ‹è¯•è¦†ç›–ç‡ä¸è¶³** (42/100) - å½“å‰6%ï¼Œç›®æ ‡80%
2. **æŠ€æœ¯å€ºåŠ¡ç§¯ç´¯** - 215é”™è¯¯ï¼Œ2,606è­¦å‘Š
3. **Realæ•°æ®å¯¹æ¥å‡†å¤‡** (75/100) - éœ€è¦æ•°æ®éªŒè¯å’ŒåŒæ­¥æ”¹é€ 

### **Realæ•°æ®å¯¹æ¥å…³é”®è·¯å¾„**

```
Week 1-2: æ•°æ®éªŒè¯å’Œé”™è¯¯å¤„ç†
Week 3-4: æ•°æ®åŒæ­¥å’Œå®æ—¶æµ
Week 5-6: é›†æˆæµ‹è¯•å’Œç”Ÿäº§å‡†å¤‡
Week 7-8: å…¨é‡åˆ‡æ¢å’Œç¨³å®šæ€§ä¿éšœ
```

### **æ ¸å¿ƒå»ºè®®**

1. **ç«‹å³å¤„ç†P0ä¼˜å…ˆçº§**ï¼ˆCSRFä¿æŠ¤ã€æµ‹è¯•è¦†ç›–ã€æ•°æ®éªŒè¯ï¼‰
2. **æŒ‰ç…§å®æ–½è·¯çº¿å›¾æ¨è¿›Realæ•°æ®å¯¹æ¥**ï¼ˆ8å‘¨è®¡åˆ’ï¼‰
3. **æŒç»­æ”¹è¿›æŠ€æœ¯å€ºåŠ¡**ï¼ˆPylinté—®é¢˜ã€ä»£ç è´¨é‡ï¼‰
4. **å¢å¼ºç›‘æ§å’Œå¯è§‚æµ‹æ€§**ï¼ˆAPMã€åˆ†å¸ƒå¼è¿½è¸ªï¼‰
5. **å®Œå–„CI/CDæµæ°´çº¿**ï¼ˆè‡ªåŠ¨åŒ–æµ‹è¯•ã€éƒ¨ç½²ï¼‰

### **æœ€ç»ˆè¯„ä¼°**

**MyStocks APIå½“å‰çŠ¶æ€**: âœ… **åŸºæœ¬å°±ç»ª**è¿›è¡ŒRealæ•°æ®å¯¹æ¥ï¼Œä½†éœ€è¦æŒ‰ç…§ä¼˜å…ˆçº§å®ŒæˆP0å’ŒP1æ”¹è¿›é¡¹ï¼Œç¡®ä¿æ•°æ®è´¨é‡ã€ç³»ç»Ÿç¨³å®šæ€§å’Œå¯è§‚æµ‹æ€§ã€‚

**æ¨èè¡ŒåŠ¨**: æŒ‰ç…§8å‘¨è·¯çº¿å›¾ï¼Œåˆ†é˜¶æ®µæ¨è¿›Realæ•°æ®å¯¹æ¥ï¼ŒåŒæ—¶å¹¶è¡Œæå‡æµ‹è¯•è¦†ç›–ç‡å’Œå®Œå–„ç›‘æ§ä½“ç³»ã€‚

---

**æŠ¥å‘Šç»“æŸ**

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**:
1. è¯„å®¡æœ¬æŠ¥å‘Šå¹¶ç¡®è®¤æ”¹è¿›ä¼˜å…ˆçº§
2. åˆ¶å®šè¯¦ç»†çš„Sprintè®¡åˆ’
3. å¯åŠ¨P0ä¼˜å…ˆçº§æ”¹è¿›é¡¹
4. åŒæ­¥å¯åŠ¨Realæ•°æ®å¯¹æ¥Phase 1

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**è¯„å®¡æ—¥æœŸ**: 2025-12-04
**è¯„å®¡äºº**: Backend Architect (Claude Code)
