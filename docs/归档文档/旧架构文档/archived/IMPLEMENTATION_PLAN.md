# MyStocks æ¶æ„ä¼˜åŒ–å®æ–½è®¡åˆ’

**åŸºäº**: `reports/architecture_analysis_revised_20251108.md`
**åˆ›å»ºæ—¥æœŸ**: 2025-11-08
**é¢„è®¡å®Œæˆ**: 4å‘¨

---

## ğŸ“‹ æ€»ä½“æ¦‚è¿°

### ä¼˜åŒ–ç›®æ ‡
- **ä»£ç å‡å°‘**: 5050è¡Œ â†’ 1550è¡Œ (69%å‡å°‘)
- **æ€§èƒ½æå‡**: å»¶è¿Ÿé™ä½47% (12-30ms â†’ 6-8ms)
- **æˆæœ¬é™ä½**: Â¥64,000/å¹´ â†’ Â¥30,000/å¹´ (53%å‰Šå‡)
- **åŠŸèƒ½ä¿ç•™**: YAMLç¾å¤‡ã€æ•°æ®å¤„ç†ã€Grafanaç›‘æ§ã€æ•°æ®åˆ†ç±»

### å››ä¸ªé˜¶æ®µ
1. **Phase 1** (1å‘¨,P0): ä¼˜åŒ–é…ç½®å’Œåˆ é™¤å†—ä½™
2. **Phase 2** (2å‘¨,P1): ç›‘æ§å’Œå¤„ç†å±‚ä¼˜åŒ–
3. **Phase 3** (1å‘¨,P1): æ¶æ„é‡æ„ä¸º4å±‚
4. **Phase 4** (3å¤©,P1): Grafanaé…ç½®å’Œæ–‡æ¡£

---

## ğŸ“… Phase 1: ä¼˜åŒ–é…ç½®å’Œåˆ é™¤å†—ä½™ (1å‘¨ - P0)

### âœ… Task 1.1: ä¼˜åŒ–YAMLé…ç½®ä¸ºç¾å¤‡ä¸“ç”¨

**å½“å‰çŠ¶æ€**:
- `table_config.yaml`: 200è¡Œ
- `core.py` ConfigDrivenTableManager: 750è¡Œ

**ç›®æ ‡çŠ¶æ€**:
- `table_config.yaml`: 100è¡Œ (-50%)
- `db_manager/disaster_recovery.py`: 300è¡Œ (-60%)

**å®æ–½æ­¥éª¤**:
```bash
# 1. ç®€åŒ–YAMLé…ç½®
cd /opt/claude/mystocks_spec
cp config/mystocks_table_config.yaml config/mystocks_table_config.yaml.backup

# ç¼–è¾‘é…ç½®,åˆ é™¤æœªä½¿ç”¨å­—æ®µ
vi config/mystocks_table_config.yaml
# ç›®æ ‡ç»“æ„:
# version: "2.0"
# disaster_recovery:
#   backup_strategy: "incremental"
#   validation_schedule: "daily"
# tables:
#   - name: xxx
#     db: postgresql/tdengine
#     type: reference/supertable
#     schema: {...}

# 2. é‡æ„ConfigDrivenTableManager
mkdir -p db_manager
cp core.py core.py.backup

# åˆ›å»ºæ–°æ–‡ä»¶
vi db_manager/disaster_recovery.py
# å®ç°DisasterRecoveryTableManagerç±»:
# - rebuild_all_tables()
# - validate_schema_consistency()
# - export_to_sql_migrations()

# 3. åˆ é™¤auto-migrationåŠŸèƒ½
# åœ¨disaster_recovery.pyä¸­ä¸å®ç°æ­¤åŠŸèƒ½

# 4. æ›´æ–°å¼•ç”¨
grep -r "ConfigDrivenTableManager" . --include="*.py" | grep -v ".backup" | awk '{print $1}' | sort -u
# é€ä¸ªæ–‡ä»¶æ›´æ–°å¼•ç”¨

# 5. æµ‹è¯•
python -c "from db_manager.disaster_recovery import DisasterRecoveryTableManager; mgr = DisasterRecoveryTableManager(); mgr.validate_schema_consistency()"
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] YAMLé…ç½®å‡å°‘åˆ°100è¡Œ
- [ ] ä»£ç ä»750è¡Œå‡å°‘åˆ°300è¡Œ
- [ ] ç¾å¤‡æ¢å¤æµ‹è¯•æˆåŠŸ (< 5åˆ†é’Ÿ)
- [ ] æ‰€æœ‰åŸæœ‰åŠŸèƒ½æ­£å¸¸å·¥ä½œ

**é¢„è®¡å·¥æ—¶**: 2å¤©

---

### âœ… Task 1.2: åˆ é™¤æœªä½¿ç”¨çš„å»é‡ç­–ç•¥

**å½“å‰çŠ¶æ€**:
- `deduplication.py`: 400è¡Œ
- 4ç§ç­–ç•¥,åªç”¨1ç§

**ç›®æ ‡çŠ¶æ€**:
- é›†æˆåˆ°`core/data_processor.py`: 100è¡Œ
- 1ç§ç­–ç•¥: FirstOccurrence

**å®æ–½æ­¥éª¤**:
```bash
# 1. æ£€æŸ¥å»é‡ç­–ç•¥ä½¿ç”¨æƒ…å†µ
grep -r "LastOccurrenceStrategy\|AverageStrategy\|CustomStrategy" . --include="*.py"

# 2. å¤‡ä»½å¹¶åˆ é™¤æ–‡ä»¶
cp deduplication.py deduplication.py.backup
rm deduplication.py

# 3. åœ¨DataProcessorä¸­é›†æˆFirstOccurrenceé€»è¾‘
vi core/data_processor.py
# æ·»åŠ æ–¹æ³•:
# def _deduplicate(self, data):
#     return data.drop_duplicates(subset=['ts_code', 'trade_date'], keep='first')

# 4. æ›´æ–°æ‰€æœ‰å¼•ç”¨
grep -r "deduplication" . --include="*.py" | grep -v ".backup"
# æ›´æ–°ä¸ºç›´æ¥è°ƒç”¨DataProcessor._deduplicate()

# 5. æµ‹è¯•
python -c "from core.data_processor import DataProcessor; import pandas as pd; df = pd.DataFrame({'ts_code': ['000001']*2, 'trade_date': ['2024-01-01']*2}); print(DataProcessor()._deduplicate(df))"
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] åˆ é™¤300è¡Œæœªä½¿ç”¨ä»£ç 
- [ ] å»é‡åŠŸèƒ½æ­£å¸¸å·¥ä½œ
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡

**é¢„è®¡å·¥æ—¶**: 1å¤©

---

### âœ… Task 1.3: åˆ é™¤å¤æ‚å‘Šè­¦ç³»ç»Ÿ

**å½“å‰çŠ¶æ€**:
- `monitoring/alerts.py`: 500è¡Œ

**ç›®æ ‡çŠ¶æ€**:
- åˆ é™¤ç‹¬ç«‹å‘Šè­¦ç³»ç»Ÿ
- ä¿ç•™åŸºç¡€Python logging

**å®æ–½æ­¥éª¤**:
```bash
# 1. æ£€æŸ¥å‘Šè­¦ç³»ç»Ÿä½¿ç”¨æƒ…å†µ
grep -r "AlertManager\|EmailAlert\|WebhookAlert" . --include="*.py"

# 2. å¤‡ä»½å¹¶åˆ é™¤
cp monitoring/alerts.py monitoring/alerts.py.backup
rm monitoring/alerts.py

# 3. ç¡®ä¿åŸºç¡€loggingä¿ç•™
vi monitoring/__init__.py
# ä¿ç•™:
# import logging
# logger = logging.getLogger('mystocks')

# 4. æ›´æ–°æ‰€æœ‰å¼•ç”¨
grep -r "from monitoring.alerts import" . --include="*.py" | grep -v ".backup"
# æ›¿æ¢ä¸ºæ ‡å‡†loggingè°ƒç”¨

# 5. å‡†å¤‡Grafanaå‘Šè­¦é…ç½®
mkdir -p config/grafana
# åç»­åœ¨Phase 4é…ç½®Grafanaå‘Šè­¦
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] åˆ é™¤500è¡Œå‘Šè­¦ä»£ç 
- [ ] åŸºç¡€æ—¥å¿—åŠŸèƒ½æ­£å¸¸
- [ ] å‡†å¤‡è¿ç§»åˆ°Grafanaå‘Šè­¦

**é¢„è®¡å·¥æ—¶**: 1å¤©

---

### âœ… Task 1.4: Phase 1 é›†æˆæµ‹è¯•å’Œæ–‡æ¡£æ›´æ–°

**å®æ–½æ­¥éª¤**:
```bash
# 1. è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest scripts/tests/ -v

# 2. æ€§èƒ½åŸºå‡†æµ‹è¯•
python -c "import time; from unified_manager import MyStocksUnifiedManager; mgr = MyStocksUnifiedManager(); start = time.time(); # æµ‹è¯•ä¿å­˜å»¶è¿Ÿ"

# 3. ä»£ç è¡Œæ•°ç»Ÿè®¡
find . -name "*.py" -path "*/db_manager/*" -o -path "*/core/*" -o -path "*/monitoring/*" | xargs wc -l

# 4. æ›´æ–°CHANGELOG.md
vi CHANGELOG.md
# æ·»åŠ :
# ## [Unreleased] - Phase 1 Complete
# ### Changed
# - ä¼˜åŒ–YAMLé…ç½®ä¸ºç¾å¤‡ä¸“ç”¨ (200è¡Œâ†’100è¡Œ)
# - é‡æ„ConfigDrivenTableManagerä¸ºDisasterRecoveryTableManager (750è¡Œâ†’300è¡Œ)
# ### Removed
# - åˆ é™¤æœªä½¿ç”¨çš„3ç§å»é‡ç­–ç•¥ (300è¡Œ)
# - åˆ é™¤å¤æ‚å‘Šè­¦ç³»ç»Ÿ (500è¡Œ)

# 5. Git commit
git add .
git commit -m "refactor(phase1): ä¼˜åŒ–é…ç½®å’Œåˆ é™¤å†—ä½™

- ä¼˜åŒ–YAMLé…ç½®ä¸ºç¾å¤‡ä¸“ç”¨ (å‡å°‘50%)
- é‡æ„ä¸ºDisasterRecoveryTableManager (å‡å°‘60%)
- åˆ é™¤æœªä½¿ç”¨å»é‡ç­–ç•¥ (åˆ é™¤300è¡Œ)
- åˆ é™¤å¤æ‚å‘Šè­¦ç³»ç»Ÿ (åˆ é™¤500è¡Œ)

æ€»è®¡: åˆ é™¤800è¡Œä»£ç ,ä»£ç ç®€åŒ–60%

Refs: reports/architecture_analysis_revised_20251108.md Phase 1"
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡
- [ ] Phase 1ç›®æ ‡è¾¾æˆ (åˆ é™¤800è¡Œ)
- [ ] CHANGELOGæ›´æ–°
- [ ] Git commitå®Œæˆ

**é¢„è®¡å·¥æ—¶**: 1å¤©

---

## ğŸ“… Phase 2: ç›‘æ§å’Œå¤„ç†å±‚ä¼˜åŒ– (2å‘¨ - P1)

### âœ… Task 2.1: åˆ›å»ºTimescaleDBç›‘æ§è¡¨

**å®æ–½æ­¥éª¤**:
```bash
# 1. åˆ›å»ºSQLè„šæœ¬
vi scripts/database/create_monitoring_tables.sql

# å†…å®¹:
cat > scripts/database/create_monitoring_tables.sql << 'EOF'
-- æŸ¥è¯¢æ€§èƒ½ç›‘æ§è¡¨
CREATE TABLE IF NOT EXISTS query_performance (
    timestamp TIMESTAMPTZ NOT NULL,
    query_type VARCHAR(50),
    table_name VARCHAR(50),
    duration_ms FLOAT,
    rows_affected INT
);
SELECT create_hypertable('query_performance', 'timestamp', if_not_exists => TRUE);

-- æ•°æ®è´¨é‡ç›‘æ§è¡¨
CREATE TABLE IF NOT EXISTS data_quality (
    timestamp TIMESTAMPTZ NOT NULL,
    table_name VARCHAR(50),
    completeness_score FLOAT,
    freshness_hours FLOAT,
    row_count BIGINT
);
SELECT create_hypertable('data_quality', 'timestamp', if_not_exists => TRUE);

-- ç³»ç»Ÿå¥åº·ç›‘æ§è¡¨
CREATE TABLE IF NOT EXISTS system_health (
    timestamp TIMESTAMPTZ NOT NULL,
    database_name VARCHAR(50),
    connection_status BOOLEAN,
    cpu_usage FLOAT,
    memory_usage FLOAT
);
SELECT create_hypertable('system_health', 'timestamp', if_not_exists => TRUE);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX IF NOT EXISTS idx_qp_timestamp ON query_performance(timestamp DESC);
CREATE INDEX IF NOT EXISTS idx_qp_duration ON query_performance(duration_ms DESC);
CREATE INDEX IF NOT EXISTS idx_dq_timestamp ON data_quality(timestamp DESC);
CREATE INDEX IF NOT EXISTS idx_sh_timestamp ON system_health(timestamp DESC);

-- è®¾ç½®æ•°æ®ä¿ç•™ç­–ç•¥ (ä¿ç•™30å¤©)
SELECT add_retention_policy('query_performance', INTERVAL '30 days', if_not_exists => TRUE);
SELECT add_retention_policy('data_quality', INTERVAL '90 days', if_not_exists => TRUE);
SELECT add_retention_policy('system_health', INTERVAL '30 days', if_not_exists => TRUE);
EOF

# 2. æ‰§è¡ŒSQL
PGPASSWORD="mystocks2025" psql -h 192.168.123.104 -U mystocks -d mystocks -f scripts/database/create_monitoring_tables.sql

# 3. éªŒè¯è¡¨åˆ›å»º
PGPASSWORD="mystocks2025" psql -h 192.168.123.104 -U mystocks -d mystocks -c "\dt query_performance data_quality system_health"
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] 3ä¸ªç›‘æ§è¡¨åˆ›å»ºæˆåŠŸ
- [ ] Hypertablesé…ç½®æ­£ç¡®
- [ ] ç´¢å¼•åˆ›å»ºæˆåŠŸ
- [ ] æ•°æ®ä¿ç•™ç­–ç•¥è®¾ç½®æ­£ç¡®

**é¢„è®¡å·¥æ—¶**: 0.5å¤©

---

### âœ… Task 2.2: å®ç°GrafanaOptimizedMonitoringç±»

**å®æ–½æ­¥éª¤**:
```bash
# 1. åˆ›å»ºæ–°ç›‘æ§æ¨¡å—
mkdir -p monitoring
vi monitoring/grafana_monitoring.py

# å®ç°å†…å®¹æ¡†æ¶:
cat > monitoring/grafana_monitoring.py << 'EOF'
"""
Grafanaä¼˜åŒ–çš„ç›‘æ§ç³»ç»Ÿ
ä½¿ç”¨PostgreSQL TimescaleDB,æ›¿ä»£ç‹¬ç«‹ç›‘æ§æ•°æ®åº“
"""
import psycopg2
from datetime import datetime
import pandas as pd
import os

class GrafanaOptimizedMonitoring:
    """Grafanaä¼˜åŒ–çš„æ—¶åºç›‘æ§"""

    def __init__(self):
        self.conn = self._get_postgres_connection()

    def _get_postgres_connection(self):
        return psycopg2.connect(
            host=os.getenv('POSTGRESQL_HOST'),
            user=os.getenv('POSTGRESQL_USER'),
            password=os.getenv('POSTGRESQL_PASSWORD'),
            database=os.getenv('POSTGRESQL_DATABASE')
        )

    def log_query(self, query_type, table_name, duration_ms, rows_affected):
        """è®°å½•æŸ¥è¯¢æ€§èƒ½"""
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO query_performance
            (timestamp, query_type, table_name, duration_ms, rows_affected)
            VALUES (NOW(), %s, %s, %s, %s)
        """, (query_type, table_name, duration_ms, rows_affected))
        self.conn.commit()

    def log_data_quality(self, table_name, completeness_score,
                        freshness_hours, row_count):
        """è®°å½•æ•°æ®è´¨é‡æŒ‡æ ‡"""
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO data_quality
            (timestamp, table_name, completeness_score, freshness_hours, row_count)
            VALUES (NOW(), %s, %s, %s, %s)
        """, (table_name, completeness_score, freshness_hours, row_count))
        self.conn.commit()

    def log_system_health(self, database_name, connection_status,
                         cpu_usage, memory_usage):
        """è®°å½•ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO system_health
            (timestamp, database_name, connection_status, cpu_usage, memory_usage)
            VALUES (NOW(), %s, %s, %s, %s)
        """, (database_name, connection_status, cpu_usage, memory_usage))
        self.conn.commit()

    def get_slow_queries(self, threshold_ms=1000, limit=10):
        """è·å–æ…¢æŸ¥è¯¢TOP N"""
        return pd.read_sql(f"""
            SELECT
                query_type,
                table_name,
                avg(duration_ms) as avg_duration,
                max(duration_ms) as max_duration,
                count(*) as query_count
            FROM query_performance
            WHERE duration_ms > {threshold_ms}
              AND timestamp > NOW() - INTERVAL '7 days'
            GROUP BY query_type, table_name
            ORDER BY avg_duration DESC
            LIMIT {limit}
        """, self.conn)

    def __del__(self):
        if hasattr(self, 'conn'):
            self.conn.close()
EOF

# 2. åˆ›å»ºç›‘æ§è£…é¥°å™¨
vi monitoring/decorators.py
# å®ç°monitor_performanceè£…é¥°å™¨

# 3. æµ‹è¯•
python -c "from monitoring.grafana_monitoring import GrafanaOptimizedMonitoring; mon = GrafanaOptimizedMonitoring(); mon.log_query('SELECT', 'stock_basic', 15.3, 100); print('Monitoring logged successfully')"
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] GrafanaOptimizedMonitoringç±»å®ç°å®Œæˆ (300è¡Œ)
- [ ] æ ¸å¿ƒæ–¹æ³•æ­£å¸¸å·¥ä½œ
- [ ] æ•°æ®æˆåŠŸå†™å…¥TimescaleDB
- [ ] è£…é¥°å™¨é›†æˆæµ‹è¯•é€šè¿‡

**é¢„è®¡å·¥æ—¶**: 2å¤©

---

### âœ… Task 2.3: åˆ é™¤æ—§ç›‘æ§ç³»ç»Ÿ

**å®æ–½æ­¥éª¤**:
```bash
# 1. å¤‡ä»½ç›‘æ§æ•°æ®
python -c "from monitoring import MonitoringDatabase; # å¯¼å‡ºå†å²æ•°æ®"

# 2. æ£€æŸ¥ä¾èµ–
grep -r "MonitoringDatabase\|monitoring.py" . --include="*.py" | grep -v ".backup"

# 3. é€æ­¥æ›¿æ¢å¼•ç”¨
# å°†æ‰€æœ‰MonitoringDatabaseå¼•ç”¨æ›¿æ¢ä¸ºGrafanaOptimizedMonitoring

# 4. åˆ é™¤æ—§æ–‡ä»¶
cp monitoring.py monitoring.py.backup
rm monitoring.py

# 5. éªŒè¯
python -c "import sys; 'monitoring' in sys.modules and print('Old monitoring removed')"
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ—§ç›‘æ§ç³»ç»Ÿä»£ç åˆ é™¤ (1700è¡Œ)
- [ ] æ‰€æœ‰å¼•ç”¨æ›´æ–°å®Œæˆ
- [ ] ç›‘æ§åŠŸèƒ½æ­£å¸¸
- [ ] æ€§èƒ½æ— æ˜æ˜¾ä¸‹é™

**é¢„è®¡å·¥æ—¶**: 2å¤©

---

### âœ… Task 2.4: ä¼˜åŒ–DataProcessor

**å®æ–½æ­¥éª¤**:
```bash
# 1. åˆ†æå½“å‰DataProcessor
wc -l data_access.py  # æŸ¥çœ‹å½“å‰è¡Œæ•°

# 2. é‡æ„ä¸ºç®€åŒ–ç‰ˆæœ¬
vi core/data_processor.py
# ä¿ç•™æ ¸å¿ƒæ–¹æ³•:
# - process()
# - _standardize()
# - _clean()
# - _deduplicate()
# - _validate()

# 3. åˆ é™¤æœªä½¿ç”¨åŠŸèƒ½
# - 6ç§éªŒè¯å™¨ â†’ 2ç§ (å®Œæ•´æ€§ã€æ–°é²œåº¦)
# - å¤æ‚é…ç½®ç®¡ç†
# - ç­–ç•¥æ¨¡å¼è¿‡åº¦æŠ½è±¡

# 4. é›†æˆå»é‡é€»è¾‘
# ç›´æ¥ä½¿ç”¨pandas drop_duplicates

# 5. æ›´æ–°æµ‹è¯•
vi scripts/tests/test_data_processor.py
pytest scripts/tests/test_data_processor.py -v
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] DataProcessorä»2000è¡Œå‡å°‘åˆ°400è¡Œ (80%å‡å°‘)
- [ ] æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸: æ¸…æ´—ã€å»é‡ã€éªŒè¯
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡
- [ ] æ€§èƒ½æ— æ˜æ˜¾ä¸‹é™

**é¢„è®¡å·¥æ—¶**: 3å¤©

---

### âœ… Task 2.5: Phase 2 é›†æˆæµ‹è¯•

**å®æ–½æ­¥éª¤**:
```bash
# 1. é›†æˆæµ‹è¯•
pytest scripts/tests/ -v

# 2. ç›‘æ§æ•°æ®éªŒè¯
python -c "from monitoring.grafana_monitoring import GrafanaOptimizedMonitoring; mon = GrafanaOptimizedMonitoring(); print(mon.get_slow_queries())"

# 3. æ€§èƒ½æµ‹è¯•
# æµ‹è¯•ç›‘æ§å¼€é”€ < 5ms

# 4. ä»£ç è¡Œæ•°ç»Ÿè®¡
find . -name "*.py" -path "*/monitoring/*" -o -path "*/core/*" | xargs wc -l

# 5. Git commit
git add .
git commit -m "refactor(phase2): ç›‘æ§å’Œå¤„ç†å±‚ä¼˜åŒ–

- åˆ›å»ºTimescaleDBç›‘æ§è¡¨ (æ›¿ä»£ç‹¬ç«‹ç›‘æ§DB)
- å®ç°GrafanaOptimizedMonitoring (300è¡Œ)
- åˆ é™¤æ—§ç›‘æ§ç³»ç»Ÿ (1700è¡Œ)
- ä¼˜åŒ–DataProcessor (2000è¡Œâ†’400è¡Œ)

æ€»è®¡: ä»£ç å‡å°‘85%,æ€§èƒ½æå‡

Refs: reports/architecture_analysis_revised_20251108.md Phase 2"
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡
- [ ] ç›‘æ§æ•°æ®æ­£å¸¸å†™å…¥
- [ ] Phase 2ç›®æ ‡è¾¾æˆ
- [ ] Git commitå®Œæˆ

**é¢„è®¡å·¥æ—¶**: 2å¤©

---

## ğŸ“… Phase 3: æ¶æ„é‡æ„ä¸º4å±‚ (1å‘¨ - P1)

### âœ… Task 3.1: å®ç°DataRouter

**å®æ–½æ­¥éª¤**:
```bash
# 1. åˆ›å»ºDataRouter
vi core/data_router.py

# å®ç°:
cat > core/data_router.py << 'EOF'
"""
æ•°æ®è·¯ç”±å™¨ - åŸºäºDataClassificationè‡ªåŠ¨è·¯ç”±
æ›¿ä»£å¤æ‚çš„StorageStrategyå±‚
"""
from core.data_classification import DataClassification
from db_manager.tdengine_access import TDengineDataAccess
from db_manager.postgres_access import PostgreSQLDataAccess

class DataRouter:
    """ç®€åŒ–çš„æ•°æ®è·¯ç”±å™¨"""

    def __init__(self):
        self.tdengine = TDengineDataAccess()
        self.postgres = PostgreSQLDataAccess()

    def save(self, data, classification: DataClassification):
        """è‡ªåŠ¨è·¯ç”±ä¿å­˜"""
        if classification.is_timeseries:
            return self.tdengine.save(data, classification.table_name)
        else:
            return self.postgres.save(data, classification.table_name)

    def load(self, classification: DataClassification, **filters):
        """è‡ªåŠ¨è·¯ç”±åŠ è½½"""
        if classification.is_timeseries:
            return self.tdengine.load(classification.table_name, **filters)
        else:
            return self.postgres.load(classification.table_name, **filters)
EOF

# 2. æµ‹è¯•
python -c "from core.data_router import DataRouter; from core.data_classification import DataClassification; router = DataRouter(); print('DataRouter created successfully')"
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] DataRouterå®ç° (100è¡Œ)
- [ ] è‡ªåŠ¨è·¯ç”±åŠŸèƒ½æ­£å¸¸
- [ ] æ€§èƒ½æµ‹è¯•é€šè¿‡

**é¢„è®¡å·¥æ—¶**: 1å¤©

---

### âœ… Task 3.2: é‡æ„UnifiedManager

**å®æ–½æ­¥éª¤**:
```bash
# 1. å¤‡ä»½å½“å‰ç‰ˆæœ¬
cp unified_manager.py unified_manager.py.backup

# 2. ç®€åŒ–è°ƒç”¨é“¾
vi unified_manager.py
# æ–°æ¶æ„:
# User â†’ UnifiedManager â†’ DataProcessor â†’ DataRouter â†’ Database

# 3. åˆ é™¤StorageStrategyå¼•ç”¨
# ç›´æ¥è°ƒç”¨DataRouter

# 4. ä¿ç•™æ ¸å¿ƒæ–¹æ³•
# - save_data_by_classification()
# - load_data_by_classification()
# - initialize_system()

# 5. æµ‹è¯•
python -c "from unified_manager import MyStocksUnifiedManager; mgr = MyStocksUnifiedManager(); mgr.initialize_system()"
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] UnifiedManagerä»500è¡Œå‡å°‘åˆ°200è¡Œ (60%å‡å°‘)
- [ ] 4å±‚æ¶æ„æ­£å¸¸å·¥ä½œ
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡

**é¢„è®¡å·¥æ—¶**: 2å¤©

---

### âœ… Task 3.3: åˆ é™¤StorageStrategyå±‚

**å®æ–½æ­¥éª¤**:
```bash
# 1. æ£€æŸ¥å¼•ç”¨
grep -r "StorageStrategy\|storage_strategy" . --include="*.py" | grep -v ".backup"

# 2. åˆ é™¤æ–‡ä»¶
cp storage_strategy.py storage_strategy.py.backup
rm storage_strategy.py

# 3. æ›´æ–°æ‰€æœ‰å¼•ç”¨
# æ›¿æ¢ä¸ºç›´æ¥ä½¿ç”¨DataRouter

# 4. æµ‹è¯•
pytest scripts/tests/ -v
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] åˆ é™¤300è¡Œä»£ç 
- [ ] æ‰€æœ‰å¼•ç”¨æ›´æ–°å®Œæˆ
- [ ] å»¶è¿Ÿæµ‹è¯•: <8ms (vs å½“å‰12-30ms)
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡

**é¢„è®¡å·¥æ—¶**: 1å¤©

---

### âœ… Task 3.4: Phase 3 æ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ–

**å®æ–½æ­¥éª¤**:
```bash
# 1. æ€§èƒ½åŸºå‡†æµ‹è¯•
python scripts/dev/performance_benchmark.py
# æµ‹è¯•æŒ‡æ ‡:
# - æ•°æ®ä¿å­˜å»¶è¿Ÿ < 8ms
# - æŸ¥è¯¢å“åº” < 50ms
# - å†…å­˜å ç”¨ < 200MB

# 2. å‹åŠ›æµ‹è¯•
# å¹¶å‘100è¯·æ±‚,æŒç»­1åˆ†é’Ÿ

# 3. ä»£ç è¡Œæ•°ç»Ÿè®¡
find . -name "*.py" | grep -v ".backup\|__pycache__\|.venv" | xargs wc -l | tail -1

# 4. Git commit
git add .
git commit -m "refactor(phase3): æ¶æ„é‡æ„ä¸º4å±‚

- å®ç°DataRouter (100è¡Œ)
- é‡æ„UnifiedManager (500è¡Œâ†’200è¡Œ)
- åˆ é™¤StorageStrategyå±‚ (300è¡Œ)
- æ€§èƒ½æå‡: å»¶è¿Ÿé™ä½47%

4å±‚æ¶æ„: User â†’ UnifiedManager â†’ DataProcessor â†’ DataRouter â†’ DB

Refs: reports/architecture_analysis_revised_20251108.md Phase 3"
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ€§èƒ½ç›®æ ‡è¾¾æˆ
- [ ] ä»£ç é‡ç›®æ ‡è¾¾æˆ (â‰¤1750è¡Œ)
- [ ] Phase 3å®Œæˆ
- [ ] Git commitå®Œæˆ

**é¢„è®¡å·¥æ—¶**: 2å¤©

---

## ğŸ“… Phase 4: Grafanaé…ç½®å’Œæ–‡æ¡£ (3å¤© - P1)

### âœ… Task 4.1: é…ç½®Grafanaæ•°æ®æº

**å®æ–½æ­¥éª¤**:
```bash
# 1. è®¿é—®Grafana (å‡è®¾å·²å®‰è£…)
# http://localhost:3000

# 2. æ·»åŠ PostgreSQLæ•°æ®æº
# Configuration â†’ Data Sources â†’ Add data source â†’ PostgreSQL
# Host: 192.168.123.104:5432
# Database: mystocks
# User: mystocks (åªè¯»ç”¨æˆ·)
# SSL Mode: require

# 3. æµ‹è¯•è¿æ¥
# æ‰§è¡Œæµ‹è¯•æŸ¥è¯¢:
# SELECT NOW()

# 4. æµ‹è¯•ç›‘æ§è¡¨æŸ¥è¯¢
# SELECT * FROM query_performance LIMIT 10
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] Grafanaæ•°æ®æºé…ç½®æˆåŠŸ
- [ ] æµ‹è¯•æŸ¥è¯¢æ­£å¸¸è¿”å›æ•°æ®
- [ ] è¿æ¥ç¨³å®š

**é¢„è®¡å·¥æ—¶**: 0.5å¤©

---

### âœ… Task 4.2: åˆ›å»ºGrafanaä»ªè¡¨ç›˜

**å®æ–½æ­¥éª¤**:
```bash
# 1. åˆ›å»ºä»ªè¡¨ç›˜JSONé…ç½®
mkdir -p config/grafana
vi config/grafana/mystocks_monitoring.json

# 2. Panel 1: æŸ¥è¯¢æ€§èƒ½è¶‹åŠ¿
# Query: SELECT timestamp as time, avg(duration_ms) as value
#        FROM query_performance
#        WHERE $__timeFilter(timestamp)
#        GROUP BY time ORDER BY time

# 3. Panel 2: æ…¢æŸ¥è¯¢TOP 10
# Query: SELECT query_type, table_name, avg(duration_ms) as avg_duration
#        FROM query_performance
#        WHERE duration_ms > 1000 AND timestamp > now() - interval '7 days'
#        GROUP BY query_type, table_name
#        ORDER BY avg_duration DESC LIMIT 10

# 4. Panel 3: æ•°æ®æ–°é²œåº¦
# Query: SELECT timestamp as time, table_name, freshness_hours
#        FROM data_quality
#        WHERE $__timeFilter(timestamp)
#        ORDER BY time

# 5. Panel 4: ç³»ç»Ÿå¥åº·
# Query: SELECT timestamp as time, database_name,
#               connection_status, cpu_usage, memory_usage
#        FROM system_health
#        WHERE $__timeFilter(timestamp)

# 6. é…ç½®å‘Šè­¦
# - æŸ¥è¯¢å»¶è¿Ÿ > 100ms
# - æ…¢æŸ¥è¯¢ > 1000ms
# - æ•°æ®æ–°é²œåº¦ > 24å°æ—¶
# - è¿æ¥å¤±è´¥ã€CPU>80%ã€å†…å­˜>90%

# 7. å¯¼å…¥ä»ªè¡¨ç›˜
# Dashboards â†’ Import â†’ ä¸Šä¼ JSONæ–‡ä»¶
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] 4ä¸ªæ ¸å¿ƒé¢æ¿åˆ›å»ºå®Œæˆ
- [ ] æ‰€æœ‰é¢æ¿æ•°æ®æ­£å¸¸æ˜¾ç¤º
- [ ] å‘Šè­¦è§„åˆ™é…ç½®å®Œæˆ
- [ ] ä»ªè¡¨ç›˜JSONé…ç½®ä¿å­˜

**é¢„è®¡å·¥æ—¶**: 1å¤©

---

### âœ… Task 4.3: åˆ›å»ºç›‘æ§æ–‡æ¡£

**å®æ–½æ­¥éª¤**:
```bash
# 1. åˆ›å»ºMONITORING.md
vi docs/guides/MONITORING.md
# å†…å®¹åŒ…æ‹¬:
# - Grafanaä»ªè¡¨ç›˜ä½¿ç”¨æŒ‡å—
# - ç›‘æ§æŒ‡æ ‡è¯´æ˜
# - å‘Šè­¦é…ç½®æŒ‡å—
# - å¸¸è§é—®é¢˜æ’æŸ¥

# 2. åˆ›å»ºDISASTER_RECOVERY.md
vi docs/guides/DISASTER_RECOVERY.md
# å†…å®¹åŒ…æ‹¬:
# - YAMLé…ç½®è¯´æ˜
# - ç¾å¤‡æ¢å¤æµç¨‹
# - è¡¨ç»“æ„éªŒè¯
# - å¸¸è§é—®é¢˜

# 3. æ›´æ–°README.md
vi README.md
# æ›´æ–°:
# - æ¶æ„è¯´æ˜ (4å±‚æ¶æ„)
# - æ€§èƒ½æŒ‡æ ‡
# - æˆæœ¬åˆ†æ

# 4. æ›´æ–°CLAUDE.md
vi CLAUDE.md
# æ›´æ–°:
# - æ–‡ä»¶ç»„ç»‡è§„åˆ™
# - å¼€å‘å‘½ä»¤
# - æ¶æ„è¯´æ˜
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] MONITORING.mdåˆ›å»ºå®Œæˆ
- [ ] DISASTER_RECOVERY.mdåˆ›å»ºå®Œæˆ
- [ ] README.mdæ›´æ–°å®Œæˆ
- [ ] CLAUDE.mdæ›´æ–°å®Œæˆ

**é¢„è®¡å·¥æ—¶**: 1.5å¤©

---

### âœ… Task 4.4: æœ€ç»ˆéªŒæ”¶å’Œäº¤ä»˜

**å®æ–½æ­¥éª¤**:
```bash
# 1. å®Œæ•´æµ‹è¯•å¥—ä»¶
pytest scripts/tests/ -v --cov=. --cov-report=html

# 2. æ€§èƒ½åŸºå‡†æµ‹è¯•
python scripts/dev/performance_benchmark.py
# éªŒè¯æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡è¾¾æ ‡

# 3. ä»£ç è´¨é‡æ£€æŸ¥
black . --check
pylint core/ db_manager/ monitoring/

# 4. æ–‡æ¡£å®Œæ•´æ€§æ£€æŸ¥
# æ£€æŸ¥æ‰€æœ‰.mdæ–‡ä»¶é“¾æ¥æœ‰æ•ˆæ€§

# 5. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
vi reports/IMPLEMENTATION_COMPLETE_20251108.md
# è®°å½•:
# - å®é™…å®Œæˆæ—¶é—´
# - ä»£ç è¡Œæ•°å˜åŒ–
# - æ€§èƒ½æ”¹è¿›æ•°æ®
# - æˆæœ¬èŠ‚çœåˆ†æ

# 6. æœ€ç»ˆGit commit
git add .
git commit -m "feat(phase4): Grafanaé…ç½®å’Œæ–‡æ¡£å®Œæˆ

- é…ç½®Grafanaæ•°æ®æºå’Œä»ªè¡¨ç›˜
- åˆ›å»ºMONITORING.mdå’ŒDISASTER_RECOVERY.md
- æ›´æ–°README.mdå’ŒCLAUDE.md
- æ‰€æœ‰æµ‹è¯•é€šè¿‡,æ€§èƒ½æŒ‡æ ‡è¾¾æ ‡

æ¶æ„ä¼˜åŒ–å®Œæˆ:
- ä»£ç : 5050è¡Œâ†’1550è¡Œ (69%å‡å°‘)
- å»¶è¿Ÿ: 12-30msâ†’6-8ms (47%æ”¹å–„)
- æˆæœ¬: Â¥64,000â†’Â¥30,000/å¹´ (53%å‰Šå‡)

Refs: reports/architecture_analysis_revised_20251108.md
Closes: æ¶æ„ä¼˜åŒ–é¡¹ç›®"

# 7. åˆ›å»ºGit tag
git tag -a v2.0-architecture-optimized -m "Architecture optimization complete - 69% code reduction, 47% performance improvement"
```

**éªŒæ”¶æ ‡å‡†**:
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡ (è¦†ç›–ç‡â‰¥80%)
- [ ] æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡è¾¾æ ‡
- [ ] æ‰€æœ‰æ–‡æ¡£å®Œæ•´
- [ ] æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆ
- [ ] Git tagåˆ›å»º

**é¢„è®¡å·¥æ—¶**: 1å¤©

---

## ğŸ“Š æˆåŠŸæ ‡å‡†æ£€æŸ¥æ¸…å•

### æŠ€æœ¯æŒ‡æ ‡
- [ ] ä»£ç å‡å°‘ â‰¥ 65% (5050è¡Œ â†’ â‰¤1750è¡Œ)
- [ ] æ•°æ®ä¿å­˜å»¶è¿Ÿ < 8ms
- [ ] æŸ¥è¯¢å“åº” < 50ms
- [ ] ç¾å¤‡æ¢å¤æ—¶é—´ < 5åˆ†é’Ÿ
- [ ] å†…å­˜å ç”¨ < 200MB

### ä¸šåŠ¡æŒ‡æ ‡
- [ ] Grafanaä»ªè¡¨ç›˜æ­£å¸¸æ˜¾ç¤ºæ‰€æœ‰ç›‘æ§æ•°æ®
- [ ] æ”¯æŒå¿«é€Ÿæ·»åŠ æ–°æ•°æ®ç±»å‹ (< 5åˆ†é’Ÿ)
- [ ] ç¾å¤‡æ¼”ç»ƒæˆåŠŸç‡ 100%
- [ ] æ•°æ®è´¨é‡ä¿æŒ â‰¥ 99.9%

### è´¨é‡æŒ‡æ ‡
- [ ] æ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡ (è¦†ç›–ç‡ â‰¥ 80%)
- [ ] æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡
- [ ] æ–‡æ¡£å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡

---

## ğŸ¯ äº¤ä»˜ç‰©æ¸…å•

### ä»£ç äº¤ä»˜ç‰©
- [x] PRDæ–‡æ¡£: `.taskmaster/docs/prd.txt`
- [x] å®æ–½è®¡åˆ’: `reports/IMPLEMENTATION_PLAN.md`
- [ ] `db_manager/disaster_recovery.py` (300è¡Œ)
- [ ] `monitoring/grafana_monitoring.py` (300è¡Œ)
- [ ] `core/data_processor.py` (400è¡Œ)
- [ ] `core/data_router.py` (100è¡Œ)
- [ ] `unified_manager.py` (é‡æ„,200è¡Œ)
- [ ] `config/mystocks_table_config.yaml` (ä¼˜åŒ–,100è¡Œ)

### é…ç½®äº¤ä»˜ç‰©
- [ ] `scripts/database/create_monitoring_tables.sql`
- [ ] `config/grafana/mystocks_monitoring.json`
- [ ] Grafanaå‘Šè­¦è§„åˆ™é…ç½®

### æ–‡æ¡£äº¤ä»˜ç‰©
- [ ] `docs/guides/MONITORING.md`
- [ ] `docs/guides/DISASTER_RECOVERY.md`
- [ ] `README.md` (æ›´æ–°)
- [ ] `CLAUDE.md` (æ›´æ–°)
- [ ] `CHANGELOG.md` (æ›´æ–°)
- [ ] `reports/IMPLEMENTATION_COMPLETE_20251108.md`

### æµ‹è¯•äº¤ä»˜ç‰©
- [ ] `scripts/tests/test_disaster_recovery.py`
- [ ] `scripts/tests/test_grafana_monitoring.py`
- [ ] `scripts/tests/test_data_processor.py` (æ›´æ–°)
- [ ] `scripts/tests/test_data_router.py`
- [ ] `scripts/tests/test_integration_4layer.py`

---

## ğŸ“š å‚è€ƒèµ„æ–™

- **åŸå§‹åˆ†ææŠ¥å‘Š**: `reports/architecture_analysis_20251108.md`
- **ä¿®è®¢åˆ†ææŠ¥å‘Š**: `reports/architecture_analysis_revised_20251108.md`
- **PRDæ–‡æ¡£**: `.taskmaster/docs/prd.txt`
- **Grafanaæ–‡æ¡£**: https://grafana.com/docs/
- **TimescaleDBæ–‡æ¡£**: https://docs.timescale.com/
- **PostgreSQLæ–‡æ¡£**: https://www.postgresql.org/docs/

---

**åˆ›å»ºæ—¥æœŸ**: 2025-11-08
**é¢„è®¡å®Œæˆæ—¶é—´**: 4å‘¨
**å½“å‰çŠ¶æ€**: Phase 0 - è§„åˆ’å®Œæˆ
**ä¸‹ä¸€æ­¥**: å¼€å§‹æ‰§è¡ŒPhase 1
