# æ•°æ®æºæ³¨å†Œä¸æ²»ç†ä¸­å¿ƒ - ä¼˜åŒ–æ–¹æ¡ˆ

> **ç‰ˆæœ¬**: v2.0
> **æ›´æ–°æ—¶é—´**: 2026-01-02
> **ä¼˜åŒ–é‡ç‚¹**: èåˆç°æœ‰æ–¹æ¡ˆä¸æ–°å¢éœ€æ±‚ï¼Œä¿ç•™Grafanaä»ªè¡¨æ¿

---

## ğŸ“Š æ–¹æ¡ˆæ€»è§ˆ

æœ¬æ–¹æ¡ˆæ—¨åœ¨å»ºç«‹ä¸€ä¸ª**é›†ä¸­å¼æ•°æ®æºæ²»ç†ä¸­å¿ƒ**ï¼Œç»Ÿä¸€ç®¡ç†æ‰€æœ‰å¤–éƒ¨æ•°æ®æºæ¥å£ï¼ˆakshare, tushare, baostock, tdxç­‰ï¼‰ï¼Œæä¾›å®Œæ•´çš„ç›‘æ§ã€è·¯ç”±ã€è°ƒåº¦å’Œæ²»ç†èƒ½åŠ›ã€‚

### æ ¸å¿ƒè®¾è®¡ç†å¿µ

1. **åŒå­˜å‚¨ç­–ç•¥**ï¼šPostgreSQLæŒä¹…åŒ– + YAMLé…ç½®æ–‡ä»¶
2. **ç»Ÿä¸€è°ƒç”¨æ¥å£**ï¼šå±è”½åº•å±‚æ•°æ®æºå·®å¼‚
3. **æ™ºèƒ½è·¯ç”±**ï¼šåŸºäºè´¨é‡è¯„åˆ†å’Œæ€§èƒ½è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ•°æ®æº
4. **å…¨ç”Ÿå‘½å‘¨æœŸç®¡ç†**ï¼šæ³¨å†Œ â†’ æµ‹è¯• â†’ ä¸Šçº¿ â†’ ç›‘æ§ â†’ ä¸‹çº¿
5. **å¯è§†åŒ–ç›‘æ§**ï¼šGrafanaä»ªè¡¨æ¿å®æ—¶å±•ç¤ºæ‰€æœ‰æ•°æ®æºçŠ¶æ€

---

## 1ï¸âƒ£ æ•°æ®æºå…ƒæ•°æ®æ³¨å†Œè¡¨ï¼ˆPostgreSQLï¼‰

### æ•°æ®åº“è¡¨ç»“æ„

```sql
-- åˆ›å»ºæ•°æ®æºæ³¨å†Œè¡¨
CREATE TABLE data_source_registry (
    id SERIAL PRIMARY KEY,

    -- åŸºç¡€ä¿¡æ¯
    source_name VARCHAR(50) NOT NULL,          -- æ•°æ®æºåç§°ï¼šakshareã€tushareã€tdxç­‰
    source_type VARCHAR(20) NOT NULL,          -- ç±»å‹ï¼šapi_library/database/crawler/file
    endpoint_name VARCHAR(100) UNIQUE NOT NULL, -- æ¥å£å”¯ä¸€æ ‡è¯†ï¼šakshare.stock_zh_a_hist

    -- è°ƒç”¨ä¿¡æ¯
    call_method VARCHAR(20),                   -- http/get/post/function_call
    endpoint_url TEXT,                         -- å®Œæ•´URLæˆ–å‡½æ•°è·¯å¾„
    parameters JSONB,                          -- å‚æ•°å®šä¹‰å’Œç¤ºä¾‹ï¼ˆJSONæ ¼å¼ï¼‰
    response_format VARCHAR(20),               -- json/csv/dataframe/protobuf

    -- åˆ†ç±»ä¸è·¯ç”±
    data_category VARCHAR(50) NOT NULL,        -- å¯¹åº”34ä¸ªåˆ†ç±»ï¼šDAILY_KLINEã€TICK_DATAç­‰
    data_classification VARCHAR(20),           -- 5å¤§åˆ†ç±»ï¼šmarket_data/reference_dataç­‰
    target_db VARCHAR(20) NOT NULL,            -- postgresql/tdengine
    table_name VARCHAR(100),                   -- å­˜å‚¨çš„ç›®æ ‡è¡¨å

    -- å…ƒæ•°æ®
    description TEXT,
    update_frequency VARCHAR(20),              -- realtime/daily/weekly/monthly
    data_quality_score FLOAT DEFAULT 8.0,      -- æ•°æ®è´¨é‡è¯„åˆ†ï¼ˆ0-10ï¼‰
    priority INT DEFAULT 10,                   -- ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
    status VARCHAR(20) DEFAULT 'active',       -- active/deprecated/maintenance/testing

    -- ç›‘æ§æŒ‡æ ‡
    last_success_time TIMESTAMP,
    last_failure_time TIMESTAMP,
    avg_response_time FLOAT DEFAULT 0,
    success_rate FLOAT DEFAULT 100.0,
    total_calls INT DEFAULT 0,
    failed_calls INT DEFAULT 0,
    consecutive_failures INT DEFAULT 0,
    quota_used INT DEFAULT 0,                  -- è°ƒç”¨é¢åº¦ä½¿ç”¨æƒ…å†µ
    quota_limit INT,                           -- è°ƒç”¨é¢åº¦ä¸Šé™

    -- æ•°æ®è´¨é‡
    data_freshness INTERVAL,                   -- æ•°æ®æ–°é²œåº¦
    last_check_time TIMESTAMP,
    health_status VARCHAR(20) DEFAULT 'unknown', -- healthy/degraded/failed/unknown

    -- ç®¡ç†ä¿¡æ¯
    owner VARCHAR(50) DEFAULT 'system',
    tags TEXT[],                               -- æ ‡ç­¾æ•°ç»„
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    -- çº¦æŸ
    CONSTRAINT chk_status CHECK (status IN ('active', 'deprecated', 'maintenance', 'testing')),
    CONSTRAINT chk_health CHECK (health_status IN ('healthy', 'degraded', 'failed', 'unknown')),
    CONSTRAINT chk_quality_score CHECK (data_quality_score >= 0 AND data_quality_score <= 10),
    CONSTRAINT chk_target_db CHECK (target_db IN ('postgresql', 'tdengine'))
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_dsr_category ON data_source_registry(data_category);
CREATE INDEX idx_dsr_status ON data_source_registry(status, health_status);
CREATE INDEX idx_dsr_source_name ON data_source_registry(source_name);
CREATE INDEX idx_dsr_quality_score ON data_source_registry(data_quality_score DESC, priority ASC);
CREATE INDEX idx_dsr_last_success ON data_source_registry(last_success_time);

-- åˆ›å»ºæ›´æ–°è§¦å‘å™¨
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER update_data_source_registry_updated_at
    BEFORE UPDATE ON data_source_registry
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

-- åˆ›å»ºè°ƒç”¨å†å²è¡¨ï¼ˆç”¨äºç›‘æ§å’Œç»Ÿè®¡ï¼‰
CREATE TABLE data_source_call_history (
    id BIGSERIAL PRIMARY KEY,
    endpoint_name VARCHAR(100) NOT NULL,
    call_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    -- è°ƒç”¨å‚æ•°
    parameters JSONB,

    -- è°ƒç”¨ç»“æœ
    success BOOLEAN NOT NULL,
    response_time FLOAT,                       -- å“åº”æ—¶é—´ï¼ˆç§’ï¼‰
    record_count INT,                          -- è¿”å›æ•°æ®æ¡æ•°

    -- é”™è¯¯ä¿¡æ¯
    error_message TEXT,
    error_type VARCHAR(100),

    -- å¤–é”®å…³è”
    CONSTRAINT fk_endpoint FOREIGN KEY (endpoint_name)
        REFERENCES data_source_registry(endpoint_name) ON DELETE CASCADE
);

CREATE INDEX idx_dsch_call_time ON data_source_call_history(call_time DESC);
CREATE INDEX idx_dsch_endpoint ON data_source_call_history(endpoint_name, call_time DESC);
CREATE INDEX idx_dsch_success ON data_source_call_history(endpoint_name, success);
```

---

## 2ï¸âƒ£ YAMLé…ç½®æ–‡ä»¶ï¼ˆåˆå§‹é…ç½®ä¸ç‰ˆæœ¬æ§åˆ¶ï¼‰

### é…ç½®æ–‡ä»¶ç»“æ„

```yaml
# config/data_sources_registry.yaml
version: "2.0"
last_updated: "2026-01-02T12:00:00"

data_sources:
  # AKShare æ•°æ®æº
  akshare_stock_daily:
    source_name: "akshare"
    source_type: "api_library"
    endpoint_name: "akshare.stock_zh_a_hist"
    call_method: "function_call"
    endpoint_url: "akshare.stock_zh_a_hist"

    data_category: "DAILY_KLINE"
    data_classification: "market_data"
    target_db: "postgresql"
    table_name: "daily_kline"

    parameters:
      symbol:
        type: "string"
        required: true
        description: "è‚¡ç¥¨ä»£ç "
        example: "000001"
      period:
        type: "string"
        required: false
        default: "daily"
        options: ["daily", "weekly", "monthly"]
      start_date:
        type: "string"
        format: "YYYYMMDD"
        required: false
        description: "å¼€å§‹æ—¥æœŸ"
      end_date:
        type: "string"
        format: "YYYYMMDD"
        required: false
        description: "ç»“æŸæ—¥æœŸ"
      adjust:
        type: "string"
        default: "qfq"
        options: ["qfq", "hfq", ""]
        description: "å¤æƒç±»å‹"

    description: "è·å–Aè‚¡æ—¥çº¿å†å²è¡Œæƒ…æ•°æ®"
    update_frequency: "daily"
    update_schedule: "16:00"
    data_quality_score: 9.5
    priority: 2
    status: "active"
    tags: ["stock", "kline", "free"]

    # æµ‹è¯•å‚æ•°ï¼ˆç”¨äºå¥åº·æ£€æŸ¥ï¼‰
    test_parameters:
      symbol: "000001"
      period: "daily"
      start_date: "20240101"
      end_date: "20240110"
      adjust: "qfq"

    # æ•°æ®æºç‰¹å®šé…ç½®
    source_config:
      module_name: "akshare"
      function_name: "stock_zh_a_hist"
      param_mapping:
        symbol: "symbol"
        period: "period"
        start_date: "start_date"
        end_date: "end_date"
        adjust: "adjust"
      quota_limit: null  # æ— é™åˆ¶

    # æ•°æ®è´¨é‡è§„åˆ™
    quality_rules:
      min_record_count: 1
      max_response_time: 10.0  # ç§’
      required_columns: ["æ—¥æœŸ", "å¼€ç›˜", "æœ€é«˜", "æœ€ä½", "æ”¶ç›˜", "æˆäº¤é‡"]

  # TuShare æ•°æ®æº
  tushare_daily:
    source_name: "tushare"
    source_type: "api_library"
    endpoint_name: "tushare.daily"
    call_method: "function_call"
    endpoint_url: "ts.pro_api.daily"

    data_category: "DAILY_KLINE"
    data_classification: "market_data"
    target_db: "postgresql"
    table_name: "daily_kline"

    parameters:
      ts_code:
        type: "string"
        required: true
        description: "è‚¡ç¥¨ä»£ç ï¼ˆTSæ ¼å¼ï¼‰"
        example: "000001.SZ"
      trade_date:
        type: "string"
        format: "YYYYMMDD"
        required: false
      start_date:
        type: "string"
        format: "YYYYMMDD"
        required: false
      end_date:
        type: "string"
        format: "YYYYMMDD"
        required: false

    description: "è·å–Aè‚¡æ—¥çº¿è¡Œæƒ…æ•°æ®ï¼ˆä¸“ä¸šç‰ˆï¼‰"
    update_frequency: "daily"
    update_schedule: "18:00"
    data_quality_score: 9.8
    priority: 1  # ä¼˜å…ˆçº§æœ€é«˜
    status: "active"
    tags: ["stock", "kline", "premium", "high-quality"]

    test_parameters:
      ts_code: "000001.SZ"
      start_date: "20240101"
      end_date: "20240110"

    source_config:
      token_env_var: "TUSHARE_TOKEN"
      module_name: "tushare"
      api_name: "daily"
      quota_limit: 5000  # æ¯æ—¥5000æ¬¡
      quota_type: "daily"

    quality_rules:
      min_record_count: 1
      max_response_time: 5.0
      required_columns: ["ts_code", "trade_date", "open", "high", "low", "close", "vol", "amount"]

  # é€šè¾¾ä¿¡æ•°æ®æº
  tdx_realtime:
    source_name: "tdx"
    source_type: "database"
    endpoint_name: "tdx.get_security_quotes"
    call_method: "tcp"
    endpoint_url: "tcp://192.168.1.100:7709"

    data_category: "REALTIME_QUOTE"
    data_classification: "market_data"
    target_db: "tdengine"
    table_name: "tick_data"

    parameters:
      symbols:
        type: "array"
        required: true
        description: "è‚¡ç¥¨ä»£ç åˆ—è¡¨"
        example: ["000001", "000002", "600000"]

    description: "é€šè¾¾ä¿¡å®æ—¶è¡Œæƒ…æ•°æ®"
    update_frequency: "realtime"
    update_schedule: "*/5 * * * *"  # æ¯5åˆ†é’Ÿ
    data_quality_score: 9.0
    priority: 3
    status: "active"
    tags: ["realtime", "tick", "low-latency"]

    test_parameters:
      symbols: ["000001"]

    source_config:
      connection_type: "tcp"
      host: "192.168.1.100"
      port: 7709
      timeout: 5

    quality_rules:
      max_response_time: 1.0  # 1ç§’å†…å“åº”
      required_fields: ["symbol", "price", "volume", "timestamp"]

  # çˆ¬è™«æ•°æ®æºç¤ºä¾‹
  eastmoney_fund_flow:
    source_name: "web_crawler"
    source_type: "crawler"
    endpoint_name: "eastmoney.fund_flow"
    call_method: "http"
    endpoint_url: "http://data.push2.eastmoney.com/api/qt/clist/get"

    data_category: "FUND_FLOW"
    data_classification: "market_data"
    target_db: "postgresql"
    table_name: "fund_flow"

    parameters:
      market:
        type: "string"
        default: "sh"
        options: ["sh", "sz"]
      date:
        type: "string"
        format: "YYYY-MM-DD"
        required: false

    description: "ä¸œæ–¹è´¢å¯Œèµ„é‡‘æµå‘æ•°æ®"
    update_frequency: "daily"
    update_schedule: "17:00"
    data_quality_score: 8.5
    priority: 5
    status: "active"
    tags: ["fund_flow", "crawler", "free"]

    test_parameters:
      market: "sh"
      date: "2024-01-10"

    source_config:
      method: "GET"
      headers:
        User-Agent: "Mozilla/5.0"
      response_format: "json"
      json_path: "$.data.diff"  # JSONè·¯å¾„æå–

    quality_rules:
      max_response_time: 15.0
      min_record_count: 10
      required_columns: ["ä»£ç ", "åç§°", "æœ€æ–°ä»·", "æ¶¨è·Œå¹…", "ä¸»åŠ›å‡€æµå…¥"]
```

---

## 3ï¸âƒ£ ç»Ÿä¸€æ•°æ®æºç®¡ç†å™¨ï¼ˆæ ¸å¿ƒç±»ï¼‰

### Pythonå®ç°

```python
# src/core/data_source_manager.py
"""
ç»Ÿä¸€æ•°æ®æºç®¡ç†å™¨ - æ‰€æœ‰å¤–éƒ¨æ•°æ®æºçš„é›†ä¸­ç®¡ç†å…¥å£
"""
import json
import time
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from pathlib import Path
import yaml
import pandas as pd

from src.storage.database import DatabaseConnectionManager
from src.monitoring import MonitoringDatabase


class DataSourceManager:
    """
    ç»Ÿä¸€æ•°æ®æºç®¡ç†å™¨

    åŠŸèƒ½ï¼š
    1. ä»æ•°æ®åº“å’ŒYAMLåŠ è½½æ•°æ®æºé…ç½®
    2. æä¾›ç»Ÿä¸€çš„æ•°æ®è·å–æ¥å£
    3. è‡ªåŠ¨å¥åº·æ£€æŸ¥å’Œè´¨é‡è¯„åˆ†
    4. è°ƒç”¨å†å²è®°å½•å’Œæ€§èƒ½ç»Ÿè®¡
    5. æ™ºèƒ½è·¯ç”±åˆ°æœ€ä½³æ•°æ®æº
    """

    def __init__(self, config_path: str = "config/data_sources_registry.yaml"):
        self.config_path = config_path
        self.registry = {}  # å†…å­˜ç¼“å­˜: {endpoint_name: {handler, metadata, cache}}
        self.db_manager = DatabaseConnectionManager()
        self.monitoring = MonitoringDatabase()

        # åŠ è½½æ‰€æœ‰æ•°æ®æºé…ç½®
        self._load_registry()

    def _load_registry(self):
        """ä»æ•°æ®åº“å’ŒYAMLåŠ è½½æ‰€æœ‰æ•°æ®æºé…ç½®"""
        print(f"[DataSourceManager] å¼€å§‹åŠ è½½æ•°æ®æºæ³¨å†Œè¡¨...")

        # 1. ä»æ•°æ®åº“åŠ è½½å·²æ³¨å†Œçš„æ•°æ®æº
        db_sources = self._load_from_database()
        print(f"[DataSourceManager] ä»æ•°æ®åº“åŠ è½½ {len(db_sources)} ä¸ªæ•°æ®æº")

        # 2. ä»YAMLåŠ è½½é…ç½®ï¼ˆç”¨äºåˆå§‹åŒ–å’Œæ›´æ–°ï¼‰
        yaml_sources = self._load_from_yaml()
        print(f"[DataSourceManager] ä»YAMLåŠ è½½ {len(yaml_sources)} ä¸ªæ•°æ®æºé…ç½®")

        # 3. åˆå¹¶é…ç½®ï¼ˆæ•°æ®åº“ä¼˜å…ˆï¼Œè¡¥å……YAMLä¸­çš„æ–°æ•°æ®æºï¼‰
        all_sources = self._merge_sources(db_sources, yaml_sources)

        # 4. åˆ›å»ºå¤„ç†å™¨å’Œç¼“å­˜
        for endpoint_name, source_config in all_sources.items():
            if source_config.get('status') != 'active':
                continue

            self.registry[endpoint_name] = {
                'handler': self._create_handler(source_config),
                'metadata': source_config,
                'cache': LRUCache(maxsize=100),
                'last_call': None,
                'call_count': 0
            }

        print(f"[DataSourceManager] æ³¨å†Œè¡¨åŠ è½½å®Œæˆï¼Œæ´»è·ƒæ•°æ®æºï¼š{len(self.registry)} ä¸ª")

    def _load_from_database(self) -> Dict:
        """ä»PostgreSQLæ•°æ®åº“åŠ è½½æ•°æ®æºæ³¨å†Œè¡¨"""
        query = """
            SELECT
                endpoint_name,
                source_name,
                source_type,
                data_category,
                target_db,
                table_name,
                parameters,
                data_quality_score,
                priority,
                status,
                health_status,
                avg_response_time,
                success_rate,
                consecutive_failures,
                last_success_time
            FROM data_source_registry
            WHERE status = 'active'
        """

        try:
            with self.db_manager.get_postgresql_connection() as conn:
                df = pd.read_sql(query, conn)

            sources = {}
            for _, row in df.iterrows():
                sources[row['endpoint_name']] = {
                    'endpoint_name': row['endpoint_name'],
                    'source_name': row['source_name'],
                    'source_type': row['source_type'],
                    'data_category': row['data_category'],
                    'target_db': row['target_db'],
                    'table_name': row['table_name'],
                    'parameters': json.loads(row['parameters']) if row['parameters'] else {},
                    'data_quality_score': row['data_quality_score'],
                    'priority': row['priority'],
                    'status': row['status'],
                    'health_status': row['health_status'],
                    'avg_response_time': row['avg_response_time'],
                    'success_rate': row['success_rate'],
                    'consecutive_failures': row['consecutive_failures'],
                    'last_success_time': row['last_success_time']
                }

            return sources
        except Exception as e:
            print(f"[DataSourceManager] ä»æ•°æ®åº“åŠ è½½å¤±è´¥: {e}")
            return {}

    def _load_from_yaml(self) -> Dict:
        """ä»YAMLé…ç½®æ–‡ä»¶åŠ è½½æ•°æ®æº"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)

            sources = {}
            for endpoint_key, source_config in config.get('data_sources', {}).items():
                # ç¡®ä¿endpoint_nameä¸€è‡´
                if 'endpoint_name' not in source_config:
                    source_config['endpoint_name'] = endpoint_key

                sources[endpoint_key] = source_config

            return sources
        except FileNotFoundError:
            print(f"[DataSourceManager] YAMLé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
            return {}
        except Exception as e:
            print(f"[DataSourceManager] ä»YAMLåŠ è½½å¤±è´¥: {e}")
            return {}

    def _merge_sources(self, db_sources: Dict, yaml_sources: Dict) -> Dict:
        """åˆå¹¶æ•°æ®åº“å’ŒYAMLé…ç½®"""
        # æ•°æ®åº“ä¼˜å…ˆï¼ˆåŒ…å«è¿è¡Œæ—¶ç»Ÿè®¡ï¼‰ï¼ŒYAMLç”¨äºè¡¥å……æ–°æ•°æ®æº
        merged = db_sources.copy()

        for endpoint_name, yaml_config in yaml_sources.items():
            if endpoint_name not in merged:
                # æ–°æ•°æ®æºï¼Œä»YAMLæ·»åŠ 
                merged[endpoint_name] = yaml_config
            else:
                # å·²å­˜åœ¨çš„æ•°æ®æºï¼Œä»…æ›´æ–°é…ç½®å­—æ®µï¼ˆä¸è¦†ç›–è¿è¡Œæ—¶ç»Ÿè®¡ï¼‰
                db_source = merged[endpoint_name]
                for key in ['parameters', 'description', 'test_parameters', 'source_config']:
                    if key in yaml_config:
                        db_source[key] = yaml_config[key]

        return merged

    def _create_handler(self, source_config: Dict):
        """å·¥å‚æ–¹æ³•åˆ›å»ºå…·ä½“æ•°æ®æºå¤„ç†å™¨"""
        from src.core.data_source_handlers import (
            AkshareHandler, TushareHandler, BaostockHandler,
            TdxHandler, WebCrawlerHandler, LocalFileHandler
        )

        source_type = source_config['source_type']
        source_name = source_config.get('source_name', '')

        handlers = {
            'akshare': AkshareHandler,
            'tushare': TushareHandler,
            'baostock': BaostockHandler,
            'tdx': TdxHandler,
            'web_crawler': WebCrawlerHandler,
            'database': TdxHandler,  # TDXä¹Ÿä½œä¸ºæ•°æ®åº“å¤„ç†
            'api_library': self._select_api_handler(source_name),
            'crawler': WebCrawlerHandler,
            'file': LocalFileHandler
        }

        handler_class = handlers.get(source_type)
        if not handler_class:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹: {source_type}")

        return handler_class(source_config)

    def _select_api_handler(self, source_name: str):
        """æ ¹æ®æ•°æ®æºåç§°é€‰æ‹©å¤„ç†å™¨"""
        from src.core.data_source_handlers import AkshareHandler, TushareHandler

        handler_map = {
            'akshare': AkshareHandler,
            'tushare': TushareHandler,
            'baostock': BaostockHandler
        }

        return handler_map.get(source_name, AkshareHandler)

    def get_data(self, endpoint_name: str, **kwargs) -> pd.DataFrame:
        """
        ç»Ÿä¸€æ•°æ®è·å–æ¥å£

        Args:
            endpoint_name: æ•°æ®æºç«¯ç‚¹åç§°ï¼ˆå¦‚ akshare.stock_zh_a_histï¼‰
            **kwargs: æ•°æ®æºç‰¹å®šå‚æ•°

        Returns:
            pandas.DataFrame: è·å–çš„æ•°æ®

        Raises:
            ValueError: æ•°æ®æºä¸å­˜åœ¨æˆ–è°ƒç”¨å¤±è´¥
        """
        # 1. æŸ¥æ‰¾æ•°æ®æº
        source = self.registry.get(endpoint_name)
        if not source:
            raise ValueError(f"æ•°æ®æº {endpoint_name} ä¸å­˜åœ¨æˆ–æœªæ¿€æ´»")

        # 2. æ£€æŸ¥å¥åº·çŠ¶æ€
        if source['metadata'].get('health_status') == 'failed':
            print(f"[WARNING] æ•°æ®æº {endpoint_name} çŠ¶æ€ä¸ºå¤±è´¥ï¼Œå°è¯•è°ƒç”¨å¯èƒ½ä¼šå¤±è´¥")

        # 3. ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._generate_cache_key(endpoint_name, kwargs)

        # 4. æ£€æŸ¥ç¼“å­˜
        if cached := source['cache'].get(cache_key):
            print(f"[DataSourceManager] ä»ç¼“å­˜è¿”å›æ•°æ®: {endpoint_name}")
            return cached

        # 5. è°ƒç”¨å…·ä½“å¤„ç†å™¨
        start_time = time.time()
        try:
            data = source['handler'].fetch(**kwargs)
            response_time = time.time() - start_time

            # 6. éªŒè¯æ•°æ®
            self._validate_data(endpoint_name, data)

            # 7. è®°å½•æˆåŠŸæŒ‡æ ‡
            self._record_success(endpoint_name, response_time, len(data) if hasattr(data, '__len__') else 0)

            # 8. æ›´æ–°ç¼“å­˜
            source['cache'][cache_key] = data
            source['last_call'] = datetime.now()
            source['call_count'] += 1

            print(f"[DataSourceManager] æˆåŠŸè·å–æ•°æ®: {endpoint_name}, è€—æ—¶: {response_time:.2f}s, è®°å½•æ•°: {len(data) if hasattr(data, '__len__') else 'N/A'}")

            return data

        except Exception as e:
            # 9. è®°å½•å¤±è´¥
            error_msg = str(e)
            self._record_failure(endpoint_name, error_msg)

            print(f"[ERROR] æ•°æ®æºè°ƒç”¨å¤±è´¥: {endpoint_name}, é”™è¯¯: {error_msg}")
            raise

    def _generate_cache_key(self, endpoint_name: str, params: Dict) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # å°†å‚æ•°è½¬æ¢ä¸ºç¨³å®šçš„å­—ç¬¦ä¸²è¡¨ç¤º
        param_str = json.dumps(params, sort_keys=True)
        return f"{endpoint_name}:{hash(param_str)}"

    def _validate_data(self, endpoint_name: str, data: Any):
        """éªŒè¯æ•°æ®è´¨é‡"""
        source = self.registry[endpoint_name]
        quality_rules = source['metadata'].get('quality_rules', {})

        if not isinstance(data, pd.DataFrame):
            return  # éDataFrameæ•°æ®è·³è¿‡éªŒè¯

        # æ£€æŸ¥æœ€å°è®°å½•æ•°
        min_count = quality_rules.get('min_record_count', 0)
        if len(data) < min_count:
            raise ValueError(f"æ•°æ®è®°å½•æ•°ä¸è¶³: {len(data)} < {min_count}")

        # æ£€æŸ¥å¿…éœ€åˆ—
        required_columns = quality_rules.get('required_columns', [])
        if required_columns:
            missing_columns = set(required_columns) - set(data.columns)
            if missing_columns:
                raise ValueError(f"ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}")

    def _record_success(self, endpoint_name: str, response_time: float, record_count: int):
        """è®°å½•æˆåŠŸè°ƒç”¨"""
        # æ›´æ–°å†…å­˜ç»Ÿè®¡
        source = self.registry[endpoint_name]
        metadata = source['metadata']

        # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
        old_avg = metadata.get('avg_response_time', 0)
        old_count = metadata.get('total_calls', 0)
        new_avg = (old_avg * old_count + response_time) / (old_count + 1)
        metadata['avg_response_time'] = new_avg
        metadata['total_calls'] = old_count + 1

        # æ›´æ–°æˆåŠŸç‡
        failed_calls = metadata.get('failed_calls', 0)
        metadata['success_rate'] = (old_count + 1 - failed_calls) / (old_count + 1) * 100

        # æ›´æ–°å¥åº·çŠ¶æ€
        if response_time > 5.0:
            metadata['health_status'] = 'degraded'
        else:
            metadata['health_status'] = 'healthy'

        metadata['consecutive_failures'] = 0
        metadata['last_success_time'] = datetime.now()

        # è®°å½•åˆ°ç›‘æ§æ•°æ®åº“
        self.monitoring.log_data_source_call(
            endpoint_name=endpoint_name,
            success=True,
            response_time=response_time,
            record_count=record_count
        )

        # è®°å½•åˆ°æ•°æ®åº“å†å²è¡¨
        self._save_call_history(endpoint_name, {}, True, response_time, record_count)

    def _record_failure(self, endpoint_name: str, error_message: str):
        """è®°å½•å¤±è´¥è°ƒç”¨"""
        source = self.registry.get(endpoint_name)
        if not source:
            return

        metadata = source['metadata']

        # æ›´æ–°å¤±è´¥ç»Ÿè®¡
        metadata['failed_calls'] = metadata.get('failed_calls', 0) + 1
        metadata['consecutive_failures'] = metadata.get('consecutive_failures', 0) + 1
        metadata['last_failure_time'] = datetime.now()

        # è¿ç»­å¤±è´¥3æ¬¡æ ‡è®°ä¸ºå¤±è´¥
        if metadata['consecutive_failures'] >= 3:
            metadata['health_status'] = 'failed'

        # è®°å½•åˆ°ç›‘æ§æ•°æ®åº“
        self.monitoring.log_data_source_call(
            endpoint_name=endpoint_name,
            success=False,
            error_message=error_message
        )

        # è®°å½•åˆ°æ•°æ®åº“å†å²è¡¨
        self._save_call_history(endpoint_name, {}, False, None, None, error_message)

    def _save_call_history(self, endpoint_name: str, parameters: Dict,
                          success: bool, response_time: Optional[float],
                          record_count: Optional[int], error_message: str = None):
        """ä¿å­˜è°ƒç”¨å†å²åˆ°æ•°æ®åº“"""
        query = """
            INSERT INTO data_source_call_history
            (endpoint_name, parameters, success, response_time, record_count, error_message)
            VALUES (%s, %s, %s, %s, %s, %s)
        """

        try:
            with self.db_manager.get_postgresql_connection() as conn:
                cursor = conn.cursor()
                cursor.execute(query, (
                    endpoint_name,
                    json.dumps(parameters),
                    success,
                    response_time,
                    record_count,
                    error_message
                ))
                conn.commit()
        except Exception as e:
            print(f"[ERROR] ä¿å­˜è°ƒç”¨å†å²å¤±è´¥: {e}")

    def find_endpoints(self, data_category: str,
                      status: str = 'active') -> List[Dict]:
        """
        æŸ¥æ‰¾æ”¯æŒç‰¹å®šæ•°æ®ç±»å‹çš„æ‰€æœ‰ç«¯ç‚¹

        Args:
            data_category: æ•°æ®åˆ†ç±»ï¼ˆå¦‚ DAILY_KLINEï¼‰
            status: çŠ¶æ€è¿‡æ»¤ï¼ˆactive, testing, deprecatedï¼‰

        Returns:
            åŒ¹é…çš„ç«¯ç‚¹åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§å’Œè´¨é‡è¯„åˆ†æ’åº
        """
        matches = []

        for endpoint_name, source in self.registry.items():
            metadata = source['metadata']

            if metadata.get('status') != status:
                continue

            if metadata.get('data_category') != data_category:
                continue

            matches.append({
                'endpoint_name': endpoint_name,
                'source_name': metadata.get('source_name'),
                'data_category': metadata.get('data_category'),
                'target_db': metadata.get('target_db'),
                'quality_score': metadata.get('data_quality_score', 0),
                'priority': metadata.get('priority', 999),
                'health_status': metadata.get('health_status', 'unknown'),
                'success_rate': metadata.get('success_rate', 100),
                'avg_response_time': metadata.get('avg_response_time', 0)
            })

        # æŒ‰ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰å’Œè´¨é‡è¯„åˆ†æ’åº
        matches.sort(key=lambda x: (x['priority'], -x['quality_score']))

        return matches

    def get_best_endpoint(self, data_category: str) -> Optional[Dict]:
        """
        è·å–æœ€ä½³æ•°æ®ç«¯ç‚¹ï¼ˆæ™ºèƒ½è·¯ç”±ï¼‰

        é€‰æ‹©æ ‡å‡†ï¼š
        1. çŠ¶æ€ä¸ºactive
        2. å¥åº·çŠ¶æ€ä¸ºhealthyæˆ–degradedï¼ˆæ’é™¤failedï¼‰
        3. æŒ‰ä¼˜å…ˆçº§æ’åº
        4. åŒä¼˜å…ˆçº§æŒ‰è´¨é‡è¯„åˆ†æ’åº
        """
        endpoints = self.find_endpoints(data_category)

        # è¿‡æ»¤æ‰å¥åº·çŠ¶æ€ä¸ºfailedçš„ç«¯ç‚¹
        healthy_endpoints = [
            ep for ep in endpoints
            if ep.get('health_status') != 'failed'
        ]

        return healthy_endpoints[0] if healthy_endpoints else None

    def search_sources(self, keyword: str = None,
                      data_category: str = None,
                      source_name: str = None,
                      tags: List[str] = None) -> List[Dict]:
        """
        æœç´¢å¯ç”¨æ•°æ®æº

        Args:
            keyword: å…³é”®è¯æœç´¢ï¼ˆåŒ¹é…endpoint_nameæˆ–descriptionï¼‰
            data_category: æ•°æ®åˆ†ç±»è¿‡æ»¤
            source_name: æ•°æ®æºåç§°è¿‡æ»¤
            tags: æ ‡ç­¾è¿‡æ»¤

        Returns:
            åŒ¹é…çš„æ•°æ®æºåˆ—è¡¨
        """
        results = []

        for endpoint_name, source in self.registry.items():
            metadata = source['metadata']

            # å…³é”®è¯åŒ¹é…
            if keyword:
                text = f"{endpoint_name} {metadata.get('description', '')}"
                if keyword.lower() not in text.lower():
                    continue

            # æ•°æ®åˆ†ç±»è¿‡æ»¤
            if data_category and metadata.get('data_category') != data_category:
                continue

            # æ•°æ®æºåç§°è¿‡æ»¤
            if source_name and metadata.get('source_name') != source_name:
                continue

            # æ ‡ç­¾è¿‡æ»¤
            if tags:
                source_tags = metadata.get('tags', [])
                if not any(tag in source_tags for tag in tags):
                    continue

            results.append({
                'endpoint_name': endpoint_name,
                'metadata': metadata
            })

        return results

    def list_all_endpoints(self) -> pd.DataFrame:
        """åˆ—å‡ºæ‰€æœ‰å·²æ³¨å†Œçš„æ•°æ®ç«¯ç‚¹ï¼ˆä¾¿äºæŸ¥çœ‹å’Œç®¡ç†ï¼‰"""
        data = []

        for endpoint_name, source in self.registry.items():
            metadata = source['metadata']

            data.append({
                'æ•°æ®æº': metadata.get('source_name'),
                'ç«¯ç‚¹åç§°': endpoint_name,
                'æ•°æ®åˆ†ç±»': metadata.get('data_category'),
                'ç›®æ ‡æ•°æ®åº“': metadata.get('target_db'),
                'ç›®æ ‡è¡¨': metadata.get('table_name'),
                'æ›´æ–°é¢‘ç‡': metadata.get('update_frequency'),
                'è´¨é‡è¯„åˆ†': metadata.get('data_quality_score'),
                'ä¼˜å…ˆçº§': metadata.get('priority'),
                'çŠ¶æ€': metadata.get('status'),
                'å¥åº·çŠ¶æ€': metadata.get('health_status'),
                'æˆåŠŸç‡': f"{metadata.get('success_rate', 100):.1f}%",
                'å¹³å‡å“åº”æ—¶é—´': f"{metadata.get('avg_response_time', 0):.2f}s",
                'è°ƒç”¨æ¬¡æ•°': metadata.get('total_calls', 0),
                'æœ€åæˆåŠŸ': metadata.get('last_success_time')
            })

        return pd.DataFrame(data)

    def get_endpoint_details(self, endpoint_name: str) -> Optional[Dict]:
        """è·å–ç«¯ç‚¹è¯¦ç»†ä¿¡æ¯"""
        source = self.registry.get(endpoint_name)
        if not source:
            return None

        return {
            'endpoint_name': endpoint_name,
            'metadata': source['metadata'],
            'call_count': source['call_count'],
            'last_call': source['last_call']
        }

    def health_check(self, endpoint_name: str = None) -> Dict:
        """
        æ‰§è¡Œå¥åº·æ£€æŸ¥

        Args:
            endpoint_name: æŒ‡å®šç«¯ç‚¹åç§°ï¼ŒNoneè¡¨ç¤ºæ£€æŸ¥æ‰€æœ‰

        Returns:
            å¥åº·æ£€æŸ¥ç»“æœ
        """
        if endpoint_name:
            return self._check_single_endpoint(endpoint_name)
        else:
            return self._check_all_endpoints()

    def _check_single_endpoint(self, endpoint_name: str) -> Dict:
        """æ£€æŸ¥å•ä¸ªç«¯ç‚¹"""
        source = self.registry.get(endpoint_name)
        if not source:
            return {
                'endpoint_name': endpoint_name,
                'status': 'not_found'
            }

        metadata = source['metadata']
        test_params = metadata.get('test_parameters', {})

        try:
            # ä½¿ç”¨æµ‹è¯•å‚æ•°è°ƒç”¨
            data = source['handler'].fetch(**test_params)

            # éªŒè¯è¿”å›æ•°æ®
            self._validate_data(endpoint_name, data)

            return {
                'endpoint_name': endpoint_name,
                'status': 'healthy',
                'response_time': metadata.get('avg_response_time', 0),
                'sample_record': data.head(1).to_dict() if hasattr(data, 'head') else str(data)[:100]
            }
        except Exception as e:
            return {
                'endpoint_name': endpoint_name,
                'status': 'unhealthy',
                'error': str(e)
            }

    def _check_all_endpoints(self) -> Dict:
        """æ£€æŸ¥æ‰€æœ‰ç«¯ç‚¹"""
        results = {}

        for endpoint_name in self.registry.keys():
            results[endpoint_name] = self._check_single_endpoint(endpoint_name)

        return {
            'total': len(results),
            'healthy': sum(1 for r in results.values() if r['status'] == 'healthy'),
            'unhealthy': sum(1 for r in results.values() if r['status'] == 'unhealthy'),
            'details': results
        }


class LRUCache:
    """ç®€å•çš„LRUç¼“å­˜å®ç°"""
    def __init__(self, maxsize=100):
        from collections import OrderedDict
        self.cache = OrderedDict()
        self.maxsize = maxsize

    def get(self, key):
        if key in self.cache:
            self.cache.move_to_end(key)
            return self.cache[key]
        return None

    def __setitem__(self, key, value):
        if key in self.cache:
            self.cache.move_to_end(key)
        self.cache[key] = value
        if len(self.cache) > self.maxsize:
            self.cache.popitem(last=False)
```

---

## 4ï¸âƒ£ æ•°æ®æºå¤„ç†å™¨å®ç°

```python
# src/core/data_source_handlers.py
"""
å…·ä½“æ•°æ®æºå¤„ç†å™¨å®ç°
"""
import importlib
from typing import Dict, Any
import pandas as pd


class BaseDataSourceHandler:
    """æ•°æ®æºå¤„ç†å™¨åŸºç±»"""

    def __init__(self, config: Dict):
        self.config = config
        self.endpoint_name = config['endpoint_name']
        self.source_name = config.get('source_name', '')

    def fetch(self, **kwargs) -> pd.DataFrame:
        """è·å–æ•°æ®ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰"""
        raise NotImplementedError

    def _map_arguments(self, args: Dict) -> Dict:
        """å‚æ•°æ˜ å°„"""
        param_mapping = self.config.get('source_config', {}).get('param_mapping', {})
        return {param_mapping.get(k, k): v for k, v in args.items()}


class AkshareHandler(BaseDataSourceHandler):
    """AKShareæ•°æ®æºå¤„ç†å™¨"""

    def __init__(self, config: Dict):
        super().__init__(config)
        self.module = importlib.import_module('akshare')
        self.function_name = config['source_config']['function_name']

    def fetch(self, **kwargs) -> pd.DataFrame:
        # å‚æ•°æ˜ å°„
        mapped_args = self._map_arguments(kwargs)

        # åŠ¨æ€è°ƒç”¨akshareå‡½æ•°
        func = getattr(self.module, self.function_name)
        return func(**mapped_args)


class TushareHandler(BaseDataSourceHandler):
    """TuShareæ•°æ®æºå¤„ç†å™¨ï¼ˆå¸¦tokenç®¡ç†ï¼‰"""

    def __init__(self, config: Dict):
        super().__init__(config)
        import tushare as ts

        token_env_var = config['source_config'].get('token_env_var')
        if token_env_var:
            import os
            token = os.getenv(token_env_var)
            if not token:
                raise ValueError(f"ç¯å¢ƒå˜é‡ {token_env_var} æœªè®¾ç½®")
        else:
            token = config['source_config'].get('token')

        self.pro = ts.pro_api(token)
        self.api_name = config['source_config']['api_name']

    def fetch(self, **kwargs) -> pd.DataFrame:
        # è°ƒç”¨tushare API
        return self.pro.query(
            self.api_name,
            **kwargs,
            fields=self.config.get('source_config', {}).get('fields')
        )


class BaostockHandler(BaseDataSourceHandler):
    """BaoStockæ•°æ®æºå¤„ç†å™¨"""

    def __init__(self, config: Dict):
        super().__init__(config)
        import baostock as bs
        self.bs = bs
        self.login()

    def login(self):
        """ç™»å½•baostock"""
        lg = self.bs.login()
        if lg.error_code != '0':
            raise ConnectionError(f"BaoStockç™»å½•å¤±è´¥: {lg.error_msg}")

    def fetch(self, **kwargs) -> pd.DataFrame:
        # è°ƒç”¨baostock query
        fields = self.config.get('source_config', {}).get('fields', '')
        return self.bs.query_stock_basic(**kwargs).get_data()

    def __del__(self):
        """é€€å‡ºç™»å½•"""
        try:
            self.bs.logout()
        except:
            pass


class TdxHandler(BaseDataSourceHandler):
    """é€šè¾¾ä¿¡æ•°æ®æºå¤„ç†å™¨ï¼ˆç›´è¿ï¼‰"""

    def __init__(self, config: Dict):
        super().__init__(config)
        from pytdx.hq import TdxHq_API
        self.api = TdxHq_API()

        conn_config = config.get('source_config', {})
        self.host = conn_config.get('host', '119.147.212.81')
        self.port = conn_config.get('port', 7709)

    def fetch(self, **kwargs) -> pd.DataFrame:
        # è¿æ¥é€šè¾¾ä¿¡
        if not self.api.connected:
            self.api.connect(self.host, self.port)

        # è°ƒç”¨ç›¸åº”æ¥å£
        symbols = kwargs.get('symbols', [])
        if not symbols:
            raise ValueError("symbolså‚æ•°ä¸èƒ½ä¸ºç©º")

        # è·å–å®æ—¶è¡Œæƒ…
        data = self.api.get_security_quotes(
            [(1, symbol) for symbol in symbols]  # 1è¡¨ç¤ºæ·±åœ³å¸‚åœº
        )

        return pd.DataFrame(data)


class WebCrawlerHandler(BaseDataSourceHandler):
    """çˆ¬è™«æ•°æ®æºå¤„ç†å™¨"""

    def __init__(self, config: Dict):
        super().__init__(config)
        import requests
        self.requests = requests
        self.endpoint_url = config['endpoint_url']
        self.method = config['source_config'].get('method', 'GET')
        self.headers = config['source_config'].get('headers', {})
        self.response_format = config['source_config'].get('response_format', 'json')
        self.json_path = config['source_config'].get('json_path')

    def fetch(self, **kwargs) -> pd.DataFrame:
        # æ„å»ºè¯·æ±‚
        url = self.endpoint_url
        params = {k: v for k, v in kwargs.items() if v is not None}

        # å‘é€è¯·æ±‚
        if self.method.upper() == 'GET':
            response = self.requests.get(url, params=params, headers=self.headers)
        else:
            response = self.requests.post(url, json=params, headers=self.headers)

        response.raise_for_status()

        # è§£æå“åº”
        if self.response_format == 'json':
            data = response.json()

            # JSONè·¯å¾„æå–
            if self.json_path:
                # ä½¿ç”¨parse_json_pathæå–æ•°æ®
                data = self._parse_json_path(data, self.json_path)

            return pd.DataFrame(data)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å“åº”æ ¼å¼: {self.response_format}")

    def _parse_json_path(self, data: Any, path: str) -> Any:
        """ç®€å•çš„JSONè·¯å¾„è§£æ"""
        # æ”¯æŒç±»ä¼¼ $.data.diff çš„è·¯å¾„
        if path.startswith('$.'):
            parts = path[2:].split('.')
            for part in parts:
                if isinstance(data, dict):
                    data = data.get(part)
                elif isinstance(data, list) and part.isdigit():
                    data = data[int(part)]
                else:
                    raise ValueError(f"æ— æ³•è§£æJSONè·¯å¾„: {path}")
            return data
        return data


class LocalFileHandler(BaseDataSourceHandler):
    """æœ¬åœ°æ–‡ä»¶æ•°æ®æºå¤„ç†å™¨"""

    def __init__(self, config: Dict):
        super().__init__(config)
        self.file_path = config['endpoint_url']
        self.file_format = config.get('response_format', 'csv')

    def fetch(self, **kwargs) -> pd.DataFrame:
        if self.file_format == 'csv':
            return pd.read_csv(self.file_path, **kwargs)
        elif self.file_format == 'excel':
            return pd.read_excel(self.file_path, **kwargs)
        elif self.file_format == 'json':
            return pd.read_json(self.file_path, **kwargs)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {self.file_format}")
```

---

## 5ï¸âƒ£ FastAPIç®¡ç†æ¥å£

```python
# web/backend/app/api/data_sources.py
"""
æ•°æ®æºç®¡ç†API
"""
from fastapi import APIRouter, Query, HTTPException
from typing import Optional, List
from pydantic import BaseModel
import pandas as pd

from src.core.data_source_manager import DataSourceManager

router = APIRouter(prefix="/api/v1/data-sources", tags=["æ•°æ®æºç®¡ç†"])

# å…¨å±€å•ä¾‹
_manager = None


def get_manager():
    """è·å–æ•°æ®æºç®¡ç†å™¨å•ä¾‹"""
    global _manager
    if _manager is None:
        _manager = DataSourceManager()
    return _manager


class DataSourceSearchRequest(BaseModel):
    keyword: Optional[str] = None
    data_category: Optional[str] = None
    source_name: Optional[str] = None
    tags: Optional[List[str]] = None


@router.get("/list")
async def list_all_sources():
    """
    åˆ—å‡ºæ‰€æœ‰å·²æ³¨å†Œçš„æ•°æ®æº

    è¿”å›å®Œæ•´çš„æ•°æ®æºæ¸…å•ï¼ŒåŒ…æ‹¬ï¼š
    - æ•°æ®æºåç§°
    - ç«¯ç‚¹åç§°
    - æ•°æ®åˆ†ç±»
    - ç›®æ ‡æ•°æ®åº“å’Œè¡¨
    - è´¨é‡è¯„åˆ†å’Œä¼˜å…ˆçº§
    - å¥åº·çŠ¶æ€å’Œæ€§èƒ½æŒ‡æ ‡
    """
    manager = get_manager()
    df = manager.list_all_endpoints()

    return {
        "total": len(df),
        "sources": df.to_dict(orient='records')
    }


@router.get("/find")
async def find_data_sources(
    data_category: str = Query(..., description="æ•°æ®åˆ†ç±»ï¼Œå¦‚ DAILY_KLINE"),
    status: str = Query("active", description="çŠ¶æ€è¿‡æ»¤")
):
    """
    æŸ¥æ‰¾æ”¯æŒç‰¹å®šæ•°æ®ç±»å‹çš„æ•°æ®æº

    ç¤ºä¾‹:
    GET /api/v1/data-sources/find?data_category=DAILY_KLINE&status=active

    è¿”å›æŒ‰ä¼˜å…ˆçº§å’Œè´¨é‡æ’åºçš„å¯ç”¨æ•°æ®æºåˆ—è¡¨
    """
    manager = get_manager()
    endpoints = manager.find_endpoints(data_category, status)

    return {
        "data_category": data_category,
        "found": len(endpoints),
        "sources": endpoints
    }


@router.get("/best")
async def get_best_source(
    data_category: str = Query(..., description="æ•°æ®åˆ†ç±»")
):
    """
    è·å–æœ€ä½³æ•°æ®æºï¼ˆæ™ºèƒ½è·¯ç”±ï¼‰

    è‡ªåŠ¨é€‰æ‹©ä¼˜å…ˆçº§æœ€é«˜ã€è´¨é‡æœ€å¥½çš„å¥åº·æ•°æ®æº

    ç¤ºä¾‹:
    GET /api/v1/data-sources/best?data_category=DAILY_KLINE
    """
    manager = get_manager()
    endpoint = manager.get_best_endpoint(data_category)

    if not endpoint:
        raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°å¯ç”¨çš„ {data_category} æ•°æ®æº")

    return endpoint


@router.post("/search")
async def search_sources(request: DataSourceSearchRequest):
    """
    é«˜çº§æœç´¢æ•°æ®æº

    æ”¯æŒå…³é”®è¯ã€åˆ†ç±»ã€æ ‡ç­¾ç­‰å¤šç»´åº¦æœç´¢
    """
    manager = get_manager()
    results = manager.search_sources(
        keyword=request.keyword,
        data_category=request.data_category,
        source_name=request.source_name,
        tags=request.tags
    )

    return {
        "total": len(results),
        "results": results
    }


@router.get("/details/{endpoint_name}")
async def get_endpoint_details(endpoint_name: str):
    """
    è·å–æ•°æ®ç«¯ç‚¹è¯¦ç»†ä¿¡æ¯

    åŒ…æ‹¬å®Œæ•´çš„é…ç½®ã€å‚æ•°å®šä¹‰ã€è´¨é‡è§„åˆ™ç­‰
    """
    manager = get_manager()
    details = manager.get_endpoint_details(endpoint_name)

    if not details:
        raise HTTPException(status_code=404, detail=f"æ•°æ®æº {endpoint_name} ä¸å­˜åœ¨")

    return details


@router.post("/health-check")
async def health_check(
    endpoint_name: Optional[str] = Query(None, description="æŒ‡å®šç«¯ç‚¹åç§°ï¼Œä¸æŒ‡å®šåˆ™æ£€æŸ¥æ‰€æœ‰")
):
    """
    æ‰§è¡Œå¥åº·æ£€æŸ¥

    å¯¹æ•°æ®æºè¿›è¡Œå®é™…è°ƒç”¨æµ‹è¯•ï¼ŒéªŒè¯å¯ç”¨æ€§

    ç¤ºä¾‹:
    POST /api/v1/data-sources/health-check?endpoint_name=akshare.stock_zh_a_hist
    """
    manager = get_manager()
    result = manager.health_check(endpoint_name)

    return result


@router.get("/call-history/{endpoint_name}")
async def get_call_history(
    endpoint_name: str,
    limit: int = Query(100, description="è¿”å›è®°å½•æ•°"),
    success_only: bool = Query(False, description="ä»…æ˜¾ç¤ºæˆåŠŸè®°å½•")
):
    """
    è·å–æ•°æ®æºè°ƒç”¨å†å²

    ç”¨äºç›‘æ§å’Œåˆ†ææ•°æ®æºä½¿ç”¨æƒ…å†µ
    """
    manager = get_manager()

    # æŸ¥è¯¢æ•°æ®åº“
    query = """
        SELECT * FROM data_source_call_history
        WHERE endpoint_name = %s
    """
    params = [endpoint_name]

    if success_only:
        query += " AND success = TRUE"

    query += " ORDER BY call_time DESC LIMIT %s"
    params.append(limit)

    with manager.db_manager.get_postgresql_connection() as conn:
        df = pd.read_sql(query, conn, params=params)

    return {
        "endpoint_name": endpoint_name,
        "total_calls": len(df),
        "history": df.to_dict(orient='records')
    }


@router.get("/statistics/summary")
async def get_statistics_summary():
    """
    è·å–æ•°æ®æºç»Ÿè®¡æ‘˜è¦

    åŒ…æ‹¬ï¼š
    - æ€»æ•°æ®æºæ•°é‡
    - å¥åº·æ•°æ®æºæ•°é‡
    - å¹³å‡æˆåŠŸç‡
    - è°ƒç”¨æ¬¡æ•°ç»Ÿè®¡
    - æ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡
    """
    manager = get_manager()

    query = """
        SELECT
            source_name,
            COUNT(*) as endpoint_count,
            AVG(success_rate) as avg_success_rate,
            AVG(avg_response_time) as avg_response_time,
            SUM(total_calls) as total_calls,
            SUM(CASE WHEN health_status = 'healthy' THEN 1 ELSE 0 END) as healthy_count,
            SUM(CASE WHEN health_status = 'failed' THEN 1 ELSE 0 END) as failed_count
        FROM data_source_registry
        WHERE status = 'active'
        GROUP BY source_name
    """

    with manager.db_manager.get_postgresql_connection() as conn:
        df = pd.read_sql(query, conn)

    return {
        "summary": {
            "total_endpoints": df['endpoint_count'].sum(),
            "total_calls": df['total_calls'].sum(),
            "overall_success_rate": df['avg_success_rate'].mean(),
            "healthy_endpoints": df['healthy_count'].sum(),
            "failed_endpoints": df['failed_count'].sum()
        },
        "by_source": df.to_dict(orient='records')
    }


@router.post("/sync-from-yaml")
async def sync_from_yaml():
    """
    ä»YAMLé…ç½®åŒæ­¥æ•°æ®æºåˆ°æ•°æ®åº“

    ç”¨äºåˆå§‹åŒ–æˆ–æ›´æ–°æ•°æ®æºæ³¨å†Œè¡¨
    """
    manager = get_manager()

    try:
        # é‡æ–°åŠ è½½æ³¨å†Œè¡¨
        manager._load_registry()

        return {
            "status": "success",
            "message": "æ•°æ®æºé…ç½®åŒæ­¥æˆåŠŸ",
            "active_sources": len(manager.registry)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åŒæ­¥å¤±è´¥: {str(e)}")
```

---

## 6ï¸âƒ£ Grafanaæ•°æ®æºç®¡ç†ä»ªè¡¨æ¿

### Prometheusç›‘æ§æŒ‡æ ‡

```python
# src/monitoring/data_source_metrics.py
"""
æ•°æ®æºç›‘æ§æŒ‡æ ‡å¯¼å‡ºå™¨
"""
from prometheus_client import Gauge, Counter, Histogram

# å®šä¹‰æ•°æ®æºç›‘æ§æŒ‡æ ‡
data_source_up = Gauge(
    'data_source_up',
    'æ•°æ®æºæ˜¯å¦å¯ç”¨ï¼ˆ1=å¯ç”¨ï¼Œ0=ä¸å¯ç”¨ï¼‰',
    ['endpoint_name', 'source_name', 'data_category']
)

data_source_response_time = Histogram(
    'data_source_response_time_seconds',
    'æ•°æ®æºå“åº”æ—¶é—´',
    ['endpoint_name', 'source_name'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]
)

data_source_calls_total = Counter(
    'data_source_calls_total',
    'æ•°æ®æºè°ƒç”¨æ€»æ¬¡æ•°',
    ['endpoint_name', 'source_name', 'status']  # status=success/failure
)

data_source_success_rate = Gauge(
    'data_source_success_rate',
    'æ•°æ®æºæˆåŠŸç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰',
    ['endpoint_name', 'source_name']
)

data_source_record_count = Histogram(
    'data_source_record_count',
    'æ•°æ®æºè¿”å›è®°å½•æ•°',
    ['endpoint_name', 'source_name'],
    buckets=[1, 10, 100, 1000, 10000]
)

data_source_quality_score = Gauge(
    'data_source_quality_score',
    'æ•°æ®æºè´¨é‡è¯„åˆ†',
    ['endpoint_name', 'source_name']
)

data_source_health_status = Gauge(
    'data_source_health_status',
    'æ•°æ®æºå¥åº·çŠ¶æ€ï¼ˆ3=healthyï¼Œ2=degradedï¼Œ1=failedï¼‰',
    ['endpoint_name', 'source_name']
)


class DataSourceMetricsExporter:
    """æ•°æ®æºæŒ‡æ ‡å¯¼å‡ºå™¨"""

    @staticmethod
    def update_call_metrics(endpoint_name: str, source_name: str,
                           success: bool, response_time: float,
                           record_count: int):
        """æ›´æ–°è°ƒç”¨æŒ‡æ ‡"""
        # è°ƒç”¨æ¬¡æ•°
        status = 'success' if success else 'failure'
        data_source_calls_total.labels(
            endpoint_name=endpoint_name,
            source_name=source_name,
            status=status
        ).inc()

        # å“åº”æ—¶é—´
        if response_time is not None:
            data_source_response_time.labels(
                endpoint_name=endpoint_name,
                source_name=source_name
            ).observe(response_time)

        # è®°å½•æ•°
        if record_count is not None:
            data_source_record_count.labels(
                endpoint_name=endpoint_name,
                source_name=source_name
            ).observe(record_count)

    @staticmethod
    def update_health_metrics(endpoint_name: str, source_name: str,
                             health_status: str, quality_score: float,
                             success_rate: float):
        """æ›´æ–°å¥åº·æŒ‡æ ‡"""
        # å¯ç”¨æ€§
        is_up = 1 if health_status != 'failed' else 0
        data_source_up.labels(
            endpoint_name=endpoint_name,
            source_name=source_name,
            data_category=''  # éœ€è¦ä»é…ç½®è·å–
        ).set(is_up)

        # å¥åº·çŠ¶æ€
        status_map = {'healthy': 3, 'degraded': 2, 'failed': 1, 'unknown': 0}
        data_source_health_status.labels(
            endpoint_name=endpoint_name,
            source_name=source_name
        ).set(status_map.get(health_status, 0))

        # è´¨é‡è¯„åˆ†
        data_source_quality_score.labels(
            endpoint_name=endpoint_name,
            source_name=source_name
        ).set(quality_score)

        # æˆåŠŸç‡
        data_source_success_rate.labels(
            endpoint_name=endpoint_name,
            source_name=source_name
        ).set(success_rate)
```

### Grafanaä»ªè¡¨æ¿JSON

```json
{
  "dashboard": {
    "title": "æ•°æ®æºç®¡ç†ä»ªè¡¨æ¿",
    "tags": ["æ•°æ®æº", "ç›‘æ§"],
    "timezone": "browser",
    "schemaVersion": 16,
    "version": 0,
    "refresh": "30s",
    "panels": [
      {
        "id": 1,
        "title": "æ•°æ®æºæ€»è§ˆ",
        "type": "stat",
        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 0},
        "targets": [
          {
            "expr": "count(data_source_up{endpoint_name=~\".*\"})",
            "legendFormat": "æ€»æ•°æ®æº"
          },
          {
            "expr": "count(data_source_up == 1)",
            "legendFormat": "å¯ç”¨æ•°æ®æº"
          },
          {
            "expr": "count(data_source_up == 0)",
            "legendFormat": "ä¸å¯ç”¨æ•°æ®æº"
          }
        ]
      },
      {
        "id": 2,
        "title": "æ•°æ®æºå¥åº·çŠ¶æ€åˆ†å¸ƒ",
        "type": "piechart",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8},
        "targets": [
          {
            "expr": "count by (health_status) (data_source_health_status)",
            "legendFormat": "{{health_status}}"
          }
        ]
      },
      {
        "id": 3,
        "title": "å„æ•°æ®æºè°ƒç”¨æ¬¡æ•°ï¼ˆ24å°æ—¶ï¼‰",
        "type": "graph",
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8},
        "targets": [
          {
            "expr": "sum by (source_name) (increase(data_source_calls_total[24h]))",
            "legendFormat": "{{source_name}}"
          }
        ]
      },
      {
        "id": 4,
        "title": "æ•°æ®æºå“åº”æ—¶é—´å¯¹æ¯”",
        "type": "graph",
        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 16},
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(data_source_response_time_seconds_bucket[5m]))",
            "legendFormat": "{{endpoint_name}} (p95)"
          },
          {
            "expr": "histogram_quantile(0.50, rate(data_source_response_time_seconds_bucket[5m]))",
            "legendFormat": "{{endpoint_name}} (p50)"
          }
        ]
      },
      {
        "id": 5,
        "title": "æ•°æ®æºæˆåŠŸç‡è¶‹åŠ¿",
        "type": "graph",
        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 24},
        "targets": [
          {
            "expr": "data_source_success_rate",
            "legendFormat": "{{endpoint_name}}"
          }
        ]
      },
      {
        "id": 6,
        "title": "æ•°æ®æºè´¨é‡è¯„åˆ†å¯¹æ¯”",
        "type": "bar gauge",
        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 32},
        "targets": [
          {
            "expr": "data_source_quality_score",
            "legendFormat": "{{endpoint_name}}"
          }
        ]
      },
      {
        "id": 7,
        "title": "æ•°æ®æºè¿”å›è®°å½•æ•°åˆ†å¸ƒ",
        "type": "heatmap",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 40},
        "targets": [
          {
            "expr": "sum by (endpoint_name) (rate(data_source_record_count_bucket[5m]))",
            "legendFormat": "{{endpoint_name}}"
          }
        ]
      },
      {
        "id": 8,
        "title": "æ•°æ®æºé”™è¯¯ç‡ç›‘æ§",
        "type": "graph",
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 40},
        "targets": [
          {
            "expr": "rate(data_source_calls_total{status=\"failure\"}[5m]) / rate(data_source_calls_total[5m]) * 100",
            "legendFormat": "{{endpoint_name}} é”™è¯¯ç‡"
          }
        ]
      },
      {
        "id": 9,
        "title": "æ•°æ®æºè°ƒç”¨æ’è¡Œæ¦œï¼ˆTop 10ï¼‰",
        "type": "table",
        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 48},
        "targets": [
          {
            "expr": "topk(10, sum by (endpoint_name) (increase(data_source_calls_total[24h])))",
            "legendFormat": "{{endpoint_name}}"
          }
        ]
      }
    ]
  }
}
```

### ä»ªè¡¨æ¿ä½¿ç”¨æŒ‡å—

**å¯¼å…¥åˆ°Grafana**:
1. ç™»å½•Grafanaï¼ˆhttp://localhost:3000ï¼‰
2. è¿›å…¥ "+" â†’ "Import"
3. ç²˜è´´ä¸Šé¢çš„JSONé…ç½®
4. é€‰æ‹©Prometheusæ•°æ®æº
5. ç‚¹å‡»"Import"

**å…³é”®é¢æ¿è¯´æ˜**:
- **æ•°æ®æºæ€»è§ˆ**: å®æ—¶æ˜¾ç¤ºå¯ç”¨/ä¸å¯ç”¨æ•°æ®æºæ•°é‡
- **å¥åº·çŠ¶æ€åˆ†å¸ƒ**: é¥¼å›¾å±•ç¤ºhealthy/degraded/failedæ¯”ä¾‹
- **å“åº”æ—¶é—´å¯¹æ¯”**: æŠ˜çº¿å›¾å±•ç¤ºå„æ•°æ®æºp50/p95å“åº”æ—¶é—´
- **æˆåŠŸç‡è¶‹åŠ¿**: ç›‘æ§æ•°æ®æºç¨³å®šæ€§
- **è´¨é‡è¯„åˆ†**: å¯¹æ¯”å„æ•°æ®æºè´¨é‡æ°´å¹³
- **é”™è¯¯ç‡ç›‘æ§**: åŠæ—¶å‘ç°å¼‚å¸¸æ•°æ®æº
- **è°ƒç”¨æ’è¡Œæ¦œ**: äº†è§£é«˜é¢‘ä½¿ç”¨çš„æ•°æ®æº

---

## 7ï¸âƒ£ ä½¿ç”¨ç¤ºä¾‹

### åœºæ™¯1: ç»Ÿä¸€è°ƒç”¨æ¥å£

```python
from src.core.data_source_manager import DataSourceManager

# åˆå§‹åŒ–ç®¡ç†å™¨
manager = DataSourceManager()

# è·å–æ—¥çº¿æ•°æ®ï¼ˆä½¿ç”¨akshareï¼‰
kline_data = manager.get_data(
    endpoint_name="akshare.stock_zh_a_hist",
    symbol="000001",
    period="daily",
    start_date="20240101",
    end_date="20240131",
    adjust="qfq"
)

# è·å–tushareæ•°æ®
basic_data = manager.get_data(
    endpoint_name="tushare.stock_basic",
    list_status="L",
    fields="ts_code,symbol,name,area,industry"
)

# è·å–å®æ—¶è¡Œæƒ…
realtime_data = manager.get_data(
    endpoint_name="tdx.get_security_quotes",
    symbols=["000001", "600000", "000002"]
)
```

### åœºæ™¯2: æ™ºèƒ½è·¯ç”±

```python
# è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ•°æ®æº
best_endpoint = manager.get_best_endpoint("DAILY_KLINE")

# è¾“å‡º: {'endpoint_name': 'tushare.daily', 'source_name': 'tushare', ...}

# ç›´æ¥ä½¿ç”¨æœ€ä½³æ•°æ®æº
data = manager.get_data(
    endpoint_name=best_endpoint['endpoint_name'],
    ts_code="000001.SZ",
    start_date="20240101",
    end_date="20240131"
)
```

### åœºæ™¯3: æŸ¥æ‰¾å’Œæœç´¢

```python
# æŸ¥æ‰¾æ‰€æœ‰æ”¯æŒæ—¥çº¿Kçº¿çš„æ•°æ®æº
endpoints = manager.find_endpoints("DAILY_KLINE")
for ep in endpoints:
    print(f"{ep['endpoint_name']}: è´¨é‡={ep['quality_score']}, ä¼˜å…ˆçº§={ep['priority']}")

# æœç´¢å…è´¹æ•°æ®æº
free_sources = manager.search_source(tags=["free"])

# å…³é”®è¯æœç´¢
results = manager.search_source(keyword="å®æ—¶è¡Œæƒ…")
```

### åœºæ™¯4: å¥åº·æ£€æŸ¥

```python
# æ£€æŸ¥æ‰€æœ‰æ•°æ®æº
health_report = manager.health_check()
print(f"æ€»è®¡: {health_report['total']}")
print(f"å¥åº·: {health_report['healthy']}")
print(f"å¼‚å¸¸: {health_report['unhealthy']}")

# æ£€æŸ¥å•ä¸ªæ•°æ®æº
status = manager.health_check("akshare.stock_zh_a_hist")
if status['status'] == 'healthy':
    print("æ•°æ®æºæ­£å¸¸")
else:
    print(f"æ•°æ®æºå¼‚å¸¸: {status.get('error')}")
```

### åœºæ™¯5: APIè°ƒç”¨

```bash
# åˆ—å‡ºæ‰€æœ‰æ•°æ®æº
curl "http://localhost:8000/api/v1/data-sources/list"

# æŸ¥æ‰¾æ—¥çº¿æ•°æ®æº
curl "http://localhost:8000/api/v1/data-sources/find?data_category=DAILY_KLINE"

# è·å–æœ€ä½³æ•°æ®æº
curl "http://localhost:8000/api/v1/data-sources/best?data_category=DAILY_KLINE"

# å¥åº·æ£€æŸ¥
curl -X POST "http://localhost:8000/api/v1/data-sources/health-check"

# æœç´¢æ•°æ®æº
curl -X POST "http://localhost:8000/api/v1/data-sources/search" \
  -H "Content-Type: application/json" \
  -d '{"tags": ["free", "realtime"]}'

# è·å–ç»Ÿè®¡æ‘˜è¦
curl "http://localhost:8000/api/v1/data-sources/statistics/summary"
```

---

## 8ï¸âƒ£ å®æ–½è·¯çº¿å›¾

### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€è®¾æ–½ï¼ˆ1å‘¨ï¼‰

- [x] åˆ›å»ºPostgreSQLæ³¨å†Œè¡¨ï¼ˆdata_source_registry + call_historyï¼‰
- [ ] åˆ›å»ºYAMLé…ç½®æ–‡ä»¶æ¨¡æ¿
- [ ] å®ç°BaseDataSourceHandlerå’ŒåŸºç¡€å¤„ç†å™¨
- [ ] å®ç°DataSourceManageræ ¸å¿ƒç±»
- [ ] å•å…ƒæµ‹è¯•

### ç¬¬äºŒé˜¶æ®µï¼šæ ¸å¿ƒåŠŸèƒ½ï¼ˆ1å‘¨ï¼‰

- [ ] å®ç°æ‰€æœ‰æ•°æ®æºå¤„ç†å™¨ï¼ˆAkshare/Tushare/Baostock/Tdx/Crawlerï¼‰
- [ ] å®ç°å‚æ•°æ˜ å°„å’ŒéªŒè¯
- [ ] å®ç°å¥åº·æ£€æŸ¥æœºåˆ¶
- [ ] å®ç°æ™ºèƒ½è·¯ç”±é€»è¾‘
- [ ] LRUç¼“å­˜ä¼˜åŒ–

### ç¬¬ä¸‰é˜¶æ®µï¼šAPIå’Œç›‘æ§ï¼ˆ1å‘¨ï¼‰

- [ ] å®ç°FastAPIç®¡ç†æ¥å£
- [ ] é›†æˆPrometheusç›‘æ§æŒ‡æ ‡
- [ ] åˆ›å»ºGrafanaä»ªè¡¨æ¿
- [ ] APIæ–‡æ¡£å’Œæµ‹è¯•

### ç¬¬å››é˜¶æ®µï¼šä¼˜åŒ–å’Œéƒ¨ç½²ï¼ˆ1å‘¨ï¼‰

- [ ] æ€§èƒ½ä¼˜åŒ–ï¼ˆè¿æ¥æ± ã€å¹¶å‘è°ƒç”¨ï¼‰
- [ ] æ•…éšœè½¬ç§»å’Œé™çº§æœºåˆ¶
- [ ] æ•°æ®æºè‡ªåŠ¨å‘ç°å’Œæ³¨å†Œ
- [ ] ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å’Œç›‘æ§

---

## 9ï¸âƒ£ ç»´æŠ¤æµç¨‹

```
æ–°æ•°æ®æº â†’ æ³¨å†Œæµ‹è¯• â†’ è´¨é‡è¯„ä¼° â†’ ç”Ÿäº§ä½¿ç”¨ â†’ å®šæœŸå·¡æ£€ â†’ ä¸‹çº¿å½’æ¡£
   â†“          â†“          â†“          â†“          â†“          â†“
 YAMLé…ç½®   å¥åº·æ£€æŸ¥   è¯„åˆ†æ‰“åˆ†    ä¸Šçº¿ç›‘æ§   æ€§èƒ½ç»Ÿè®¡   çŠ¶æ€æ ‡è®°
```

**å…³é”®ç»´æŠ¤ä»»åŠ¡**:
1. **æ¯æ—¥**: æŸ¥çœ‹Grafanaä»ªè¡¨æ¿ï¼Œå…³æ³¨å¼‚å¸¸æ•°æ®æº
2. **æ¯å‘¨**: è¯„ä¼°æ•°æ®æºè´¨é‡è¯„åˆ†ï¼Œè°ƒæ•´ä¼˜å…ˆçº§
3. **æ¯æœˆ**: æ¸…ç†åºŸå¼ƒæ•°æ®æºï¼Œæ›´æ–°é…ç½®
4. **æ¯å­£åº¦**: å…¨é¢å®¡è®¡ï¼Œä¼˜åŒ–æ•°æ®æºç»„åˆ

---

## ğŸ æ–¹æ¡ˆä¼˜åŠ¿æ€»ç»“

| ç‰¹æ€§ | å®ç°æ•ˆæœ | å¯¹æ¯”ä¼˜åŒ–å‰ |
|------|---------|----------|
| **é›†ä¸­ç®¡ç†** | PostgreSQL + YAMLåŒå­˜å‚¨ | âœ… é…ç½®åŒ–+æŒä¹…åŒ– |
| **å¿«é€ŸæŸ¥æ‰¾** | æŒ‰åˆ†ç±»/æ ‡ç­¾/å…³é”®è¯æœç´¢ | âœ… å¤šç»´åº¦æœç´¢ |
| **æ™ºèƒ½è·¯ç”±** | è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ•°æ®æº | âœ… åŸºäºè´¨é‡+æ€§èƒ½ |
| **å¥åº·ç›‘æ§** | å®æ—¶å¥åº·æ£€æŸ¥å’Œå‘Šè­¦ | âœ… ä¸»åŠ¨å‘ç°å¼‚å¸¸ |
| **æ€§èƒ½è¿½è¸ª** | è°ƒç”¨å†å²å’Œç»Ÿè®¡ | âœ… å®Œæ•´æ•°æ®é“¾è·¯ |
| **å¯è§†åŒ–** | Grafanaä»ªè¡¨æ¿ | âœ… 8å¤§ç›‘æ§é¢æ¿ |
| **ç»Ÿä¸€è°ƒç”¨** | å•ä¸€å…¥å£å±è”½å·®å¼‚ | âœ… ç®€åŒ–ä½¿ç”¨ |
| **æ•…éšœè½¬ç§»** | è‡ªåŠ¨é™çº§å¤±è´¥æ•°æ®æº | âœ… æé«˜å¯ç”¨æ€§ |
| **å…¨ç”Ÿå‘½å‘¨æœŸ** | ä»æ³¨å†Œåˆ°ä¸‹çº¿å…¨ç®¡ç† | âœ… è§„èŒƒåŒ–æµç¨‹ |

---

**æ–‡æ¡£ç‰ˆæœ¬**: v2.0
**æœ€åæ›´æ–°**: 2026-01-02
**çŠ¶æ€**: ä¼˜åŒ–å®Œæˆï¼Œç­‰å¾…å®æ–½
