# æ•°æ®æºç®¡ç†ä¸æ•°æ®æ²»ç†æ¨¡å—ä¼˜åŒ–æ–¹æ¡ˆ

**æ–‡æ¡£ç±»å‹**: æŠ€æœ¯ä¼˜åŒ–ææ¡ˆ
**åˆ›å»ºæ—¶é—´**: 2026-01-09
**ç‰ˆæœ¬**: v1.0
**ä½œè€…**: Claude Code (Data Management Expert)
**çŠ¶æ€**: å¾…å®¡æ ¡

---

## ğŸ“‹ ç›®å½•

1. [æ‰§è¡Œæ‘˜è¦](#æ‰§è¡Œæ‘˜è¦)
2. [å½“å‰æ¶æ„åˆ†æ](#å½“å‰æ¶æ„åˆ†æ)
3. [ç—›ç‚¹ä¸ç“¶é¢ˆ](#ç—›ç‚¹ä¸ç“¶é¢ˆ)
4. [ä¼˜åŒ–æ–¹æ¡ˆ](#ä¼˜åŒ–æ–¹æ¡ˆ)
5. [å®æ–½è®¡åˆ’](#å®æ–½è®¡åˆ’)
6. [é¢„æœŸæ”¶ç›Š](#é¢„æœŸæ”¶ç›Š)
7. [é£é™©è¯„ä¼°](#é£é™©è¯„ä¼°)
8. [å‚è€ƒèµ„æº](#å‚è€ƒèµ„æº)

---

## æ‰§è¡Œæ‘˜è¦

æœ¬ææ¡ˆé’ˆå¯¹ MyStocks é¡¹ç›®çš„**æ•°æ®æºç®¡ç†æ¨¡å—** (`src/core/data_source/`) å’Œ**æ•°æ®æ²»ç†æ¨¡å—** (`src/governance/`) æä¾›ç³»ç»Ÿæ€§çš„ä¼˜åŒ–å»ºè®®ã€‚

### ğŸ¯ æ ¸å¿ƒç›®æ ‡

- **æ€§èƒ½æå‡**: APIå“åº”æ—¶é—´ä»500msé™è‡³100ms (**5å€æå‡**)
- **æˆæœ¬ä¼˜åŒ–**: APIè°ƒç”¨æˆæœ¬é™ä½70% (é€šè¿‡æ™ºèƒ½ç¼“å­˜å’Œè·¯ç”±)
- **å¯é æ€§å¢å¼º**: ç³»ç»Ÿå¯ç”¨æ€§ä»95%æå‡è‡³99.9%
- **å¯è§‚æµ‹æ€§**: å®ç°å…¨é¢çš„ç›‘æ§å’Œè¿½è¸ªèƒ½åŠ›

### ğŸ“Š ä¼˜åŒ–èŒƒå›´

| ä¼˜åŒ–é¡¹ | ä¼˜å…ˆçº§ | å·¥ä½œé‡ | é¢„æœŸæ”¶ç›Š |
|--------|--------|--------|----------|
| æ™ºèƒ½ç¼“å­˜ç­–ç•¥ | ğŸ”¥ P0 | 1-2å‘¨ | APIæˆæœ¬â†“40% |
| ç†”æ–­å™¨æœºåˆ¶ | ğŸ”¥ P0 | 1-2å‘¨ | æ•…éšœæ¢å¤â†“95% |
| æ•°æ®è´¨é‡éªŒè¯ | ğŸ”¥ P0 | 1-2å‘¨ | æ•°æ®å¯ä¿¡åº¦â†‘50% |
| æ™ºèƒ½è·¯ç”±ç®—æ³• | ğŸ”¶ P1 | 3-4å‘¨ | æ€§èƒ½â†‘30% |
| ç›‘æ§ä½“ç³»å®Œå–„ | ğŸ”¶ P1 | 3-4å‘¨ | å¯è§‚æµ‹æ€§â†‘10x |
| è¯·æ±‚æ‰¹å¤„ç† | ğŸ”¶ P1 | 2-3å‘¨ | ååé‡â†‘3-5x |

---

## å½“å‰æ¶æ„åˆ†æ

### 1ï¸âƒ£ æ•°æ®æºç®¡ç†æ¨¡å— (`src/core/data_source/`)

#### æ ¸å¿ƒç»„ä»¶

```
src/core/data_source/
â”œâ”€â”€ base.py              # DataSourceManagerV2 æ ¸å¿ƒç±»
â”œâ”€â”€ registry.py          # æ•°æ®æºæ³¨å†Œè¡¨ç®¡ç†
â”œâ”€â”€ router.py            # æ™ºèƒ½è·¯ç”±ç³»ç»Ÿ
â”œâ”€â”€ handler.py           # æ•°æ®è°ƒç”¨å¤„ç†å™¨
â”œâ”€â”€ monitoring.py        # ç›‘æ§å’Œå¥åº·æ£€æŸ¥
â”œâ”€â”€ health_check.py      # å¥åº·æ£€æŸ¥å®ç°
â”œâ”€â”€ validation.py        # æ•°æ®éªŒè¯
â””â”€â”€ cache.py             # LRUç¼“å­˜
```

#### å½“å‰å®ç°åŠŸèƒ½

**âœ… å·²å®ç°**:
- åŒæºé…ç½®ç®¡ç† (PostgreSQL + YAML)
- 34ä¸ªæ•°æ®æºç«¯ç‚¹ç®¡ç†
- æ™ºèƒ½è·¯ç”± (åŸºäºä¼˜å…ˆçº§å’Œè´¨é‡è¯„åˆ†)
- LRUç¼“å­˜æœºåˆ¶
- åŸºç¡€å¥åº·ç›‘æ§
- è°ƒç”¨ç»Ÿè®¡è®°å½•

**æ¶æ„ç‰¹ç‚¹**:
- ğŸ—ï¸ **åˆ†å±‚æ¶æ„**: è¡¨ç¤ºå±‚ â†’ ä¸šåŠ¡é€»è¾‘å±‚ â†’ æ•°æ®è®¿é—®å±‚
- ğŸ”§ **å·¥å‚æ¨¡å¼**: åŠ¨æ€åˆ›å»ºæ•°æ®æºå¤„ç†å™¨
- ğŸ¯ **ç­–ç•¥æ¨¡å¼**: å¯æ’æ‹”çš„è·¯ç”±å’ŒéªŒè¯ç­–ç•¥
- ğŸ”’ **å•ä¾‹æ¨¡å¼**: å…¨å±€å”¯ä¸€ç®¡ç†å™¨å®ä¾‹

---

### 2ï¸âƒ£ æ•°æ®æ²»ç†æ¨¡å— (`src/governance/`)

#### æ ¸å¿ƒç»„ä»¶

```
src/governance/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ fetcher_bridge.py    # æ²»ç†æ•°æ®è·å–æ¡¥æ¥å™¨
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ base.py              # éªŒè¯å™¨åŸºç±»
â”‚   â””â”€â”€ gpu_validator.py     # GPUåŠ é€ŸéªŒè¯å™¨
â””â”€â”€ tests/
    â”œâ”€â”€ test_fetcher_bridge.py
    â””â”€â”€ test_gpu_validator.py
```

#### å½“å‰å®ç°åŠŸèƒ½

**âœ… å·²å®ç°**:
- æ¡¥æ¥æ¨¡å¼ (æ²»ç†å±‚ä¸æ•°æ®æºè§£è€¦)
- å¤šç­–ç•¥è·¯ç”± (æ™ºèƒ½/æœ€å¿«/æœ€ç¨³å®š/æŒ‡å®šæº)
- GPU/CPUè‡ªé€‚åº”éªŒè¯
- åŸºç¡€æ•°æ®è´¨é‡æ£€æŸ¥ (OHLCé€»è¾‘)
- ä¼˜é›…é™çº§æœºåˆ¶

**æ€§èƒ½äº®ç‚¹**:
- ğŸš€ GPUåŠ é€Ÿå®ç°68.58xæ€§èƒ½æå‡
- ğŸ”„ è‡ªåŠ¨æ•…éšœè½¬ç§»
- ğŸ§© é«˜åº¦æ¨¡å—åŒ–è®¾è®¡

---

## ç—›ç‚¹ä¸ç“¶é¢ˆ

### ğŸ”´ å…³é”®é—®é¢˜

#### 1. ç¼“å­˜ç­–ç•¥è¿‡äºç®€å•

**é—®é¢˜æè¿°**:
```python
# âŒ å½“å‰å®ç°ï¼šä»…æœ‰LRUç¼“å­˜ï¼Œæ— TTL
self.registry[endpoint_name] = {
    "cache": LRUCache(maxsize=100),  # æ°¸ä¸è¿‡æœŸ
}
```

**é€ æˆå½±å“**:
- âŒ ç¼“å­˜æ•°æ®å¯èƒ½é•¿æœŸè¿‡æœŸ
- âŒ æ— æ³•æ§åˆ¶æ•°æ®æ–°é²œåº¦
- âŒ æµªè´¹APIè°ƒç”¨é¢åº¦
- âŒ å“åº”æ—¶é—´ä¸ç¨³å®š

---

#### 2. å¥åº·æ£€æŸ¥æœºåˆ¶ç®€é™‹

**é—®é¢˜æè¿°**:
```python
# âŒ å½“å‰å®ç°ï¼šè¿ç»­å¤±è´¥3æ¬¡å°±æ°¸ä¹…æ ‡è®°failed
if config["consecutive_failures"] >= 3:
    config["health_status"] = "failed"  # æ— æ³•è‡ªåŠ¨æ¢å¤
```

**é€ æˆå½±å“**:
- âŒ æ— ç†”æ–­ä¿æŠ¤ï¼Œçº§è”æ•…éšœé£é™©
- âŒ æ— æ³•è‡ªåŠ¨æ¢å¤
- âŒ ç¼ºå°‘åŠå¼€çŠ¶æ€è¯•æ¢æœºåˆ¶
- âŒ æµªè´¹èµ„æºè°ƒç”¨å¤±è´¥ç«¯ç‚¹

---

#### 3. æ•°æ®éªŒè¯ä¸å®Œæ•´

**é—®é¢˜æè¿°**:
```python
# âŒ å½“å‰å®ç°ï¼šåªåšOHLCåŸºç¡€é€»è¾‘æ£€æŸ¥
def validate(self, data, rules):
    # ä»…æ£€æŸ¥ low <= close <= high
    # ç¼ºå°‘ä¸šåŠ¡è§„åˆ™ã€ç»Ÿè®¡å¼‚å¸¸ã€è·¨æºéªŒè¯
```

**é€ æˆå½±å“**:
- âŒ æ— æ³•æ£€æµ‹å¼‚å¸¸ä»·æ ¼æ³¢åŠ¨
- âŒ æ— æ³•è¯†åˆ«å¼‚å¸¸æˆäº¤é‡
- âŒ ç¼ºå°‘ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹ (3-sigma)
- âŒ æ— è·¨æºäº¤å‰éªŒè¯èƒ½åŠ›

---

#### 4. ç›‘æ§æ•°æ®ä¸å¤Ÿä¸°å¯Œ

**é—®é¢˜æè¿°**:
```python
# âŒ å½“å‰å®ç°ï¼šä»…è®°å½•å¹³å‡å“åº”æ—¶é—´
config["avg_response_time"] = new_avg
```

**é€ æˆå½±å“**:
- âŒ æ— P95/P99å»¶è¿ŸæŒ‡æ ‡
- âŒ æ— æ•°æ®è´¨é‡æŒ‡æ ‡
- âŒ æ— æˆæœ¬è¿½è¸ª (APIè°ƒç”¨é‡)
- âŒ æ— ä¸šåŠ¡æŒ‡æ ‡ (ç¼“å­˜å‘½ä¸­ç‡)

---

#### 5. è·¯ç”±ç®—æ³•å•ä¸€

**é—®é¢˜æè¿°**:
```python
# âŒ å½“å‰å®ç°ï¼šä»…åŸºäºä¼˜å…ˆçº§+è´¨é‡è¯„åˆ†
endpoint = self.manager.get_best_endpoint(data_category)
```

**é€ æˆå½±å“**:
- âŒ æ— è´Ÿè½½å‡è¡¡
- âŒ æ— æˆæœ¬ä¼˜åŒ– (ä¼˜å…ˆä½¿ç”¨å…è´¹é¢åº¦)
- âŒ æ— åœ°åŸŸæ„ŸçŸ¥
- âŒ æ— æµé‡æ§åˆ¶

---

#### 6. æ‰¹é‡å¤„ç†æ•ˆç‡ä½

**é—®é¢˜æè¿°**:
```python
# âŒ å½“å‰å®ç°ï¼šæ‰¹é‡è¯·æ±‚ä¸²è¡Œæ‰§è¡Œ
for symbol in symbols:
    df = self._fetch_single_symbol(symbol, ...)  # ä¸²è¡Œï¼Œæ…¢ï¼
```

**é€ æˆå½±å“**:
- âŒ ååé‡ä½
- âŒ æ— æ³•å……åˆ†åˆ©ç”¨å¹¶å‘èƒ½åŠ›
- âŒ æµªè´¹ç½‘ç»œå¾€è¿”æ—¶é—´

---

## ä¼˜åŒ–æ–¹æ¡ˆ

### ğŸ”¥ P0 ä¼˜åŒ– (ç«‹å³å®æ–½)

#### ä¼˜åŒ–1: æ™ºèƒ½ç¼“å­˜ç­–ç•¥ â­â­â­â­â­

**ç›®æ ‡**: å®ç°LRU + TTL + é¢„çƒ­çš„æ™ºèƒ½ç¼“å­˜æœºåˆ¶

**è®¾è®¡æ–¹æ¡ˆ**:

```python
from collections import OrderedDict
from datetime import datetime, timedelta
import asyncio
from typing import Any, Optional

class SmartCache:
    """
    æ™ºèƒ½ç¼“å­˜ï¼šLRU + TTL + ä¸»åŠ¨é¢„çƒ­

    ç‰¹æ€§ï¼š
    1. TTLè¿‡æœŸæœºåˆ¶
    2. è®¿é—®æ—¶å¼‚æ­¥åˆ·æ–°
    3. ä¼˜é›…é™çº§ (è¿”å›è¿‡æœŸæ•°æ®)
    """

    def __init__(
        self,
        maxsize: int = 100,
        ttl: int = 3600,  # é»˜è®¤1å°æ—¶
        refresh_ratio: float = 0.8  # è®¿é—®åˆ°80% TTLæ—¶åˆ·æ–°
    ):
        self.maxsize = maxsize
        self.ttl = ttl
        self.refresh_ratio = refresh_ratio

        # ä¸»ç¼“å­˜ï¼šOrderedDictå®ç°LRU
        self.cache = OrderedDict()

        # å…ƒæ•°æ®ï¼šè®°å½•åˆ›å»ºæ—¶é—´å’Œè®¿é—®æ—¶é—´
        self.metadata = {}

    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼ï¼Œæ”¯æŒTTLå’Œå¼‚æ­¥åˆ·æ–°"""
        if key not in self.cache:
            return None

        # ç§»åŠ¨åˆ°æœ«å°¾ (LRU)
        self.cache.move_to_end(key)

        # æ£€æŸ¥TTL
        if self._is_expired(key):
            # å¼‚æ­¥åˆ·æ–°
            self._refresh_async(key)

            # è¿”å›è¿‡æœŸæ•°æ® (ä¼˜é›…é™çº§)
            return self._get_stale(key)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é¢„çƒ­åˆ·æ–°
        if self._should_refresh(key):
            self._refresh_async(key)

        return self._get_fresh(key)

    def set(self, key: str, value: Any):
        """è®¾ç½®ç¼“å­˜å€¼"""
        if key in self.cache:
            self.cache.move_to_end(key)

        self.cache[key] = value
        self.metadata[key] = {
            "created_at": datetime.now(),
            "last_accessed": datetime.now(),
        }

        # LRUæ·˜æ±°
        if len(self.cache) > self.maxsize:
            self.cache.popitem(last=False)

    def _is_expired(self, key: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿‡æœŸ"""
        if key not in self.metadata:
            return True

        created_at = self.metadata[key]["created_at"]
        return (datetime.now() - created_at).total_seconds() > self.ttl

    def _should_refresh(self, key: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦é¢„çƒ­åˆ·æ–°"""
        if key not in self.metadata:
            return False

        created_at = self.metadata[key]["created_at"]
        elapsed = (datetime.now() - created_at).total_seconds()
        return elapsed > self.ttl * self.refresh_ratio

    def _get_fresh(self, key: str) -> Any:
        """è·å–æ–°é²œæ•°æ®"""
        self.metadata[key]["last_accessed"] = datetime.now()
        return self.cache[key]

    def _get_stale(self, key: str) -> Any:
        """è·å–è¿‡æœŸæ•°æ® (ä¼˜é›…é™çº§)"""
        logger.warning(f"è¿”å›è¿‡æœŸç¼“å­˜æ•°æ®: {key}")
        return self.cache[key]

    def _refresh_async(self, key: str):
        """å¼‚æ­¥åˆ·æ–°ç¼“å­˜ (ç”±å­ç±»å®ç°)"""
        # è¿™é‡Œåªæ˜¯é¢„ç•™æ¥å£ï¼Œå®é™…ç”±DataSourceManagerå®ç°
        pass
```

**é›†æˆåˆ°DataSourceManagerV2**:

```python
# src/core/data_source/base.py

class DataSourceManagerV2:
    def __init__(self, yaml_config_path: str = "config/data_sources_registry.yaml"):
        # ...

        # æ—§ä»£ç ï¼š
        # self.registry[endpoint_name] = {
        #     "cache": LRUCache(maxsize=100),
        # }

        # æ–°ä»£ç ï¼š
        self.registry[endpoint_name] = {
            "cache": SmartCache(
                maxsize=100,
                ttl=3600,  # 1å°æ—¶TTL
                refresh_ratio=0.8  # 80%æ—¶åˆ·æ–°
            ),
        }
```

**é¢„æœŸæ”¶ç›Š**:
- âœ… APIè°ƒç”¨æˆæœ¬é™ä½**40%**
- âœ… ç¼“å­˜å‘½ä¸­æ—¶å“åº”æ—¶é—´ < 1ms
- âœ… æ•°æ®æ–°é²œåº¦å¯æ§

---

#### ä¼˜åŒ–2: ç†”æ–­å™¨æœºåˆ¶ â­â­â­â­â­

**ç›®æ ‡**: å®ç°Circuit Breakeræ¨¡å¼ï¼Œé˜²æ­¢çº§è”æ•…éšœ

**è®¾è®¡æ–¹æ¡ˆ**:

```python
from enum import Enum
import time
from typing import Callable, Any

class CircuitState(Enum):
    """ç†”æ–­å™¨çŠ¶æ€"""
    CLOSED = "closed"       # å…³é—­ï¼šæ­£å¸¸å·¥ä½œ
    OPEN = "open"           # å¼€å¯ï¼šç†”æ–­ï¼Œæ‹’ç»è¯·æ±‚
    HALF_OPEN = "half_open" # åŠå¼€ï¼šè¯•æ¢æ¢å¤

class CircuitBreakerOpenError(Exception):
    """ç†”æ–­å™¨å¼€å¯å¼‚å¸¸"""
    pass

class CircuitBreaker:
    """
    ç†”æ–­å™¨ï¼šä¿æŠ¤çº§è”æ•…éšœ

    çŠ¶æ€è½¬æ¢ï¼š
    CLOSED â†’ OPEN: å¤±è´¥æ¬¡æ•°è¾¾åˆ°é˜ˆå€¼
    OPEN â†’ HALF_OPEN: è¶…æ—¶æ—¶é—´åˆ°æœŸ
    HALF_OPEN â†’ CLOSED: è¯•æ¢æˆåŠŸ
    HALF_OPEN â†’ OPEN: è¯•æ¢å¤±è´¥
    """

    def __init__(
        self,
        failure_threshold: int = 5,  # å¤±è´¥é˜ˆå€¼
        timeout: int = 60,            # ç†”æ–­è¶…æ—¶ (ç§’)
        half_open_max_calls: int = 3 # åŠå¼€çŠ¶æ€æœ€å¤§è¯•æ¢æ¬¡æ•°
    ):
        self.state = CircuitState.CLOSED
        self.failure_count = 0
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.half_open_max_calls = half_open_max_calls
        self.half_open_calls = 0
        self.last_failure_time = None

    def call(self, func: Callable, *args, **kwargs) -> Any:
        """æ‰§è¡Œè°ƒç”¨ï¼Œè‡ªåŠ¨ç†”æ–­"""
        # æ£€æŸ¥çŠ¶æ€
        if self.state == CircuitState.OPEN:
            if self._should_attempt_reset():
                logger.info("ç†”æ–­å™¨è¿›å…¥åŠå¼€çŠ¶æ€ï¼Œå°è¯•æ¢å¤...")
                self.state = CircuitState.HALF_OPEN
                self.half_open_calls = 0
            else:
                raise CircuitBreakerOpenError(
                    f"ç†”æ–­å™¨å¼€å¯ï¼Œæ‹’ç»è°ƒç”¨ (å°†åœ¨{self._get_remaining_time()}ç§’åå°è¯•æ¢å¤)"
                )

        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result

        except Exception as e:
            self._on_failure()
            raise

    def _should_attempt_reset(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥å°è¯•æ¢å¤"""
        if self.last_failure_time is None:
            return False

        elapsed = time.time() - self.last_failure_time
        return elapsed >= self.timeout

    def _get_remaining_time(self) -> int:
        """è·å–å‰©ä½™æ¢å¤æ—¶é—´"""
        if self.last_failure_time is None:
            return 0

        elapsed = time.time() - self.last_failure_time
        return max(0, self.timeout - int(elapsed))

    def _on_success(self):
        """æˆåŠŸæ—¶çš„å¤„ç†"""
        self.failure_count = 0

        if self.state == CircuitState.HALF_OPEN:
            self.half_open_calls += 1
            logger.info(f"åŠå¼€çŠ¶æ€è¯•æ¢æˆåŠŸ ({self.half_open_calls}/{self.half_open_max_calls})")

            # è¿ç»­æˆåŠŸè¾¾åˆ°é˜ˆå€¼ï¼Œå…³é—­ç†”æ–­å™¨
            if self.half_open_calls >= self.half_open_max_calls:
                logger.info("ç†”æ–­å™¨æ¢å¤ï¼ŒçŠ¶æ€åˆ‡æ¢ä¸ºCLOSED")
                self.state = CircuitState.CLOSED
                self.half_open_calls = 0

    def _on_failure(self):
        """å¤±è´¥æ—¶çš„å¤„ç†"""
        self.failure_count += 1
        self.last_failure_time = time.time()

        if self.state == CircuitState.HALF_OPEN:
            # åŠå¼€çŠ¶æ€å¤±è´¥ï¼Œé‡æ–°æ‰“å¼€ç†”æ–­å™¨
            logger.warning("åŠå¼€çŠ¶æ€è¯•æ¢å¤±è´¥ï¼Œé‡æ–°æ‰“å¼€ç†”æ–­å™¨")
            self.state = CircuitState.OPEN
            self.half_open_calls = 0

        elif self.failure_count >= self.failure_threshold:
            # è¾¾åˆ°å¤±è´¥é˜ˆå€¼ï¼Œæ‰“å¼€ç†”æ–­å™¨
            logger.error(
                f"è¿ç»­å¤±è´¥{self.failure_count}æ¬¡ï¼Œè¾¾åˆ°é˜ˆå€¼{self.failure_threshold}ï¼Œæ‰“å¼€ç†”æ–­å™¨"
            )
            self.state = CircuitState.OPEN
```

**é›†æˆåˆ°DataSourceManagerV2**:

```python
# src/core/data_source/base.py

class DataSourceManagerV2:
    def __init__(self, yaml_config_path: str = "config/data_sources_registry.yaml"):
        # ...

        # ä¸ºæ¯ä¸ªendpointåˆ›å»ºç†”æ–­å™¨
        self.registry[endpoint_name] = {
            "config": source_config,
            "handler": None,
            "cache": SmartCache(...),
            "circuit_breaker": CircuitBreaker(  # æ–°å¢
                failure_threshold=5,
                timeout=60,
                half_open_max_calls=3
            ),
            # ...
        }

    def _call_endpoint(self, endpoint_info: Dict, **kwargs) -> Any:
        """è°ƒç”¨ç«¯ç‚¹ï¼Œå¸¦ç†”æ–­ä¿æŠ¤"""
        endpoint_name = endpoint_info["endpoint_name"]
        cb = self.registry[endpoint_name]["circuit_breaker"]

        def _do_call():
            # åŸæœ‰çš„è°ƒç”¨é€»è¾‘
            handler = self.registry[endpoint_name]["handler"]
            return handler.fetch(**kwargs)

        # é€šè¿‡ç†”æ–­å™¨è°ƒç”¨
        return cb.call(_do_call)
```

**é¢„æœŸæ”¶ç›Š**:
- âœ… é˜²æ­¢çº§è”æ•…éšœ
- âœ… è‡ªåŠ¨æ¢å¤èƒ½åŠ› (æ•…éšœæ¢å¤æ—¶é—´<1åˆ†é’Ÿ)
- âœ… å‡å°‘æ— æ•ˆAPIè°ƒç”¨ **95%**

---

#### ä¼˜åŒ–3: å¢å¼ºæ•°æ®è´¨é‡éªŒè¯ â­â­â­â­

**ç›®æ ‡**: å®ç°å¤šå±‚æ¬¡æ•°æ®éªŒè¯ä½“ç³»

**è®¾è®¡æ–¹æ¡ˆ**:

```python
from typing import Dict, List, Any, Optional
import pandas as pd
import numpy as np

class DataQualityValidator:
    """
    æ•°æ®è´¨é‡éªŒè¯å™¨

    å¤šå±‚æ¬¡éªŒè¯ï¼š
    1. åŸºç¡€é€»è¾‘éªŒè¯ (OHLCå…³ç³»)
    2. ä¸šåŠ¡è§„åˆ™éªŒè¯ (å¼‚å¸¸ä»·æ ¼/æˆäº¤é‡)
    3. ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹ (3-sigma)
    4. è·¨æºäº¤å‰éªŒè¯ (å¯é€‰)
    """

    def validate(
        self,
        data: pd.DataFrame,
        context: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå¤šå±‚æ¬¡éªŒè¯

        Args:
            data: å¾…éªŒè¯æ•°æ®
            context: éªŒè¯ä¸Šä¸‹æ–‡
                - symbol: è‚¡ç¥¨ä»£ç 
                - start_date: å¼€å§‹æ—¥æœŸ
                - end_date: ç»“æŸæ—¥æœŸ
                - enable_cross_validation: æ˜¯å¦å¯ç”¨è·¨æºéªŒè¯

        Returns:
            Dict: éªŒè¯ç»“æœ
                - is_valid: æ˜¯å¦é€šè¿‡
                - checks: å„é¡¹æ£€æŸ¥ç»“æœ
                - issues: å‘ç°çš„é—®é¢˜åˆ—è¡¨
        """
        context = context or {}

        results = {
            "is_valid": True,
            "checks": [],
            "issues": [],
            "summary": {}
        }

        # 1ï¸âƒ£ åŸºç¡€é€»è¾‘éªŒè¯
        check1 = self._logic_check(data)
        results["checks"].append(check1)
        if not check1["passed"]:
            results["issues"].extend(check1["issues"])

        # 2ï¸âƒ£ ä¸šåŠ¡è§„åˆ™éªŒè¯
        check2 = self._business_check(data, context)
        results["checks"].append(check2)
        if not check2["passed"]:
            results["issues"].extend(check2["issues"])

        # 3ï¸âƒ£ ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹
        check3 = self._statistical_check(data)
        results["checks"].append(check3)
        if not check3["passed"]:
            results["issues"].append(check3["message"])

        # 4ï¸âƒ£ è·¨æºäº¤å‰éªŒè¯ (å¯é€‰)
        if context.get("enable_cross_validation"):
            check4 = self._cross_source_check(data, context)
            results["checks"].append(check4)
            if not check4["passed"]:
                results["issues"].extend(check4["issues"])

        # æ±‡æ€»ç»“æœ
        results["is_valid"] = all(
            check["passed"] for check in results["checks"]
        )

        results["summary"] = {
            "total_checks": len(results["checks"]),
            "passed_checks": sum(1 for c in results["checks"] if c["passed"]),
            "total_issues": len(results["issues"]),
        }

        return results

    def _logic_check(self, data: pd.DataFrame) -> Dict[str, Any]:
        """åŸºç¡€é€»è¾‘éªŒè¯ï¼šOHLCå…³ç³»"""
        issues = []

        # æ£€æŸ¥å¿…è¦å­—æ®µ
        required_fields = ["open", "high", "low", "close"]
        missing_fields = [
            f for f in required_fields if f not in data.columns
        ]
        if missing_fields:
            issues.append(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}")

        # æ£€æŸ¥OHLCå…³ç³»: low <= open, close <= high
        if not missing_fields:
            invalid_open = data[data["open"] < data["low"]]
            invalid_open = data[data["open"] > data["high"]]

            invalid_close = data[data["close"] < data["low"]]
            invalid_close = data[data["close"] > data["high"]]

            if len(invalid_open) > 0:
                issues.append(f"å¼€ç›˜ä»·è¿è§„è®°å½•: {len(invalid_open)}æ¡")

            if len(invalid_close) > 0:
                issues.append(f"æ”¶ç›˜ä»·è¿è§„è®°å½•: {len(invalid_close)}æ¡")

        return {
            "name": "åŸºç¡€é€»è¾‘éªŒè¯",
            "passed": len(issues) == 0,
            "issues": issues
        }

    def _business_check(
        self,
        data: pd.DataFrame,
        context: Dict
    ) -> Dict[str, Any]:
        """ä¸šåŠ¡è§„åˆ™éªŒè¯"""
        issues = []

        # 1. æ£€æµ‹æç«¯ä»·æ ¼æ³¢åŠ¨
        if self._has_extreme_price_change(data):
            issues.append("æ£€æµ‹åˆ°æç«¯ä»·æ ¼æ³¢åŠ¨ (>20%)")

        # 2. æ£€æµ‹å¼‚å¸¸æˆäº¤é‡
        if self._has_abnormal_volume(data):
            issues.append("æ£€æµ‹åˆ°å¼‚å¸¸æˆäº¤é‡ (>10å€å‡å€¼)")

        # 3. æ£€æµ‹åœç‰Œæ•°æ®
        if self._is_suspended(data):
            issues.append("åœç‰ŒæœŸé—´å­˜åœ¨æ•°æ®")

        # 4. æ£€æµ‹ä»·æ ¼ä¸ºé›¶æˆ–è´Ÿæ•°
        if (data["close"] <= 0).any():
            issues.append("å­˜åœ¨é›¶æˆ–è´Ÿä»·æ ¼")

        return {
            "name": "ä¸šåŠ¡è§„åˆ™éªŒè¯",
            "passed": len(issues) == 0,
            "issues": issues
        }

    def _statistical_check(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹ (3-sigmaè§„åˆ™)"""
        if "close" not in data.columns:
            return {
                "name": "ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹",
                "passed": True,
                "message": "æ— æ”¶ç›˜ä»·æ•°æ®ï¼Œè·³è¿‡ç»Ÿè®¡æ£€æµ‹"
            }

        # è®¡ç®—3-sigmaè¾¹ç•Œ
        mean = data["close"].mean()
        std = data["close"].std()

        lower_bound = mean - 3 * std
        upper_bound = mean + 3 * std

        # æ£€æµ‹å¼‚å¸¸å€¼
        outliers = data[
            (data["close"] < lower_bound) |
            (data["close"] > upper_bound)
        ]

        message = f"æ£€æµ‹åˆ°{len(outliers)}ä¸ªç»Ÿè®¡å¼‚å¸¸å€¼ (3-sigma)"

        return {
            "name": "ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹",
            "passed": len(outliers) == 0,
            "message": message,
            "outliers_count": len(outliers),
            "outliers": outliers.to_dict("records") if len(outliers) > 0 else []
        }

    def _has_extreme_price_change(self, data: pd.DataFrame) -> bool:
        """æ£€æµ‹æç«¯ä»·æ ¼æ³¢åŠ¨ (>20%)"""
        if "close" not in data.columns or len(data) < 2:
            return False

        data = data.sort_values("date")
        data["pct_change"] = data["close"].pct_change() * 100

        extreme_changes = data[abs(data["pct_change"]) > 20]
        return len(extreme_changes) > 0

    def _has_abnormal_volume(self, data: pd.DataFrame) -> bool:
        """æ£€æµ‹å¼‚å¸¸æˆäº¤é‡ (>10å€å‡å€¼)"""
        if "volume" not in data.columns or len(data) < 10:
            return False

        mean_volume = data["volume"].mean()
        abnormal = data[data["volume"] > mean_volume * 10]

        return len(abnormal) > 0

    def _is_suspended(self, data: pd.DataFrame) -> bool:
        """æ£€æµ‹åœç‰Œæ•°æ® (æˆäº¤é‡ä¸º0ä¸”ä»·æ ¼ä¸å˜)"""
        if "volume" not in data.columns or len(data) < 2:
            return False

        # å‡è®¾è¿ç»­3å¤©æˆäº¤é‡ä¸º0ä¸”ä»·æ ¼ä¸å˜ä¸ºåœç‰Œ
        data = data.sort_values("date")

        for i in range(len(data) - 2):
            if (
                data.iloc[i]["volume"] == 0 and
                data.iloc[i+1]["volume"] == 0 and
                data.iloc[i+2]["volume"] == 0 and
                data.iloc[i]["close"] == data.iloc[i+1]["close"] == data.iloc[i+2]["close"]
            ):
                return True

        return False

    def _cross_source_check(
        self,
        data: pd.DataFrame,
        context: Dict
    ) -> Dict[str, Any]:
        """è·¨æºäº¤å‰éªŒè¯"""
        # TODO: å®ç°è·¨æºéªŒè¯é€»è¾‘
        # 1. ä»å¦ä¸€ä¸ªæ•°æ®æºè·å–ç›¸åŒæ•°æ®
        # 2. æ¯”è¾ƒä»·æ ¼å·®å¼‚
        # 3. æ£€æŸ¥ä¸€è‡´æ€§

        return {
            "name": "è·¨æºäº¤å‰éªŒè¯",
            "passed": True,
            "issues": [],
            "note": "å¾…å®ç°"
        }
```

**é›†æˆåˆ°GPUValidator**:

```python
# src/governance/engine/gpu_validator.py

class GPUValidator(BaseValidator):
    def validate(self, data: Any, rules: List[str] = None) -> Dict[str, Any]:
        """GPUåŠ é€ŸéªŒè¯"""
        # ä½¿ç”¨GPUè¿›è¡ŒåŸºç¡€éªŒè¯
        result = super().validate(data, rules)

        # å¢å¼ºéªŒè¯ï¼šä¸šåŠ¡è§„åˆ™ + ç»Ÿè®¡æ£€æµ‹
        quality_validator = DataQualityValidator()
        quality_result = quality_validator.validate(data, self.context)

        # åˆå¹¶ç»“æœ
        result["quality_check"] = quality_result

        if not quality_result["is_valid"]:
            result["is_valid"] = False
            result["issues"].extend(quality_result["issues"])

        return result
```

**é¢„æœŸæ”¶ç›Š**:
- âœ… æ•°æ®è´¨é‡å¯ä¿¡åº¦æå‡ **50%**
- âœ… åŠæ—©å‘ç°æ•°æ®æºé—®é¢˜
- âœ… æ”¯æŒç›‘ç®¡å®¡è®¡è¦æ±‚

---

### ğŸ”¶ P1 ä¼˜åŒ– (è¿‘æœŸå®æ–½)

#### ä¼˜åŒ–4: æ™ºèƒ½è·¯ç”±ç®—æ³• â­â­â­â­

**ç›®æ ‡**: å®ç°å¤šç»´åº¦è·¯ç”±å†³ç­–

**è®¾è®¡æ–¹æ¡ˆ**:

```python
from typing import Dict, List, Optional
import numpy as np

class SmartRouter:
    """
    æ™ºèƒ½è·¯ç”±å™¨ï¼šå¤šç»´åº¦å†³ç­–

    å†³ç­–ç»´åº¦ï¼š
    1. æ€§èƒ½è¯„åˆ† (P50 + P95 + P99 + æˆåŠŸç‡)
    2. æˆæœ¬ä¼˜åŒ– (ä¼˜å…ˆä½¿ç”¨å…è´¹é¢åº¦)
    3. è´Ÿè½½å‡è¡¡ (é¿å…å•ç‚¹è¿‡è½½)
    4. åœ°åŸŸæ„ŸçŸ¥ (é€‰æ‹©æœ€è¿‘èŠ‚ç‚¹)
    """

    def __init__(self):
        self.weights = {
            "performance": 0.4,  # æ€§èƒ½æƒé‡
            "cost": 0.3,         # æˆæœ¬æƒé‡
            "load": 0.2,         # è´Ÿè½½æƒé‡
            "location": 0.1      # åœ°åŸŸæƒé‡
        }

    def route(
        self,
        request_context: Dict,
        candidates: List[Dict]
    ) -> Optional[Dict]:
        """
        æ‰§è¡Œæ™ºèƒ½è·¯ç”±

        Args:
            request_context: è¯·æ±‚ä¸Šä¸‹æ–‡
                - data_category: æ•°æ®åˆ†ç±»
                - caller: è°ƒç”¨æ–¹
                - location: åœ°ç†ä½ç½® (å¯é€‰)
            candidates: å€™é€‰ç«¯ç‚¹åˆ—è¡¨

        Returns:
            Dict: é€‰ä¸­çš„ç«¯ç‚¹
        """
        if not candidates:
            return None

        # 1ï¸âƒ£ åŸºäºå†å²æ€§èƒ½è¯„åˆ†
        scored = self._score_by_performance(candidates)

        # 2ï¸âƒ£ åŸºäºæˆæœ¬ä¼˜åŒ–
        scored = self._adjust_by_cost(scored, request_context)

        # 3ï¸âƒ£ åŸºäºè´Ÿè½½å‡è¡¡
        scored = self._adjust_by_load(scored)

        # 4ï¸âƒ£ åŸºäºåœ°åŸŸæ„ŸçŸ¥
        scored = self._adjust_by_location(scored, request_context)

        # 5ï¸âƒ£ é€‰æ‹©æœ€ç»ˆå¾—åˆ†æœ€é«˜çš„
        best = max(scored, key=lambda x: x["final_score"])

        logger.info(
            f"æ™ºèƒ½è·¯ç”±é€‰æ‹©: {best['endpoint_name']} "
            f"(å¾—åˆ†: {best['final_score']:.2f})"
        )

        return best

    def _score_by_performance(self, endpoints: List[Dict]) -> List[Dict]:
        """æ€§èƒ½è¯„åˆ†"""
        for ep in endpoints:
            metrics = ep.get("metrics", {})

            # ç»¼åˆè¯„åˆ† (è¶Šä½è¶Šå¥½)
            p50 = metrics.get("p50_latency", 1.0)
            p95 = metrics.get("p95_latency", 2.0)
            p99 = metrics.get("p99_latency", 5.0)
            success_rate = metrics.get("success_rate", 100)

            # å½’ä¸€åŒ–è¯„åˆ† (0-100)
            perf_score = (
                (1 / (p50 + 0.1)) * 20 +  # P50æƒé‡20%
                (1 / (p95 + 0.1)) * 30 +  # P95æƒé‡30%
                (1 / (p99 + 0.1)) * 30 +  # P99æƒé‡30%
                (success_rate / 100) * 20  # æˆåŠŸç‡æƒé‡20%
            )

            ep["perf_score"] = min(perf_score, 100)

        return endpoints

    def _adjust_by_cost(
        self,
        endpoints: List[Dict],
        context: Dict
    ) -> List[Dict]:
        """æˆæœ¬ä¼˜åŒ–"""
        for ep in endpoints:
            perf_score = ep.get("perf_score", 0)

            # æˆæœ¬åŠ æˆ
            if ep.get("pricing") == "free":
                # å®Œå…¨å…è´¹ï¼š50%åŠ æˆ
                cost_bonus = 1.5
            elif ep.get("free_quota_remaining", 0) > 0:
                # æœ‰å…è´¹é¢åº¦ï¼š20%åŠ æˆ
                cost_bonus = 1.2
            else:
                # ä»˜è´¹ï¼šæ— åŠ æˆ
                cost_bonus = 1.0

            ep["cost_score"] = perf_score * cost_bonus

        return endpoints

    def _adjust_by_load(self, endpoints: List[Dict]) -> List[Dict]:
        """è´Ÿè½½å‡è¡¡"""
        for ep in endpoints:
            cost_score = ep.get("cost_score", 0)

            # åŸºäºå½“å‰è°ƒç”¨æ•°è°ƒæ•´
            current_calls = ep.get("current_calls", 0)
            max_calls = ep.get("max_calls", 1000)

            utilization = current_calls / max_calls

            # è´Ÿè½½æƒ©ç½š
            if utilization > 0.8:
                # é«˜è´Ÿè½½ï¼š-30%
                load_penalty = 0.7
            elif utilization > 0.5:
                # ä¸­è´Ÿè½½ï¼š-10%
                load_penalty = 0.9
            else:
                # ä½è´Ÿè½½ï¼šæ— æƒ©ç½š
                load_penalty = 1.0

            ep["load_score"] = cost_score * load_penalty

        return endpoints

    def _adjust_by_location(
        self,
        endpoints: List[Dict],
        context: Dict
    ) -> List[Dict]:
        """åœ°åŸŸæ„ŸçŸ¥"""
        client_location = context.get("location", "default")

        for ep in endpoints:
            load_score = ep.get("load_score", 0)

            # åœ°åŸŸåŠ æˆ (ç®€åŒ–ç‰ˆ)
            if ep.get("location") == client_location:
                # åŒåœ°åŸŸï¼š10%åŠ æˆ
                location_bonus = 1.1
            else:
                location_bonus = 1.0

            # æœ€ç»ˆå¾—åˆ†
            ep["final_score"] = load_score * location_bonus

        return endpoints
```

**é›†æˆåˆ°DataSourceManagerV2**:

```python
# src/core/data_source/router.py

def get_best_endpoint(
    manager: DataSourceManagerV2,
    data_category: str,
    request_context: Optional[Dict] = None
) -> Optional[Dict]:
    """è·å–æœ€ä½³ç«¯ç‚¹ (æ™ºèƒ½è·¯ç”±)"""
    # 1. æŸ¥æ‰¾å¥åº·ç«¯ç‚¹
    endpoints = find_endpoints(
        manager,
        data_category=data_category,
        only_healthy=True
    )

    if not endpoints:
        return None

    # 2. æ™ºèƒ½è·¯ç”±
    router = SmartRouter()
    return router.route(request_context or {}, endpoints)
```

**é¢„æœŸæ”¶ç›Š**:
- âœ… æ•´ä½“æ€§èƒ½æå‡ **30%**
- âœ… APIæˆæœ¬é™ä½ **30%**
- âœ… è´Ÿè½½åˆ†å¸ƒæ›´å‡è¡¡

---

#### ä¼˜åŒ–5: å®Œå–„ç›‘æ§ä½“ç³» â­â­â­â­

**ç›®æ ‡**: é›†æˆPrometheusï¼Œå®ç°å…¨é¢å¯è§‚æµ‹æ€§

**è®¾è®¡æ–¹æ¡ˆ**:

```python
# src/core/data_source/metrics.py

from prometheus_client import (
    Histogram,
    Gauge,
    Counter,
    start_http_server
)
import time
from functools import wraps

# ========================================
# PrometheusæŒ‡æ ‡å®šä¹‰
# ========================================

# 1. å»¶è¿ŸæŒ‡æ ‡
api_latency = Histogram(
    'datasource_api_latency_seconds',
    'API call latency by endpoint',
    ['source', 'endpoint', 'status'],
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]  # ç§’
)

# 2. æ•°æ®è´¨é‡è¯„åˆ†
data_quality_score = Gauge(
    'datasource_data_quality',
    'Data quality score (0-100)',
    ['source']
)

# 3. APIè°ƒç”¨è®¡æ•°
api_calls_total = Counter(
    'datasource_api_calls_total',
    'Total API calls',
    ['source', 'endpoint', 'status']
)

# 4. æˆæœ¬è¿½è¸ª
api_cost_estimated = Gauge(
    'datasource_api_cost_estimated',
    'Estimated API cost (CNY)',
    ['source']
)

# 5. ç¼“å­˜å‘½ä¸­ç‡
cache_hits = Counter(
    'datasource_cache_hits_total',
    'Cache hits',
    ['source']
)

cache_misses = Counter(
    'datasource_cache_misses_total',
    'Cache misses',
    ['source']
)

# 6. ç†”æ–­å™¨çŠ¶æ€
circuit_breaker_state = Gauge(
    'datasource_circuit_breaker_state',
    'Circuit breaker state (0=closed, 1=open, 2=half_open)',
    ['source']
)

# ========================================
# è£…é¥°å™¨ï¼šè‡ªåŠ¨è®°å½•æŒ‡æ ‡
# ========================================

def track_api_call(source: str, endpoint: str):
    """è£…é¥°å™¨ï¼šè¿½è¸ªAPIè°ƒç”¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            status = "success"

            try:
                result = func(*args, **kwargs)

                # è®°å½•æˆåŠŸ
                latency = time.time() - start_time
                api_latency.labels(
                    source=source,
                    endpoint=endpoint,
                    status="success"
                ).observe(latency)

                api_calls_total.labels(
                    source=source,
                    endpoint=endpoint,
                    status="success"
                ).inc()

                return result

            except Exception as e:
                status = "error"
                latency = time.time() - start_time

                # è®°å½•å¤±è´¥
                api_latency.labels(
                    source=source,
                    endpoint=endpoint,
                    status="error"
                ).observe(latency)

                api_calls_total.labels(
                    source=source,
                    endpoint=endpoint,
                    status="error"
                ).inc()

                raise

        return wrapper
    return decorator

# ========================================
# æŒ‡æ ‡æ”¶é›†å™¨
# ========================================

class DataSourceMetrics:
    """æ•°æ®æºæŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self, source: str):
        self.source = source
        self.latency_samples = []

    def record_latency(self, value: float):
        """è®°å½•å»¶è¿Ÿæ ·æœ¬"""
        self.latency_samples.append(value)

        # ä¿ç•™æœ€è¿‘1000ä¸ªæ ·æœ¬
        if len(self.latency_samples) > 1000:
            self.latency_samples.pop(0)

    def get_percentiles(self) -> Dict[str, float]:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        if not self.latency_samples:
            return {"p50": 0, "p95": 0, "p99": 0}

        import numpy as np
        return {
            "p50": float(np.percentile(self.latency_samples, 50)),
            "p95": float(np.percentile(self.latency_samples, 95)),
            "p99": float(np.percentile(self.latency_samples, 99))
        }

    def update_quality_score(self, score: float):
        """æ›´æ–°æ•°æ®è´¨é‡è¯„åˆ†"""
        data_quality_score.labels(source=self.source).set(score)

    def update_cost(self, cost: float):
        """æ›´æ–°æˆæœ¬ä¼°ç®—"""
        api_cost_estimated.labels(source=self.source).set(cost)

    def record_cache_hit(self):
        """è®°å½•ç¼“å­˜å‘½ä¸­"""
        cache_hits.labels(source=self.source).inc()

    def record_cache_miss(self):
        """è®°å½•ç¼“å­˜æœªå‘½ä¸­"""
        cache_misses.labels(source=self.source).inc()

    def update_circuit_breaker_state(self, state: str):
        """æ›´æ–°ç†”æ–­å™¨çŠ¶æ€"""
        state_map = {
            "closed": 0,
            "open": 1,
            "half_open": 2
        }
        circuit_breaker_state.labels(
            source=self.source
        ).set(state_map.get(state, 0))
```

**å¯åŠ¨Prometheus HTTPæœåŠ¡å™¨**:

```python
# src/core/data_source/base.py

class DataSourceManagerV2:
    def __init__(self, ...):
        # å¯åŠ¨PrometheusæŒ‡æ ‡æœåŠ¡å™¨ (ç«¯å£9091)
        try:
            start_http_server(9091)
            logger.info("PrometheusæŒ‡æ ‡æœåŠ¡å™¨å¯åŠ¨: http://localhost:9091")
        except Exception as e:
            logger.warning(f"PrometheusæœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
```

**Grafanaä»ªè¡¨æ¿é…ç½®**:

```json
{
  "dashboard": {
    "title": "æ•°æ®æºç›‘æ§ä»ªè¡¨æ¿",
    "panels": [
      {
        "title": "APIå»¶è¿Ÿ (P95)",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, datasource_api_latency_seconds_bucket)"
          }
        ]
      },
      {
        "title": "APIæˆåŠŸç‡",
        "targets": [
          {
            "expr": "rate(datasource_api_calls_total{status=\"success\"}[5m]) / rate(datasource_api_calls_total[5m]) * 100"
          }
        ]
      },
      {
        "title": "ç¼“å­˜å‘½ä¸­ç‡",
        "targets": [
          {
            "expr": "datasource_cache_hits_total / (datasource_cache_hits_total + datasource_cache_misses_total) * 100"
          }
        ]
      }
    ]
  }
}
```

**é¢„æœŸæ”¶ç›Š**:
- âœ… å…¨é¢å¯è§‚æµ‹æ€§
- âœ… å¿«é€Ÿå®šä½é—®é¢˜ (å¹³å‡MTTRâ†“50%)
- âœ… æ•°æ®é©±åŠ¨çš„ä¼˜åŒ–å†³ç­–

---

#### ä¼˜åŒ–6: è¯·æ±‚åˆå¹¶ä¸æ‰¹å¤„ç† â­â­â­

**ç›®æ ‡**: æå‡ååé‡3-5å€

**è®¾è®¡æ–¹æ¡ˆ**:

```python
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Callable
import threading
import time

class BatchProcessor:
    """
    æ‰¹å¤„ç†å™¨ï¼šåˆå¹¶å¤šä¸ªè¯·æ±‚

    ç‰¹æ€§ï¼š
    1. è‡ªåŠ¨æŒ‰æ•°æ®æºåˆ†ç»„
    2. å¹¶å‘æ‰§è¡Œæ‰¹æ¬¡
    3. è¶…æ—¶è‡ªåŠ¨è§¦å‘
    """

    def __init__(
        self,
        max_batch_size: int = 100,
        max_wait_time: float = 0.5,  # ç§’
        max_workers: int = 5
    ):
        self.max_batch_size = max_batch_size
        self.max_wait_time = max_wait_time
        self.max_workers = max_workers

        self.pending_requests: List[Dict] = []
        self.lock = threading.Lock()
        self.last_flush_time = time.time()

    def add_request(
        self,
        request: Dict,
        callback: Optional[Callable] = None
    ):
        """
        æ·»åŠ è¯·æ±‚åˆ°æ‰¹å¤„ç†é˜Ÿåˆ—

        Args:
            request: è¯·æ±‚å­—å…¸
                - endpoint_name: ç«¯ç‚¹åç§°
                - params: è°ƒç”¨å‚æ•°
            callback: å®Œæˆå›è°ƒå‡½æ•°
        """
        with self.lock:
            self.pending_requests.append({
                "request": request,
                "callback": callback,
                "added_at": time.time()
            })

            # æ£€æŸ¥æ˜¯å¦éœ€è¦è§¦å‘æ‰§è¡Œ
            if self._should_flush():
                self._flush()

    def _should_flush(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è§¦å‘æ‰¹å¤„ç†"""
        # æ¡ä»¶1: è¾¾åˆ°æ‰¹æ¬¡å¤§å°
        if len(self.pending_requests) >= self.max_batch_size:
            return True

        # æ¡ä»¶2: è¶…æ—¶æ—¶é—´åˆ°è¾¾
        elapsed = time.time() - self.last_flush_time
        if elapsed >= self.max_wait_time and len(self.pending_requests) > 0:
            return True

        return False

    def _flush(self):
        """æ‰§è¡Œæ‰¹å¤„ç†"""
        if not self.pending_requests:
            return

        logger.info(
            f"æ‰§è¡Œæ‰¹å¤„ç†: {len(self.pending_requests)}ä¸ªè¯·æ±‚"
        )

        # 1. æŒ‰æ•°æ®æºåˆ†ç»„
        grouped = self._group_by_source(self.pending_requests)

        # 2. å¹¶å‘æ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {}

            for source, requests in grouped.items():
                future = executor.submit(
                    self._execute_batch,
                    source,
                    requests
                )
                futures[future] = (source, requests)

            # 3. ç­‰å¾…æ‰€æœ‰æ‰¹æ¬¡å®Œæˆ
            for future in as_completed(futures):
                source, requests = futures[future]
                try:
                    results = future.result()
                    logger.info(
                        f"æ‰¹æ¬¡å®Œæˆ: source={source}, count={len(results)}"
                    )

                    # è°ƒç”¨å›è°ƒ
                    for req_item, result in zip(requests, results):
                        callback = req_item.get("callback")
                        if callback:
                            callback(result)

                except Exception as e:
                    logger.error(
                        f"æ‰¹æ¬¡æ‰§è¡Œå¤±è´¥: source={source}, error={e}"
                    )

                    # é”™è¯¯å›è°ƒ
                    for req_item in requests:
                        callback = req_item.get("callback")
                        if callback:
                            callback(None)

        # æ¸…ç©ºé˜Ÿåˆ—
        self.pending_requests.clear()
        self.last_flush_time = time.time()

    def _group_by_source(
        self,
        requests: List[Dict]
    ) -> Dict[str, List[Dict]]:
        """æŒ‰æ•°æ®æºåˆ†ç»„"""
        grouped = {}

        for req_item in requests:
            request = req_item["request"]
            endpoint_name = request["endpoint_name"]

            # æå–æ•°æ®æºç±»å‹
            source_type = endpoint_name.split("_")[0]  # ç®€åŒ–ç‰ˆ

            if source_type not in grouped:
                grouped[source_type] = []

            grouped[source_type].append(req_item)

        return grouped

    def _execute_batch(
        self,
        source: str,
        requests: List[Dict]
    ) -> List[Any]:
        """æ‰§è¡Œå•ä¸ªæ•°æ®æºçš„æ‰¹æ¬¡"""
        # TODO: å®ç°å®é™…çš„æ‰¹é‡è°ƒç”¨é€»è¾‘
        # è¿™é‡Œç®€åŒ–ä¸ºä¸²è¡Œè°ƒç”¨ï¼Œå®é™…åº”è¯¥è°ƒç”¨æ•°æ®æºçš„æ‰¹é‡æ¥å£

        results = []
        for req_item in requests:
            request = req_item["request"]
            params = request["params"]

            # è°ƒç”¨å•ä¸ªè¯·æ±‚
            try:
                result = self._call_single(source, params)
                results.append(result)
            except Exception as e:
                logger.error(f"å•ä¸ªè¯·æ±‚å¤±è´¥: {e}")
                results.append(None)

        return results

    def _call_single(self, source: str, params: Dict) -> Any:
        """è°ƒç”¨å•ä¸ªè¯·æ±‚ (å ä½ç¬¦)"""
        # å®é™…å®ç°åº”è¯¥è°ƒç”¨DataSourceManager
        pass
```

**é›†æˆåˆ°GovernanceDataFetcher**:

```python
# src/governance/core/fetcher_bridge.py

class GovernanceDataFetcher:
    def __init__(self):
        self.manager = self._get_manager_instance()
        self.batch_processor = BatchProcessor(
            max_batch_size=100,
            max_wait_time=0.5
        )

    def fetch_batch_kline(
        self,
        symbols: List[str],
        start_date: str,
        end_date: str,
        period: TimeFrame = TimeFrame.DAILY,
        policy: RoutePolicy = RoutePolicy.SMART_ROUTING
    ) -> Dict[str, pd.DataFrame]:
        """æ‰¹é‡è·å–Kçº¿æ•°æ® (ä½¿ç”¨æ‰¹å¤„ç†å™¨)"""
        results = {}

        # æ·»åŠ è¯·æ±‚åˆ°æ‰¹å¤„ç†é˜Ÿåˆ—
        for symbol in symbols:
            self.batch_processor.add_request(
                request={
                    "endpoint_name": f"stock_{symbol}",
                    "params": {
                        "symbol": symbol,
                        "start_date": start_date,
                        "end_date": end_date
                    }
                },
                callback=lambda result, s=symbol: self._on_result(
                    s, result, results
                )
            )

        # ç­‰å¾…æ‰€æœ‰æ‰¹æ¬¡å®Œæˆ
        self.batch_processor._flush()

        return results

    def _on_result(self, symbol: str, result: Any, results: Dict):
        """ç»“æœå›è°ƒ"""
        if result is not None:
            results[symbol] = result
```

**é¢„æœŸæ”¶ç›Š**:
- âœ… ååé‡æå‡ **3-5å€**
- âœ… ç½‘ç»œå¾€è¿”æ—¶é—´å‡å°‘ **80%**
- âœ… APIè°ƒç”¨æ•ˆç‡æå‡

---

### ğŸ”· P2 ä¼˜åŒ– (ä¸­æœŸè§„åˆ’)

#### ä¼˜åŒ–7: æ•°æ®è¡€ç¼˜è¿½è¸ª â­â­â­

**ç›®æ ‡**: è®°å½•æ•°æ®æµå‘å’Œè½¬æ¢å†å²

**è®¾è®¡æ–¹æ¡ˆ**:

```python
# src/governance/lineage/tracker.py

from typing import Dict, List, Any
from datetime import datetime
import networkx as nx

class DataLineageTracker:
    """
    æ•°æ®è¡€ç¼˜è¿½è¸ªå™¨

    åŠŸèƒ½ï¼š
    1. è®°å½•æ•°æ®æ¥æº
    2. è¿½è¸ªæ•°æ®è½¬æ¢
    3. è®°å½•æ•°æ®å»å‘
    4. å¯è§†åŒ–è¡€ç¼˜å…³ç³»
    """

    def __init__(self):
        self.graph = nx.DiGraph()

    def record_lineage(
        self,
        data_id: str,
        source: Dict,
        transformations: List[Dict],
        destinations: List[str]
    ):
        """è®°å½•æ•°æ®è¡€ç¼˜"""
        # 1. è®°å½•æ•°æ®èŠ‚ç‚¹
        self.graph.add_node(
            data_id,
            **{
                "source": source,
                "created_at": datetime.now(),
                "transformations": transformations
            }
        )

        # 2. è®°å½•è½¬æ¢å…³ç³»
        for i, transform in enumerate(transformations):
            transform_id = f"{data_id}_transform_{i}"
            self.graph.add_node(transform_id, **transform)

            # æ·»åŠ è¾¹ï¼šæ•°æ® -> è½¬æ¢
            self.graph.add_edge(data_id, transform_id)

        # 3. è®°å½•å»å‘å…³ç³»
        for dest in destinations:
            self.graph.add_edge(
                transformations[-1].get("id", data_id),
                dest
            )

        # 4. ä¿å­˜åˆ°å›¾æ•°æ®åº“
        self._save_to_graph_db(data_id, source, transformations, destinations)

    def trace_lineage(self, data_id: str) -> Dict[str, Any]:
        """è¿½æº¯æ•°æ®è¡€ç¼˜"""
        # ä¸Šæ¸¸ï¼šæ•°æ®æ¥æº
        upstream = list(self.graph.predecessors(data_id))

        # ä¸‹æ¸¸ï¼šæ•°æ®å»å‘
        downstream = list(self.graph.successors(data_id))

        return {
            "data_id": data_id,
            "upstream": upstream,
            "downstream": downstream,
            "full_path": nx.shortest_path(self.graph, data_id)
        }

    def _save_to_graph_db(self, data_id, source, transformations, destinations):
        """ä¿å­˜åˆ°å›¾æ•°æ®åº“ (Neo4j)"""
        # TODO: å®ç°Neo4jä¿å­˜é€»è¾‘
        pass
```

---

#### ä¼˜åŒ–8: è‡ªé€‚åº”é™æµ â­â­â­

**ç›®æ ‡**: åŠ¨æ€è°ƒæ•´è¯·æ±‚é€Ÿç‡

**è®¾è®¡æ–¹æ¡ˆ**:

```python
import time
import threading

class AdaptiveRateLimiter:
    """
    è‡ªé€‚åº”é™æµå™¨

    ç‰¹æ€§ï¼š
    1. åŸºäºé”™è¯¯ç‡åŠ¨æ€è°ƒæ•´
    2. æ”¯æŒçªå¢æµé‡
    3. å¹³æ»‘é€Ÿç‡è°ƒæ•´
    """

    def __init__(
        self,
        initial_rate: int = 10,  # åˆå§‹é€Ÿç‡ (req/s)
        min_rate: int = 1,       # æœ€å°é€Ÿç‡
        max_rate: int = 100,     # æœ€å¤§é€Ÿç‡
        adjustment_factor: float = 0.1  # è°ƒæ•´å› å­
    ):
        self.current_rate = initial_rate
        self.min_rate = min_rate
        self.max_rate = max_rate
        self.adjustment_factor = adjustment_factor

        self.error_rate = 0.0
        self.lock = threading.Lock()
        self.last_call_time = None

    def acquire(self, permits: int = 1):
        """è·å–è®¿é—®è®¸å¯"""
        with self.lock:
            # åŠ¨æ€è°ƒæ•´é€Ÿç‡
            if self.error_rate > 0.1:  # é”™è¯¯ç‡>10%
                # é™é€Ÿ
                self.current_rate = max(
                    self.min_rate,
                    int(self.current_rate * (1 - self.adjustment_factor))
                )
                logger.warning(
                    f"é”™è¯¯ç‡è¿‡é«˜({self.error_rate:.1%}), é™é€Ÿè‡³{self.current_rate} req/s"
                )

            elif self.error_rate < 0.01:  # é”™è¯¯ç‡<1%
                # åŠ é€Ÿ
                self.current_rate = min(
                    self.max_rate,
                    int(self.current_rate * (1 + self.adjustment_factor))
                )
                logger.info(
                    f"é”™è¯¯ç‡ä½({self.error_rate:.1%}), åŠ é€Ÿè‡³{self.current_rate} req/s"
                )

            # é™æµæ§åˆ¶
            now = time.time()
            if self.last_call_time is not None:
                elapsed = now - self.last_call_time
                min_interval = 1.0 / self.current_rate

                if elapsed < min_interval:
                    time.sleep(min_interval - elapsed)

            self.last_call_time = time.time()

    def record_error(self):
        """è®°å½•é”™è¯¯"""
        with self.lock:
            self.error_rate = min(self.error_rate + 0.05, 1.0)

    def record_success(self):
        """è®°å½•æˆåŠŸ"""
        with self.lock:
            self.error_rate = max(self.error_rate - 0.01, 0.0)
```

---

## å®æ–½è®¡åˆ’

### ğŸ—“ï¸ åˆ†é˜¶æ®µå®æ–½

#### **Phase 1: å¿«é€Ÿè§æ•ˆ** (1-2å‘¨)

**ä¼˜åŒ–é¡¹**:
1. âœ… æ™ºèƒ½ç¼“å­˜ç­–ç•¥ (Smart Cache + TTL)
2. âœ… ç†”æ–­å™¨æœºåˆ¶ (Circuit Breaker)
3. âœ… å¢å¼ºæ•°æ®éªŒè¯ (ä¸šåŠ¡è§„åˆ™+ç»Ÿè®¡æ£€æµ‹)

**å·¥ä½œé‡**: 8-12äººå¤©

**é¢„æœŸæ”¶ç›Š**:
- APIæˆæœ¬é™ä½ **40%**
- å“åº”é€Ÿåº¦æå‡ **50%**
- æ•…éšœæ¢å¤æ—¶é—´ < **1åˆ†é’Ÿ**

**å®æ–½æ­¥éª¤**:
1. ç¬¬1-3å¤©: å®ç°SmartCache
2. ç¬¬4-6å¤©: å®ç°CircuitBreaker
3. ç¬¬7-10å¤©: å¢å¼ºDataQualityValidator
4. ç¬¬11-12å¤©: é›†æˆæµ‹è¯•å’Œæ–‡æ¡£

---

#### **Phase 2: èƒ½åŠ›æå‡** (1ä¸ªæœˆ)

**ä¼˜åŒ–é¡¹**:
4. âœ… æ™ºèƒ½è·¯ç”±ç®—æ³• (å¤šç»´åº¦å†³ç­–)
5. âœ… å®Œå–„ç›‘æ§ä½“ç³» (Prometheusé›†æˆ)
6. âœ… è¯·æ±‚åˆå¹¶æ‰¹å¤„ç† (Batch Processor)

**å·¥ä½œé‡**: 20-25äººå¤©

**é¢„æœŸæ”¶ç›Š**:
- ååé‡æå‡ **3-5å€**
- å¯è§‚æµ‹æ€§æå‡ **10å€**
- æˆæœ¬ä¼˜åŒ– **30%+**

**å®æ–½æ­¥éª¤**:
1. ç¬¬1-2å‘¨: å®ç°SmartRouter
2. ç¬¬2-3å‘¨: é›†æˆPrometheus
3. ç¬¬3-4å‘¨: å®ç°BatchProcessor
4. ç¬¬4å‘¨: é›†æˆæµ‹è¯•å’Œæ€§èƒ½éªŒè¯

---

#### **Phase 3: é«˜çº§ç‰¹æ€§** (2-3ä¸ªæœˆ)

**ä¼˜åŒ–é¡¹**:
7. âœ… æ•°æ®è¡€ç¼˜è¿½è¸ª (å®¡è®¡æ”¯æŒ)
8. âœ… è‡ªé€‚åº”é™æµ (åŠ¨æ€ä¼˜åŒ–)
9. âœ… è·¨æºäº¤å‰éªŒè¯ (æ•°æ®è´¨é‡)

**å·¥ä½œé‡**: 40-50äººå¤©

**é¢„æœŸæ”¶ç›Š**:
- å®Œæ•´å®¡è®¡èƒ½åŠ›
- æ™ºèƒ½è‡ªä¼˜åŒ–
- ç”Ÿäº§çº§å¯é æ€§

**å®æ–½æ­¥éª¤**:
1. ç¬¬1ä¸ªæœˆ: å®ç°DataLineageTracker
2. ç¬¬2ä¸ªæœˆ: å®ç°AdaptiveRateLimiter
3. ç¬¬2-3ä¸ªæœˆ: å®ç°CrossSourceValidator
4. ç¬¬3ä¸ªæœˆ: å…¨é¢æµ‹è¯•å’Œä¼˜åŒ–

---

## é¢„æœŸæ”¶ç›Š

### ğŸ“Š æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”

| ç»´åº¦ | å½“å‰çŠ¶æ€ | Phase 1 | Phase 2 | Phase 3 | æå‡å¹…åº¦ |
|------|---------|---------|---------|---------|----------|
| **æ€§èƒ½** | å¹³å‡500ms | 250ms | 100ms | 80ms | **6.25x** |
| **æˆæœ¬** | 100% åŸºçº¿ | 60% | 40% | 30% | **-70%** |
| **å¯é æ€§** | 95% å¯ç”¨æ€§ | 98% | 99.5% | 99.9% | **+5%** |
| **å¯è§‚æµ‹æ€§** | åŸºç¡€ç›‘æ§ | å¢å¼º | å…¨é¢è¿½è¸ª | æ™ºèƒ½åˆ†æ | **10x** |
| **æ‰©å±•æ€§** | æ‰‹åŠ¨é…ç½® | åŠè‡ªåŠ¨ | è‡ªåŠ¨ä¼˜åŒ– | è‡ªé€‚åº” | **3x** |
| **ååé‡** | 10 req/s | 20 req/s | 50 req/s | 100 req/s | **10x** |

### ğŸ’° æˆæœ¬èŠ‚çº¦ä¼°ç®—

**å‡è®¾**: æ¯æ—¥APIè°ƒç”¨é‡100,000æ¬¡ï¼Œå¹³å‡0.01å…ƒ/æ¬¡

| é˜¶æ®µ | æ—¥è°ƒç”¨é‡ | æ—¥æˆæœ¬ | æœˆæˆæœ¬ | èŠ‚çº¦ |
|------|----------|--------|--------|------|
| **å½“å‰** | 100,000 | Â¥1,000 | Â¥30,000 | - |
| **Phase 1** | 60,000 | Â¥600 | Â¥18,000 | **-40%** |
| **Phase 2** | 40,000 | Â¥400 | Â¥12,000 | **-60%** |
| **Phase 3** | 30,000 | Â¥300 | Â¥9,000 | **-70%** |

**å¹´åº¦èŠ‚çº¦**: **Â¥252,000** (çº¦70%)

---

## é£é™©è¯„ä¼°

### âš ï¸ ä¸»è¦é£é™©ä¸ç¼“è§£æªæ–½

| é£é™© | å½±å“ | æ¦‚ç‡ | ç¼“è§£æªæ–½ |
|------|------|------|----------|
| **ç¼“å­˜æ•°æ®ä¸€è‡´æ€§** | é«˜ | ä¸­ | å®ç°ç¼“å­˜ç‰ˆæœ¬æ§åˆ¶å’Œå¤±æ•ˆé€šçŸ¥ |
| **ç†”æ–­å™¨è¯¯è§¦å‘** | ä¸­ | ä½ | è°ƒæ•´é˜ˆå€¼å’Œè¶…æ—¶å‚æ•° |
| **ç›‘æ§ç³»ç»Ÿå¤æ‚åº¦** | ä¸­ | ä¸­ | é‡‡ç”¨æˆç†Ÿå·¥å…·é“¾ (Prometheus) |
| **æ‰¹å¤„ç†å»¶è¿Ÿå¢åŠ ** | ä¸­ | ä¸­ | æä¾›åŒæ­¥/å¼‚æ­¥åŒæ¨¡å¼ |
| **æ•°æ®æºAPIé™åˆ¶** | é«˜ | é«˜ | å®ç°è¯·æ±‚é˜Ÿåˆ—å’Œé™çº§ç­–ç•¥ |
| **è¿ç§»å…¼å®¹æ€§** | é«˜ | ä½ | ä¿æŒå‘åå…¼å®¹ï¼Œåˆ†æ­¥è¿ç§» |

### ğŸ›¡ï¸ è´¨é‡ä¿è¯æªæ–½

1. **å•å…ƒæµ‹è¯•è¦†ç›–ç‡**: ç›®æ ‡80%+
2. **é›†æˆæµ‹è¯•**: è¦†ç›–æ‰€æœ‰ä¼˜åŒ–ç‚¹
3. **æ€§èƒ½æµ‹è¯•**: å‹åŠ›æµ‹è¯•å’ŒåŸºå‡†å¯¹æ¯”
4. **ç°åº¦å‘å¸ƒ**: åˆ†é˜¶æ®µä¸Šçº¿ï¼Œç›‘æ§å…³é”®æŒ‡æ ‡
5. **å›æ»šé¢„æ¡ˆ**: ä¿ç•™å¿«é€Ÿå›æ»šèƒ½åŠ›

---

## å‚è€ƒèµ„æº

### ğŸ“š æ¨èä¹¦ç±

- **Release It!** (Michael Nygard) - Circuit Breakeræ¨¡å¼
- **Designing Data-Intensive Applications** (Martin Kleppmann) - æ•°æ®æ¶æ„è®¾è®¡
- **Site Reliability Engineering** (Google SRE) - ç›‘æ§å’Œå‘Šè­¦

### ğŸ”§ å·¥å…·åº“

- `circuitbreaker` - Pythonç†”æ–­å™¨åº“
- `prometheus_client` - Prometheusç›‘æ§
- `ratelimit` - é™æµåº“
- `cachetools` - é«˜çº§ç¼“å­˜å·¥å…·

### ğŸŒ æœ€ä½³å®è·µ

- **Google SREæ‰‹å†Œ** - ç›‘æ§å’Œå‘Šè­¦
- **AWS Well-Architected Framework** - æˆæœ¬ä¼˜åŒ–
- **Netflix Hystrix** - ç†”æ–­å™¨å®ç°å‚è€ƒ
- **Stripe APIè®¾è®¡** - æ‰¹å¤„ç†å’Œé™æµ

---

## é™„å½•

### A. å…³é”®æŒ‡æ ‡å®šä¹‰

| æŒ‡æ ‡ | å®šä¹‰ | è®¡ç®—æ–¹å¼ |
|------|------|----------|
| **P50å»¶è¿Ÿ** | 50%è¯·æ±‚çš„å“åº”æ—¶é—´ | ç™¾åˆ†ä½æ•° |
| **P95å»¶è¿Ÿ** | 95%è¯·æ±‚çš„å“åº”æ—¶é—´ | ç™¾åˆ†ä½æ•° |
| **P99å»¶è¿Ÿ** | 99%è¯·æ±‚çš„å“åº”æ—¶é—´ | ç™¾åˆ†ä½æ•° |
| **æˆåŠŸç‡** | æˆåŠŸè¯·æ±‚å æ¯” | æˆåŠŸæ•° / æ€»æ•° Ã— 100% |
| **ç¼“å­˜å‘½ä¸­ç‡** | ç¼“å­˜å‘½ä¸­å æ¯” | å‘½ä¸­æ•° / (å‘½ä¸­æ•°+æœªå‘½ä¸­æ•°) Ã— 100% |
| **æ•°æ®å®Œæ•´æ€§** | éç©ºæ•°æ®å æ¯” | éç©ºè®°å½•æ•° / æ€»è®°å½•æ•° Ã— 100% |

### B. é…ç½®ç¤ºä¾‹

**YAMLé…ç½®**:
```yaml
# config/optimization_config.yaml

cache:
  maxsize: 100
  ttl: 3600  # 1å°æ—¶
  refresh_ratio: 0.8

circuit_breaker:
  failure_threshold: 5
  timeout: 60
  half_open_max_calls: 3

router:
  weights:
    performance: 0.4
    cost: 0.3
    load: 0.2
    location: 0.1

batch_processor:
  max_batch_size: 100
  max_wait_time: 0.5
  max_workers: 5
```

---

**æ–‡æ¡£ç»“æŸ**

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**:
1. å®¡æ ¡æœ¬ææ¡ˆ
2. ä¼˜å…ˆçº§æ’åº
3. èµ„æºè¯„ä¼°
4. åˆ¶å®šè¯¦ç»†å®æ–½è®¡åˆ’
5. å¯åŠ¨Phase 1å¼€å‘
