"""
GPU Acceleration Performance Benchmark - GPUåŠ é€Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•

æµ‹è¯•GPUæŒ‡æ ‡è®¡ç®—çš„æ€§èƒ½æå‡å’Œå‡†ç¡®æ€§ï¼š
- CPU vs GPU æ€§èƒ½å¯¹æ¯”
- å‡†ç¡®æ€§éªŒè¯
- å†…å­˜ä½¿ç”¨åˆ†æ
- æ‰¹é‡å¤„ç†ä¼˜åŒ–

ä½œè€…: Claude Code (Sisyphus)
æ—¥æœŸ: 2026-01-14
"""

import time
import logging
from typing import Dict, Any, List
import numpy as np
import pandas as pd
from dataclasses import dataclass
from pathlib import Path

from web.backend.app.services.indicators.gpu_adapter import (
    GPUIndicatorAdapter,
    GPUIndicatorFactory,
    GPU_AVAILABLE,
)
from src.indicators.indicator_factory import IndicatorFactory
from web.backend.app.services.indicators.gpu_adapter import IndicatorConfig

logger = logging.getLogger(__name__)


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""

    indicator_name: str
    data_size: int
    gpu_time: float
    cpu_time: float
    speedup_ratio: float
    gpu_accuracy: float
    cpu_accuracy: float
    gpu_memory_mb: float
    cpu_memory_mb: float
    batch_size: int


class GPUPerformanceBenchmark:
    """
    GPUæ€§èƒ½åŸºå‡†æµ‹è¯•å™¨

    æä¾›å…¨é¢çš„GPUåŠ é€Ÿæ€§èƒ½è¯„ä¼°ï¼š
    - æ€§èƒ½å¯¹æ¯”æµ‹è¯•
    - å‡†ç¡®æ€§éªŒè¯
    - å†…å­˜ä½¿ç”¨åˆ†æ
    - æ‰¹é‡å¤§å°ä¼˜åŒ–
    """

    def __init__(self):
        self.results = []
        self.test_data_sizes = [1000, 5000, 10000, 50000, 100000]
        self.test_indicators = ["macd", "rsi", "bbands"]

        # åˆ›å»ºæµ‹è¯•æ•°æ®ç”Ÿæˆå™¨
        self.data_generator = TestDataGenerator()

        logger.info("âœ… GPU Performance Benchmark initialized")

    def run_full_benchmark(self) -> List[BenchmarkResult]:
        """
        è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•

        Returns:
            åŸºå‡†æµ‹è¯•ç»“æœåˆ—è¡¨
        """
        logger.info("ğŸš€ Starting GPU Performance Benchmark...")

        for indicator_name in self.test_indicators:
            logger.info(f"Testing indicator: {indicator_name}")

            for data_size in self.test_data_sizes:
                try:
                    result = self._benchmark_indicator(indicator_name, data_size)
                    self.results.append(result)
                    logger.info(
                        f"âœ… {indicator_name} ({data_size}): {result.speedup_ratio:.1f}x speedup"
                    )

                except Exception as e:
                    logger.error(
                        f"âŒ Benchmark failed for {indicator_name} ({data_size}): {e}"
                    )

        logger.info("âœ… GPU Performance Benchmark completed")
        return self.results

    def _benchmark_indicator(
        self, indicator_name: str, data_size: int
    ) -> BenchmarkResult:
        """æµ‹è¯•å•ä¸ªæŒ‡æ ‡çš„æ€§èƒ½"""
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        test_data = self.data_generator.generate_stock_data(data_size)

        # GPUæµ‹è¯•
        gpu_result = self._test_gpu_indicator(indicator_name, test_data)

        # CPUæµ‹è¯•
        cpu_result = self._test_cpu_indicator(indicator_name, test_data)

        # è®¡ç®—åŠ é€Ÿæ¯”
        speedup_ratio = (
            cpu_result["time"] / gpu_result["time"] if gpu_result["time"] > 0 else 0
        )

        # éªŒè¯å‡†ç¡®æ€§
        accuracy_score = self._validate_accuracy(gpu_result["data"], cpu_result["data"])

        return BenchmarkResult(
            indicator_name=indicator_name,
            data_size=data_size,
            gpu_time=gpu_result["time"],
            cpu_time=cpu_result["time"],
            speedup_ratio=speedup_ratio,
            gpu_accuracy=accuracy_score,
            cpu_accuracy=1.0,  # CPUä½œä¸ºåŸºå‡†
            gpu_memory_mb=gpu_result["memory_mb"],
            cpu_memory_mb=cpu_result["memory_mb"],
            batch_size=data_size,  # ç®€åŒ–å¤„ç†
        )

    def _test_gpu_indicator(
        self, indicator_name: str, test_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æµ‹è¯•GPUæŒ‡æ ‡è®¡ç®—"""
        try:
            # åˆ›å»ºGPUæŒ‡æ ‡
            config = IndicatorConfig(
                name=f"{indicator_name}_gpu",
                type=indicator_name,
                parameters=self._get_indicator_params(indicator_name),
            )

            indicator = GPUIndicatorFactory.create_indicator(indicator_name, config)

            # è®°å½•å¼€å§‹æ—¶é—´å’Œå†…å­˜
            start_time = time.time()
            start_memory = self._get_memory_usage()

            # æ‰§è¡Œè®¡ç®—
            result = indicator.calculate(test_data)

            # è®°å½•ç»“æŸæ—¶é—´å’Œå†…å­˜
            end_time = time.time()
            end_memory = self._get_memory_usage()

            return {
                "time": end_time - start_time,
                "memory_mb": end_memory - start_memory,
                "data": result.data,
            }

        except Exception as e:
            logger.warning(f"GPU test failed, falling back to CPU: {e}")
            return self._test_cpu_indicator(indicator_name, test_data)

    def _test_cpu_indicator(
        self, indicator_name: str, test_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æµ‹è¯•CPUæŒ‡æ ‡è®¡ç®—"""
        try:
            # ä½¿ç”¨ç°æœ‰çš„CPUæŒ‡æ ‡å·¥å‚
            indicator = IndicatorFactory.create_indicator(indicator_name)

            # è®°å½•å¼€å§‹æ—¶é—´å’Œå†…å­˜
            start_time = time.time()
            start_memory = self._get_memory_usage()

            # æ‰§è¡Œè®¡ç®—
            result = indicator.calculate(test_data)

            # è®°å½•ç»“æŸæ—¶é—´å’Œå†…å­˜
            end_time = time.time()
            end_memory = self._get_memory_usage()

            return {
                "time": end_time - start_time,
                "memory_mb": end_memory - start_memory,
                "data": result.data if hasattr(result, "data") else result,
            }

        except Exception as e:
            logger.error(f"CPU test failed: {e}")
            return {
                "time": 1.0,  # é»˜è®¤1ç§’
                "memory_mb": 0.0,
                "data": {},
            }

    def _get_indicator_params(self, indicator_name: str) -> Dict[str, Any]:
        """è·å–æŒ‡æ ‡é»˜è®¤å‚æ•°"""
        param_map = {
            "macd": {"fast_period": 12, "slow_period": 26, "signal_period": 9},
            "rsi": {"period": 14},
            "bbands": {"period": 20, "std_dev": 2.0},
        }
        return param_map.get(indicator_name, {})

    def _get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        try:
            import psutil

            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024  # MB
        except ImportError:
            return 0.0

    def _validate_accuracy(
        self, gpu_data: Dict[str, Any], cpu_data: Dict[str, Any]
    ) -> float:
        """éªŒè¯GPUå’ŒCPUç»“æœçš„å‡†ç¡®æ€§"""
        try:
            accuracy_scores = []

            for key in gpu_data.keys():
                if key in cpu_data:
                    gpu_values = np.array(gpu_data[key])
                    cpu_values = np.array(cpu_data[key])

                    # è®¡ç®—ç›¸å¯¹è¯¯å·®
                    diff = np.abs(gpu_values - cpu_values)
                    relative_error = diff / (np.abs(cpu_values) + 1e-10)  # é¿å…é™¤é›¶

                    # è®¡ç®—å‡†ç¡®ç‡ï¼ˆè¯¯å·®å°äº1%çš„æ¯”ä¾‹ï¼‰
                    accuracy = np.mean(relative_error < 0.01)
                    accuracy_scores.append(accuracy)

            return np.mean(accuracy_scores) if accuracy_scores else 0.0

        except Exception as e:
            logger.error(f"Accuracy validation failed: {e}")
            return 0.0

    def generate_report(
        self, output_path: str = "docs/reports/GPU_ACCELERATION_BENCHMARK.md"
    ) -> None:
        """
        ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š

        Args:
            output_path: æŠ¥å‘Šè¾“å‡ºè·¯å¾„
        """
        if not self.results:
            logger.warning("No benchmark results to report")
            return

        report = self._create_markdown_report()

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)

        # å†™å…¥æŠ¥å‘Š
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(report)

        logger.info(f"âœ… Benchmark report generated: {output_path}")

    def _create_markdown_report(self) -> str:
        """åˆ›å»ºMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        lines = []

        # æ ‡é¢˜
        lines.append("# GPU Acceleration Performance Benchmark Report")
        lines.append("")
        lines.append("**Generated:** 2026-01-14")
        lines.append("**Test Environment:** MyStocks Quantitative Trading System")
        lines.append("")

        # æ¦‚è¿°
        total_tests = len(self.results)
        gpu_available = GPU_AVAILABLE
        avg_speedup = np.mean(
            [r.speedup_ratio for r in self.results if r.speedup_ratio > 0]
        )

        lines.append("## Executive Summary")
        lines.append("")
        lines.append(f"- **Total Tests:** {total_tests}")
        lines.append(f"- **GPU Available:** {'âœ… Yes' if gpu_available else 'âŒ No'}")
        lines.append(".1f")
        lines.append(
            f"- **Average Accuracy:** {np.mean([r.gpu_accuracy for r in self.results]):.1%}"
        )
        lines.append("")

        # è¯¦ç»†ç»“æœè¡¨æ ¼
        lines.append("## Detailed Results")
        lines.append("")
        lines.append(
            "| Indicator | Data Size | GPU Time | CPU Time | Speedup | Accuracy | GPU Memory | CPU Memory |"
        )
        lines.append(
            "|-----------|-----------|----------|----------|---------|----------|------------|------------|"
        )

        for result in self.results:
            lines.append("3.3f3.3f.1f.1%.1f.1f")

        lines.append("")

        # æ€§èƒ½åˆ†æ
        lines.append("## Performance Analysis")
        lines.append("")

        # æŒ‰æŒ‡æ ‡åˆ†ç»„çš„åŠ é€Ÿæ¯”
        for indicator in self.test_indicators:
            indicator_results = [
                r for r in self.results if r.indicator_name == indicator
            ]
            if indicator_results:
                avg_speedup = np.mean([r.speedup_ratio for r in indicator_results])
                max_speedup = np.max([r.speedup_ratio for r in indicator_results])
                lines.append(f"### {indicator.upper()}")
                lines.append(".1f")
                lines.append(".1f")
                lines.append("")

        # å»ºè®®
        lines.append("## Recommendations")
        lines.append("")
        if gpu_available and avg_speedup > 2.0:
            lines.append("âœ… **GPU acceleration is highly effective**")
            lines.append(
                "- Consider enabling GPU acceleration for production workloads"
            )
            lines.append("- Focus on optimizing data transfer between CPU and GPU")
        elif gpu_available and avg_speedup > 1.2:
            lines.append("âš ï¸ **GPU acceleration provides moderate benefits**")
            lines.append(
                "- Consider GPU acceleration for large datasets (>10K data points)"
            )
            lines.append("- Evaluate cost-benefit ratio for production deployment")
        else:
            lines.append("âŒ **GPU acceleration not recommended**")
            lines.append("- Stick with CPU implementation for better reliability")
            lines.append("- Consider optimizing CPU algorithms instead")

        lines.append("")
        lines.append("## Technical Notes")
        lines.append("")
        lines.append("- All tests use synthetic stock market data")
        lines.append("- Accuracy validation uses 1% relative error threshold")
        lines.append("- Memory measurements may include system overhead")
        lines.append("- Results may vary based on specific hardware configuration")

        return "\n".join(lines)


class TestDataGenerator:
    """æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨"""

    def __init__(self, seed: int = 42):
        np.random.seed(seed)
        self.seed = seed

    def generate_stock_data(self, size: int) -> Dict[str, Any]:
        """ç”Ÿæˆè‚¡ç¥¨æµ‹è¯•æ•°æ®"""
        # ç”ŸæˆåŸºç¡€ä»·æ ¼æ•°æ®
        base_price = 100.0
        price_changes = np.random.normal(0.001, 0.02, size)  # æ¯æ—¥æ”¶ç›Šç‡
        prices = base_price * np.cumprod(1 + price_changes)

        # ç”ŸæˆOHLCVæ•°æ®
        high_multipliers = 1 + np.random.uniform(0, 0.05, size)
        low_multipliers = 1 - np.random.uniform(0, 0.05, size)
        volume_base = 1000000

        return {
            "open": prices * (1 + np.random.normal(0, 0.01, size)),
            "high": prices * high_multipliers,
            "low": prices * low_multipliers,
            "close": prices,
            "volume": volume_base * (1 + np.random.uniform(0, 2, size)),
        }


# ä¾¿æ·å‡½æ•°
def run_gpu_benchmark(save_report: bool = True) -> List[BenchmarkResult]:
    """è¿è¡ŒGPUåŸºå‡†æµ‹è¯•ï¼ˆä¾¿æ·å‡½æ•°ï¼‰"""
    benchmark = GPUPerformanceBenchmark()
    results = benchmark.run_full_benchmark()

    if save_report:
        benchmark.generate_report()

    return results


if __name__ == "__main__":
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    results = run_gpu_benchmark()

    # æ‰“å°æ‘˜è¦
    print("\nğŸ¯ GPU Benchmark Summary:")
    print(f"Tests completed: {len(results)}")

    if results:
        avg_speedup = np.mean([r.speedup_ratio for r in results if r.speedup_ratio > 0])
        avg_accuracy = np.mean([r.gpu_accuracy for r in results])
        print(".1f")
        print(".1%")

    print("\nğŸ“Š Detailed report saved to: docs/reports/GPU_ACCELERATION_BENCHMARK.md")
