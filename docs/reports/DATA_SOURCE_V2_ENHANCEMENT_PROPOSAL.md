# æ•°æ®æºç®¡ç†V2.0 - åŠŸèƒ½å¢å¼ºåˆ†æä¸å®æ–½å»ºè®®

> **æ—¥æœŸ**: 2026-01-02
> **å½“å‰ç‰ˆæœ¬**: v2.0 (Phase 1-4å®Œæˆ)
> **ç›®çš„**: åˆ†æå½“å‰å®ç°ä¸ç†æƒ³æ¶æ„çš„å·®è·ï¼Œæä¾›å¢å¼ºå»ºè®®

---

## é—®é¢˜1: è‡ªåŠ¨å‘ç°ä¸æ³¨å†Œæµç¨‹å®ç°åˆ†æ

### ç†æƒ³æµç¨‹

```
æ–°æ•°æ®æº â†’ æ³¨å†Œæµ‹è¯• â†’ è´¨é‡è¯„ä¼° â†’ ç”Ÿäº§ä½¿ç”¨ â†’ å®šæœŸå·¡æ£€ â†’ ä¸‹çº¿å½’æ¡£
```

### å½“å‰å®ç°çŠ¶æ€

| é˜¶æ®µ | å®ç°çŠ¶æ€ | å…·ä½“åŠŸèƒ½ | å·®è· |
|------|---------|---------|------|
| **æ–°æ•°æ®æº** | âš ï¸ éƒ¨åˆ†å®ç° | âœ… YAMLæ‰‹åŠ¨æ³¨å†Œ<br>âœ… sync_sources.pyåŒæ­¥<br>âŒ æ— è‡ªåŠ¨å‘ç° | éœ€è¦DataSourceDiscoverer |
| **æ³¨å†Œæµ‹è¯•** | âš ï¸ éƒ¨åˆ†å®ç° | âœ… test_parameterså­—æ®µ<br>âœ… health_check()æ–¹æ³•<br>âŒ æ— ç‹¬ç«‹æµ‹è¯•æ¡†æ¶ | éœ€è¦æµ‹è¯•è‡ªåŠ¨åŒ– |
| **è´¨é‡è¯„ä¼°** | âœ… å·²å®ç° | âœ… data_quality_score<br>âœ… success_rateç›‘æ§<br>âœ… avg_response_time<br>âœ… æ™ºèƒ½è·¯ç”±é€‰æ‹© | åŠŸèƒ½å®Œæ•´ |
| **ç”Ÿäº§ä½¿ç”¨** | âœ… å·²å®ç° | âœ… statuså­—æ®µç®¡ç†<br>âœ… æ™ºèƒ½è·¯ç”±<br>âœ… æ•…éšœé™çº§ | åŠŸèƒ½å®Œæ•´ |
| **å®šæœŸå·¡æ£€** | âš ï¸ éƒ¨åˆ†å®ç° | âœ… å¥åº·æ£€æŸ¥åŠŸèƒ½<br>âŒ æ— è‡ªåŠ¨è°ƒåº¦<br>âŒ æ— å·¡æ£€æŠ¥å‘Š | éœ€è¦å®šæ—¶ä»»åŠ¡ |
| **ä¸‹çº¿å½’æ¡£** | âŒ æœªå®ç° | âŒ æ— ä¸‹çº¿æµç¨‹<br>âŒ æ— å½’æ¡£æœºåˆ¶<br>âŒ æ— å†å²æ•°æ®æ¸…ç† | éœ€è¦å®Œæ•´çš„ç”Ÿå‘½å‘¨æœŸç®¡ç† |

---

## é—®é¢˜2: Grafanaç®¡ç†åŠŸèƒ½å®ç°åˆ†æ

### ç”¨æˆ·æœŸæœ›çš„æ ¸å¿ƒåŠŸèƒ½

#### 2.1 æ¥å£æ³¨å†Œè¡¨æŸ¥è¯¢

**æœŸæœ›åŠŸèƒ½**:
- æŒ‰5å±‚åˆ†ç±»ç­›é€‰
- æŒ‰æ•°æ®æºç±»å‹ç­›é€‰
- æŒ‰å¯ç”¨çŠ¶æ€ç­›é€‰
- æ”¯æŒæ¨¡ç³Šæœç´¢ï¼ˆå¦‚æœç´¢"æ—¥çº¿"ï¼‰

**å½“å‰å®ç°**: âŒ **æœªå®ç°**

**ç°çŠ¶**: Grafanaä»…å±•ç¤ºç›‘æ§æ•°æ®ï¼Œä¸æä¾›æ³¨å†Œè¡¨æŸ¥è¯¢åŠŸèƒ½

**å»ºè®®æ–¹æ¡ˆ**:

##### æ–¹æ¡ˆA: FastAPIåç«¯ + Vueå‰ç«¯ï¼ˆæ¨èï¼‰

```python
# web/backend/app/api/data_source_registry.py
from fastapi import APIRouter, Query
from typing import Optional

router = APIRouter()

@router.get("/api/v1/data-sources")
async def search_data_sources(
    data_category: Optional[str] = Query(None, description="5å±‚åˆ†ç±»"),
    source_type: Optional[str] = Query(None, description="æ•°æ®æºç±»å‹"),
    status: Optional[str] = Query("active", description="å¯ç”¨çŠ¶æ€"),
    keyword: Optional[str] = Query(None, description="æ¨¡ç³Šæœç´¢å…³é”®è¯")
):
    """
    æœç´¢æ•°æ®æºæ¥å£

    ç¤ºä¾‹:
        GET /api/v1/data-sources?data_category=DAILY_KLINE&keyword=æ—¥çº¿
    """
    from src.core.data_source_manager_v2 import DataSourceManagerV2

    manager = DataSourceManagerV2()

    # ä½¿ç”¨ç°æœ‰æŸ¥è¯¢åŠŸèƒ½
    endpoints = manager.find_endpoints(
        data_category=data_category,
        source_type=source_type,
        only_healthy=(status == "active")
    )

    # å…³é”®è¯è¿‡æ»¤
    if keyword:
        endpoints = [
            ep for ep in endpoints
            if keyword.lower() in ep['endpoint_name'].lower() or
               keyword.lower() in ep.get('description', '').lower()
        ]

    return {
        "total": len(endpoints),
        "data_sources": endpoints
    }

@router.get("/api/v1/data-sources/categories")
async def get_categories():
    """è·å–æ‰€æœ‰5å±‚æ•°æ®åˆ†ç±»åŠç»Ÿè®¡"""
    from src.core.data_source_manager_v2 import DataSourceManagerV2

    manager = DataSourceManagerV2()

    # æŒ‰åˆ†ç±»åˆ†ç»„ç»Ÿè®¡
    categories = {}
    for endpoint_name, source_data in manager.registry.items():
        category = source_data['config'].get('data_category', 'UNKNOWN')
        if category not in categories:
            categories[category] = {
                'category': category,
                'total': 0,
                'healthy': 0,
                'endpoints': []
            }

        categories[category]['total'] += 1
        if source_data['config'].get('health_status') == 'healthy':
            categories[category]['healthy'] += 1

        categories[category]['endpoints'].append(endpoint_name)

    # æŒ‰åˆ†ç±»æ’åº
    return sorted(categories.values(), key=lambda x: x['category'])
```

##### æ–¹æ¡ˆB: Grafana + PostgreSQLç›´æ¥æŸ¥è¯¢ï¼ˆå¿«é€Ÿæ–¹æ¡ˆï¼‰

åœ¨Grafanaä¸­åˆ›å»ºæ–°çš„Panelï¼Œç›´æ¥æŸ¥è¯¢PostgreSQL:

```sql
-- æŒ‰åˆ†ç±»æŸ¥è¯¢æ•°æ®æº
SELECT
    data_category,
    endpoint_name,
    source_name,
    health_status,
    data_quality_score,
    priority,
    success_rate,
    avg_response_time
FROM data_source_registry
WHERE
    status = 'active'
    AND (${data_category:raw} IS NULL OR data_category = ${data_category})
    AND (${keyword:raw} IS NULL OR endpoint_name ILIKE '%' || ${keyword} || '%')
ORDER BY priority ASC, data_quality_score DESC
```

**Grafanaå˜é‡é…ç½®**:
```yaml
variables:
  - name: data_category
    type: query
    query: "SELECT DISTINCT data_category FROM data_source_registry WHERE status='active' ORDER BY data_category"

  - name: keyword
    type: textbox
```

#### 2.2 åˆ†ç±»åˆ†ç»„å±•ç¤º

**æœŸæœ›åŠŸèƒ½**:
- æŒ‰5å±‚åˆ†ç±»æŠ˜å å±•ç¤º
- å±•å¼€åå¯è§æ‰€æœ‰åŸå§‹æ¥å£
- æ˜¾ç¤ºæ¥å£ä¼˜å…ˆçº§ã€å¯ç”¨çŠ¶æ€

**å½“å‰å®ç°**: âŒ **æœªå®ç°**

**å»ºè®®æ–¹æ¡ˆ**: ä½¿ç”¨Grafana **Table Panel** + **Repeat by Variable**

```json
{
  "type": "table",
  "title": "æ•°æ®æºæ¥å£åˆ—è¡¨ï¼ˆæŒ‰åˆ†ç±»ï¼‰",
  "repeat": "data_category",
  "repeatDirection": "h",
  "targets": [
    {
      "sql": "SELECT
        endpoint_name,
        source_name,
        priority,
        health_status,
        data_quality_score,
        success_rate,
        avg_response_time
      FROM data_source_registry
      WHERE data_category = '$data_category'
        AND status = 'active'
      ORDER BY priority ASC, data_quality_score DESC"
    }
  ],
  "transformations": [
    {
      "id": "organize",
      "options": {
        "excludeByName": {},
        "indexByName": {},
        "renameByName": {
          "endpoint_name": "æ¥å£åç§°",
          "source_name": "æ•°æ®æº",
          "priority": "ä¼˜å…ˆçº§",
          "health_status": "å¥åº·çŠ¶æ€",
          "data_quality_score": "è´¨é‡è¯„åˆ†",
          "success_rate": "æˆåŠŸç‡(%)",
          "avg_response_time": "å“åº”æ—¶é—´(s)"
        }
      }
    }
  ]
}
```

#### 2.3 ç›‘æ§ä»ªè¡¨ç›˜ï¼ˆå·²å®ç°âœ…ï¼‰

**æœŸæœ›åŠŸèƒ½**:
- æŒ‰åˆ†ç±»å±•ç¤ºæ¥å£å¯ç”¨æ€§ï¼ˆæˆåŠŸç‡ï¼‰
- å¹³å‡å“åº”æ—¶é—´
- æ•°æ®è´¨é‡è¯„åˆ†
- æŠ˜çº¿å›¾/æŸ±çŠ¶å›¾

**å½“å‰å®ç°**: âœ… **å·²å®ç°** (12ä¸ªGrafanaé¢æ¿)

å·²åŒ…å«çš„ç›‘æ§é¢æ¿:
1. âœ… æ•°æ®æºå¯ç”¨æ€§çŠ¶æ€ (Staté¢æ¿)
2. âœ… æ•°æ®æºè°ƒç”¨é€Ÿç‡ QPS (Time Series)
3. âœ… æ•°æ®æºå¥åº·çŠ¶æ€ (Stat)
4. âœ… å“åº”æ—¶é—´åˆ†å¸ƒ (Histogram)
5. âœ… æ•°æ®è´¨é‡è¯„åˆ† (Gauge)
6. âœ… æˆåŠŸç‡è¶‹åŠ¿ (Time Series)
7. âœ… è°ƒç”¨æ€»æ¬¡æ•° (Stat)
8. âœ… è¿”å›æ•°æ®é‡åˆ†å¸ƒ (Heatmap)
9. âœ… è¿ç»­å¤±è´¥æ¬¡æ•° (Table)
10. âœ… æ¥å£å¯¹æ¯” (Bar Chart)
11. âœ… å®æ—¶è°ƒç”¨æ—¥å¿— (Table)
12. âœ… æ•°æ®æºåˆ—è¡¨ (Table)

**å½“å‰PromQLæŸ¥è¯¢ç¤ºä¾‹**:
```promql
# æŒ‰æ•°æ®åˆ†ç±»çš„æˆåŠŸç‡
rate(data_source_calls_total{status="success"}[5m]) /
rate(data_source_calls_total[5m]) * 100

# æŒ‰æ•°æ®åˆ†ç±»çš„å“åº”æ—¶é—´
rate(data_source_response_time_seconds_sum[5m]) /
rate(data_source_response_time_seconds_count[5m])

# æ•°æ®è´¨é‡è¯„åˆ†
data_source_quality_score
```

**å¢å¼ºå»ºè®®**: æ·»åŠ æŒ‰`data_category`æ ‡ç­¾åˆ†ç»„

```promql
# æŒ‰åˆ†ç±»çš„æˆåŠŸç‡
rate(data_source_calls_total{status="success", data_category="DAILY_KLINE"}[5m]) /
rate(data_source_calls_total{data_category="DAILY_KLINE"}[5m]) * 100
```

#### 2.4 å¼‚å¸¸æ¥å£æ ‡çº¢æç¤ºï¼ˆå·²å®ç°âœ…ï¼‰

**æœŸæœ›åŠŸèƒ½**:
- ç¬¬1ç±»æ¥å£æˆåŠŸç‡<90%æ ‡çº¢
- å“åº”æ—¶é—´>1ç§’æ ‡çº¢

**å½“å‰å®ç°**: âœ… **å·²å®ç°** (Grafanaé˜ˆå€¼å‘Šè­¦)

å·²åœ¨Panelä¸­é…ç½®:
```json
{
  "fieldConfig": {
    "defaults": {
      "thresholds": {
        "mode": "absolute",
        "steps": [
          {"color": "red", "value": null},
          {"color": "yellow", "value": 90},
          {"color": "green", "value": 95}
        ]
      }
    }
  }
}
```

**Prometheuså‘Šè­¦è§„åˆ™** (å¯æ·»åŠ ):
```yaml
groups:
  - name: data_source_alerts
    rules:
      # æˆåŠŸç‡å‘Šè­¦
      - alert: DataSourceSuccessRateLow
        expr: |
          rate(data_source_calls_total{status="success"}[5m]) /
          rate(data_source_calls_total[5m]) < 0.9
        for: 10m
        labels:
          severity: warning
          category: reliability
        annotations:
          summary: "æ•°æ®æº {{ $endpoint_name }} æˆåŠŸç‡ä½äº90%"
          description: "æˆåŠŸç‡: {{ $value | humanizePercentage }}"

      # å“åº”æ—¶é—´å‘Šè­¦
      - alert: DataSourceResponseTimeHigh
        expr: |
          rate(data_source_response_time_seconds_sum[5m]) /
          rate(data_source_response_time_seconds_count[5m]) > 1.0
        for: 5m
        labels:
          severity: warning
          category: performance
        annotations:
          summary: "æ•°æ®æº {{ $endpoint_name }} å“åº”æ—¶é—´è¶…è¿‡1ç§’"
          description: "å¹³å‡å“åº”æ—¶é—´: {{ $value }}s"
```

#### 2.5 é…ç½®ç¼–è¾‘ï¼ˆæœªå®ç°âŒï¼‰

**æœŸæœ›åŠŸèƒ½**:
- ç›´æ¥åœ¨é¢æ¿ä¸Šä¿®æ”¹æ¥å£é…ç½®
- ä¿®æ”¹å¯ç”¨çŠ¶æ€ã€ä¼˜å…ˆçº§
- ä¿å­˜åè‡ªåŠ¨æ›´æ–°æ³¨å†Œè¡¨

**å½“å‰å®ç°**: âŒ **æœªå®ç°**

**å»ºè®®æ–¹æ¡ˆ**:

##### æ–¹æ¡ˆA: Webç®¡ç†ç•Œé¢ï¼ˆæ¨èï¼‰

åˆ›å»ºVue.jsç®¡ç†ç•Œé¢:

```vue
<!-- web/frontend/src/views/DataSourceManagement.vue -->
<template>
  <div class="data-source-management">
    <!-- æœç´¢ç­›é€‰ -->
    <el-form :inline="true">
      <el-form-item label="æ•°æ®åˆ†ç±»">
        <el-select v-model="searchForm.data_category">
          <el-option label="å…¨éƒ¨" value=""></el-option>
          <el-option
            v-for="cat in categories"
            :key="cat"
            :label="cat"
            :value="cat">
          </el-option>
        </el-select>
      </el-form-item>

      <el-form-item label="å…³é”®è¯">
        <el-input v-model="searchForm.keyword"></el-input>
      </el-form-item>

      <el-button type="primary" @click="searchDataSources">æœç´¢</el-button>
    </el-form>

    <!-- æ•°æ®æºåˆ—è¡¨ -->
    <el-table :data="dataSources" style="margin-top: 20px">
      <el-table-column prop="endpoint_name" label="æ¥å£åç§°"></el-table-column>
      <el-table-column prop="data_category" label="æ•°æ®åˆ†ç±»"></el-table-column>
      <el-table-column prop="priority" label="ä¼˜å…ˆçº§">
        <template #default="scope">
          <el-input-number
            v-model="scope.row.priority"
            :min="1"
            :max="10"
            @change="updatePriority(scope.row)">
          </el-input-number>
        </template>
      </el-table-column>
      <el-table-column prop="health_status" label="çŠ¶æ€">
        <template #default="scope">
          <el-switch
            v-model="scope.row.active"
            active-text="å¯ç”¨"
            inactive-text="ç¦ç”¨"
            @change="toggleStatus(scope.row)">
          </el-switch>
        </template>
      </el-table-column>
      <el-table-column label="æ“ä½œ">
        <template #default="scope">
          <el-button size="small" @click="editDataSource(scope.row)">ç¼–è¾‘</el-button>
          <el-button size="small" @click="testDataSource(scope.row)">æµ‹è¯•</el-button>
        </template>
      </el-table-column>
    </el-table>

    <!-- ç¼–è¾‘å¯¹è¯æ¡† -->
    <el-dialog v-model="editDialogVisible" title="ç¼–è¾‘æ•°æ®æº" width="60%">
      <el-form :model="editForm">
        <el-form-item label="æ¥å£åç§°">
          <el-input v-model="editForm.endpoint_name" disabled></el-input>
        </el-form-item>

        <el-form-item label="ä¼˜å…ˆçº§">
          <el-input-number v-model="editForm.priority" :min="1" :max="10"></el-input-number>
        </el-form-item>

        <el-form-item label="è´¨é‡è¯„åˆ†">
          <el-slider v-model="editForm.data_quality_score" :min="0" :max="10"></el-slider>
        </el-form-item>

        <el-form-item label="çŠ¶æ€">
          <el-radio-group v-model="editForm.status">
            <el-radio label="active">å¯ç”¨</el-radio>
            <el-radio label="maintenance">ç»´æŠ¤ä¸­</el-radio>
            <el-radio label="deprecated">å·²åºŸå¼ƒ</el-radio>
          </el-radio-group>
        </el-form-item>

        <el-form-item label="æè¿°">
          <el-input type="textarea" v-model="editForm.description"></el-input>
        </el-form-item>
      </el-form>

      <template #footer>
        <el-button @click="editDialogVisible = false">å–æ¶ˆ</el-button>
        <el-button type="primary" @click="saveDataSource">ä¿å­˜</el-button>
      </template>
    </el-dialog>
  </div>
</template>

<script setup>
import { ref, onMounted } from 'vue'
import axios from 'axios'

const dataSources = ref([])
const categories = ref([])
const editDialogVisible = ref(false)
const editForm = ref({})

// æœç´¢æ•°æ®æº
const searchDataSources = async () => {
  const { data } = await axios.get('/api/v1/data-sources', {
    params: searchForm.value
  })
  dataSources.value = data.data_sources
}

// æ›´æ–°ä¼˜å…ˆçº§
const updatePriority = async (row) => {
  await axios.put(`/api/v1/data-sources/${row.endpoint_name}`, {
    priority: row.priority
  })
  ElMessage.success('ä¼˜å…ˆçº§å·²æ›´æ–°')
}

// åˆ‡æ¢çŠ¶æ€
const toggleStatus = async (row) => {
  await axios.put(`/api/v1/data-sources/${row.endpoint_name}`, {
    status: row.active ? 'active' : 'maintenance'
  })
  ElMessage.success('çŠ¶æ€å·²æ›´æ–°')
}

// ä¿å­˜æ•°æ®æºé…ç½®
const saveDataSource = async () => {
  await axios.put(`/api/v1/data-sources/${editForm.value.endpoint_name}`, editForm.value)
  ElMessage.success('é…ç½®å·²ä¿å­˜')
  editDialogVisible.value = false
  searchDataSources()
}

onMounted(() => {
  searchDataSources()
})
</script>
```

**åç«¯API**:
```python
# web/backend/app/api/data_source_registry.py
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

router = APIRouter()

class DataSourceUpdate(BaseModel):
    priority: Optional[int] = None
    data_quality_score: Optional[float] = None
    status: Optional[str] = None
    description: Optional[str] = None

@router.put("/api/v1/data-sources/{endpoint_name}")
async def update_data_source(endpoint_name: str, update: DataSourceUpdate):
    """æ›´æ–°æ•°æ®æºé…ç½®"""
    import psycopg2
    from dotenv import load_dotenv
    load_dotenv()

    conn = psycopg2.connect(
        host=os.getenv('POSTGRESQL_HOST'),
        port=int(os.getenv('POSTGRESQL_PORT')),
        user=os.getenv('POSTGRESQL_USER'),
        password=os.getenv('POSTGRESQL_PASSWORD'),
        database=os.getenv('POSTGRESQL_DATABASE')
    )
    cursor = conn.cursor()

    # æ„å»ºæ›´æ–°SQL
    updates = {k: v for k, v in update.dict().items() if v is not None}

    if not updates:
        raise HTTPException(status_code=400, detail="æ— æ›´æ–°å†…å®¹")

    set_clause = ", ".join([f"{k} = %({k})s" for k in updates.keys()])

    sql = f"""
        UPDATE data_source_registry
        SET {set_clause}, updated_at = NOW()
        WHERE endpoint_name = %(endpoint_name)s
    """

    cursor.execute(sql, {**updates, "endpoint_name": endpoint_name})
    conn.commit()
    cursor.close()
    conn.close()

    return {"success": True, "message": "é…ç½®å·²æ›´æ–°"}
```

##### æ–¹æ¡ˆB: Grafana + JSON APIï¼ˆå¿«é€Ÿæ–¹æ¡ˆï¼‰

ä½¿ç”¨Grafanaçš„**Table Panel** + **Data Link**åŠŸèƒ½ï¼Œåœ¨è¡¨æ ¼ä¸­æ·»åŠ æ“ä½œåˆ—:

```json
{
  "type": "table",
  "transformations": [
    {
      "id": "links",
      "options": {
        "links": [
          {
            "title": "ç¼–è¾‘",
            "url": "http://localhost:8000/api/v1/data-sources/edit?endpoint=${__value.fields.endpoint_name}"
          },
          {
            "title": "æµ‹è¯•",
            "url": "http://localhost:8000/api/v1/data-sources/test?endpoint=${__value.fields.endpoint_name}"
          }
        ]
      }
    }
  ]
}
```

#### 2.6 æ‰‹åŠ¨æµ‹è¯•ï¼ˆæœªå®ç°âŒï¼‰

**æœŸæœ›åŠŸèƒ½**:
- ç®¡ç†å‘˜é€‰æ‹©æ¥å£
- è¾“å…¥æµ‹è¯•å‚æ•°ï¼ˆè‚¡ç¥¨ä»£ç ã€æ—¥æœŸèŒƒå›´ï¼‰
- æ‰‹åŠ¨è§¦å‘è°ƒç”¨
- æŸ¥çœ‹è¿”å›ç»“æœå’Œæ•°æ®è´¨é‡
- æ— éœ€ç¼–å†™æµ‹è¯•è„šæœ¬

**å½“å‰å®ç°**: âŒ **æœªå®ç°**

**å»ºè®®æ–¹æ¡ˆ**:

åˆ›å»ºç‹¬ç«‹çš„æµ‹è¯•å·¥å…·:

```python
# scripts/tools/manual_data_source_tester.py
"""
æ•°æ®æºæ‰‹åŠ¨æµ‹è¯•å·¥å…·

ä½¿ç”¨ç¤ºä¾‹:
    python scripts/tools/manual_data_source_tester.py --endpoint akshare.stock_zh_a_hist --symbol 000001 --start-date 20240101 --end-date 20240131
"""

import argparse
import json
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.data_source_manager_v2 import DataSourceManagerV2


def test_data_source(endpoint_name: str, test_params: dict, verbose: bool = True):
    """
    æ‰‹åŠ¨æµ‹è¯•æ•°æ®æº

    Args:
        endpoint_name: æ¥å£åç§°ï¼ˆå¦‚ akshare.stock_zh_a_histï¼‰
        test_params: æµ‹è¯•å‚æ•°
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    """
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•æ•°æ®æº: {endpoint_name}")
    print(f"{'='*60}")

    manager = DataSourceManagerV2()

    # 1. æ£€æŸ¥æ¥å£æ˜¯å¦å­˜åœ¨
    if endpoint_name not in manager.registry:
        print(f"âŒ æ¥å£ä¸å­˜åœ¨: {endpoint_name}")
        print(f"   å¯ç”¨æ¥å£: {list(manager.registry.keys())}")
        return False

    source_config = manager.registry[endpoint_name]['config']

    # 2. æ˜¾ç¤ºæ¥å£é…ç½®
    print(f"\nğŸ“‹ æ¥å£é…ç½®:")
    print(f"   æ•°æ®æº: {source_config.get('source_name')}")
    print(f"   æ•°æ®åˆ†ç±»: {source_config.get('data_category')}")
    print(f"   ç›®æ ‡æ•°æ®åº“: {source_config.get('target_db')}")
    print(f"   è´¨é‡è¯„åˆ†: {source_config.get('data_quality_score')}")
    print(f"   å¥åº·çŠ¶æ€: {source_config.get('health_status')}")

    # 3. æ˜¾ç¤ºæµ‹è¯•å‚æ•°
    print(f"\nğŸ”§ æµ‹è¯•å‚æ•°:")
    for key, value in test_params.items():
        print(f"   {key}: {value}")

    # 4. æ‰§è¡Œæµ‹è¯•
    print(f"\nâ³ æ­£åœ¨è°ƒç”¨æ¥å£...")
    start_time = datetime.now()

    try:
        # è°ƒç”¨æ•°æ®æº
        handler = manager._get_handler(endpoint_name)
        data = handler.fetch(**test_params)

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        # 5. æ˜¾ç¤ºç»“æœ
        print(f"âœ… è°ƒç”¨æˆåŠŸ")
        print(f"   å“åº”æ—¶é—´: {duration:.3f}ç§’")
        print(f"   è¿”å›æ•°æ®é‡: {len(data) if data is not None else 0}æ¡")

        if verbose and data is not None and len(data) > 0:
            print(f"\nğŸ“Š æ•°æ®é¢„è§ˆ:")
            print(f"   {data.head(3).to_string() if hasattr(data, 'head') else str(data)[:200]}")

            # æ•°æ®è´¨é‡æ£€æŸ¥
            print(f"\nğŸ“ˆ æ•°æ®è´¨é‡åˆ†æ:")

            # å®Œæ•´æ€§æ£€æŸ¥
            expected_cols = source_config.get('parameters', {}).keys()
            if hasattr(data, 'columns'):
                actual_cols = data.columns.tolist()
                missing_cols = set(expected_cols) - set(actual_cols)
                if missing_cols:
                    print(f"   âš ï¸  ç¼ºå¤±åˆ—: {missing_cols}")
                else:
                    print(f"   âœ… åˆ—å®Œæ•´: {len(actual_cols)}åˆ—")

            # æ•°æ®èŒƒå›´æ£€æŸ¥
            if hasattr(data, 'empty'):
                print(f"   {'âœ… æ•°æ®éç©º' if not data.empty else 'âŒ æ•°æ®ä¸ºç©º'}")

        # 6. è®°å½•æˆåŠŸ
        manager._record_success(endpoint_name, duration, len(data) if data is not None else 0)

        return True

    except Exception as e:
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        print(f"âŒ è°ƒç”¨å¤±è´¥")
        print(f"   å“åº”æ—¶é—´: {duration:.3f}ç§’")
        print(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")

        # 7. è®°å½•å¤±è´¥
        manager._record_failure(endpoint_name, str(e))

        if verbose:
            import traceback
            print(f"\nè¯¦ç»†é”™è¯¯å †æ ˆ:")
            traceback.print_exc()

        return False


def interactive_mode():
    """äº¤äº’å¼æµ‹è¯•æ¨¡å¼"""
    print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘       MyStocks æ•°æ®æºæ‰‹åŠ¨æµ‹è¯•å·¥å…· v1.0              â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

    manager = DataSourceManagerV2()

    # 1. é€‰æ‹©æ¥å£
    print(f"\nå¯ç”¨æ¥å£åˆ—è¡¨ (å…±{len(manager.registry)}ä¸ª):")

    # æŒ‰åˆ†ç±»åˆ†ç»„æ˜¾ç¤º
    categories = {}
    for endpoint_name, source_data in manager.registry.items():
        category = source_data['config'].get('data_category', 'UNKNOWN')
        if category not in categories:
            categories[category] = []
        categories[category].append(endpoint_name)

    for i, (category, endpoints) in enumerate(sorted(categories.items()), 1):
        print(f"\n[{i}] {category} ({len(endpoints)}ä¸ªæ¥å£):")
        for endpoint in sorted(endpoints)[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"    - {endpoint}")
        if len(endpoints) > 5:
            print(f"    ... è¿˜æœ‰ {len(endpoints) - 5} ä¸ªæ¥å£")

    # 2. é€‰æ‹©æ¥å£
    endpoint_input = input(f"\nè¯·è¾“å…¥æ¥å£åç§°ï¼ˆæˆ–è¾“å…¥åˆ†ç±»ç¼–å·ï¼‰: ").strip()

    if endpoint_input.isdigit():
        # ç”¨æˆ·è¾“å…¥äº†åˆ†ç±»ç¼–å·
        category_list = sorted(categories.items())
        idx = int(endpoint_input) - 1
        if 0 <= idx < len(category_list):
            selected_category, endpoints = category_list[idx]
            print(f"\n{selected_category} çš„æ¥å£åˆ—è¡¨:")
            for i, endpoint in enumerate(sorted(endpoints), 1):
                print(f"  [{i}] {endpoint}")

            sub_idx = int(input(f"\nè¯·é€‰æ‹©æ¥å£ç¼–å·: ").strip()) - 1
            endpoint_name = sorted(endpoints)[sub_idx]
        else:
            print(f"âŒ æ— æ•ˆçš„ç¼–å·")
            return
    else:
        endpoint_name = endpoint_input

    # 3. è¾“å…¥æµ‹è¯•å‚æ•°
    print(f"\nè¯·è¾“å…¥æµ‹è¯•å‚æ•° (JSONæ ¼å¼ï¼Œç•™ç©ºä½¿ç”¨é»˜è®¤å‚æ•°):")
    param_input = input("> ").strip()

    if param_input:
        try:
            test_params = json.loads(param_input)
        except json.JSONDecodeError:
            print(f"âŒ JSONæ ¼å¼é”™è¯¯")
            return
    else:
        # ä½¿ç”¨é»˜è®¤æµ‹è¯•å‚æ•°
        source_config = manager.registry[endpoint_name]['config']
        test_params = source_config.get('test_parameters', {})
        print(f"ä½¿ç”¨é»˜è®¤å‚æ•°: {test_params}")

    # 4. æ‰§è¡Œæµ‹è¯•
    test_data_source(endpoint_name, test_params, verbose=True)


def main():
    parser = argparse.ArgumentParser(description="æ•°æ®æºæ‰‹åŠ¨æµ‹è¯•å·¥å…·")
    parser.add_argument("--endpoint", help="æ¥å£åç§°")
    parser.add_argument("--symbol", help="è‚¡ç¥¨ä»£ç ")
    parser.add_argument("--start-date", help="å¼€å§‹æ—¥æœŸ")
    parser.add_argument("--end-date", help="ç»“æŸæ—¥æœŸ")
    parser.add_argument("--interactive", "-i", action="store_true", help="äº¤äº’å¼æ¨¡å¼")
    parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")

    args = parser.parse_args()

    if args.interactive:
        interactive_mode()
    elif args.endpoint:
        test_params = {}
        if args.symbol:
            test_params['symbol'] = args.symbol
        if args.start_date:
            test_params['start_date'] = args.start_date
        if args.end_date:
            test_params['end_date'] = args.end_date

        test_data_source(args.endpoint, test_params, args.verbose)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
```

**é›†æˆåˆ°Webç•Œé¢**:

```python
# web/backend/app/api/data_source_registry.py
@router.post("/api/v1/data-sources/{endpoint_name}/test")
async def test_data_source(endpoint_name: str, test_params: dict):
    """
    æ‰‹åŠ¨æµ‹è¯•æ•°æ®æº

    Bodyç¤ºä¾‹:
        {
            "symbol": "000001",
            "start_date": "20240101",
            "end_date": "20240131"
        }
    """
    from src.core.data_source_manager_v2 import DataSourceManagerV2

    manager = DataSourceManagerV2()

    if endpoint_name not in manager.registry:
        raise HTTPException(status_code=404, detail="æ¥å£ä¸å­˜åœ¨")

    try:
        # è°ƒç”¨æ•°æ®æº
        handler = manager._get_handler(endpoint_name)
        data = handler.fetch(**test_params)

        return {
            "success": True,
            "endpoint_name": endpoint_name,
            "test_params": test_params,
            "result": {
                "row_count": len(data) if data is not None else 0,
                "preview": data.head(3).to_dict() if hasattr(data, 'head') and data is not None else None,
                "columns": list(data.columns) if hasattr(data, 'columns') else None
            }
        }
    except Exception as e:
        return {
            "success": False,
            "endpoint_name": endpoint_name,
            "test_params": test_params,
            "error": str(e)
        }
```

---

## å®æ–½å»ºè®®æ€»ç»“

### ä¼˜å…ˆçº§çŸ©é˜µ

| åŠŸèƒ½ | ä¼˜å…ˆçº§ | å¤æ‚åº¦ | é¢„è®¡å·¥ä½œé‡ | å»ºè®® |
|------|--------|--------|-----------|------|
| **æ¥å£æ³¨å†Œè¡¨æŸ¥è¯¢** | P0 | ä¸­ | 2-3å¤© | ç«‹å³å®æ–½ |
| **ç›‘æ§ä»ªè¡¨ç›˜å¢å¼º** | P0 | ä½ | 1å¤© | ç«‹å³å®æ–½ |
| **æ‰‹åŠ¨æµ‹è¯•å·¥å…·** | P0 | ä½ | 1-2å¤© | ç«‹å³å®æ–½ |
| **é…ç½®ç¼–è¾‘ï¼ˆWebç•Œé¢ï¼‰** | P1 | é«˜ | 5-7å¤© | ç¬¬äºŒé˜¶æ®µ |
| **è‡ªåŠ¨å‘ç°** | P2 | ä¸­ | 3-5å¤© | ç¬¬ä¸‰é˜¶æ®µ |
| **å®šæœŸå·¡æ£€** | P1 | ä¸­ | 2-3å¤© | ç¬¬äºŒé˜¶æ®µ |
| **ä¸‹çº¿å½’æ¡£** | P2 | ä½ | 2å¤© | ç¬¬ä¸‰é˜¶æ®µ |

### ç¬¬ä¸€é˜¶æ®µå®æ–½è®¡åˆ’ï¼ˆ1å‘¨ï¼‰

**ç›®æ ‡**: å®ç°æ ¸å¿ƒç®¡ç†åŠŸèƒ½

1. **Day 1-2**: æ¥å£æ³¨å†Œè¡¨æŸ¥è¯¢
   - å®ç°FastAPIæœç´¢æ¥å£
   - æ·»åŠ Grafanaè¡¨æ ¼é¢æ¿ï¼ˆPostgreSQLæŸ¥è¯¢ï¼‰
   - æä¾›æŒ‰åˆ†ç±»ã€çŠ¶æ€ã€å…³é”®è¯ç­›é€‰

2. **Day 3**: ç›‘æ§ä»ªè¡¨ç›˜å¢å¼º
   - æ·»åŠ æŒ‰åˆ†ç±»åˆ†ç»„å±•ç¤º
   - é…ç½®é˜ˆå€¼å‘Šè­¦
   - ä¼˜åŒ–ç°æœ‰12ä¸ªé¢æ¿

3. **Day 4-5**: æ‰‹åŠ¨æµ‹è¯•å·¥å…·
   - å®ç°å‘½ä»¤è¡Œæµ‹è¯•å·¥å…·
   - é›†æˆåˆ°Web API
   - æ·»åŠ æµ‹è¯•æŠ¥å‘ŠåŠŸèƒ½

**äº¤ä»˜ç‰©**:
- FastAPIæœç´¢æ¥å£ (`/api/v1/data-sources`)
- Grafanaå¢å¼ºä»ªè¡¨ç›˜ï¼ˆæŒ‰åˆ†ç±»å±•ç¤ºï¼‰
- æ‰‹åŠ¨æµ‹è¯•å·¥å…· (`scripts/tools/manual_data_source_tester.py`)

### ç¬¬äºŒé˜¶æ®µå®æ–½è®¡åˆ’ï¼ˆ1-2å‘¨ï¼‰

**ç›®æ ‡**: å®ç°Webç®¡ç†ç•Œé¢

1. **Week 1**: é…ç½®ç¼–è¾‘ç•Œé¢
   - Vue.jsç®¡ç†é¡µé¢
   - CRUD APIå®ç°
   - å®æ—¶é…ç½®æ›´æ–°

2. **Week 2**: å®šæœŸå·¡æ£€ç³»ç»Ÿ
   - å®šæ—¶å¥åº·æ£€æŸ¥
   - å·¡æ£€æŠ¥å‘Šç”Ÿæˆ
   - é‚®ä»¶/é’‰é’‰é€šçŸ¥

### ç¬¬ä¸‰é˜¶æ®µå®æ–½è®¡åˆ’ï¼ˆ1-2å‘¨ï¼‰

**ç›®æ ‡**: å®ç°è‡ªåŠ¨åŒ–æµç¨‹

1. **Week 1**: è‡ªåŠ¨å‘ç°åŠŸèƒ½
   - DataSourceDiscovererå®ç°
   - akshare/tushareè‡ªåŠ¨æ‰«æ
   - è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•å‚æ•°

2. **Week 2**: ç”Ÿå‘½å‘¨æœŸç®¡ç†
   - ä¸‹çº¿æµç¨‹
   - æ•°æ®å½’æ¡£
   - å†å²æ•°æ®æ¸…ç†

---

## æŠ€æœ¯é€‰å‹å»ºè®®

### æ¨èæ–¹æ¡ˆ: FastAPI + Vue.js + PostgreSQL + Grafana

**æ¶æ„**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Vue.js å‰ç«¯                          â”‚
â”‚  - æ¥å£æŸ¥è¯¢å’Œç­›é€‰                                        â”‚
â”‚  - é…ç½®ç¼–è¾‘ç•Œé¢                                          â”‚
â”‚  - å®æ—¶ç›‘æ§å±•ç¤º                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ REST API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 FastAPI åç«¯                            â”‚
â”‚  - /api/v1/data-sources (æœç´¢ã€æ›´æ–°)                      â”‚
â”‚  - /api/v1/data-sources/{id}/test (æ‰‹åŠ¨æµ‹è¯•)              â”‚
â”‚  - /api/v1/data-sources/categories (åˆ†ç±»ç»Ÿè®¡)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          DataSourceManagerV2 (æ ¸å¿ƒé€»è¾‘)                 â”‚
â”‚  - æ™ºèƒ½è·¯ç”±                                               â”‚
â”‚  - å¥åº·æ£€æŸ¥                                               â”‚
â”‚  - ç›‘æ§è®°å½•                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        PostgreSQL (æ³¨å†Œè¡¨) + Prometheus (ç›‘æ§)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¼˜åŠ¿**:
- âœ… åˆ©ç”¨ç°æœ‰åŸºç¡€è®¾æ–½ï¼ˆFastAPIã€PostgreSQLã€Grafanaï¼‰
- âœ… å‰åç«¯åˆ†ç¦»ï¼Œæ˜“äºç»´æŠ¤
- âœ… RESTful APIï¼Œæ˜“äºæ‰©å±•
- âœ… å®æ—¶ç›‘æ§ï¼Œæ— éœ€é¢å¤–æ­å»º

---

## ç»“è®º

### å½“å‰å®ç°æ€»ç»“

**å·²å®ç°** âœ…:
- æ ¸å¿ƒæ³¨å†Œè¡¨å’Œæ™ºèƒ½è·¯ç”±
- Prometheusç›‘æ§æŒ‡æ ‡å¯¼å‡º
- GrafanaåŸºç¡€ç›‘æ§ä»ªè¡¨ç›˜ï¼ˆ12ä¸ªé¢æ¿ï¼‰
- å¥åº·æ£€æŸ¥å’Œæ•…éšœé™çº§

**éœ€è¦å¢å¼º** âš ï¸:
- æ¥å£æ³¨å†Œè¡¨æŸ¥è¯¢ç•Œé¢
- é…ç½®ç¼–è¾‘åŠŸèƒ½
- æ‰‹åŠ¨æµ‹è¯•å·¥å…·
- è‡ªåŠ¨å‘ç°å’Œç”Ÿå‘½å‘¨æœŸç®¡ç†

### æ¨èå®æ–½è·¯å¾„

**ç¬¬ä¸€é˜¶æ®µ** (1å‘¨ï¼ŒP0ä¼˜å…ˆçº§):
1. å®ç°æ¥å£æ³¨å†Œè¡¨æŸ¥è¯¢ï¼ˆFastAPI + Grafana Tableï¼‰
2. å¢å¼ºç›‘æ§ä»ªè¡¨ç›˜ï¼ˆæŒ‰åˆ†ç±»å±•ç¤ºï¼‰
3. å®ç°æ‰‹åŠ¨æµ‹è¯•å·¥å…·ï¼ˆå‘½ä»¤è¡Œ + APIï¼‰

**ç¬¬äºŒé˜¶æ®µ** (2å‘¨ï¼ŒP1ä¼˜å…ˆçº§):
4. å¼€å‘Webé…ç½®ç®¡ç†ç•Œé¢ï¼ˆVue.jsï¼‰
5. å®ç°å®šæœŸå·¡æ£€ç³»ç»Ÿ

**ç¬¬ä¸‰é˜¶æ®µ** (2å‘¨ï¼ŒP2ä¼˜å…ˆçº§):
6. å®ç°è‡ªåŠ¨å‘ç°åŠŸèƒ½
7. å®Œå–„ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆä¸‹çº¿ã€å½’æ¡£ï¼‰

---

## é—®é¢˜3: æ•°æ®æ¸…æ´—ä¸éªŒè¯åŠŸèƒ½å®ç°åˆ†æ

### å½“å‰å®ç°çŠ¶æ€

| åŠŸèƒ½ | å®ç°çŠ¶æ€ | å…·ä½“æ–‡ä»¶ | å·®è· |
|------|---------|---------|------|
| **è¡Œä¸šæ•°æ®æ¸…æ´—** | âœ… å·²å®ç° | `scripts/data_cleaning/clean_industry_data.py` | åŠŸèƒ½å®Œæ•´ |
| **æ•°æ®åº“éªŒè¯è„šæœ¬** | âœ… å·²å®ç° | `scripts/data_cleaning/verify_db_data.py` | åŠŸèƒ½å®Œæ•´ |
| **Kçº¿æ•°æ®éªŒè¯** | âœ… å·²å®ç° | verify_db_data.py --check-structure | åŠŸèƒ½å®Œæ•´ |
| **adj_factoréªŒè¯** | âœ… å·²å®ç° | verify_db_data.py --check-adj-factor | åŠŸèƒ½å®Œæ•´ |
| **è‡ªåŠ¨åŒ–æ¸…æ´—ä»»åŠ¡** | âŒ æœªå®ç° | - | éœ€è¦å®šæ—¶ä»»åŠ¡ |
| **å…¥åº“å‰éªŒè¯** | âš ï¸ éƒ¨åˆ†å®ç° | DataManagerä¸­æœ‰åŸºç¡€éªŒè¯ | éœ€è¦å¢å¼º |
| **æ•°æ®æ²»ç†è§„åˆ™** | âŒ æœªå®ç° | - | éœ€è¦è§„åˆ™å¼•æ“ |

### çŸ­æœŸå»ºè®®ï¼ˆç«‹å³å¯å®æ–½ï¼‰

#### 3.1 å®Œå–„ç°æœ‰éªŒè¯è„šæœ¬

**ç°çŠ¶**: âœ… ä¸¤ä¸ªè„šæœ¬å·²å®Œæ•´å®ç°
- `scripts/data_cleaning/clean_industry_data.py` (437è¡Œ)
- `scripts/data_cleaning/verify_db_data.py` (544è¡Œ)

**å¢å¼ºç‚¹**:
1. æ·»åŠ TDengineæ•°æ®éªŒè¯æ”¯æŒ
2. å¢åŠ æ›´å¤šéªŒè¯è§„åˆ™ï¼ˆæ•°æ®ç±»å‹ã€èŒƒå›´ã€é‡å¤ï¼‰
3. æ”¯æŒæ‰¹é‡è¡¨éªŒè¯
4. å¢åŠ æ€§èƒ½ä¼˜åŒ–ï¼ˆå¹¶è¡ŒéªŒè¯ï¼‰

#### 3.2 åˆ›å»ºè‡ªåŠ¨åŒ–æ¸…æ´—ä»»åŠ¡

**å»ºè®®**: åˆ›å»ºå®šæ—¶ä»»åŠ¡è„šæœ¬

```python
# scripts/data_cleaning/auto_clean_scheduler.py
"""
è‡ªåŠ¨åŒ–æ•°æ®æ¸…æ´—è°ƒåº¦å™¨

åŠŸèƒ½:
1. æ¯æ—¥æ”¶ç›˜åè‡ªåŠ¨éªŒè¯Kçº¿æ•°æ®
2. æ¯å‘¨æ£€æŸ¥è¡Œä¸šæ•°æ®è´¨é‡
3. è‡ªåŠ¨ä¿®å¤adj_factorç¼ºå¤±å€¼
4. ç”Ÿæˆæ¸…æ´—æŠ¥å‘Šå¹¶å‘Šè­¦
"""

import schedule
import time
from pathlib import Path
from datetime import datetime
from scripts.data_cleaning.verify_db_data import DatabaseVerifier
from scripts.data_cleaning.clean_industry_data import DataCleaner


class AutoCleanScheduler:
    """è‡ªåŠ¨åŒ–æ•°æ®æ¸…æ´—è°ƒåº¦å™¨"""

    def __init__(self):
        self.verifier = DatabaseVerifier()

    def daily_kline_check(self):
        """æ¯æ—¥æ£€æŸ¥Kçº¿æ•°æ®"""
        print(f"\n[{datetime.now()}] æ‰§è¡Œæ¯æ—¥Kçº¿æ•°æ®æ£€æŸ¥...")

        # æ£€æŸ¥adj_factor
        result = self.verifier.check_adj_factor("stocks_daily")

        # å¦‚æœæ— æ•ˆç‡è¶…è¿‡5%ï¼Œè‡ªåŠ¨ä¿®å¤
        if result['valid_percent'] < 95:
            print(f"âš ï¸ adj_factoræœ‰æ•ˆç‡ä¸º{result['valid_percent']:.2f}%ï¼Œè‡ªåŠ¨ä¿®å¤...")
            fix_result = self.verifier.fix_adj_factor(
                "stocks_daily",
                default_value=1.0,
                dry_run=False
            )
            print(f"âœ… å·²ä¿®å¤{fix_result['fixed_count']}æ¡è®°å½•")

    def weekly_industry_check(self):
        """æ¯å‘¨æ£€æŸ¥è¡Œä¸šæ•°æ®"""
        print(f"\n[{datetime.now()}] æ‰§è¡Œæ¯å‘¨è¡Œä¸šæ•°æ®æ£€æŸ¥...")

        result = self.verifier.check_industry_data("stocks_basic")

        # å¦‚æœè„æ•°æ®è¶…è¿‡10%ï¼Œå‘Šè­¦
        if result['dirty_percent'] > 10:
            print(f"âš ï¸ è„æ•°æ®ç‡ä¸º{result['dirty_percent']:.2f}%ï¼Œéœ€è¦äººå·¥å®¡æ ¸")
            # è¿™é‡Œå¯ä»¥é›†æˆé‚®ä»¶/é’‰é’‰å‘Šè­¦

    def run(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        print("âœ… è‡ªåŠ¨åŒ–æ¸…æ´—è°ƒåº¦å™¨å·²å¯åŠ¨")

        # æ¯æ—¥æ£€æŸ¥Kçº¿æ•°æ®ï¼ˆæ”¶ç›˜åï¼‰
        schedule.every().day.at("16:00").do(self.daily_kline_check)

        # æ¯å‘¨ä¸€æ£€æŸ¥è¡Œä¸šæ•°æ®
        schedule.every().monday.at("09:00").do(self.weekly_industry_check)

        while True:
            schedule.run_pending()
            time.sleep(60)


if __name__ == "__main__":
    scheduler = AutoCleanScheduler()
    scheduler.run()
```

#### 3.3 å¢å¼ºå…¥åº“å‰éªŒè¯

**å»ºè®®**: åœ¨DataManagerä¸­å¢åŠ éªŒè¯é’©å­

```python
# src/core/data_validator.py
"""
æ•°æ®éªŒè¯å™¨ - å…¥åº“å‰éªŒè¯

éªŒè¯è§„åˆ™:
1. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
2. æ•°æ®ç±»å‹éªŒè¯
3. æ•°æ®èŒƒå›´éªŒè¯
4. é‡å¤æ•°æ®æ£€æµ‹
5. ä¸šåŠ¡é€»è¾‘éªŒè¯ï¼ˆå¦‚OHLCä»·æ ¼åˆç†æ€§ï¼‰
"""

from typing import Dict, Any, List, Optional
import pandas as pd
import numpy as np


class DataValidator:
    """æ•°æ®éªŒè¯å™¨"""

    def __init__(self):
        self.rules = {}

    def register_rule(self, table_name: str, rule: Dict):
        """æ³¨å†ŒéªŒè¯è§„åˆ™"""
        if table_name not in self.rules:
            self.rules[table_name] = []
        self.rules[table_name].append(rule)

    def validate(self, table_name: str, data: pd.DataFrame) -> Dict[str, Any]:
        """
        éªŒè¯æ•°æ®

        è¿”å›:
            {
                "is_valid": bool,
                "errors": List[str],
                "warnings": List[str]
            }
        """
        result = {
            "is_valid": True,
            "errors": [],
            "warnings": []
        }

        if table_name not in self.rules:
            # æ²¡æœ‰è§„åˆ™ï¼Œè·³è¿‡éªŒè¯
            return result

        for rule in self.rules[table_name]:
            rule_result = self._apply_rule(data, rule)

            if not rule_result["is_valid"]:
                result["is_valid"] = False
                result["errors"].extend(rule_result["errors"])

            if rule_result["warnings"]:
                result["warnings"].extend(rule_result["warnings"])

        return result

    def _apply_rule(self, data: pd.DataFrame, rule: Dict) -> Dict:
        """åº”ç”¨å•ä¸ªéªŒè¯è§„åˆ™"""
        rule_type = rule.get("type")

        if rule_type == "required_columns":
            return self._check_required_columns(data, rule)
        elif rule_type == "column_types":
            return self._check_column_types(data, rule)
        elif rule_type == "ohlc_logic":
            return self._check_ohlc_logic(data, rule)
        elif rule_type == "no_duplicates":
            return self._check_no_duplicates(data, rule)
        elif rule_type == "value_range":
            return self._check_value_range(data, rule)
        else:
            return {"is_valid": True, "errors": [], "warnings": []}

    def _check_required_columns(self, data: pd.DataFrame, rule: Dict) -> Dict:
        """æ£€æŸ¥å¿…éœ€åˆ—"""
        required = rule.get("columns", [])
        missing = [col for col in required if col not in data.columns]

        if missing:
            return {
                "is_valid": False,
                "errors": [f"ç¼ºå°‘å¿…éœ€åˆ—: {', '.join(missing)}"],
                "warnings": []
            }

        return {"is_valid": True, "errors": [], "warnings": []}

    def _check_column_types(self, data: pd.DataFrame, rule: Dict) -> Dict:
        """æ£€æŸ¥åˆ—ç±»å‹"""
        type_mappings = rule.get("mappings", {})
        errors = []

        for col, expected_type in type_mappings.items():
            if col not in data.columns:
                continue

            actual_type = str(data[col].dtype)

            if expected_type == "numeric" and not pd.api.types.is_numeric_dtype(data[col]):
                errors.append(f"åˆ— '{col}' åº”ä¸ºæ•°å€¼ç±»å‹ï¼Œå®é™…ä¸º {actual_type}")
            elif expected_type == "datetime" and not pd.api.types.is_datetime64_any_dtype(data[col]):
                errors.append(f"åˆ— '{col}' åº”ä¸ºæ—¥æœŸæ—¶é—´ç±»å‹ï¼Œå®é™…ä¸º {actual_type}")

        return {
            "is_valid": len(errors) == 0,
            "errors": errors,
            "warnings": []
        }

    def _check_ohlc_logic(self, data: pd.DataFrame, rule: Dict) -> Dict:
        """æ£€æŸ¥OHLCä»·æ ¼é€»è¾‘"""
        errors = []

        required_cols = ["open", "high", "low", "close"]
        if not all(col in data.columns for col in required_cols):
            return {"is_valid": True, "errors": [], "warnings": []}

        # high >= max(open, close)
        invalid_high = data["high"] < data[["open", "close"]].max(axis=1)
        if invalid_high.any():
            errors.append(f"å‘ç° {invalid_high.sum()} æ¡è®°å½•çš„high < max(open, close)")

        # low <= min(open, close)
        invalid_low = data["low"] > data[["open", "close"]].min(axis=1)
        if invalid_low.any():
            errors.append(f"å‘ç° {invalid_low.sum()} æ¡è®°å½•çš„low > min(open, close)")

        # open, high, low, close > 0
        negative_prices = (data[["open", "high", "low", "close"]] <= 0).any(axis=1)
        if negative_prices.any():
            errors.append(f"å‘ç° {negative_prices.sum()} æ¡è®°å½•çš„ä»·æ ¼ <= 0")

        return {
            "is_valid": len(errors) == 0,
            "errors": errors,
            "warnings": []
        }

    def _check_no_duplicates(self, data: pd.DataFrame, rule: Dict) -> Dict:
        """æ£€æŸ¥é‡å¤æ•°æ®"""
        key_columns = rule.get("keys", [])

        if not key_columns or not all(col in data.columns for col in key_columns):
            return {"is_valid": True, "errors": [], "warnings": []}

        duplicates = data.duplicated(subset=key_columns)
        dup_count = duplicates.sum()

        if dup_count > 0:
            return {
                "is_valid": False,
                "errors": [f"å‘ç° {dup_count} æ¡é‡å¤æ•°æ®ï¼ˆåŸºäºåˆ—: {', '.join(key_columns)}ï¼‰"],
                "warnings": []
            }

        return {"is_valid": True, "errors": [], "warnings": []}

    def _check_value_range(self, data: pd.DataFrame, rule: Dict) -> Dict:
        """æ£€æŸ¥æ•°å€¼èŒƒå›´"""
        column = rule.get("column")
        min_val = rule.get("min")
        max_val = rule.get("max")

        if column not in data.columns:
            return {"is_valid": True, "errors": [], "warnings": []}

        out_of_range = pd.Series([False] * len(data))

        if min_val is not None:
            out_of_range |= (data[column] < min_val)

        if max_val is not None:
            out_of_range |= (data[column] > max_val)

        count = out_of_range.sum()

        if count > 0:
            return {
                "is_valid": False,
                "errors": [f"åˆ— '{column}' æœ‰ {count} æ¡æ•°æ®è¶…å‡ºèŒƒå›´ [{min_val}, {max_val}]"],
                "warnings": []
            }

        return {"is_valid": True, "errors": [], "warnings": []}


# å…¨å±€éªŒè¯å™¨å®ä¾‹
_validator = DataValidator()


# æ³¨å†Œå¸¸ç”¨éªŒè¯è§„åˆ™
def setup_default_rules():
    """è®¾ç½®é»˜è®¤éªŒè¯è§„åˆ™"""

    # Kçº¿æ•°æ®è§„åˆ™
    _validator.register_rule("stocks_daily", {
        "type": "required_columns",
        "columns": ["symbol", "trade_date", "open", "high", "low", "close", "volume"]
    })

    _validator.register_rule("stocks_daily", {
        "type": "ohlc_logic"
    })

    _validator.register_rule("stocks_daily", {
        "type": "no_duplicates",
        "keys": ["symbol", "trade_date"]
    })

    # è¡Œä¸šæ•°æ®è§„åˆ™
    _validator.register_rule("stocks_basic", {
        "type": "no_duplicates",
        "keys": ["symbol"]
    })

    # å‰”é™¤è¡Œä¸šè„æ•°æ®
    _validator.register_rule("stocks_basic", {
        "type": "custom",
        "description": "è¡Œä¸šæ•°æ®ä¸åº”ç­‰äºè‚¡ç¥¨åç§°",
        "validator": lambda df: {
            "is_valid": not (df["industry"] == df["name"]).any(),
            "errors": [],
            "warnings": []
        }
    })


def get_validator() -> DataValidator:
    """è·å–éªŒè¯å™¨å®ä¾‹"""
    return _validator
```

### ä¸­æœŸå»ºè®®ï¼ˆ1-2å‘¨å®æ–½ï¼‰

#### 3.4 æ•°æ®æ²»ç†è§„åˆ™å¼•æ“

**å»ºè®®**: åˆ›å»ºçµæ´»çš„è§„åˆ™å¼•æ“

```python
# src/core/data_governance_engine.py
"""
æ•°æ®æ²»ç†è§„åˆ™å¼•æ“

åŠŸèƒ½:
1. å¯é…ç½®çš„éªŒè¯è§„åˆ™
2. è§„åˆ™ä¼˜å…ˆçº§ç®¡ç†
3. è§„åˆ™æ‰§è¡Œå†å²è®°å½•
4. è§„åˆ™çƒ­æ›´æ–°
"""

from typing import Dict, Any, List, Optional
from datetime import datetime
import json
from pathlib import Path


class DataGovernanceEngine:
    """æ•°æ®æ²»ç†å¼•æ“"""

    def __init__(self, config_path: str = "config/data_governance_rules.json"):
        self.config_path = config_path
        self.rules = {}
        self.load_rules()

    def load_rules(self):
        """åŠ è½½è§„åˆ™é…ç½®"""
        if Path(self.config_path).exists():
            with open(self.config_path, 'r') as f:
                self.rules = json.load(f)
        else:
            # é»˜è®¤è§„åˆ™
            self.rules = self._get_default_rules()

    def save_rules(self):
        """ä¿å­˜è§„åˆ™é…ç½®"""
        Path(self.config_path).parent.mkdir(parents=True, exist_ok=True)
        with open(self.config_path, 'w') as f:
            json.dump(self.rules, f, indent=2)

    def _get_default_rules(self) -> Dict:
        """è·å–é»˜è®¤è§„åˆ™"""
        return {
            "rules": [
                {
                    "id": "KLINE_001",
                    "name": "Kçº¿æ•°æ®å®Œæ•´æ€§æ£€æŸ¥",
                    "table": "stocks_daily",
                    "enabled": True,
                    "priority": "HIGH",
                    "actions": [
                        {
                            "type": "required_columns",
                            "columns": ["symbol", "trade_date", "open", "high", "low", "close", "volume"]
                        },
                        {
                            "type": "ohlc_logic"
                        }
                    ],
                    "on_failure": "REJECT"
                },
                {
                    "id": "INDUSTRY_001",
                    "name": "è¡Œä¸šæ•°æ®æ¸…æ´—",
                    "table": "stocks_basic",
                    "enabled": True,
                    "priority": "MEDIUM",
                    "actions": [
                        {
                            "type": "custom",
                            "description": "å‰”é™¤ industry = name çš„è„æ•°æ®",
                            "operation": "SET_NULL"
                        }
                    ],
                    "on_failure": "FIX"
                },
                {
                    "id": "ADJFACTOR_001",
                    "name": "å¤æƒå› å­å¡«å……",
                    "table": "stocks_daily",
                    "enabled": True,
                    "priority": "LOW",
                    "actions": [
                        {
                            "type": "fill_null",
                            "column": "adj_factor",
                            "default_value": 1.0
                        }
                    ],
                    "on_failure": "FIX"
                }
            ]
        }

    def apply_rules(self, table_name: str, data: pd.DataFrame) -> Dict[str, Any]:
        """
        åº”ç”¨è§„åˆ™

        è¿”å›:
            {
                "is_valid": bool,
                "data": pd.DataFrame,
                "errors": List[str],
                "warnings": List[str]
            }
        """
        result = {
            "is_valid": True,
            "data": data,
            "errors": [],
            "warnings": []
        }

        table_rules = [r for r in self.rules.get("rules", [])
                      if r.get("table") == table_name and r.get("enabled", True)]

        for rule in sorted(table_rules, key=lambda x: self._get_priority_score(x.get("priority"))):
            rule_result = self._apply_rule(data, rule)

            if not rule_result["is_valid"]:
                on_failure = rule.get("on_failure", "REJECT")

                if on_failure == "REJECT":
                    result["is_valid"] = False
                    result["errors"].append(f"è§„åˆ™ {rule['id']} å¤±è´¥: {rule['name']}")
                    break
                elif on_failure == "FIX":
                    # å°è¯•è‡ªåŠ¨ä¿®å¤
                    data = rule_result.get("fixed_data", data)
                    result["data"] = data
                    result["warnings"].append(f"è§„åˆ™ {rule['id']} å·²è‡ªåŠ¨ä¿®å¤: {rule['name']}")
                elif on_failure == "WARN":
                    result["warnings"].append(f"è§„åˆ™ {rule['id']} è§¦å‘è­¦å‘Š: {rule['name']}")

        return result

    def _apply_rule(self, data: pd.DataFrame, rule: Dict) -> Dict:
        """åº”ç”¨å•ä¸ªè§„åˆ™"""
        # å…·ä½“å®ç°...
        pass

    def _get_priority_score(self, priority: str) -> int:
        """è·å–ä¼˜å…ˆçº§åˆ†æ•°"""
        scores = {"HIGH": 3, "MEDIUM": 2, "LOW": 1}
        return scores.get(priority, 0)


# é…ç½®æ–‡ä»¶ç¤ºä¾‹: config/data_governance_rules.json
"""
{
  "rules": [
    {
      "id": "KLINE_001",
      "name": "Kçº¿æ•°æ®å®Œæ•´æ€§æ£€æŸ¥",
      "table": "stocks_daily",
      "enabled": true,
      "priority": "HIGH",
      "actions": [...],
      "on_failure": "REJECT"
    }
  ]
}
"""
```

### é•¿æœŸå»ºè®®ï¼ˆ2-4å‘¨å®æ–½ï¼‰

#### 3.5 æ¥å…¥æƒå¨è¡Œä¸šæ•°æ®æº

**å»ºè®®**: é›†æˆç¬¬ä¸‰æ–¹æƒå¨æ•°æ®æºè¿›è¡Œäº¤å‰éªŒè¯

```python
# src/industry_data_validator.py
"""
è¡Œä¸šæ•°æ®æƒå¨éªŒè¯å™¨

åŠŸèƒ½:
1. ä»æƒå¨æ•°æ®æºè·å–è¡Œä¸šåˆ†ç±»
2. äº¤å‰éªŒè¯æœ¬åœ°æ•°æ®
3. ä¿®æ­£ä¸ä¸€è‡´çš„è¡Œä¸šä¿¡æ¯
4. è®°å½•æ•°æ®æ¥æºå’Œç½®ä¿¡åº¦
"""

import requests
import pandas as pd
from typing import Dict, List, Optional


class IndustryDataValidator:
    """è¡Œä¸šæ•°æ®æƒå¨éªŒè¯å™¨"""

    def __init__(self):
        self.authoritative_sources = {
            "eastmoney": self._fetch_eastmoney_industry,
            "tushare": self._fetch_tushare_industry,
            # å¯ä»¥æ·»åŠ æ›´å¤šæ•°æ®æº
        }

    def validate_industry(self, symbol: str, current_industry: str) -> Dict:
        """
        éªŒè¯è¡Œä¸šæ•°æ®

        è¿”å›:
            {
                "is_valid": bool,
                "suggested_industry": Optional[str],
                "confidence": float,
                "sources": List[Dict]
            }
        """
        suggestions = []

        for source_name, fetch_func in self.authoritative_sources.items():
            try:
                industry = fetch_func(symbol)
                if industry:
                    suggestions.append({
                        "source": source_name,
                        "industry": industry,
                        "matches": industry == current_industry
                    })
            except Exception as e:
                print(f"æ•°æ®æº {source_name} æŸ¥è¯¢å¤±è´¥: {e}")

        if not suggestions:
            return {
                "is_valid": True,
                "suggested_industry": None,
                "confidence": 0.0,
                "sources": []
            }

        # ç»Ÿè®¡æœ€ä¸€è‡´çš„è¡Œä¸šåˆ†ç±»
        industry_votes = {}
        for s in suggestions:
            industry = s["industry"]
            industry_votes[industry] = industry_votes.get(industry, 0) + 1

        best_industry = max(industry_votes, key=industry_votes.get)
        best_count = industry_votes[best_industry]

        is_valid = best_industry == current_industry
        confidence = best_count / len(suggestions)

        return {
            "is_valid": is_valid,
            "suggested_industry": best_industry if not is_valid else None,
            "confidence": confidence,
            "sources": suggestions
        }

    def _fetch_eastmoney_industry(self, symbol: str) -> Optional[str]:
        """ä»ä¸œæ–¹è´¢å¯Œè·å–è¡Œä¸šä¿¡æ¯"""
        # å®ç°ç»†èŠ‚...
        pass

    def _fetch_tushare_industry(self, symbol: str) -> Optional[str]:
        """ä»Tushareè·å–è¡Œä¸šä¿¡æ¯"""
        # å®ç°ç»†èŠ‚...
        pass

    def batch_validate(self, symbols: List[str]) -> pd.DataFrame:
        """æ‰¹é‡éªŒè¯"""
        results = []

        for symbol in symbols:
            # ä»æ•°æ®åº“è·å–å½“å‰è¡Œä¸š
            # current_industry = ...

            # éªŒè¯
            result = self.validate_industry(symbol, "")
            result["symbol"] = symbol
            results.append(result)

        return pd.DataFrame(results)
```

#### 3.6 Kçº¿æ•°æ®æ²»ç†å®Œæ•´æ–¹æ¡ˆ

| æ–¹é¢ | çŸ­æœŸï¼ˆ1å‘¨ï¼‰ | ä¸­æœŸï¼ˆ2å‘¨ï¼‰ | é•¿æœŸï¼ˆ4å‘¨ï¼‰ |
|------|-----------|-----------|-----------|
| **æ•°æ®å®Œæ•´æ€§** | âœ… éªŒè¯è„šæœ¬<br>âœ… ä¿®å¤adj_factor | âœ… å…¥åº“å‰éªŒè¯<br>âœ… è‡ªåŠ¨åŒ–æ¸…æ´— | âœ… æƒå¨æ•°æ®æºäº¤å‰éªŒè¯ |
| **å¤æƒå› å­** | âœ… éªŒè¯å®Œæ•´æ€§<br>âœ… å¡«å……é»˜è®¤å€¼ | âœ… æ¯æ—¥è‡ªåŠ¨è®¡ç®—<br>âœ… å†å²æ•°æ®å›å¡« | âœ… å¤šæºæ•°æ®èåˆ |
| **æ•°æ®éªŒè¯** | âœ… OHLCé€»è¾‘æ£€æŸ¥<br>âœ… é‡å¤æ•°æ®æ£€æµ‹ | âœ… è§„åˆ™å¼•æ“<br>âœ… å¯é…ç½®è§„åˆ™ | âœ… æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹ |
| **å¼‚å¸¸å¤„ç†** | âœ… è®°å½•æ—¥å¿—<br>âœ… æ‹’ç»å…¥åº“ | âœ… è‡ªåŠ¨ä¿®å¤<br>âœ… å‘Šè­¦é€šçŸ¥ | âœ… æ™ºèƒ½ä¿®æ­£ |

---

## å®Œæ•´å®æ–½è·¯çº¿å›¾ï¼ˆæ›´æ–°ç‰ˆï¼‰

### Phase 1: æ•°æ®è´¨é‡åŸºç¡€è®¾æ–½ï¼ˆ1å‘¨ï¼‰

**ç›®æ ‡**: å®Œå–„éªŒè¯è„šæœ¬å’ŒåŸºç¡€éªŒè¯

**ä»»åŠ¡**:
1. âœ… å¢å¼ºç°æœ‰éªŒè¯è„šæœ¬
   - æ·»åŠ TDengineæ”¯æŒ
   - å¢åŠ å¹¶è¡ŒéªŒè¯
   - æ€§èƒ½ä¼˜åŒ–

2. âœ… åˆ›å»ºæ•°æ®éªŒè¯å™¨æ¨¡å—
   - `src/core/data_validator.py`
   - æ³¨å†Œé»˜è®¤éªŒè¯è§„åˆ™

3. âœ… é›†æˆåˆ°DataManager
   - ä¿å­˜å‰è‡ªåŠ¨éªŒè¯
   - éªŒè¯å¤±è´¥å¤„ç†ç­–ç•¥

**äº¤ä»˜ç‰©**:
- å¢å¼ºçš„éªŒè¯è„šæœ¬
- DataValidatoræ¨¡å—
- éªŒè¯è§„åˆ™é…ç½®æ–‡ä»¶

### Phase 2: è‡ªåŠ¨åŒ–æ¸…æ´—ç³»ç»Ÿï¼ˆ1-2å‘¨ï¼‰

**ç›®æ ‡**: å»ºç«‹è‡ªåŠ¨åŒ–æ¸…æ´—å’Œè°ƒåº¦ç³»ç»Ÿ

**ä»»åŠ¡**:
1. åˆ›å»ºè‡ªåŠ¨åŒ–æ¸…æ´—è°ƒåº¦å™¨
   - `scripts/data_cleaning/auto_clean_scheduler.py`
   - æ¯æ—¥Kçº¿æ£€æŸ¥
   - æ¯å‘¨è¡Œä¸šæ£€æŸ¥

2. åˆ›å»ºæ•°æ®æ²»ç†å¼•æ“
   - `src/core/data_governance_engine.py`
   - å¯é…ç½®è§„åˆ™
   - è§„åˆ™ä¼˜å…ˆçº§ç®¡ç†

3. é›†æˆå‘Šè­¦ç³»ç»Ÿ
   - é‚®ä»¶é€šçŸ¥
   - é’‰é’‰é€šçŸ¥
   - PrometheusæŒ‡æ ‡

**äº¤ä»˜ç‰©**:
- è‡ªåŠ¨åŒ–è°ƒåº¦å™¨
- æ•°æ®æ²»ç†å¼•æ“
- è§„åˆ™é…ç½®æ–‡ä»¶
- å‘Šè­¦é›†æˆ

### Phase 3: æƒå¨æ•°æ®æºé›†æˆï¼ˆ2-4å‘¨ï¼‰

**ç›®æ ‡**: æ¥å…¥æƒå¨æ•°æ®æºè¿›è¡Œäº¤å‰éªŒè¯

**ä»»åŠ¡**:
1. è¡Œä¸šæ•°æ®éªŒè¯å™¨
   - ä¸œæ–¹è´¢å¯ŒAPIé›†æˆ
   - Tushare APIé›†æˆ
   - äº¤å‰éªŒè¯é€»è¾‘

2. Kçº¿æ•°æ®èåˆ
   - å¤šæºæ•°æ®å¯¹æ¯”
   - æ•°æ®è´¨é‡è¯„åˆ†
   - æ™ºèƒ½é€‰æ‹©

3. å†å²æ•°æ®å›å¡«
   - æ‰¹é‡éªŒè¯å†å²æ•°æ®
   - è‡ªåŠ¨ä¿®å¤
   - æŠ¥å‘Šç”Ÿæˆ

**äº¤ä»˜ç‰©**:
- IndustryDataValidatoræ¨¡å—
- å¤šæºæ•°æ®èåˆç³»ç»Ÿ
- å†å²æ•°æ®å›å¡«è„šæœ¬

### Phase 4: é«˜çº§ç‰¹æ€§ï¼ˆ4å‘¨+ï¼‰

**ç›®æ ‡**: æœºå™¨å­¦ä¹ å¼‚å¸¸æ£€æµ‹å’Œæ™ºèƒ½ä¿®æ­£

**ä»»åŠ¡**:
1. å¼‚å¸¸æ£€æµ‹æ¨¡å‹
   - åŸºäºç»Ÿè®¡çš„å¼‚å¸¸æ£€æµ‹
   - åŸºäºæœºå™¨å­¦ä¹ çš„å¼‚å¸¸æ£€æµ‹

2. æ™ºèƒ½ä¿®æ­£
   - è‡ªåŠ¨è¯†åˆ«å¼‚å¸¸æ¨¡å¼
   - æ™ºèƒ½å¡«å……ç¼ºå¤±å€¼
   - è‡ªåŠ¨ä¿®æ­£é”™è¯¯

3. æ•°æ®è¡€ç¼˜è¿½è¸ª
   - è®°å½•æ•°æ®æ¥æº
   - è¿½è¸ªæ•°æ®å˜æ›´
   - æ•°æ®è´¨é‡å½±å“åˆ†æ

---

**æŠ¥å‘Šç‰ˆæœ¬**: v2.0
**åˆ›å»ºæ—¥æœŸ**: 2026-01-02
**æ›´æ–°æ—¥æœŸ**: 2026-01-07
**ä½œè€…**: Claude Code
**çŠ¶æ€**: âœ… å·²æ›´æ–°ï¼ŒåŒ…å«æ•°æ®æ²»ç†è§„åˆ’
