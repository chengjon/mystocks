#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MyStocksæŠ€æœ¯è´Ÿå€ºåˆ†æå™¨

å…¨é¢åˆ†æé¡¹ç›®ä¸­çš„æŠ€æœ¯è´Ÿå€ºï¼ŒåŒ…æ‹¬ï¼š
1. ä»£ç è´¨é‡é—®é¢˜
2. æ¶æ„å€ºåŠ¡
3. æ€§èƒ½é—®é¢˜
4. å®‰å…¨é—®é¢˜
5. ä¾èµ–é—®é¢˜
6. æµ‹è¯•è¦†ç›–
7. æ–‡æ¡£é—®é¢˜

ä½œè€…: iFlow CLI
æ—¥æœŸ: 2025-11-25
ç‰ˆæœ¬: v1.0
"""

import ast
import json
import logging
import os
import re
import subprocess
from collections import Counter, defaultdict
from pathlib import Path
from typing import Any, Dict, List, Tuple

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class TechnicalDebtAnalyzer:
    """æŠ€æœ¯è´Ÿå€ºåˆ†æå™¨"""

    def __init__(self, project_root: str = "/opt/claude/mystocks_spec"):
        self.project_root = Path(project_root)
        self.issues = defaultdict(list)
        self.stats = {
            "total_files": 0,
            "python_files": 0,
            "total_lines": 0,
            "code_lines": 0,
            "comment_lines": 0,
            "blank_lines": 0,
        }

    def analyze_all(self) -> Dict[str, Any]:
        """æ‰§è¡Œå…¨é¢çš„æŠ€æœ¯è´Ÿå€ºåˆ†æ"""
        logger.info("å¼€å§‹æŠ€æœ¯è´Ÿå€ºåˆ†æ...")

        # 1. ä»£ç è´¨é‡åˆ†æ
        self.analyze_code_quality()

        # 2. æ¶æ„å€ºåŠ¡åˆ†æ
        self.analyze_architecture_debt()

        # 3. æ€§èƒ½é—®é¢˜åˆ†æ
        self.analyze_performance_issues()

        # 4. å®‰å…¨é—®é¢˜åˆ†æ
        self.analyze_security_issues()

        # 5. ä¾èµ–é—®é¢˜åˆ†æ
        self.analyze_dependency_issues()

        # 6. æµ‹è¯•è¦†ç›–åˆ†æ
        self.analyze_test_coverage()

        # 7. æ–‡æ¡£é—®é¢˜åˆ†æ
        self.analyze_documentation_issues()

        # 8. é…ç½®ç®¡ç†é—®é¢˜åˆ†æ
        self.analyze_configuration_issues()

        return {
            "analysis_summary": self.generate_summary(),
            "detailed_issues": dict(self.issues),
            "recommendations": self.generate_recommendations(),
            "technical_debt_score": self.calculate_debt_score(),
            "priority_actions": self.get_priority_actions(),
        }

    def analyze_code_quality(self):
        """åˆ†æä»£ç è´¨é‡é—®é¢˜"""
        logger.info("åˆ†æä»£ç è´¨é‡...")

        python_files = list(self.project_root.rglob("*.py"))
        self.stats["total_files"] = len(list(self.project_root.rglob("*"))) - len(
            list(self.project_root.rglob("__pycache__"))
        )
        self.stats["python_files"] = len(python_files)

        for py_file in python_files:
            if self._should_skip_file(py_file):
                continue

            try:
                with open(py_file, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()
                    lines = content.split("\n")
                    self.stats["total_lines"] += len(lines)

                # è§£æAST
                tree = ast.parse(content, filename=str(py_file))

                # åˆ†æå„ç§ä»£ç è´¨é‡é—®é¢˜
                self._analyze_long_functions(py_file, tree, lines)
                self._analyze_complex_functions(py_file, tree)
                self._analyze_dead_imports(py_file, tree, content)
                self._analyze_code_duplication(py_file, content, lines)
                self._analyze_naming_issues(py_file, tree)
                self._analyze_file_complexity(py_file, tree)

            except Exception as e:
                self.issues["parsing_errors"].append(
                    {"file": str(py_file), "error": str(e), "category": "code_quality"}
                )

    def _should_skip_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡æ–‡ä»¶"""
        skip_patterns = [
            "__pycache__",
            ".git",
            ".pytest_cache",
            "node_modules",
            ".mypy_cache",
            ".opencode",
            ".cursor",
            ".specify",
            ".taskmaster",
            ".archive",
            "tests/e2e/node_modules",
        ]

        skip_patterns.extend([".pyc", ".pyo", ".pyd"])

        for pattern in skip_patterns:
            if pattern in str(file_path):
                return True
        return False

    def _analyze_long_functions(self, file_path: Path, tree: ast.AST, lines: List[str]):
        """åˆ†æè¿‡é•¿çš„å‡½æ•°"""

        class FunctionAnalyzer(ast.NodeVisitor):
            def __init__(self, file_path, lines):
                self.file_path = file_path
                self.lines = lines

            def visit_FunctionDef(self, node):
                # è®¡ç®—å‡½æ•°è¡Œæ•°ï¼ˆåŒ…æ‹¬æ³¨é‡Šå’Œç©ºè¡Œï¼‰
                func_start = node.lineno - 1
                func_end = node.end_lineno if hasattr(node, "end_lineno") else len(self.lines)
                func_lines = func_end - func_start

                # è­¦å‘Šé˜ˆå€¼ï¼š50è¡Œä»¥ä¸Šçš„å‡½æ•°
                if func_lines > 50:
                    self.file_path.parent.parent.parent.parent.issues["long_functions"].append(
                        {
                            "file": str(self.file_path),
                            "function": node.name,
                            "line_count": func_lines,
                            "start_line": node.lineno,
                            "end_line": func_end,
                            "category": "code_quality",
                            "severity": "high" if func_lines > 100 else "medium",
                        }
                    )

                self.generic_visit(node)

        analyzer = FunctionAnalyzer(file_path, lines)
        analyzer.visit(tree)

    def _analyze_complex_functions(self, file_path: Path, tree: ast.AST):
        """åˆ†æå¤æ‚å‡½æ•°ï¼ˆé«˜åœˆå¤æ‚åº¦ï¼‰"""

        class ComplexityAnalyzer(ast.NodeVisitor):
            def __init__(self, file_path):
                self.file_path = file_path
                self.complexity = 0

            def visit_FunctionDef(self, node):
                complexity = 1  # åŸºç¡€å¤æ‚åº¦

                # è®¡ç®—æ¡ä»¶è¯­å¥å¤æ‚åº¦
                for child in ast.walk(node):
                    if isinstance(child, (ast.If, ast.While, ast.For, ast.Try, ast.With)):
                        complexity += 1
                    elif isinstance(child, ast.BoolOp):
                        complexity += len(child.values) - 1

                # å¤æ‚åº¦è¶…è¿‡10è¢«è®¤ä¸ºå¤æ‚
                if complexity > 10:
                    self.file_path.parent.parent.parent.parent.issues["complex_functions"].append(
                        {
                            "file": str(self.file_path),
                            "function": node.name,
                            "complexity": complexity,
                            "category": "code_quality",
                            "severity": "high" if complexity > 20 else "medium",
                        }
                    )

                self.generic_visit(node)

        analyzer = ComplexityAnalyzer(file_path)
        analyzer.visit(tree)

    def _analyze_dead_imports(self, file_path: Path, tree: ast.AST, content: str):
        """åˆ†ææœªä½¿ç”¨çš„å¯¼å…¥"""

        class ImportAnalyzer(ast.NodeVisitor):
            def __init__(self, file_path, content):
                self.file_path = file_path
                self.content = content
                self.used_names = set()
                self.imported_names = {}

                # æ”¶é›†ä½¿ç”¨çš„åç§°
                for node in ast.walk(tree):
                    if isinstance(node, ast.Name):
                        self.used_names.add(node.id)
                    elif isinstance(node, ast.Attribute):
                        if isinstance(node.value, ast.Name):
                            self.used_names.add(f"{node.value.id}.{node.attr}")

                # æ”¶é›†å¯¼å…¥çš„åç§°
                for node in ast.walk(tree):
                    if isinstance(node, ast.Import):
                        for alias in node.names:
                            self.imported_names[alias.asname or alias.name] = alias.name
                    elif isinstance(node, ast.ImportFrom):
                        module = node.module or ""
                        for alias in node.names:
                            name = alias.asname or alias.name
                            self.imported_names[name] = f"{module}.{alias.name}"

            def visit_Import(self, node):
                for alias in node.names:
                    name = alias.asname or alias.name
                    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨
                    if name not in self.used_names and f"{name}" not in self.used_names:
                        self.file_path.parent.parent.parent.parent.issues["dead_imports"].append(
                            {
                                "file": str(self.file_path),
                                "import": alias.name,
                                "asname": alias.asname,
                                "category": "code_quality",
                                "severity": "low",
                            }
                        )

            def visit_ImportFrom(self, node):
                module = node.module or ""
                for alias in node.names:
                    name = alias.asname or alias.name
                    if name not in self.used_names:
                        self.file_path.parent.parent.parent.parent.issues["dead_imports"].append(
                            {
                                "file": str(self.file_path),
                                "import": f"{module}.{alias.name}",
                                "asname": alias.asname,
                                "category": "code_quality",
                                "severity": "low",
                            }
                        )

        analyzer = ImportAnalyzer(file_path, content)
        analyzer.visit(tree)

    def _analyze_code_duplication(self, file_path: Path, content: str, lines: List[str]):
        """åˆ†æä»£ç é‡å¤"""
        # ç®€å•çš„é‡å¤ä»£ç æ£€æµ‹ï¼šæŸ¥æ‰¾é‡å¤çš„è¡Œ
        line_counts = Counter()
        for line in lines:
            stripped = line.strip()
            if len(stripped) > 10 and not stripped.startswith("#"):  # å¿½ç•¥å¤ªçŸ­çš„è¡Œå’Œæ³¨é‡Š
                line_counts[stripped] += 1

        for line, count in line_counts.items():
            if count > 2:  # é‡å¤è¶…è¿‡2æ¬¡
                self.issues["code_duplication"].append(
                    {
                        "file": str(file_path),
                        "code": line[:100],
                        "occurrence_count": count,
                        "category": "code_quality",
                        "severity": "medium",
                    }
                )

    def _analyze_naming_issues(self, file_path: Path, tree: ast.AST):
        """åˆ†æå‘½åé—®é¢˜"""

        class NamingAnalyzer(ast.NodeVisitor):
            def __init__(self, file_path):
                self.file_path = file_path

            def visit_FunctionDef(self, node):
                # æ£€æŸ¥å‡½æ•°å‘½åæ˜¯å¦ç¬¦åˆè§„èŒƒ
                if not re.match(r"^[a-z_][a-z0-9_]*$", node.name):
                    self.file_path.parent.parent.parent.parent.issues["naming_issues"].append(
                        {
                            "file": str(self.file_path),
                            "type": "function",
                            "name": node.name,
                            "issue": "function_name_convention",
                            "category": "code_quality",
                            "severity": "low",
                        }
                    )

            def visit_ClassDef(self, node):
                # æ£€æŸ¥ç±»å‘½åæ˜¯å¦ç¬¦åˆè§„èŒƒ
                if not re.match(r"^[A-Z][A-Za-z0-9]*$", node.name):
                    self.file_path.parent.parent.parent.parent.issues["naming_issues"].append(
                        {
                            "file": str(self.file_path),
                            "type": "class",
                            "name": node.name,
                            "issue": "class_name_convention",
                            "category": "code_quality",
                            "severity": "medium",
                        }
                    )

        analyzer = NamingAnalyzer(file_path)
        analyzer.visit(tree)

    def _analyze_file_complexity(self, file_path: Path, tree: ast.AST):
        """åˆ†ææ–‡ä»¶å¤æ‚åº¦"""

        class FileComplexityAnalyzer(ast.NodeVisitor):
            def __init__(self, file_path):
                self.file_path = file_path
                self.classes = 0
                self.functions = 0
                self.imports = 0

            def visit_ClassDef(self, node):
                self.classes += 1

            def visit_FunctionDef(self, node):
                self.functions += 1

            def visit_Import(self, node):
                self.imports += len(node.names)

            def visit_ImportFrom(self, node):
                self.imports += len(node.names)

        analyzer = FileComplexityAnalyzer(file_path)
        analyzer.visit(tree)

        # æ–‡ä»¶å¤ªå¤æ‚ï¼ˆè¶…è¿‡500è¡Œæˆ–ç±»/å‡½æ•°å¤ªå¤šï¼‰
        file_lines = len(tree.body) if hasattr(tree, "body") else 0

        # è¿™ä¸ªæ£€æŸ¥éœ€è¦åŸºäºå®é™…æ–‡ä»¶é•¿åº¦ï¼Œè®©æˆ‘ä»¬é‡æ–°è¯»å–
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                actual_lines = len(f.readlines())

            if actual_lines > 500:
                self.issues["large_files"].append(
                    {"file": str(file_path), "line_count": actual_lines, "category": "code_quality", "severity": "high"}
                )
        except:
            pass

    def analyze_architecture_debt(self):
        """åˆ†ææ¶æ„å€ºåŠ¡"""
        logger.info("åˆ†ææ¶æ„å€ºåŠ¡...")

        # åˆ†ææ¨¡å—è€¦åˆ
        self._analyze_coupling()

        # åˆ†æè¿åå•ä¸€èŒè´£åŸåˆ™
        self._analyze_single_responsibility()

        # åˆ†æå¾ªç¯ä¾èµ–
        self._analyze_circular_dependencies()

        # åˆ†æä¾èµ–å€’ç½®
        self._analyze_dependency_inversion()

    def _analyze_coupling(self):
        """åˆ†ææ¨¡å—è€¦åˆ"""
        import_graph = defaultdict(set)

        python_files = list(self.project_root.rglob("*.py"))
        for py_file in python_files:
            if self._should_skip_file(py_file):
                continue

            try:
                with open(py_file, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()
                    tree = ast.parse(content)

                # åˆ†æå¯¼å…¥ä¾èµ–
                for node in ast.walk(tree):
                    if isinstance(node, ast.Import):
                        for alias in node.names:
                            import_graph[str(py_file)].add(alias.name)
                    elif isinstance(node, ast.ImportFrom):
                        module = node.module or ""
                        import_graph[str(py_file)].add(module)
            except:
                continue

        # æ‰¾å‡ºé«˜è€¦åˆæ¨¡å—
        for module, deps in import_graph.items():
            if len(deps) > 20:  # ä¾èµ–è¶…è¿‡20ä¸ªæ¨¡å—
                self.issues["high_coupling"].append(
                    {
                        "file": module,
                        "dependency_count": len(deps),
                        "dependencies": list(deps),
                        "category": "architecture",
                        "severity": "high",
                    }
                )

    def _analyze_single_responsibility(self):
        """åˆ†æå•ä¸€èŒè´£åŸåˆ™è¿å"""
        # è¿™é‡Œéœ€è¦æ›´å¤æ‚çš„åˆ†æï¼Œæš‚æ—¶æ ‡è®°ä¸ºæ¶æ„å€ºåŠ¡
        self.issues["architecture_concerns"].append(
            {
                "category": "architecture",
                "issue": "å•ä¸€èŒè´£åŸåˆ™éœ€è¦è¿›ä¸€æ­¥åˆ†æ",
                "severity": "medium",
                "recommendation": "å»ºè®®è¿›è¡Œæ›´æ·±å…¥çš„æ¶æ„åˆ†æ",
            }
        )

    def _analyze_circular_dependencies(self):
        """åˆ†æå¾ªç¯ä¾èµ–"""
        # ç®€åŒ–çš„å¾ªç¯ä¾èµ–æ£€æµ‹
        python_files = [f for f in list(self.project_root.rglob("*.py")) if not self._should_skip_file(f)]

        # æ„å»ºä¾èµ–å›¾
        dependencies = defaultdict(set)

        for py_file in python_files:
            try:
                with open(py_file, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()
                    tree = ast.parse(content)

                for node in ast.walk(tree):
                    if isinstance(node, ast.ImportFrom):
                        module = node.module
                        if module and module.startswith("src."):
                            dependencies[str(py_file)].add(module)
            except:
                continue

        # ç®€åŒ–æ£€æµ‹å¾ªç¯ä¾èµ–ï¼ˆéœ€è¦æ›´å¤æ‚ç®—æ³•ï¼‰
        self.issues["architecture_concerns"].append(
            {
                "category": "architecture",
                "issue": "å¾ªç¯ä¾èµ–æ£€æµ‹éœ€è¦å®Œå–„",
                "severity": "medium",
                "recommendation": "å»ºè®®ä½¿ç”¨ä¸“ä¸šå·¥å…·å¦‚pycircularè¿›è¡Œæ£€æµ‹",
            }
        )

    def _analyze_dependency_inversion(self):
        """åˆ†æä¾èµ–å€’ç½®åŸåˆ™"""
        # æ£€æŸ¥æ˜¯å¦æ­£ç¡®ä½¿ç”¨ä¾èµ–æ³¨å…¥
        self.issues["architecture_concerns"].append(
            {
                "category": "architecture",
                "issue": "ä¾èµ–æ³¨å…¥æ¨¡å¼ä½¿ç”¨æƒ…å†µéœ€è¦è¯„ä¼°",
                "severity": "medium",
                "recommendation": "æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨ä¾èµ–æ³¨å…¥å®¹å™¨",
            }
        )

    def analyze_performance_issues(self):
        """åˆ†ææ€§èƒ½é—®é¢˜"""
        logger.info("åˆ†ææ€§èƒ½é—®é¢˜...")

        python_files = list(self.project_root.rglob("*.py"))
        for py_file in python_files:
            if self._should_skip_file(py_file):
                continue

            try:
                with open(py_file, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()

                # æ£€æŸ¥å¸¸è§çš„æ€§èƒ½åæ¨¡å¼
                self._check_n_plus_one_queries(py_file, content)
                self._check_synchronous_io(py_file, content)
                self._check_memory_intensive_operations(py_file, content)
                self._check_inefficient_data_structures(py_file, content)

            except Exception as e:
                logger.warning(f"åˆ†ææ€§èƒ½é—®é¢˜å¤±è´¥ {py_file}: {e}")

    def _check_n_plus_one_queries(self, file_path: Path, content: str):
        """æ£€æŸ¥N+1æŸ¥è¯¢é—®é¢˜"""
        # æŸ¥æ‰¾æ•°æ®åº“æŸ¥è¯¢æ¨¡å¼
        query_patterns = [
            r"\.query\s*\(",
            r"\.execute\s*\(",
            r"\.fetch\s*\(",
            r"sql\.execute",
        ]

        for pattern in query_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if len(matches) > 5:  # è¶…è¿‡5ä¸ªæŸ¥è¯¢å¯èƒ½æœ‰é—®é¢˜
                self.issues["performance_issues"].append(
                    {
                        "file": str(file_path),
                        "issue": "potential_n_plus_one",
                        "query_count": len(matches),
                        "category": "performance",
                        "severity": "medium",
                    }
                )
                break

    def _check_synchronous_io(self, file_path: Path, content: str):
        """æ£€æŸ¥åŒæ­¥I/Oæ“ä½œ"""
        sync_patterns = [
            r"requests\.get\s*\(",
            r"requests\.post\s*\(",
            r"open\s*\(",
            r"file\.read\s*\(",
        ]

        for pattern in sync_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                self.issues["performance_issues"].append(
                    {
                        "file": str(file_path),
                        "issue": "synchronous_io",
                        "pattern": pattern,
                        "category": "performance",
                        "severity": "low",
                    }
                )
                break

    def _check_memory_intensive_operations(self, file_path: Path, content: str):
        """æ£€æŸ¥å†…å­˜å¯†é›†å‹æ“ä½œ"""
        memory_patterns = [
            r"\.read\s*\(\)",
            r"json\.loads\s*\(",
            r"eval\s*\(",
        ]

        for pattern in memory_patterns:
            if re.search(pattern, content):
                self.issues["performance_issues"].append(
                    {
                        "file": str(file_path),
                        "issue": "memory_intensive",
                        "pattern": pattern,
                        "category": "performance",
                        "severity": "medium",
                    }
                )
                break

    def _check_inefficient_data_structures(self, file_path: Path, content: str):
        """æ£€æŸ¥ä½æ•ˆæ•°æ®ç»“æ„"""
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨listä½œä¸ºdictionaryçš„key
        if "list(" in content and "dict(" in content:
            self.issues["performance_issues"].append(
                {
                    "file": str(file_path),
                    "issue": "inefficient_data_structure",
                    "category": "performance",
                    "severity": "low",
                }
            )

    def analyze_security_issues(self):
        """åˆ†æå®‰å…¨é—®é¢˜"""
        logger.info("åˆ†æå®‰å…¨é—®é¢˜...")

        python_files = list(self.project_root.rglob("*.py"))
        for py_file in python_files:
            if self._should_skip_file(py_file):
                continue

            try:
                with open(py_file, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()

                # æ£€æŸ¥å¸¸è§å®‰å…¨æ¼æ´
                self._check_hardcoded_secrets(py_file, content)
                self._check_sql_injection(py_file, content)
                self._check_insecure_file_operations(py_file, content)
                self._check_unsafe_eval(py_file, content)

            except Exception as e:
                logger.warning(f"åˆ†æå®‰å…¨é—®é¢˜å¤±è´¥ {py_file}: {e}")

    def _check_hardcoded_secrets(self, file_path: Path, content: str):
        """æ£€æŸ¥ç¡¬ç¼–ç å¯†é’¥"""
        secret_patterns = [
            r'password\s*=\s*["\'][^"\']+["\']',
            r'api_key\s*=\s*["\'][^"\']+["\']',
            r'secret\s*=\s*["\'][^"\']+["\']',
            r'token\s*=\s*["\'][^"\']+["\']',
            r'key\s*=\s*["\'][^"\']+["\']',
        ]

        for pattern in secret_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                self.issues["security_issues"].append(
                    {
                        "file": str(file_path),
                        "issue": "hardcoded_secret",
                        "matches": matches[:3],  # åªè®°å½•å‰3ä¸ª
                        "category": "security",
                        "severity": "high",
                    }
                )
                break

    def _check_sql_injection(self, file_path: Path, content: str):
        """æ£€æŸ¥SQLæ³¨å…¥é£é™©"""
        sql_patterns = [
            r'\.execute\s*\(\s*["\'].*%.*["\'].*\)',
            r'cursor\.execute\s*\(\s*f["\'].*\{{.*\}}.*["\'].*\)',
        ]

        for pattern in sql_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                self.issues["security_issues"].append(
                    {"file": str(file_path), "issue": "sql_injection_risk", "category": "security", "severity": "high"}
                )
                break

    def _check_insecure_file_operations(self, file_path: Path, content: str):
        """æ£€æŸ¥ä¸å®‰å…¨çš„æ–‡ä»¶æ“ä½œ"""
        insecure_patterns = [
            r"os\.system\s*\(",
            r"subprocess\.call\s*\(",
            r"exec\s*\(",
            r"eval\s*\(",
        ]

        for pattern in insecure_patterns:
            if re.search(pattern, content):
                self.issues["security_issues"].append(
                    {
                        "file": str(file_path),
                        "issue": "insecure_file_operation",
                        "pattern": pattern,
                        "category": "security",
                        "severity": "medium",
                    }
                )
                break

    def _check_unsafe_eval(self, file_path: Path, content: str):
        """æ£€æŸ¥ä¸å®‰å…¨çš„evalä½¿ç”¨"""
        if "eval(" in content:
            self.issues["security_issues"].append(
                {"file": str(file_path), "issue": "unsafe_eval", "category": "security", "severity": "high"}
            )

    def analyze_dependency_issues(self):
        """åˆ†æä¾èµ–é—®é¢˜"""
        logger.info("åˆ†æä¾èµ–é—®é¢˜...")

        # åˆ†ærequirements.txt
        requirements_file = self.project_root / "requirements.txt"
        if requirements_file.exists():
            self._analyze_requirements(requirements_file)

        # åˆ†æDockerä¾èµ–
        dockerfiles = list(self.project_root.rglob("Dockerfile*"))
        for dockerfile in dockerfiles:
            self._analyze_docker_dependencies(dockerfile)

        # åˆ†æpackage.json
        package_json = self.project_root / "package.json"
        if package_json.exists():
            self._analyze_package_json(package_json)

    def _analyze_requirements(self, requirements_file: Path):
        """åˆ†ærequirements.txt"""
        try:
            with open(requirements_file, "r", encoding="utf-8") as f:
                lines = f.readlines()

            for line in lines:
                line = line.strip()
                if line and not line.startswith("#"):
                    # æ£€æŸ¥ç‰ˆæœ¬å›ºå®š
                    if "==" not in line and ">=" not in line and "<=" not in line:
                        self.issues["dependency_issues"].append(
                            {
                                "file": str(requirements_file),
                                "package": line,
                                "issue": "unpinned_version",
                                "category": "dependencies",
                                "severity": "medium",
                            }
                        )
        except Exception as e:
            logger.warning(f"åˆ†ærequirements.txtå¤±è´¥: {e}")

    def _analyze_docker_dependencies(self, dockerfile: Path):
        """åˆ†æDockerä¾èµ–"""
        try:
            with open(dockerfile, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()

            # æ£€æŸ¥latestæ ‡ç­¾ä½¿ç”¨
            if "FROM.*:latest" in content:
                self.issues["dependency_issues"].append(
                    {
                        "file": str(dockerfile),
                        "issue": "using_latest_tag",
                        "category": "dependencies",
                        "severity": "low",
                    }
                )
        except Exception as e:
            logger.warning(f"åˆ†æDockerfileå¤±è´¥ {dockerfile}: {e}")

    def _analyze_package_json(self, package_json: Path):
        """åˆ†æpackage.json"""
        try:
            with open(package_json, "r", encoding="utf-8") as f:
                data = json.load(f)

            # æ£€æŸ¥devDependencieså’Œdependenciesçš„ç‰ˆæœ¬å›ºå®š
            for dep_type in ["dependencies", "devDependencies"]:
                if dep_type in data:
                    for package, version in data[dep_type].items():
                        if version == "*" or version == "latest":
                            self.issues["dependency_issues"].append(
                                {
                                    "file": str(package_json),
                                    "package": package,
                                    "version": version,
                                    "issue": "unpinned_version",
                                    "dependency_type": dep_type,
                                    "category": "dependencies",
                                    "severity": "medium",
                                }
                            )
        except Exception as e:
            logger.warning(f"åˆ†æpackage.jsonå¤±è´¥: {e}")

    def analyze_test_coverage(self):
        """åˆ†ææµ‹è¯•è¦†ç›–"""
        logger.info("åˆ†ææµ‹è¯•è¦†ç›–...")

        # æŸ¥æ‰¾æµ‹è¯•æ–‡ä»¶
        test_files = list(self.project_root.rglob("test_*.py"))
        test_files.extend(list(self.project_root.rglob("*_test.py")))
        test_files.extend(list(self.project_root.rglob("tests/**/*.py")))

        # æŸ¥æ‰¾æºä»£ç æ–‡ä»¶
        source_files = [
            f for f in list(self.project_root.rglob("*.py")) if not self._should_skip_file(f) and "test" not in str(f)
        ]

        test_to_source_ratio = len(test_files) / max(len(source_files), 1)

        if test_to_source_ratio < 0.1:  # æµ‹è¯•æ–‡ä»¶æ¯”ä¾‹å°äº10%
            self.issues["test_issues"].append(
                {
                    "issue": "low_test_coverage_ratio",
                    "test_files": len(test_files),
                    "source_files": len(source_files),
                    "ratio": test_to_source_ratio,
                    "category": "testing",
                    "severity": "high",
                }
            )

        # æ£€æŸ¥æ˜¯å¦æœ‰e2eæµ‹è¯•
        e2e_files = list(self.project_root.rglob("e2e/**/*.py"))
        if not e2e_files:
            self.issues["test_issues"].append(
                {"issue": "missing_e2e_tests", "category": "testing", "severity": "medium"}
            )

        # æ£€æŸ¥æµ‹è¯•é…ç½®
        pytest_ini = self.project_root / "pytest.ini"
        if not pytest_ini.exists():
            self.issues["test_issues"].append(
                {"issue": "missing_pytest_config", "category": "testing", "severity": "low"}
            )

    def analyze_documentation_issues(self):
        """åˆ†ææ–‡æ¡£é—®é¢˜"""
        logger.info("åˆ†ææ–‡æ¡£é—®é¢˜...")

        # ç»Ÿè®¡æ–‡æ¡£æ–‡ä»¶
        doc_files = {
            "markdown": list(self.project_root.rglob("*.md")),
            "rst": list(self.project_root.rglob("*.rst")),
            "txt": list(self.project_root.rglob("*.txt")),
        }

        total_doc_files = sum(len(files) for files in doc_files.values())

        # æ£€æŸ¥READMEæ–‡ä»¶
        readme_files = list(self.project_root.rglob("README*"))
        if not readme_files:
            self.issues["documentation_issues"].append(
                {"issue": "missing_readme", "category": "documentation", "severity": "high"}
            )

        # æ£€æŸ¥APIæ–‡æ¡£
        if "docs/api" not in [str(d) for d in self.project_root.rglob("docs/api")]:
            self.issues["documentation_issues"].append(
                {"issue": "missing_api_docs", "category": "documentation", "severity": "medium"}
            )

        # æ£€æŸ¥docstringsè¦†ç›–ç‡
        python_files = [f for f in list(self.project_root.rglob("*.py")) if not self._should_skip_file(f)]

        files_without_docstrings = 0
        for py_file in python_files:
            try:
                with open(py_file, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()
                    tree = ast.parse(content)

                has_docstring = False
                if (
                    isinstance(tree, ast.Module)
                    and tree.body
                    and isinstance(tree.body[0], ast.Expr)
                    and isinstance(tree.body[0].value, ast.Constant)
                    and isinstance(tree.body[0].value.value, str)
                ):
                    has_docstring = True

                if not has_docstring:
                    files_without_docstrings += 1
            except:
                continue

        if files_without_docstrings > len(python_files) * 0.7:  # è¶…è¿‡70%çš„æ–‡ä»¶æ²¡æœ‰docstring
            self.issues["documentation_issues"].append(
                {
                    "issue": "low_docstring_coverage",
                    "files_without_docstrings": files_without_docstrings,
                    "total_files": len(python_files),
                    "ratio": files_without_docstrings / len(python_files),
                    "category": "documentation",
                    "severity": "medium",
                }
            )

    def analyze_configuration_issues(self):
        """åˆ†æé…ç½®ç®¡ç†é—®é¢˜"""
        logger.info("åˆ†æé…ç½®ç®¡ç†é—®é¢˜...")

        # æ£€æŸ¥ç¡¬ç¼–ç é…ç½®
        python_files = list(self.project_root.rglob("*.py"))
        for py_file in python_files:
            if self._should_skip_file(py_file):
                continue

            try:
                with open(py_file, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()

                # æ£€æŸ¥ç¡¬ç¼–ç é…ç½®
                if re.search(r'=\s*["\'][^"\']*[\d\.]+["\']', content):  # ç¡¬ç¼–ç æ•°å­—
                    self.issues["configuration_issues"].append(
                        {
                            "file": str(py_file),
                            "issue": "hardcoded_numbers",
                            "category": "configuration",
                            "severity": "medium",
                        }
                    )
                    break

                if re.search(r'=\s*["\'][^"\']*(?:localhost|127\.0\.0\.1|3306|5432)["\']', content):
                    self.issues["configuration_issues"].append(
                        {
                            "file": str(py_file),
                            "issue": "hardcoded_config",
                            "category": "configuration",
                            "severity": "high",
                        }
                    )
                    break

            except Exception as e:
                logger.warning(f"åˆ†æé…ç½®é—®é¢˜å¤±è´¥ {py_file}: {e}")

        # æ£€æŸ¥ç¯å¢ƒå˜é‡ä½¿ç”¨
        env_vars_used = False
        for py_file in python_files:
            if self._should_skip_file(py_file):
                continue
            try:
                with open(py_file, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()
                if "os.environ" in content or "getenv" in content:
                    env_vars_used = True
                    break
            except:
                continue

        if not env_vars_used:
            self.issues["configuration_issues"].append(
                {"issue": "no_environment_variables", "category": "configuration", "severity": "medium"}
            )

    def generate_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        total_issues = sum(len(issues) for issues in self.issues.values())

        category_counts = defaultdict(int)
        severity_counts = defaultdict(int)

        for category, issues in self.issues.items():
            category_counts[category] = len(issues)
            for issue in issues:
                severity_counts[issue.get("severity", "unknown")] += 1

        return {
            "total_issues": total_issues,
            "categories": dict(category_counts),
            "severities": dict(severity_counts),
            "project_stats": dict(self.stats),
            "analysis_date": "2025-11-25",
        }

    def generate_recommendations(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # åŸºäºå‘ç°çš„é—®é¢˜ç”Ÿæˆå»ºè®®
        if len(self.issues["long_functions"]) > 10:
            recommendations.append(
                {
                    "priority": "high",
                    "category": "code_quality",
                    "title": "é‡æ„é•¿å‡½æ•°",
                    "description": f'å‘ç°{len(self.issues["long_functions"])}ä¸ªè¿‡é•¿å‡½æ•°ï¼Œå»ºè®®è¿›è¡Œé‡æ„',
                    "actions": ["å°†é•¿å‡½æ•°æ‹†åˆ†ä¸ºå¤šä¸ªå°å‡½æ•°", "æå–å…¬å…±é€»è¾‘åˆ°ç‹¬ç«‹å‡½æ•°", "ä½¿ç”¨è£…é¥°å™¨ç®€åŒ–æ¨ªåˆ‡å…³æ³¨ç‚¹"],
                }
            )

        if len(self.issues["security_issues"]) > 0:
            recommendations.append(
                {
                    "priority": "critical",
                    "category": "security",
                    "title": "ä¿®å¤å®‰å…¨æ¼æ´",
                    "description": f'å‘ç°{len(self.issues["security_issues"])}ä¸ªå®‰å…¨é—®é¢˜ï¼Œéœ€è¦ç«‹å³å¤„ç†',
                    "actions": ["ç§»é™¤ç¡¬ç¼–ç çš„å¯†é’¥å’Œå¯†ç ", "ä½¿ç”¨ç¯å¢ƒå˜é‡ç®¡ç†æ•æ„Ÿé…ç½®", "å®æ–½è¾“å…¥éªŒè¯å’ŒSQLæ³¨å…¥é˜²æŠ¤"],
                }
            )

        if len(self.issues["test_issues"]) > 0:
            recommendations.append(
                {
                    "priority": "high",
                    "category": "testing",
                    "title": "æé«˜æµ‹è¯•è¦†ç›–ç‡",
                    "description": "æµ‹è¯•è¦†ç›–ç‡ä¸è¶³ï¼Œå»ºè®®å¢åŠ å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•",
                    "actions": ["ä¸ºå…³é”®ä¸šåŠ¡é€»è¾‘ç¼–å†™å•å…ƒæµ‹è¯•", "å®æ–½è‡ªåŠ¨åŒ–æµ‹è¯•", "å¢åŠ ç«¯åˆ°ç«¯æµ‹è¯•"],
                }
            )

        return recommendations

    def calculate_debt_score(self) -> float:
        """è®¡ç®—æŠ€æœ¯è´Ÿå€ºè¯„åˆ†ï¼ˆ0-100ï¼Œ100ä¸ºæ— è´Ÿå€ºï¼‰"""
        score = 100.0

        # æ ¹æ®é—®é¢˜æ•°é‡å’Œä¸¥é‡ç¨‹åº¦æ‰£åˆ†
        severity_weights = {"critical": 10, "high": 5, "medium": 2, "low": 1}

        total_deduction = 0
        for category, issues in self.issues.items():
            for issue in issues:
                severity = issue.get("severity", "low")
                weight = severity_weights.get(severity, 1)
                total_deduction += weight

        # æ ¹æ®ä»£ç è¡Œæ•°è°ƒæ•´è¯„åˆ†
        if self.stats["code_lines"] > 100000:
            total_deduction *= 1.5
        elif self.stats["code_lines"] > 50000:
            total_deduction *= 1.2

        score = max(0, 100 - total_deduction)
        return round(score, 2)

    def get_priority_actions(self) -> List[Dict[str, Any]]:
        """è·å–ä¼˜å…ˆå¤„ç†è¡ŒåŠ¨"""
        actions = []

        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åºæ‰€æœ‰é—®é¢˜
        all_issues = []
        for category, issues in self.issues.items():
            for issue in issues:
                all_issues.append({**issue, "category": category})

        all_issues.sort(
            key=lambda x: {"critical": 4, "high": 3, "medium": 2, "low": 1}.get(x.get("severity", "low"), 1),
            reverse=True,
        )

        # å–å‰10ä¸ªæœ€ä¸¥é‡çš„é—®é¢˜
        for issue in all_issues[:10]:
            actions.append(
                {
                    "priority": issue.get("severity", "low"),
                    "category": issue["category"],
                    "file": issue.get("file", "N/A"),
                    "issue": issue.get("issue", issue.get("category", "unknown")),
                    "description": f"åœ¨{issue.get('file', 'æœªçŸ¥æ–‡ä»¶')}ä¸­å‘ç°{issue.get('issue', 'é—®é¢˜')}",
                }
            )

        return actions


def main():
    """ä¸»å‡½æ•°"""
    analyzer = TechnicalDebtAnalyzer()
    results = analyzer.analyze_all()

    # ç”ŸæˆæŠ¥å‘Š
    report_file = "/opt/claude/mystocks_spec/technical_debt_assessment_report.md"

    with open(report_file, "w", encoding="utf-8") as f:
        f.write("# MyStocks æŠ€æœ¯è´Ÿå€ºè¯„ä¼°æŠ¥å‘Š\n\n")
        f.write(f"**è¯„ä¼°æ—¥æœŸ**: {results['analysis_summary']['analysis_date']}\n")
        f.write(f"**æŠ€æœ¯è´Ÿå€ºè¯„åˆ†**: {results['technical_debt_score']}/100\n\n")

        # æ€»ä½“æ¦‚å†µ
        f.write("## ğŸ“Š æ€»ä½“æ¦‚å†µ\n\n")
        summary = results["analysis_summary"]
        f.write(f"- **é—®é¢˜æ€»æ•°**: {summary['total_issues']}\n")
        f.write(f"- **ä»£ç æ–‡ä»¶æ•°**: {summary['project_stats']['python_files']}\n")
        f.write(f"- **æ€»ä»£ç è¡Œæ•°**: {summary['project_stats']['total_lines']:,}\n")
        f.write(f"- **Pythonæ–‡ä»¶æ•°**: {summary['project_stats']['python_files']}\n\n")

        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        f.write("## ğŸ“‹ é—®é¢˜åˆ†ç±»ç»Ÿè®¡\n\n")
        for category, count in summary["categories"].items():
            severity_info = []
            for issue in results["detailed_issues"][category]:
                severity = issue.get("severity", "unknown")
                if severity not in [s[0] for s in severity_info]:
                    severity_count = sum(
                        1 for i in results["detailed_issues"][category] if i.get("severity") == severity
                    )
                    severity_info.append((severity, severity_count))

            f.write(f"### {category.replace('_', ' ').title()}\n")
            f.write(f"- æ€»æ•°: {count}\n")
            for severity, count in severity_info:
                f.write(f"- {severity}: {count}\n")
            f.write("\n")

        # ä¼˜å…ˆè¡ŒåŠ¨
        f.write("## ğŸš¨ ä¼˜å…ˆå¤„ç†è¡ŒåŠ¨\n\n")
        for i, action in enumerate(results["priority_actions"][:5], 1):
            f.write(f"{i}. **{action['priority'].upper()}** - {action['description']}\n")
            f.write(f"   - æ–‡ä»¶: `{action['file']}`\n")
            f.write(f"   - ç±»åˆ«: {action['category']}\n\n")

        # ä¼˜åŒ–å»ºè®®
        f.write("## ğŸ’¡ ä¼˜åŒ–å»ºè®®\n\n")
        for rec in results["recommendations"]:
            f.write(f"### {rec['title']} ({rec['priority'].upper()})\n")
            f.write(f"{rec['description']}\n\n")
            f.write("**è¡ŒåŠ¨å»ºè®®**:\n")
            for action in rec["actions"]:
                f.write(f"- {action}\n")
            f.write("\n")

        # è¯¦ç»†é—®é¢˜åˆ—è¡¨
        f.write("## ğŸ“ è¯¦ç»†é—®é¢˜åˆ—è¡¨\n\n")
        for category, issues in results["detailed_issues"].items():
            if issues:
                f.write(f"### {category.replace('_', ' ').title()}\n\n")
                for issue in issues[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ªé—®é¢˜
                    f.write(f"- **æ–‡ä»¶**: `{issue.get('file', 'N/A')}`\n")
                    f.write(f"  - **é—®é¢˜**: {issue.get('issue', issue.get('category', 'unknown'))}\n")
                    f.write(f"  - **ä¸¥é‡ç¨‹åº¦**: {issue.get('severity', 'unknown')}\n\n")

                if len(issues) > 20:
                    f.write(f"*... è¿˜æœ‰{len(issues) - 20}ä¸ªç±»ä¼¼é—®é¢˜*\n\n")

        f.write("---\n")
        f.write("*æœ¬æŠ¥å‘Šç”±iFlow CLIè‡ªåŠ¨ç”Ÿæˆ - æŠ€æœ¯è´Ÿå€ºåˆ†æå™¨ v1.0*\n")

    logger.info(f"æŠ€æœ¯è´Ÿå€ºè¯„ä¼°æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

    # è¾“å‡ºåˆ°æ§åˆ¶å°
    print(f"\n{'='*60}")
    print(f"ğŸ” MyStocks æŠ€æœ¯è´Ÿå€ºè¯„ä¼°æŠ¥å‘Š")
    print(f"{'='*60}")
    print(f"ğŸ“Š æŠ€æœ¯è´Ÿå€ºè¯„åˆ†: {results['technical_debt_score']}/100")
    print(f"ğŸ“‹ é—®é¢˜æ€»æ•°: {results['analysis_summary']['total_issues']}")
    print(f"ğŸ Pythonæ–‡ä»¶: {results['analysis_summary']['project_stats']['python_files']}")
    print(f"ğŸ“„ æ€»ä»£ç è¡Œ: {results['analysis_summary']['project_stats']['total_lines']:,}")
    print(f"{'='*60}")
    print(f"ğŸ“ è¯¦ç»†æŠ¥å‘Š: {report_file}")
    print(f"{'='*60}\n")

    return results


if __name__ == "__main__":
    main()
