#!/usr/bin/env python3
"""
åˆ†æå®é™…çš„GPUæ–‡ä»¶
è¯†åˆ«éœ€è¦è¿ç§»çš„å…³é”®æ–‡ä»¶ï¼Œåˆ¶å®šå…·ä½“çš„è¿ç§»è®¡åˆ’
"""

import os
import re
from pathlib import Path
from typing import List, Dict, Any, Tuple
import json
from dataclasses import dataclass, field
from collections import defaultdict


@dataclass
class GPUFileInfo:
    """GPUæ–‡ä»¶ä¿¡æ¯"""

    path: str
    name: str
    size: int
    has_direct_gpu: bool = False
    gpu_libraries: List[str] = field(default_factory=list)
    migration_complexity: str = "MEDIUM"
    migration_priority: str = "MEDIUM"
    recommended_action: str = ""


class GPUFileAnalyzer:
    """GPUæ–‡ä»¶åˆ†æå™¨"""

    def __init__(self):
        self.gpu_files: List[GPUFileInfo] = []
        self.analysis_results = {}

    def analyze_gpu_files(self) -> Dict[str, Any]:
        """åˆ†æGPUæ–‡ä»¶"""
        print("ğŸ” åˆ†æå®é™…GPUæ–‡ä»¶...")

        # 1. æŸ¥æ‰¾æ‰€æœ‰GPUç›¸å…³æ–‡ä»¶
        gpu_files = self._find_gpu_files()

        if not gpu_files:
            print("   æœªæ‰¾åˆ°GPUæ–‡ä»¶")
            return {}

        # 2. åˆ†ææ¯ä¸ªæ–‡ä»¶
        analyzed_files = []
        for file_path in gpu_files:
            file_info = self._analyze_gpu_file(file_path)
            analyzed_files.append(file_info)
            print(f"   ğŸ“ åˆ†æ: {file_info.name} ({file_info.size:,} bytes)")

        self.gpu_files = analyzed_files

        # 3. ç”Ÿæˆåˆ†æç»“æœ
        results = self._generate_analysis_results()

        self.analysis_results = results
        return results

    def _find_gpu_files(self) -> List[str]:
        """æŸ¥æ‰¾GPUæ–‡ä»¶"""
        gpu_files = []
        project_root = Path("src")

        # é€’å½’æŸ¥æ‰¾Pythonæ–‡ä»¶
        for py_file in project_root.rglob("*.py"):
            if "gpu" in str(py_file).lower():
                gpu_files.append(str(py_file))

        # æŒ‰å¤§å°æ’åº
        gpu_files.sort(key=lambda x: os.path.getsize(x), reverse=True)
        return gpu_files

    def _analyze_gpu_file(self, file_path: str) -> GPUFileInfo:
        """åˆ†æå•ä¸ªGPUæ–‡ä»¶"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
        except Exception as e:
            print(f"   âš ï¸  æ— æ³•è¯»å– {file_path}: {e}")
            return GPUFileInfo(
                path=file_path,
                name=os.path.basename(file_path),
                size=0,
                migration_complexity="HIGH",
                recommended_action="File read error - manual review required",
            )

        file_size = len(content)
        file_name = os.path.basename(file_path)

        # æ£€æµ‹GPUåº“ä½¿ç”¨
        gpu_libraries = self._detect_gpu_libraries(content)

        # æ£€æµ‹ç›´æ¥GPUè°ƒç”¨
        has_direct_gpu = self._detect_direct_gpu_calls(content)

        # ç¡®å®šè¿ç§»å¤æ‚åº¦å’Œä¼˜å…ˆçº§
        complexity, priority = self._assess_migration_complexity(
            file_path, content, has_direct_gpu, gpu_libraries
        )

        # æ¨èè¡ŒåŠ¨
        recommended_action = self._recommend_action(
            file_path, content, has_direct_gpu, gpu_libraries
        )

        return GPUFileInfo(
            path=file_path,
            name=file_name,
            size=file_size,
            has_direct_gpu=has_direct_gpu,
            gpu_libraries=gpu_libraries,
            migration_complexity=complexity,
            migration_priority=priority,
            recommended_action=recommended_action,
        )

    def _detect_gpu_libraries(self, content: str) -> List[str]:
        """æ£€æµ‹GPUåº“ä½¿ç”¨"""
        gpu_libs = []

        # GPUåº“æ¨¡å¼
        gpu_patterns = [
            (r"import\s+cupy", "CuPy"),
            (r"import\s+torch", "PyTorch"),
            (r"import\s+numba", "Numba"),
            (r"import\s+pycuda", "PyCUDA"),
            (r"from\s+cupy", "CuPy"),
            (r"from\s+torch", "PyTorch"),
            (r"from\s+numba", "Numba"),
            (r"cuda\.", "CUDA Direct"),
            (r"\.cuda\(\)", "PyTorch CUDA"),
            (r"\.to\(['\"]cuda", "PyTorch CUDA"),
        ]

        for pattern, lib_name in gpu_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                gpu_libs.append(lib_name)

        return list(set(gpu_libs))

    def _detect_direct_gpu_calls(self, content: str) -> bool:
        """æ£€æµ‹ç›´æ¥GPUè°ƒç”¨"""
        direct_patterns = [
            r"cp\.array\(",
            r"cp\.zeros\(",
            r"cp\.ones\(",
            r"torch\.tensor\(",
            r"torch\.zeros\(",
            r"torch\.ones\(",
            r"cuda\.Device\(",
            r"torch\.cuda\.device\(",
            r"\.cuda\(\)",
            r"\.to\(['\"]cuda",
            r"torch\.no_grad\(\)",
        ]

        for pattern in direct_patterns:
            if re.search(pattern, content):
                return True

        return False

    def _assess_migration_complexity(
        self,
        file_path: str,
        content: str,
        has_direct_gpu: bool,
        gpu_libraries: List[str],
    ) -> Tuple[str, str]:
        """è¯„ä¼°è¿ç§»å¤æ‚åº¦å’Œä¼˜å…ˆçº§"""
        file_size = len(content)
        file_name = os.path.basename(file_path).lower()

        # å¤æ‚åº¦è¯„ä¼°
        complexity = "MEDIUM"  # é»˜è®¤

        if file_size > 2000:  # å¤§æ–‡ä»¶
            complexity = "HIGH"
        elif has_direct_gpu and len(gpu_libraries) > 2:  # å¤šGPUåº“
            complexity = "HIGH"
        elif not has_direct_gpu and not gpu_libraries:
            complexity = "LOW"

        # ä¼˜å…ˆçº§è¯„ä¼°
        priority = "MEDIUM"  # é»˜è®¤

        if file_name.startswith("gpu_") and (
            "manager" in file_name or "resource" in file_name
        ):
            priority = "HIGH"
        elif file_name.startswith("gpu_") and (
            "api" in file_name or "server" in file_name
        ):
            priority = "HIGH"
        elif file_name.startswith("gpu_") and (
            "accelerated" in file_name or "engine" in file_name
        ):
            priority = "HIGH"
        elif "test" in file_name:
            priority = "LOW"

        return complexity, priority

    def _recommend_action(
        self,
        file_path: str,
        content: str,
        has_direct_gpu: bool,
        gpu_libraries: List[str],
    ) -> str:
        """æ¨èè¡ŒåŠ¨"""
        file_name = os.path.basename(file_path).lower()

        if not has_direct_gpu and not gpu_libraries:
            return "No GPU usage - no migration needed"

        if "api_system" in file_path or "server" in file_name:
            return "High priority - replace GPU calls with HAL API"

        if "test" in file_name:
            return "Low priority - update test to use HAL interfaces"

        if "manager" in file_name or "resource" in file_name:
            return "High priority - migrate to GPUResourceManager"

        if "accelerated" in file_name or "engine" in file_name:
            return "High priority - migrate to kernel engines"

        if has_direct_gpu:
            return "Medium priority - replace direct GPU calls with HAL layer"

        return "Review for HAL integration opportunities"

    def _generate_analysis_results(self) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†æç»“æœ"""
        total_files = len(self.gpu_files)
        total_size = sum(f.size for f in self.gpu_files)
        files_with_gpu = sum(1 for f in self.gpu_files if f.has_direct_gpu)

        # HALå±‚åˆ†å¸ƒç»Ÿè®¡
        hal_layer_stats = defaultdict(int)
        for file_info in self.gpu_files:
            if (
                "manager" in file_info.name.lower()
                or "resource" in file_info.name.lower()
            ):
                hal_layer_stats["GPUResourceManager"] += 1
            elif (
                "accelerated" in file_info.name.lower()
                or "engine" in file_info.name.lower()
            ):
                hal_layer_stats["AcceleratedEngine"] += 1
            elif "api" in file_info.name.lower() or "server" in file_info.name.lower():
                hal_layer_stats["APIServer"] += 1
            else:
                hal_layer_stats["General"] += 1

        # ä¼˜å…ˆçº§åˆ†å¸ƒ
        priority_stats = defaultdict(int)
        for file_info in self.gpu_files:
            priority_stats[file_info.migration_priority] += 1

        # å¤æ‚åº¦åˆ†å¸ƒ
        complexity_stats = defaultdict(int)
        for file_info in self.gpu_files:
            complexity_stats[file_info.migration_complexity] += 1

        # GPUåº“ä½¿ç”¨ç»Ÿè®¡
        gpu_lib_stats = defaultdict(int)
        for file_info in self.gpu_files:
            for lib in file_info.gpu_libraries:
                gpu_lib_stats[lib] += 1

        # ç”Ÿæˆè¿ç§»è®¡åˆ’
        migration_plan = self._create_migration_plan()

        return {
            "summary": {
                "total_files": total_files,
                "total_size_bytes": total_size,
                "files_with_direct_gpu": files_with_gpu,
                "gpu_usage_percentage": (files_with_gpu / total_files * 100)
                if total_files > 0
                else 0,
            },
            "statistics": {
                "priority_distribution": dict(priority_stats),
                "complexity_distribution": dict(complexity_stats),
                "hal_layer_distribution": dict(hal_layer_stats),
                "gpu_library_distribution": dict(gpu_lib_stats),
            },
            "files": [
                {
                    "path": f.path,
                    "name": f.name,
                    "size": f.size,
                    "has_direct_gpu": f.has_direct_gpu,
                    "gpu_libraries": f.gpu_libraries,
                    "complexity": f.migration_complexity,
                    "priority": f.migration_priority,
                    "recommended_action": f.recommended_action,
                }
                for f in self.gpu_files
            ],
            "migration_plan": migration_plan,
        }

    def _create_migration_plan(self) -> Dict[str, Any]:
        """åˆ›å»ºè¿ç§»è®¡åˆ’"""
        # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„
        high_priority = [f for f in self.gpu_files if f.migration_priority == "HIGH"]
        medium_priority = [
            f for f in self.gpu_files if f.migration_priority == "MEDIUM"
        ]
        low_priority = [f for f in self.gpu_files if f.migration_priority == "LOW"]

        # å…³é”®æ–‡ä»¶è¯†åˆ«
        key_files = []
        for f in self.gpu_files:
            if any(
                keyword in f.name.lower()
                for keyword in [
                    "gpu_acceleration_engine",
                    "gpu_resource_manager",
                    "main_server",
                    "integrated_services",
                    "acceleration_engine",
                ]
            ):
                key_files.append(f)

        return {
            "phases": [
                {
                    "phase": 1,
                    "name": "Key Infrastructure Files",
                    "files": key_files,
                    "count": len(key_files),
                    "description": "Critical GPU infrastructure files",
                    "estimated_days": 3,
                },
                {
                    "phase": 2,
                    "name": "High Priority Files",
                    "files": high_priority,
                    "count": len(high_priority),
                    "description": "High priority GPU files",
                    "estimated_days": 2,
                },
                {
                    "phase": 3,
                    "name": "Medium Priority Files",
                    "files": medium_priority,
                    "count": len(medium_priority),
                    "description": "Standard priority GPU files",
                    "estimated_days": 3,
                },
                {
                    "phase": 4,
                    "name": "Low Priority Files",
                    "files": low_priority,
                    "count": len(low_priority),
                    "description": "Low priority files including tests",
                    "estimated_days": 2,
                },
            ],
            "total_files": len(self.gpu_files),
            "total_estimated_days": 10,
            "success_criteria": [
                "All direct GPU calls replaced with HAL interfaces",
                "GPU resource management centralized",
                "Proper error handling and fallback mechanisms",
                "Performance maintained or improved",
                "All tests updated and passing",
            ],
        }

    def print_summary(self):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        if not self.analysis_results:
            print("âŒ å°šæœªè¿›è¡Œåˆ†æ")
            return

        print("\n" + "=" * 60)
        print("ğŸ“Š GPUæ–‡ä»¶åˆ†ææ‘˜è¦")
        print("=" * 60)

        summary = self.analysis_results["summary"]
        stats = self.analysis_results["statistics"]

        # åŸºæœ¬ä¿¡æ¯
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {summary['total_files']}")
        print(f"ğŸ“ æ€»ä»£ç è¡Œæ•°: {summary['total_size_bytes']:,}")
        print(f"ğŸ”¥ åŒ…å«GPUè°ƒç”¨çš„æ–‡ä»¶: {summary['files_with_direct_gpu']}")
        print(f"ğŸ“Š GPUä½¿ç”¨ç‡: {summary['gpu_usage_percentage']:.1f}%")

        # ä¼˜å…ˆçº§åˆ†å¸ƒ
        print("\nğŸ¯ ä¼˜å…ˆçº§åˆ†å¸ƒ:")
        for priority, count in stats["priority_distribution"].items():
            print(f"   {priority}: {count} æ–‡ä»¶")

        # å¤æ‚åº¦åˆ†å¸ƒ
        print("\nâš™ï¸ å¤æ‚åº¦åˆ†å¸ƒ:")
        for complexity, count in stats["complexity_distribution"].items():
            print(f"   {complexity}: {count} æ–‡ä»¶")

        # HALå±‚åˆ†å¸ƒ
        print("\nğŸ—ï¸ æ¨èHALå±‚:")
        for hal_layer, count in stats["hal_layer_distribution"].items():
            print(f"   {hal_layer}: {count} æ–‡ä»¶")

        # GPUåº“åˆ†å¸ƒ
        print("\nğŸ“š GPUåº“ä½¿ç”¨:")
        for lib, count in sorted(stats["gpu_library_distribution"].items()):
            print(f"   {lib}: {count} æ–‡ä»¶")

        # è¿ç§»è®¡åˆ’
        plan = self.analysis_results["migration_plan"]
        print("\nğŸ“‹ è¿ç§»è®¡åˆ’:")
        total_days = 0
        for phase in plan["phases"]:
            print(f"   é˜¶æ®µ{phase['phase']}: {phase['name']} ({phase['count']}æ–‡ä»¶)")
            print(f"      é¢„ä¼°: {phase['estimated_days']}å¤©")
            total_days += phase["estimated_days"]

        print(f"\nâ±ï¸ æ€»é¢„ä¼°å·¥ä½œé‡: {total_days}å¤©")

        print("\n" + "=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    analyzer = GPUFileAnalyzer()

    print("ğŸš€ å¼€å§‹åˆ†æå®é™…GPUæ–‡ä»¶...")

    # æ‰§è¡Œåˆ†æ
    results = analyzer.analyze_gpu_files()

    if not results:
        print("âŒ æœªæ‰¾åˆ°GPUæ–‡ä»¶æˆ–åˆ†æå¤±è´¥")
        return

    # ä¿å­˜åˆ†æç»“æœ
    report_path = "gpu_files_analysis_report.json"
    try:
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

    # æ‰“å°æ‘˜è¦
    analyzer.print_summary()

    return results


if __name__ == "__main__":
    main()
