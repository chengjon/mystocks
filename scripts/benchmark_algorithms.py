#!/usr/bin/env python3
"""
ç®—æ³•æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬

å¯¹æ¯”æµ‹è¯•ä¸‰ä¸ªåˆ†ç±»ç®—æ³•çš„æ€§èƒ½ï¼š
- SVM (Support Vector Machine)
- Decision Tree (Random Forest)
- Naive Bayes (Gaussian)

æµ‹è¯•ç»´åº¦ï¼š
- è®­ç»ƒæ—¶é—´
- é¢„æµ‹æ—¶é—´
- GPU vs CPU æ€§èƒ½å¯¹æ¯”
- å‡†ç¡®ç‡å¯¹æ¯”

ä½œè€…: MyStockså›¢é˜Ÿ
æ—¥æœŸ: 2026-01-12
"""

import sys
import asyncio
import logging
import numpy as np
import pandas as pd
import time
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = "/opt/claude/mystocks_spec"
sys.path.insert(0, project_root)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


class AlgorithmBenchmark:
    """ç®—æ³•æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""

    def __init__(self):
        self.algorithms = {}
        self.results = {}

    async def setup_algorithms(self):
        """è®¾ç½®æ‰€æœ‰ç®—æ³•å®ä¾‹"""
        from src.algorithms.classification.svm_algorithm import SVMAlgorithm
        from src.algorithms.classification.decision_tree_algorithm import DecisionTreeAlgorithm
        from src.algorithms.classification.naive_bayes_algorithm import NaiveBayesAlgorithm
        from src.algorithms.metadata import AlgorithmFingerprint

        # SVMç®—æ³•
        svm_metadata = AlgorithmFingerprint.from_config(
            {
                "name": "SVM_Benchmark",
                "description": "SVM algorithm benchmark",
                "algorithm_type": "classification",
                "gpu_enabled": True,
            }
        )
        self.algorithms["SVM"] = SVMAlgorithm(svm_metadata)

        # Decision Treeç®—æ³•
        dt_metadata = AlgorithmFingerprint.from_config(
            {
                "name": "DecisionTree_Benchmark",
                "description": "Decision Tree algorithm benchmark",
                "algorithm_type": "classification",
                "gpu_enabled": True,
            }
        )
        self.algorithms["Decision Tree"] = DecisionTreeAlgorithm(dt_metadata)

        # Naive Bayesç®—æ³•
        nb_metadata = AlgorithmFingerprint.from_config(
            {
                "name": "NaiveBayes_Benchmark",
                "description": "Naive Bayes algorithm benchmark",
                "algorithm_type": "classification",
                "gpu_enabled": True,
            }
        )
        self.algorithms["Naive Bayes"] = NaiveBayesAlgorithm(nb_metadata)

        logger.info(f"âœ“ å·²è®¾ç½® {len(self.algorithms)} ä¸ªç®—æ³•è¿›è¡ŒåŸºå‡†æµ‹è¯•")

    def generate_benchmark_data(self, n_samples: int = 5000, n_features: int = 20) -> pd.DataFrame:
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æ•°æ®"""
        np.random.seed(42)

        data = {}
        # ç”Ÿæˆç‰¹å¾æ•°æ®
        for i in range(n_features):
            data[f"feature_{i}"] = np.random.randn(n_samples)

        # ä¸ºNaive Bayesç”Ÿæˆæ›´åˆé€‚çš„æ­£å€¼ç‰¹å¾
        for i in range(5):  # å‰5ä¸ªç‰¹å¾ä¸ºæ­£å€¼
            data[f"feature_{i}"] = np.abs(data[f"feature_{i}"])

        # ç”Ÿæˆç›®æ ‡å˜é‡ (3åˆ†ç±»é—®é¢˜)
        data["target"] = np.random.randint(0, 3, n_samples)

        df = pd.DataFrame(data)
        logger.info(f"âœ“ ç”ŸæˆåŸºå‡†æµ‹è¯•æ•°æ®: {n_samples} ä¸ªæ ·æœ¬, {n_features} ä¸ªç‰¹å¾")
        return df

    async def benchmark_algorithm(self, name: str, data: pd.DataFrame, n_test_samples: int = 1000) -> Dict[str, Any]:
        """å¯¹å•ä¸ªç®—æ³•è¿›è¡ŒåŸºå‡†æµ‹è¯•"""
        logger.info(f"å¼€å§‹æµ‹è¯•ç®—æ³•: {name}")

        algorithm = self.algorithms[name]
        results = {"algorithm": name}

        # å‡†å¤‡è®­ç»ƒé…ç½®
        feature_cols = [col for col in data.columns if col.startswith("feature_")]
        config = {
            "feature_columns": feature_cols,
            "target_column": "target",
        }

        # æ·»åŠ ç®—æ³•ç‰¹å®šå‚æ•°
        if name == "SVM":
            config["svm_params"] = {"C": 1.0, "kernel": "rbf", "random_state": 42}
        elif name == "Decision Tree":
            config["dt_params"] = {"max_depth": 10, "random_state": 42}
        elif name == "Naive Bayes":
            config["nb_params"] = {"var_smoothing": 1e-9}

        # è®­ç»ƒåŸºå‡†æµ‹è¯•
        train_start = time.time()
        train_result = await algorithm.train(data, config)
        train_time = time.time() - train_start

        results.update(
            {
                "training_time": train_time,
                "training_accuracy": train_result.get("training_metrics", {}).get("accuracy", 0.0),
                "gpu_used_training": train_result.get("training_metrics", {}).get("gpu_used", False),
            }
        )

        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_data = data.sample(n=n_test_samples, random_state=42).reset_index(drop=True)

        # é¢„æµ‹åŸºå‡†æµ‹è¯•
        predict_start = time.time()
        predict_result = await algorithm.predict(test_data, train_result)
        predict_time = time.time() - predict_start

        results.update(
            {
                "prediction_time": predict_time,
                "gpu_used_prediction": predict_result.get("gpu_used", False),
                "predictions_count": len(predict_result.get("predictions", [])),
            }
        )

        # è¯„ä¼°åŸºå‡†æµ‹è¯•
        actual_labels = test_data["target"].values
        eval_start = time.time()
        eval_result = algorithm.evaluate(predict_result, actual_labels)
        eval_time = time.time() - eval_start

        results.update(
            {
                "evaluation_time": eval_time,
                "test_accuracy": eval_result.get("accuracy", 0.0),
                "precision": eval_result.get("precision", 0.0),
                "recall": eval_result.get("recall", 0.0),
                "f1_score": eval_result.get("f1_score", 0.0),
            }
        )

        logger.info(".4f")
        logger.info(".4f")
        logger.info(f"  - æµ‹è¯•å‡†ç¡®ç‡: {results['test_accuracy']:.4f}")

        return results

    async def run_full_benchmark(self, data_sizes: List[int] = None) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•"""
        if data_sizes is None:
            data_sizes = [1000, 2500, 5000]

        logger.info("=" * 80)
        logger.info("ç®—æ³•æ€§èƒ½åŸºå‡†æµ‹è¯•")
        logger.info("=" * 80)

        all_results = {}

        for n_samples in data_sizes:
            logger.info(f"\nğŸ“Š æµ‹è¯•æ•°æ®é›†å¤§å°: {n_samples} æ ·æœ¬")
            logger.info("-" * 50)

            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            data = self.generate_benchmark_data(n_samples=n_samples)

            # æµ‹è¯•æ¯ä¸ªç®—æ³•
            dataset_results = {}
            for algo_name in self.algorithms.keys():
                try:
                    result = await self.benchmark_algorithm(algo_name, data)
                    dataset_results[algo_name] = result
                except Exception as e:
                    logger.error(f"âŒ {algo_name} åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
                    dataset_results[algo_name] = {"algorithm": algo_name, "error": str(e)}

            all_results[f"samples_{n_samples}"] = dataset_results

        return all_results

    def print_benchmark_summary(self, results: Dict[str, Any]):
        """æ‰“å°åŸºå‡†æµ‹è¯•æ‘˜è¦"""
        logger.info("\n" + "=" * 80)
        logger.info("åŸºå‡†æµ‹è¯•æ‘˜è¦")
        logger.info("=" * 80)

        for dataset_key, dataset_results in results.items():
            sample_size = dataset_key.split("_")[1]
            logger.info(f"\nğŸ“Š æ•°æ®é›†: {sample_size} æ ·æœ¬")

            # åˆ›å»ºæ¯”è¾ƒè¡¨æ ¼
            header = ".4f"
            logger.info(header)
            logger.info("-" * len(header))

            for algo_name, result in dataset_results.items():
                if "error" in result:
                    logger.info("<15")
                else:
                    gpu_train = "âœ“" if result.get("gpu_used_training") else "âœ—"
                    gpu_pred = "âœ“" if result.get("gpu_used_prediction") else "âœ—"
                    logger.info("<15")

        logger.info("\n" + "=" * 80)
        logger.info("âœ… åŸºå‡†æµ‹è¯•å®Œæˆ")
        logger.info("=" * 80)


async def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºåŸºå‡†æµ‹è¯•å®ä¾‹
        benchmark = AlgorithmBenchmark()

        # è®¾ç½®ç®—æ³•
        await benchmark.setup_algorithms()

        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        results = await benchmark.run_full_benchmark()

        # æ‰“å°æ‘˜è¦
        benchmark.print_benchmark_summary(results)

        logger.info("\nğŸ‰ æ‰€æœ‰ç®—æ³•æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ!")

    except Exception as e:
        logger.error(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        logger.error(traceback.format_exc())
        exit(1)


if __name__ == "__main__":
    asyncio.run(main())
