#!/usr/bin/env python3
"""
æ€§èƒ½å›å½’æµ‹è¯•è„šæœ¬
ä¸ºæ ¸å¿ƒåŠŸèƒ½å»ºç«‹æ€§èƒ½åŸºå‡†å’Œå›å½’æ£€æµ‹
ç¡®ä¿ä»£ç ä¿®æ”¹ä¸ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™
"""

import sys
import os
import time
import statistics
from pathlib import Path
from typing import Dict, List, Tuple
import pandas as pd
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


class PerformanceRegressionTester:
    """æ€§èƒ½å›å½’æµ‹è¯•å™¨"""

    def __init__(self):
        self.baseline_file = project_root / "scripts" / "performance" / "baseline.json"
        self.baseline_data = {}
        self.test_results = {}

    def run_all_tests(self) -> Dict[str, Dict]:
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹è¿è¡Œæ€§èƒ½å›å½’æµ‹è¯•...")

        tests = [
            (
                "data_validator_symbol_validation",
                self.test_data_validator_symbol_validation,
            ),
            (
                "data_validator_date_validation",
                self.test_data_validator_date_validation,
            ),
            (
                "data_validator_price_validation",
                self.test_data_validator_price_validation,
            ),
            ("dataframe_operations", self.test_dataframe_operations),
            ("exception_creation", self.test_exception_creation),
            ("exception_serialization", self.test_exception_serialization),
        ]

        results = {}

        for test_name, test_func in tests:
            print(f"\nğŸ“Š è¿è¡Œæµ‹è¯•: {test_name}")
            try:
                results[test_name] = test_func()
                print(f"âœ… {test_name}: æµ‹è¯•é€šè¿‡")
            except Exception as e:
                print(f"âŒ {test_name}: æµ‹è¯•å¤±è´¥ - {e}")
                results[test_name] = {"error": str(e)}

        self.test_results = results
        return results

    def test_data_validator_symbol_validation(self) -> Dict:
        """æµ‹è¯•æ•°æ®éªŒè¯å™¨çš„è‚¡ç¥¨ä»£ç éªŒè¯æ€§èƒ½"""
        from src.adapters.data_validator import DataValidator

        validator = DataValidator()
        test_symbols = ["000001", "600000", "300001", "002415"] * 1000  # 4000ä¸ªæµ‹è¯•

        # é¢„çƒ­
        for symbol in test_symbols[:100]:
            validator.validate_stock_symbol(symbol)

        # å®é™…æµ‹è¯•
        start_time = time.time()
        for symbol in test_symbols:
            validator.validate_stock_symbol(symbol)
        end_time = time.time()

        duration = end_time - start_time
        operations_per_second = len(test_symbols) / duration

        return {
            "duration": duration,
            "operations": len(test_symbols),
            "ops_per_second": operations_per_second,
            "avg_time_per_op": duration / len(test_symbols) * 1000,  # è½¬æ¢ä¸ºæ¯«ç§’
        }

    def test_data_validator_date_validation(self) -> Dict:
        """æµ‹è¯•æ•°æ®éªŒè¯å™¨çš„æ—¥æœŸéªŒè¯æ€§èƒ½"""
        from src.adapters.data_validator import DataValidator

        validator = DataValidator()
        test_dates = ["2024-01-01", "2024-06-15", "2024-12-31"] * 1000  # 3000ä¸ªæµ‹è¯•

        # é¢„çƒ­
        for date in test_dates[:100]:
            validator.validate_date_format(date)

        # å®é™…æµ‹è¯•
        start_time = time.time()
        for date in test_dates:
            validator.validate_date_format(date)
        end_time = time.time()

        duration = end_time - start_time
        operations_per_second = len(test_dates) / duration

        return {
            "duration": duration,
            "operations": len(test_dates),
            "ops_per_second": operations_per_second,
            "avg_time_per_op": duration / len(test_dates) * 1000,
        }

    def test_data_validator_price_validation(self) -> Dict:
        """æµ‹è¯•æ•°æ®éªŒè¯å™¨çš„ä»·æ ¼æ•°æ®éªŒè¯æ€§èƒ½"""
        from src.adapters.data_validator import DataValidator

        validator = DataValidator()

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_cases = []
        for i in range(100):  # 100ä¸ªDataFrame
            df = pd.DataFrame(
                {
                    "open": [10.0 + i * 0.1] * 100,
                    "high": [10.5 + i * 0.1] * 100,
                    "low": [9.5 + i * 0.1] * 100,
                    "close": [10.2 + i * 0.1] * 100,
                    "volume": [1000 + i * 10] * 100,
                }
            )
            test_cases.append(df)

        # é¢„çƒ­
        for df in test_cases[:10]:
            validator.validate_price_data(df)

        # å®é™…æµ‹è¯•
        start_time = time.time()
        for df in test_cases:
            validator.validate_price_data(df)
        end_time = time.time()

        duration = end_time - start_time
        total_rows = sum(len(df) for df in test_cases)

        return {
            "duration": duration,
            "dataframes": len(test_cases),
            "total_rows": total_rows,
            "rows_per_second": total_rows / duration,
            "avg_time_per_df": duration / len(test_cases) * 1000,
        }

    def test_dataframe_operations(self) -> Dict:
        """æµ‹è¯•DataFrameæ“ä½œæ€§èƒ½"""
        # åˆ›å»ºå¤§å‹DataFrame
        large_df = pd.DataFrame(
            {
                "date": pd.date_range("2020-01-01", periods=10000),
                "open": np.random.uniform(10, 100, 10000),
                "high": np.random.uniform(10, 100, 10000),
                "low": np.random.uniform(10, 100, 10000),
                "close": np.random.uniform(10, 100, 10000),
                "volume": np.random.randint(1000, 10000, 10000),
            }
        )

        operations = []

        # æµ‹è¯•åŸºæœ¬æ“ä½œ
        start_time = time.time()

        # 1. è¿‡æ»¤æ“ä½œ
        filtered = large_df[large_df["close"] > 50]
        operations.append(time.time() - start_time)

        # 2. åˆ†ç»„æ“ä½œ
        start_time = time.time()
        grouped = large_df.groupby(large_df["date"].dt.month).mean()
        operations.append(time.time() - start_time)

        # 3. æ’åºæ“ä½œ
        start_time = time.time()
        sorted_df = large_df.sort_values("close")
        operations.append(time.time() - start_time)

        # 4. èšåˆæ“ä½œ
        start_time = time.time()
        aggregated = large_df.agg(
            {
                "open": "first",
                "high": "max",
                "low": "min",
                "close": "last",
                "volume": "sum",
            }
        )
        operations.append(time.time() - start_time)

        total_time = sum(operations)
        avg_time = statistics.mean(operations)

        return {
            "rows": len(large_df),
            "total_time": total_time,
            "avg_operation_time": avg_time * 1000,  # è½¬æ¢ä¸ºæ¯«ç§’
            "operations": operations,
            "min_time": min(operations) * 1000,
            "max_time": max(operations) * 1000,
        }

    def test_exception_creation(self) -> Dict:
        """æµ‹è¯•å¼‚å¸¸åˆ›å»ºæ€§èƒ½"""
        from src.core.exceptions import MyStocksException, NetworkError

        # æµ‹è¯•ä¸åŒå¼‚å¸¸ç±»å‹çš„åˆ›å»ºæ—¶é—´
        exception_types = [
            MyStocksException,
            NetworkError,
        ]

        results = {}

        for exc_class in exception_types:
            # é¢„çƒ­
            for _ in range(100):
                exc_class("Warmup")

            # å®é™…æµ‹è¯•
            start_time = time.time()
            for i in range(1000):
                exc = exc_class(f"Test message {i}", context={"index": i})
            end_time = time.time()

            duration = end_time - start_time
            class_name = exc_class.__name__

            results[class_name] = {
                "duration": duration,
                "ops_per_second": 1000 / duration,
                "avg_time_per_op": duration / 1000 * 1000,
            }

        return results

    def test_exception_serialization(self) -> Dict:
        """æµ‹è¯•å¼‚å¸¸åºåˆ—åŒ–æ€§èƒ½"""
        from src.core.exceptions import MyStocksException

        # åˆ›å»ºæµ‹è¯•å¼‚å¸¸
        exceptions = []
        for i in range(1000):
            exc = MyStocksException(
                f"Test message {i}",
                context={"index": i, "data": "x" * 50},  # å¢åŠ ä¸Šä¸‹æ–‡å¤§å°
            )
            exceptions.append(exc)

        # æµ‹è¯•to_dictåºåˆ—åŒ–
        start_time = time.time()
        for exc in exceptions:
            data = exc.to_dict()
        end_time = time.time()

        dict_serialization_time = end_time - start_time

        # æµ‹è¯•å­—ç¬¦ä¸²åºåˆ—åŒ–
        start_time = time.time()
        for exc in exceptions:
            str_repr = str(exc)
        end_time = time.time()

        str_serialization_time = end_time - start_time

        return {
            "exception_count": len(exceptions),
            "dict_serialization_time": dict_serialization_time,
            "dict_ops_per_second": len(exceptions) / dict_serialization_time,
            "str_serialization_time": str_serialization_time,
            "str_ops_per_second": len(exceptions) / str_serialization_time,
            "avg_dict_time": dict_serialization_time / len(exceptions) * 1000,
            "avg_str_time": str_serialization_time / len(exceptions) * 1000,
        }

    def load_baseline(self) -> bool:
        """åŠ è½½åŸºå‡†æ•°æ®"""
        if not self.baseline_file.exists():
            print(f"âš ï¸  åŸºå‡†æ–‡ä»¶ä¸å­˜åœ¨: {self.baseline_file}")
            return False

        try:
            import json

            with open(self.baseline_file, "r") as f:
                self.baseline_data = json.load(f)
            print(f"âœ… åŠ è½½åŸºå‡†æ•°æ®: {len(self.baseline_data)} ä¸ªæµ‹è¯•")
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½åŸºå‡†æ•°æ®å¤±è´¥: {e}")
            return False

    def save_baseline(self):
        """ä¿å­˜å½“å‰æµ‹è¯•ç»“æœä½œä¸ºåŸºå‡†"""
        try:
            self.baseline_file.parent.mkdir(parents=True, exist_ok=True)

            import json

            with open(self.baseline_file, "w") as f:
                json.dump(self.test_results, f, indent=2)
            print(f"âœ… ä¿å­˜åŸºå‡†æ•°æ®åˆ°: {self.baseline_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜åŸºå‡†æ•°æ®å¤±è´¥: {e}")

    def compare_with_baseline(self) -> Tuple[bool, List[str]]:
        """ä¸åŸºå‡†æ•°æ®æ¯”è¾ƒ"""
        if not self.baseline_data:
            return True, ["æ²¡æœ‰åŸºå‡†æ•°æ®å¯ä¾›æ¯”è¾ƒ"]

        all_passed = True
        messages = []

        for test_name, current_result in self.test_results.items():
            if "error" in current_result:
                messages.append(f"âš ï¸  {test_name}: å½“å‰æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•æ¯”è¾ƒ")
                continue

            if test_name not in self.baseline_data:
                messages.append(f"ğŸ†• {test_name}: æ–°æµ‹è¯•ï¼Œå»ºç«‹åŸºå‡†")
                continue

            baseline_result = self.baseline_data[test_name]

            # æ¯”è¾ƒæ€§èƒ½æŒ‡æ ‡
            regression_detected = self._compare_performance(
                test_name, current_result, baseline_result
            )

            if regression_detected:
                all_passed = False
                messages.append(f"ğŸ“‰ {test_name}: æ£€æµ‹åˆ°æ€§èƒ½å›å½’")
                messages.append(f"    å½“å‰: {self._format_performance(current_result)}")
                messages.append(
                    f"    åŸºå‡†: {self._format_performance(baseline_result)}"
                )
            else:
                messages.append(f"âœ… {test_name}: æ€§èƒ½æ­£å¸¸æˆ–æ”¹å–„")

        return all_passed, messages

    def _compare_performance(
        self, test_name: str, current: Dict, baseline: Dict
    ) -> bool:
        """æ¯”è¾ƒæ€§èƒ½æŒ‡æ ‡"""
        # å®šä¹‰æ€§èƒ½å›å½’é˜ˆå€¼ï¼ˆå…è®¸10%çš„æ€§èƒ½ä¸‹é™ï¼‰
        regression_threshold = 1.1  # 10%æ€§èƒ½ä¸‹é™é˜ˆå€¼

        if test_name == "exception_creation":
            # å¼‚å¸¸åˆ›å»ºæµ‹è¯•
            for exc_class in current_result:
                if exc_class in baseline:
                    current_ops = current_result[exc_class]["ops_per_second"]
                    baseline_ops = baseline[exc_class]["ops_per_second"]

                    if current_ops < baseline_ops / regression_threshold:
                        return True

        elif "ops_per_second" in current:
            # é€šç”¨æ¯ç§’æ“ä½œæ•°æ¯”è¾ƒ
            current_ops = current_result["ops_per_second"]
            baseline_ops = baseline.get("ops_per_second", 0)

            if baseline_ops > 0 and current_ops < baseline_ops / regression_threshold:
                return True

        elif "rows_per_second" in current:
            # DataFrameå¤„ç†é€Ÿåº¦æ¯”è¾ƒ
            current_speed = current_result["rows_per_second"]
            baseline_speed = baseline.get("rows_per_second", 0)

            if (
                baseline_speed > 0
                and current_speed < baseline_speed / regression_threshold
            ):
                return True

        return False

    def _format_performance(self, result: Dict) -> str:
        """æ ¼å¼åŒ–æ€§èƒ½æ•°æ®"""
        if "ops_per_second" in result:
            return f"{result['ops_per_second']:.1f} ops/s"
        elif "rows_per_second" in result:
            return f"{result['rows_per_second']:.1f} rows/s"
        elif "duration" in result:
            return f"{result['duration']:.3f}s"
        else:
            return str(result)

    def generate_report(self, comparison_passed: bool, messages: List[str]) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = []
        report.append("=" * 60)
        report.append("âš¡ æ€§èƒ½å›å½’æµ‹è¯•æŠ¥å‘Š")
        report.append("=" * 60)
        report.append(f"æµ‹è¯•æ—¶é—´: {self._get_current_time()}")
        report.append("")

        # æ€»ä½“çŠ¶æ€
        status = "âœ… é€šè¿‡" if comparison_passed else "âŒ æ£€æµ‹åˆ°æ€§èƒ½å›å½’"
        report.append(f"æ€»ä½“çŠ¶æ€: {status}")
        report.append("")

        # æµ‹è¯•ç»“æœ
        report.append("ğŸ“Š æµ‹è¯•ç»“æœè¯¦æƒ…:")
        report.append("-" * 30)

        for test_name, result in self.test_results.items():
            if "error" in result:
                report.append(f"  âŒ {test_name}: æµ‹è¯•å¤±è´¥")
            else:
                report.append(f"  âœ… {test_name}: æµ‹è¯•é€šè¿‡")

        report.append("")
        report.append("ğŸ“‹ æ€§èƒ½æ¯”è¾ƒ:")
        report.append("-" * 30)

        for message in messages:
            report.append(f"  {message}")

        # å»ºè®®
        report.append("")
        report.append("ğŸ’¡ å»ºè®®:")
        if comparison_passed:
            report.append("  âœ… æ‰€æœ‰æ€§èƒ½æµ‹è¯•æ­£å¸¸")
            report.append("  âœ… å¯ä»¥ç»§ç»­å¼€å‘æˆ–é‡æ„")
        else:
            report.append("  âŒ æ£€æµ‹åˆ°æ€§èƒ½å›å½’")
            report.append("  âŒ å»ºè®®åˆ†ææ€§èƒ½ç“¶é¢ˆ")
            report.append("  âŒ è€ƒè™‘ä¼˜åŒ–åå†æäº¤ä»£ç ")

        report.append("")
        report.append("=" * 60)

        return "\n".join(report)

    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´"""
        from datetime import datetime

        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    def run(self, save_baseline: bool = False) -> bool:
        """è¿è¡Œæ€§èƒ½å›å½’æµ‹è¯•"""
        print("ğŸš€ å¯åŠ¨æ€§èƒ½å›å½’æµ‹è¯•...")

        # è¿è¡Œæµ‹è¯•
        self.run_all_tests()

        if save_baseline:
            print("\nğŸ’¾ ä¿å­˜åŸºå‡†æ•°æ®...")
            self.save_baseline()
            return True

        # åŠ è½½åŸºå‡†å¹¶æ¯”è¾ƒ
        self.load_baseline()
        comparison_passed, messages = self.compare_with_baseline()

        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_report(comparison_passed, messages)
        print("\n" + report)

        # ä¿å­˜æŠ¥å‘Š
        report_file = project_root / "performance_report.txt"
        try:
            with open(report_file, "w", encoding="utf-8") as f:
                f.write(report)
            print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

        return comparison_passed


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="æ€§èƒ½å›å½’æµ‹è¯•å·¥å…·")
    parser.add_argument(
        "--save-baseline", action="store_true", help="ä¿å­˜å½“å‰æµ‹è¯•ç»“æœä½œä¸ºåŸºå‡†æ•°æ®"
    )
    parser.add_argument(
        "--baseline-only", action="store_true", help="åªè¿è¡ŒåŸºå‡†æµ‹è¯•ï¼Œä¸è¿›è¡Œæ¯”è¾ƒ"
    )

    args = parser.parse_args()

    tester = PerformanceRegressionTester()

    try:
        if args.baseline_only:
            success = tester.run(save_baseline=True)
        else:
            success = tester.run(save_baseline=args.save_baseline)

        if success:
            print("\nğŸ‰ æ€§èƒ½å›å½’æµ‹è¯•é€šè¿‡!")
            sys.exit(0)
        else:
            print("\nâš ï¸  æ€§èƒ½å›å½’æµ‹è¯•æœªé€šè¿‡ï¼Œè¯·æ£€æŸ¥æ€§èƒ½ç“¶é¢ˆ")
            sys.exit(1)

    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
