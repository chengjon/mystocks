#!/usr/bin/env python3
"""
å°†api_data_inventory.jsonæ‹†åˆ†æˆå¤šä¸ªå°æ–‡ä»¶ï¼Œå¹¶åˆ›å»ºç´¢å¼•
"""

import json
from pathlib import Path
from collections import defaultdict


def split_api_inventory():
    """æ‹†åˆ†APIæ¸…å•æ–‡ä»¶"""

    print("ğŸ“ æ‹†åˆ†APIæ•°æ®æ¸…å•æ–‡ä»¶...")

    # è¯»å–åŸå§‹æ–‡ä»¶
    with open("docs/reports/api_data_inventory.json", "r", encoding="utf-8") as f:
        data = json.load(f)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("docs/reports/api_split")
    output_dir.mkdir(parents=True, exist_ok=True)

    # æŒ‰è·¯å¾„å‰ç¼€åˆ†ç»„
    api_by_prefix = defaultdict(list)

    for endpoint in data["endpoints"]:
        path = endpoint["path"]
        # è·¯å¾„å‰ç¼€ï¼ˆå¦‚ /auth, /data, /dashboardï¼‰
        parts = path.split("/")
        if len(parts) > 1:
            prefix = f"/{parts[1]}"
        else:
            prefix = "/root"

        api_by_prefix[prefix].append(endpoint)

    # ä¸ºæ¯ä¸ªå‰ç¼€åˆ›å»ºå•ç‹¬çš„æ–‡ä»¶
    split_files = []

    for prefix, endpoints in sorted(api_by_prefix.items()):
        # æ¸…ç†å‰ç¼€ä½œä¸ºæ–‡ä»¶å
        filename = prefix.replace("/", "_").replace("{", "").replace("}", "")
        if filename.startswith("_"):
            filename = filename[1:]

        # åˆ›å»ºå­æ–‡ä»¶æ•°æ®
        split_data = {
            "generated_at": data["generated_at"],
            "prefix": prefix,
            "total_endpoints": len(endpoints),
            "endpoints": endpoints,
        }

        # ä¿å­˜æ–‡ä»¶
        output_file = output_dir / f"api_{filename}.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(split_data, f, indent=2, ensure_ascii=False)

        split_files.append(
            {"prefix": prefix, "filename": f"api_{filename}.json", "count": len(endpoints), "file": output_file.name}
        )

        print(f"  âœ… ç”Ÿæˆ: {output_file.name} ({len(endpoints)} ä¸ªç«¯ç‚¹)")

    # åˆ›å»ºç´¢å¼•æ–‡ä»¶
    index_data = {
        "generated_at": data["generated_at"],
        "total_endpoints": data["total_endpoints"],
        "split_files": split_files,
        "summary": {
            "total_files": len(split_files),
            "endpoints_per_file_avg": data["total_endpoints"] // len(split_files) if split_files else 0,
            "files": [{"prefix": f["prefix"], "file": f["file"], "count": f["count"]} for f in split_files],
        },
    }

    index_file = output_dir / "api_index.json"
    with open(index_file, "w", encoding="utf-8") as f:
        json.dump(index_data, f, indent=2, ensure_ascii=False)

    print(f"  âœ… ç”Ÿæˆç´¢å¼•: {index_file.name}")

    # åˆ›å»ºMarkdownç´¢å¼•æ–‡æ¡£
    create_markdown_index(index_data, output_dir)

    print(f"\nâœ… æ‹†åˆ†å®Œæˆï¼")
    print(f"   æ€»æ–‡ä»¶æ•°: {len(split_files) + 2}")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")


def create_markdown_index(index_data: dict, output_dir: Path):
    """åˆ›å»ºMarkdownæ ¼å¼çš„ç´¢å¼•æ–‡æ¡£"""

    with open(output_dir / "API_SPLIT_INDEX.md", "w", encoding="utf-8") as f:
        # å†™å…¥å¤´éƒ¨
        f.write("# APIæ•°æ®æ¸…å•ç´¢å¼•\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {index_data['generated_at']}\n\n")

        # å†™å…¥æ¦‚è§ˆ
        f.write("## æ¦‚è§ˆ\n\n")
        f.write(f"- **æ€»APIç«¯ç‚¹æ•°**: {index_data['total_endpoints']}\n")
        f.write(f"- **æ‹†åˆ†æ–‡ä»¶æ•°**: {index_data['summary']['total_files']}\n")
        f.write(f"- **å¹³å‡æ¯æ–‡ä»¶**: {index_data['summary']['endpoints_per_file_avg']} ä¸ªç«¯ç‚¹\n\n")

        # å†™å…¥æ–‡ä»¶åˆ—è¡¨
        f.write("## æ‹†åˆ†æ–‡ä»¶åˆ—è¡¨\n\n")
        f.write("| è·¯å¾„å‰ç¼€ | æ–‡ä»¶å | ç«¯ç‚¹æ•°é‡ | é“¾æ¥ |\n")
        f.write("|---------|--------|----------|------|\n")

        for file_info in index_data["split_files"]:
            # è®¡ç®—å æ¯”
            percentage = file_info["count"] / index_data["total_endpoints"] * 100
            # åˆ›å»ºç›¸å¯¹é“¾æ¥
            link = f"[{file_info['file']}]({file_info['file']})"
            f.write(
                f"| {file_info['prefix']} | {file_info['file']} | {file_info['count']} ({percentage:.1f}%) | {link} |\n"
            )

        # å†™å…¥ä½¿ç”¨è¯´æ˜
        f.write("\n## ä½¿ç”¨è¯´æ˜\n\n")
        f.write("### æŸ¥æ‰¾ç‰¹å®šAPI\n\n")
        f.write("1. æ ¹æ®APIè·¯å¾„å‰ç¼€ï¼ˆå¦‚ `/auth`, `/data`, `/dashboard`ï¼‰æ‰¾åˆ°å¯¹åº”æ–‡ä»¶\n")
        f.write("2. ç‚¹å‡»é“¾æ¥æ‰“å¼€å¯¹åº”çš„JSONæ–‡ä»¶\n")
        f.write("3. åœ¨æ–‡ä»¶ä¸­æŸ¥æ‰¾å…·ä½“çš„APIç«¯ç‚¹\n\n")

        f.write("### å¿«é€Ÿå¯¼èˆª\n\n")
        f.write("```bash\n")
        f.write("# æŸ¥çœ‹æ‰€æœ‰æ–‡ä»¶\n")
        f.write(f"ls -lh {output_dir}/\n\n")
        f.write("# æŸ¥çœ‹ç‰¹å®šæ–‡ä»¶\n")
        f.write(f"cat {output_dir}/api_auth.json\n\n")
        f.write("# æœç´¢ç‰¹å®šAPI\n")
        f.write(f'grep -r "/login" {output_dir}/\n')
        f.write("```\n\n")

        f.write("### æ–‡ä»¶è¯´æ˜\n\n")
        f.write("- **`api_index.json`**: ç´¢å¼•æ–‡ä»¶ï¼ŒåŒ…å«æ‰€æœ‰æ‹†åˆ†æ–‡ä»¶çš„å…ƒæ•°æ®\n")
        f.write("- **`api_*.json`**: æŒ‰è·¯å¾„å‰ç¼€æ‹†åˆ†çš„APIæ•°æ®æ–‡ä»¶\n")
        f.write("- **`API_SPLIT_INDEX.md`**: æœ¬æ–‡æ¡£ï¼Œæä¾›å‹å¥½çš„ç´¢å¼•ç•Œé¢\n\n")

        f.write("### æ•°æ®æ ¼å¼\n\n")
        f.write("æ¯ä¸ªæ‹†åˆ†æ–‡ä»¶åŒ…å«ä»¥ä¸‹å­—æ®µï¼š\n\n")
        f.write("```json\n")
        f.write("{\n")
        f.write('  "generated_at": "2026-01-02T00:32:22.264253",\n')
        f.write('  "prefix": "/auth",\n')
        f.write('  "total_endpoints": 5,\n')
        f.write('  "endpoints": [\n')
        f.write("    {\n")
        f.write('      "path": "/login",\n')
        f.write('      "method": "POST",\n')
        f.write('      "file": "auth.py",\n')
        f.write('      "function": "login_for_access_token",\n')
        f.write('      "return_model": "dict",\n')
        f.write('      "data_fields": [],\n')
        f.write('      "db_dependencies": [],\n')
        f.write('      "source_type": "postgresql",\n')
        f.write('      "line_number": 172\n')
        f.write("    }\n")
        f.write("  ]\n")
        f.write("}\n")
        f.write("```\n\n")

        # å†™å…¥æŒ‰æ–‡ä»¶åˆ†ç»„çš„ä¿¡æ¯
        f.write("## æŒ‰APIæ–‡ä»¶åˆ†ç»„\n\n")
        endpoints_by_file = defaultdict(list)

        for file_info in index_data["split_files"]:
            # è¯»å–æ‹†åˆ†æ–‡ä»¶è·å–è¯¦ç»†ä¿¡æ¯
            split_file = output_dir / file_info["file"]
            try:
                with open(split_file, "r", encoding="utf-8") as sf:
                    split_data = json.load(sf)
                    for ep in split_data["endpoints"]:
                        endpoints_by_file[ep["file"]].append(ep)
            except:
                pass

        if endpoints_by_file:
            f.write("| åç«¯æ–‡ä»¶ | APIç«¯ç‚¹æ•° | è·¯å¾„ç¤ºä¾‹ |\n")
            f.write("|---------|-----------|----------|\n")

            for filename, endpoints in sorted(endpoints_by_file.items()):
                examples = ", ".join([ep["path"] for ep in endpoints[:3]])
                if len(endpoints) > 3:
                    examples += f" ... (+{len(endpoints) - 3})"
                f.write(f"| {filename} | {len(endpoints)} | {examples} |\n")

    print(f"  âœ… ç”ŸæˆMarkdownç´¢å¼•: API_SPLIT_INDEX.md")


if __name__ == "__main__":
    split_api_inventory()
