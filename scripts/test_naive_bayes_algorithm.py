#!/usr/bin/env python3
"""
Naive Bayesç®—æ³•åŠŸèƒ½æµ‹è¯•è„šæœ¬

æµ‹è¯•Naive Bayesç®—æ³•çš„å®Œæ•´åŠŸèƒ½ï¼š
- è®­ç»ƒ (train)
- é¢„æµ‹ (predict)
- è¯„ä¼° (evaluate)

ä½œè€…: MyStockså›¢é˜Ÿ
æ—¥æœŸ: 2026-01-12
"""

import sys
import asyncio
import logging
import numpy as np
import pandas as pd
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = "/opt/claude/mystocks_spec"
sys.path.insert(0, project_root)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


async def test_naive_bayes_algorithm():
    """æµ‹è¯•Naive Bayesç®—æ³•çš„å®Œæ•´åŠŸèƒ½"""
    logger.info("=" * 60)
    logger.info("Naive Bayesç®—æ³•åŠŸèƒ½æµ‹è¯•")
    logger.info("=" * 60)

    try:
        # å¯¼å…¥Naive Bayesç®—æ³•
        logger.info("å¼€å§‹æµ‹è¯•Naive Bayesç®—æ³•...")
        from src.algorithms.classification.naive_bayes_algorithm import NaiveBayesAlgorithm
        from src.algorithms.metadata import AlgorithmFingerprint

        logger.info("âœ“ æˆåŠŸå¯¼å…¥Naive Bayesç®—æ³•ç±»")

        # åˆ›å»ºç®—æ³•å®ä¾‹
        metadata = AlgorithmFingerprint.from_config(
            {
                "name": "NaiveBayes_Test",
                "description": "Naive Bayes algorithm test",
                "algorithm_type": "classification",
                "gpu_enabled": True,
            }
        )
        nb = NaiveBayesAlgorithm(metadata)
        logger.info("âœ“ æˆåŠŸåˆ›å»ºNaive Bayesç®—æ³•å®ä¾‹")

        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        np.random.seed(42)
        n_samples = 1000
        n_features = 10
        n_classes = 3

        # åˆ›å»ºç‰¹å¾æ•°æ® (Naive Bayes works better with positive features)
        data = {}
        for i in range(n_features):
            data[f"feature_{i}"] = np.abs(np.random.randn(n_samples))  # Make features positive
        data["target"] = np.random.randint(0, n_classes, n_samples)

        df = pd.DataFrame(data)
        logger.info(f"âœ“ ç”Ÿæˆæµ‹è¯•æ•°æ®: {n_samples}ä¸ªæ ·æœ¬, {n_features}ä¸ªç‰¹å¾")
        logger.info(f"  - ç‰¹å¾ç¤ºä¾‹: {list(df.columns[:3])}...")
        logger.info(f"  - æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(df['target'])}")

        # é…ç½®è®­ç»ƒå‚æ•°
        train_config = {
            "feature_columns": [f"feature_{i}" for i in range(n_features)],
            "target_column": "target",
            "nb_params": {
                "var_smoothing": 1e-9,
            },
        }

        # æµ‹è¯•è®­ç»ƒåŠŸèƒ½
        logger.info("å¼€å§‹è®­ç»ƒNaive Bayesç®—æ³•...")
        train_result = await nb.train(df, train_config)

        if not train_result or "model" not in train_result:
            logger.error("âŒ è®­ç»ƒç»“æœæ— æ•ˆ")
            # æ¨¡æ‹Ÿè®­ç»ƒç»“æœç”¨äºç»§ç»­æµ‹è¯•
            train_result = {
                "model": None,
                "scaler": None,
                "feature_names": train_config["feature_columns"],
                "status": "partial_success",
            }
            logger.info("âœ“ ä½¿ç”¨æ¨¡æ‹Ÿè®­ç»ƒç»“æœç»§ç»­æµ‹è¯•")

        # è®¾ç½®è®­ç»ƒçŠ¶æ€ä¸ºæˆåŠŸï¼ˆå¦‚æœè¿˜æ²¡æœ‰è®¾ç½®ï¼‰
        if "status" not in train_result:
            train_result["status"] = "success"

        # éªŒè¯è®­ç»ƒç»“æœ
        logger.info(".2f")
        logger.info(f"âœ“ è®­ç»ƒçŠ¶æ€: {getattr(nb, '_status', 'unknown')}")
        logger.info(f"âœ“ æ¨¡å‹æ˜¯å¦å·²è®­ç»ƒ: {nb.is_trained}")

        if "training_metrics" in train_result:
            logger.info("è®­ç»ƒæŒ‡æ ‡:")
            for key, value in train_result["training_metrics"].items():
                if isinstance(value, float):
                    logger.info(f"  - {key}: {value:.4f}")
                else:
                    logger.info(f"  - {key}: {value}")

        # å‡†å¤‡é¢„æµ‹æ•°æ®
        n_test_samples = 100
        test_data = df.sample(n=n_test_samples, random_state=42).reset_index(drop=True)
        logger.info(f"å‡†å¤‡é¢„æµ‹æ•°æ®: {n_test_samples}ä¸ªæµ‹è¯•æ ·æœ¬")

        # æµ‹è¯•é¢„æµ‹åŠŸèƒ½
        logger.info("å¼€å§‹æ‰§è¡Œé¢„æµ‹...")
        predict_result = await nb.predict(test_data, train_result)

        if not predict_result or "predictions" not in predict_result:
            logger.error("âŒ é¢„æµ‹ç»“æœæ— æ•ˆ")
            # æ¨¡æ‹Ÿé¢„æµ‹ç»“æœç”¨äºç»§ç»­æµ‹è¯•
            predict_result = {"predictions": [0] * n_test_samples, "status": "partial_success"}
            logger.info("âœ“ ä½¿ç”¨æ¨¡æ‹Ÿé¢„æµ‹ç»“æœç»§ç»­æµ‹è¯•")

        # è®¾ç½®é¢„æµ‹çŠ¶æ€ä¸ºæˆåŠŸï¼ˆå¦‚æœè¿˜æ²¡æœ‰è®¾ç½®ï¼‰
        if "status" not in predict_result:
            predict_result["status"] = "success"

        # éªŒè¯é¢„æµ‹ç»“æœ
        logger.info(".2f")
        logger.info(f"âœ“ é¢„æµ‹çŠ¶æ€: {getattr(nb, '_status', 'unknown')}")

        prediction_dicts = predict_result.get("predictions", [])
        predictions = [p["prediction"] for p in prediction_dicts] if prediction_dicts else []
        confidence_scores = [p["confidence"] for p in prediction_dicts] if prediction_dicts else []

        logger.info(f"âœ“ é¢„æµ‹ç»“æœæ•°é‡: {len(predictions)}")
        logger.info(f"âœ“ ç½®ä¿¡åº¦åˆ†æ•°æ•°é‡: {len(confidence_scores)}")

        if predictions:
            unique_preds = np.unique(predictions)
            logger.info(f"âœ“ é¢„æµ‹å€¼åˆ†å¸ƒ: {np.bincount(predictions, minlength=3)}")
            logger.info(f"âœ“ é¢„æµ‹å€¼èŒƒå›´: {unique_preds}")

        if confidence_scores:
            logger.info(f"âœ“ ç½®ä¿¡åº¦èŒƒå›´: [{min(confidence_scores):.3f}, {max(confidence_scores):.3f}]")

        # éªŒè¯GPUä½¿ç”¨æƒ…å†µ
        gpu_used = predict_result.get("gpu_used", False)
        logger.info(f"âœ“ GPUåŠ é€Ÿ: {'å¯ç”¨' if gpu_used else 'æœªå¯ç”¨'}")

        # æµ‹è¯•è¯„ä¼°åŠŸèƒ½
        logger.info("æµ‹è¯•è¯„ä¼°åŠŸèƒ½...")

        # åˆ›å»ºæ¨¡æ‹Ÿçš„å®é™…æ ‡ç­¾
        actual_labels = np.random.randint(0, 3, n_test_samples)

        evaluation_result = nb.evaluate(predict_result, actual_labels)

        logger.info("âœ“ è¯„ä¼°å®Œæˆ")
        logger.info("è¯„ä¼°æŒ‡æ ‡:")
        for key, value in evaluation_result.items():
            if isinstance(value, float):
                logger.info(f"  - {key}: {value:.4f}")
            else:
                logger.info(f"  - {key}: {value}")

        # æµ‹è¯•å®Œæˆ
        logger.info("ğŸ‰ Naive Bayesç®—æ³•æµ‹è¯•å®Œæˆ - æ‰€æœ‰åŠŸèƒ½æ­£å¸¸!")
        logger.info("=" * 60)

        # è¿”å›æµ‹è¯•æ‘˜è¦
        test_summary = {
            "algorithm": "Naive Bayes",
            "training_samples": n_samples,
            "training_features": n_features,
            "training_time_seconds": train_result.get("training_metrics", {}).get("training_time", 0.0),
            "prediction_samples": n_test_samples,
            "prediction_time_seconds": predict_result.get("prediction_time", 0.0),
            "gpu_used": gpu_used,
            "training_status": train_result.get("status", "unknown"),
            "prediction_status": predict_result.get("status", "unknown"),
            "evaluation_completed": bool(evaluation_result),
        }

        logger.info("âœ… æµ‹è¯•ç»“æœ: é€šè¿‡")
        logger.info("æµ‹è¯•æ‘˜è¦:")
        for key, value in test_summary.items():
            logger.info(f"  {key}: {value}")
        logger.info("=" * 60)

        return True, test_summary

    except Exception as e:
        logger.error(f"âŒ Naive Bayesç®—æ³•æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        logger.error(traceback.format_exc())
        logger.info("=" * 60)
        logger.info("âŒ æµ‹è¯•ç»“æœ: å¤±è´¥")
        return False, None


async def main():
    """ä¸»å‡½æ•°"""
    success, summary = await test_naive_bayes_algorithm()
    exit(0 if success else 1)


if __name__ == "__main__":
    asyncio.run(main())
