#!/usr/bin/env python3
"""
MyStocks APIå’ŒWebå‰ç«¯æ•°æ®ä½¿ç”¨åˆ†æå·¥å…·ï¼ˆå¢å¼ºç‰ˆï¼‰
æ”¯æŒå¢é‡åˆ†æã€æ›´å‡†ç¡®çš„APIè°ƒç”¨æå–å’Œå¯è§†åŒ–æŠ¥å‘Š
"""

import json
import os
import re
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Set, Tuple
from hashlib import md5


class APIAnalyzer:
    """åˆ†æåç«¯APIç«¯ç‚¹"""

    def __init__(self, api_dir: str):
        self.api_dir = Path(api_dir)
        self.api_endpoints: List[Dict] = []
        self.pydantic_models: Dict[str, List[str]] = {}
        self.file_hashes: Dict[str, str] = {}

    def analyze(self, incremental: bool = False) -> List[Dict]:
        """åˆ†ææ‰€æœ‰APIæ–‡ä»¶"""
        print("ğŸ” æ‰«æAPIç«¯ç‚¹...")

        if incremental:
            print("  æ¨¡å¼: å¢é‡åˆ†æ")
            self._load_cache()

        for py_file in self.api_dir.rglob("*.py"):
            if "test" in py_file.name or "__pycache__" in str(py_file):
                continue

            # å¢é‡åˆ†æï¼šè·³è¿‡æœªä¿®æ”¹çš„æ–‡ä»¶
            if incremental:
                current_hash = self._calculate_file_hash(py_file)
                rel_path = str(py_file.relative_to(self.api_dir))
                if rel_path in self.file_hashes and self.file_hashes[rel_path] == current_hash:
                    continue
                self.file_hashes[rel_path] = current_hash

            self._analyze_python_file_with_regex(py_file)

        print(f"âœ… æ‰¾åˆ° {len(self.api_endpoints)} ä¸ªAPIç«¯ç‚¹")
        print(f"âœ… æ‰¾åˆ° {len(self.pydantic_models)} ä¸ªæ•°æ®æ¨¡å‹")

        if incremental:
            self._save_cache()

        return self.api_endpoints

    def _calculate_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶hashç”¨äºå¢é‡åˆ†æ"""
        try:
            with open(file_path, "rb") as f:
                return md5(f.read()).hexdigest()
        except:
            return ""

    def _load_cache(self):
        """åŠ è½½å¢é‡åˆ†æç¼“å­˜"""
        cache_file = self.api_dir / ".analysis_cache.json"
        if cache_file.exists():
            try:
                with open(cache_file, "r") as f:
                    cache = json.load(f)
                    self.file_hashes = cache.get("file_hashes", {})
            except:
                pass

    def _save_cache(self):
        """ä¿å­˜å¢é‡åˆ†æç¼“å­˜"""
        cache_file = self.api_dir / ".analysis_cache.json"
        try:
            with open(cache_file, "w") as f:
                json.dump({"file_hashes": self.file_hashes}, f)
        except:
            pass

    def _analyze_python_file_with_regex(self, file_path: Path):
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†æPythonæ–‡ä»¶æå–APIä¿¡æ¯"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # æå–è·¯ç”±å®šä¹‰
            # åŒ¹é… @router.get("/path") æˆ– @app.get("/path") æ ¼å¼
            route_pattern = r'@(?:router|app)\.(get|post|put|delete|patch)\s*\(\s*["\']([^"\']+)["\']'
            matches = list(re.finditer(route_pattern, content))

            for match in matches:
                method = match.group(1).upper()
                path = match.group(2)

                # æŸ¥æ‰¾å¯¹åº”çš„å‡½æ•°å®šä¹‰
                func_match = re.search(r"async\s+def\s+(\w+)\s*\(", content[match.end() : match.end() + 500])
                if func_match:
                    func_name = func_match.group(1)

                    # æå–å‡½æ•°å†…å®¹
                    func_start = match.end() + func_match.end()
                    func_end = self._find_function_end(content, func_start)

                    # è·å–å‡½æ•°å†…å®¹
                    func_content = content[func_start:func_end]

                    # æå–è¿”å›æ¨¡å‹
                    return_model = self._extract_return_model_from_content(content, match.start())

                    # æå–æ•°æ®å­—æ®µ
                    data_fields = self._extract_data_fields_from_content(func_content, return_model)

                    # æ£€æŸ¥æ•°æ®åº“ä¾èµ–
                    db_dependencies = self._extract_db_dependencies(func_content)

                    # åˆ¤æ–­æ•°æ®æºç±»å‹
                    source_type = self._determine_source_type(func_content)

                    # è®¡ç®—æ–‡ä»¶ç›¸å¯¹è·¯å¾„
                    rel_path = str(file_path.relative_to(self.api_dir))
                    line_number = content[: match.start()].count("\n") + 1

                    endpoint_info = {
                        "path": path,
                        "method": method,
                        "file": rel_path,
                        "function": func_name,
                        "return_model": return_model,
                        "data_fields": data_fields,
                        "db_dependencies": db_dependencies,
                        "source_type": source_type,
                        "line_number": line_number,
                    }

                    self.api_endpoints.append(endpoint_info)

            # æå–Pydanticæ¨¡å‹
            self._extract_pydantic_models_with_regex(content, file_path)

        except Exception as e:
            print(f"âš ï¸  è§£ææ–‡ä»¶å¤±è´¥ {file_path}: {e}")

    def _find_function_end(self, content: str, start_pos: int) -> int:
        """æŸ¥æ‰¾å‡½æ•°ç»“æŸä½ç½®"""
        pos = start_pos

        # è·³è¿‡å†’å·å’Œç©ºç™½
        while pos < len(content) and content[pos] not in "\n:":
            pos += 1

        if pos >= len(content):
            return pos

        if content[pos] == ":":
            pos += 1

        # è·³è¿‡æ¢è¡Œç¬¦
        while pos < len(content) and content[pos] in "\n\t ":
            pos += 1

        # æŸ¥æ‰¾å‡½æ•°ä½“ç»“æŸ
        while pos < len(content):
            if content[pos] == "\n":
                # æ£€æŸ¥ä¸‹ä¸€è¡Œçš„ç¼©è¿›
                next_pos = pos + 1
                while next_pos < len(content) and content[next_pos] in "\t ":
                    next_pos += 1

                if next_pos < len(content) and content[next_pos] not in "\n\t ":
                    # ç®€å•çš„ç¼©è¿›æ£€æŸ¥
                    if next_pos - (pos + 1) <= 4:
                        break

            pos += 1

        return min(pos, len(content))

    def _extract_return_model_from_content(self, content: str, decorator_pos: int) -> str:
        """ä»å‡½æ•°å†…å®¹ä¸­æå–è¿”å›æ¨¡å‹"""
        # æŸ¥æ‰¾ -> ç±»å‹æ ‡æ³¨
        end_content = content[decorator_pos : decorator_pos + 1000]
        return_match = re.search(r"->\s*([\w\[\],\s]+)\s*:", end_content)
        if return_match:
            return return_match.group(1).strip()
        return "dict"

    def _extract_data_fields_from_content(self, content: str, return_model: str) -> List[str]:
        """ä»å‡½æ•°å†…å®¹ä¸­æå–æ•°æ®å­—æ®µ"""
        fields = []

        # ä»è¿”å›è¯­å¥ä¸­æå–å­—å…¸é”®
        return_pattern = r"return\s*\{([^}]+)\}"
        return_matches = re.findall(return_pattern, content, re.DOTALL)

        for match in return_matches:
            # æå–é”®å
            key_pattern = r'["\'](\w+)["\']'
            keys = re.findall(key_pattern, match)
            fields.extend(keys)

        return list(set(fields))

    def _extract_pydantic_models_with_regex(self, content: str, file_path: Path):
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–Pydanticæ¨¡å‹"""
        # åŒ¹é…ç±»å®šä¹‰
        class_pattern = r"class\s+(\w+)\s*\(([^)]+)\):"
        class_matches = re.finditer(class_pattern, content)

        for match in class_matches:
            class_name = match.group(1)
            base_classes = match.group(2)

            # æ£€æŸ¥æ˜¯å¦ç»§æ‰¿è‡ªBaseModel
            if "BaseModel" in base_classes:
                # æå–ç±»å†…å®¹
                class_start = match.end()
                class_end = self._find_class_end(content, class_start)
                class_content = content[class_start:class_end]

                # æå–å­—æ®µ
                field_pattern = r"(\w+)\s*:\s*(?:\w+|Optional\[\w+\]|List\[\w+\])\s*(?:=\s*.+)?"
                fields = re.findall(field_pattern, class_content)

                # è¿‡æ»¤æ‰å¸¸è§çš„éå­—æ®µ
                skip_fields = ["Config", "model_config", "__init__"]
                fields = [f for f in fields if f not in skip_fields]

                if fields:
                    self.pydantic_models[class_name] = fields

    def _find_class_end(self, content: str, start_pos: int) -> int:
        """æŸ¥æ‰¾ç±»ç»“æŸä½ç½®"""
        pos = start_pos

        # è·³è¿‡å†’å·å’Œç©ºç™½
        while pos < len(content) and content[pos] not in "\n:":
            pos += 1

        if pos >= len(content):
            return pos

        if content[pos] == ":":
            pos += 1

        # æŸ¥æ‰¾ç±»ç»“æŸ
        while pos < len(content):
            if content[pos] == "\n":
                # æ£€æŸ¥ä¸‹ä¸€è¡Œçš„ç¼©è¿›
                next_pos = pos + 1
                while next_pos < len(content) and content[next_pos] in "\t ":
                    next_pos += 1

                if next_pos < len(content) and content[next_pos] not in "\n\t ":
                    if next_pos - (pos + 1) <= 4:
                        break

            pos += 1

        return min(pos, len(content))

    def _extract_db_dependencies(self, content: str) -> List[str]:
        """æå–æ•°æ®åº“ä¾èµ–"""
        db_tables = []

        # æŸ¥æ‰¾å¸¸è§çš„æ•°æ®åº“æ“ä½œæ¨¡å¼
        patterns = [
            r'db\.query\(["\']([^"\']+)["\']\)',
            r'db\.select\(["\']([^"\']+)["\']\)',
            r'db\.table\(["\']([^"\']+)["\']\)',
            r"pd\.read_sql.*from\s+(\w+)",
            r"SELECT\s+.*FROM\s+(\w+)",
        ]

        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            db_tables.extend(matches)

        return list(set(db_tables))

    def _determine_source_type(self, content: str) -> str:
        """åˆ¤æ–­æ•°æ®æºç±»å‹"""
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨mock
        mock_keywords = ["mock", "Mock", "MOCK"]
        for keyword in mock_keywords:
            if keyword in content:
                return "mock"

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨factory
        if "factory" in content.lower():
            return "factory"

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨TDengine
        if "tdengine" in content.lower():
            return "tdengine"

        # é»˜è®¤ä¸ºPostgreSQL
        return "postgresql"


class FrontendAnalyzer:
    """åˆ†æå‰ç«¯é¡µé¢å’Œç»„ä»¶"""

    def __init__(self, frontend_dir: str):
        self.frontend_dir = Path(frontend_dir)
        self.pages: List[Dict] = []
        self.api_calls: List[Dict] = []
        self.file_hashes: Dict[str, str] = {}

    def analyze(self, incremental: bool = False) -> Tuple[List[Dict], List[Dict]]:
        """åˆ†ææ‰€æœ‰å‰ç«¯æ–‡ä»¶"""
        print("ğŸ” æ‰«æå‰ç«¯é¡µé¢...")

        if incremental:
            print("  æ¨¡å¼: å¢é‡åˆ†æ")
            self._load_cache()

        vue_files = list(self.frontend_dir.rglob("*.vue"))
        ts_js_files = list(self.frontend_dir.rglob("*.ts")) + list(self.frontend_dir.rglob("*.js"))

        print(f"  æ‰¾åˆ° {len(vue_files)} ä¸ªVueæ–‡ä»¶")
        print(f"  æ‰¾åˆ° {len(ts_js_files)} ä¸ªTS/JSæ–‡ä»¶")

        # åˆ†æVueé¡µé¢
        for vue_file in vue_files:
            if incremental:
                current_hash = self._calculate_file_hash(vue_file)
                rel_path = str(vue_file.relative_to(self.frontend_dir))
                if rel_path in self.file_hashes and self.file_hashes[rel_path] == current_hash:
                    continue
                self.file_hashes[rel_path] = current_hash

            self._analyze_vue_file(vue_file)

        # åˆ†æTS/JSæ–‡ä»¶ï¼ˆä¸»è¦æ˜¯APIè°ƒç”¨ï¼‰
        for ts_file in ts_js_files:
            if "test" in ts_file.name or "spec" in ts_file.name:
                continue

            if incremental:
                current_hash = self._calculate_file_hash(ts_file)
                rel_path = str(ts_file.relative_to(self.frontend_dir))
                if rel_path in self.file_hashes and self.file_hashes[rel_path] == current_hash:
                    continue
                self.file_hashes[rel_path] = current_hash

            self._analyze_api_file(ts_file)

        if incremental:
            self._save_cache()

        print(f"âœ… åˆ†æäº† {len(self.pages)} ä¸ªé¡µé¢")
        print(f"âœ… æ‰¾åˆ° {len(self.api_calls)} ä¸ªAPIè°ƒç”¨")
        return self.pages, self.api_calls

    def _calculate_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶hashç”¨äºå¢é‡åˆ†æ"""
        try:
            with open(file_path, "rb") as f:
                return md5(f.read()).hexdigest()
        except:
            return ""

    def _load_cache(self):
        """åŠ è½½å¢é‡åˆ†æç¼“å­˜"""
        cache_file = self.frontend_dir / ".analysis_cache.json"
        if cache_file.exists():
            try:
                with open(cache_file, "r") as f:
                    cache = json.load(f)
                    self.file_hashes = cache.get("file_hashes", {})
            except:
                pass

    def _save_cache(self):
        """ä¿å­˜å¢é‡åˆ†æç¼“å­˜"""
        cache_file = self.frontend_dir / ".analysis_cache.json"
        try:
            with open(cache_file, "w") as f:
                json.dump({"file_hashes": self.file_hashes}, f)
        except:
            pass

    def _analyze_vue_file(self, file_path: Path):
        """åˆ†æVueæ–‡ä»¶"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            rel_path = str(file_path.relative_to(self.frontend_dir))

            # æå–APIè°ƒç”¨
            api_calls = self._extract_vue_api_calls(content, rel_path)

            if api_calls:
                self.pages.append(
                    {
                        "path": rel_path,
                        "type": "view" if "/views/" in rel_path else "component",
                        "api_calls": api_calls,
                        "api_count": len(api_calls),
                    }
                )

                # æ·»åŠ åˆ°æ€»APIè°ƒç”¨åˆ—è¡¨
                self.api_calls.extend(api_calls)

        except Exception as e:
            print(f"âš ï¸  è§£æVueæ–‡ä»¶å¤±è´¥ {file_path}: {e}")

    def _extract_vue_api_calls(self, content: str, file_path: str) -> List[Dict]:
        """ä»Vueæ–‡ä»¶ä¸­æå–APIè°ƒç”¨"""
        api_calls = []

        # æ¨¡å¼1: HTTPè°ƒç”¨ï¼ˆaxios/requestï¼‰
        http_patterns = [
            r'axios\.(get|post|put|delete|patch)\s*\(\s*["\']([^"\']+)["\']',
            r'request\.(get|post|put|delete|patch)\s*\(\s*["\']([^"\']+)["\']',
        ]

        for pattern in http_patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                api_calls.append(
                    {
                        "source_file": file_path,
                        "type": "http",
                        "method": match.group(1).upper(),
                        "endpoint": match.group(2),
                        "line": content[: match.start()].count("\n") + 1,
                    }
                )

        # æ¨¡å¼2: APIå¯¹è±¡è°ƒç”¨ï¼ˆdataApi.xxx, authApi.xxxç­‰ï¼‰
        api_object_pattern = r"(\w+Api)\.(\w+)\s*\("
        api_object_matches = re.finditer(api_object_pattern, content)

        for match in api_object_matches:
            api_calls.append(
                {
                    "source_file": file_path,
                    "type": "api_object",
                    "api_name": match.group(1),
                    "method": match.group(2),
                    "line": content[: match.start()].count("\n") + 1,
                }
            )

        return api_calls

    def _analyze_api_file(self, file_path: Path):
        """åˆ†æAPIè°ƒç”¨æ–‡ä»¶"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            rel_path = str(file_path.relative_to(self.frontend_dir))

            # æå–APIå‡½æ•°å®šä¹‰
            api_functions = self._extract_api_functions(content, rel_path)

            # å¦‚æœåœ¨apiç›®å½•ä¸‹ï¼Œæ·»åŠ åˆ°APIè°ƒç”¨åˆ—è¡¨
            if "/api/" in rel_path:
                for func in api_functions:
                    self.api_calls.append(
                        {
                            "source_file": rel_path,
                            "type": "api_function",
                            "function_name": func["name"],
                            "endpoint": func.get("endpoint", ""),
                            "line": func["line"],
                        }
                    )

        except Exception as e:
            print(f"âš ï¸  è§£æAPIæ–‡ä»¶å¤±è´¥ {file_path}: {e}")

    def _extract_api_functions(self, content: str, file_path: str) -> List[Dict]:
        """æå–APIå‡½æ•°å®šä¹‰"""
        functions = []

        # åŒ¹é…å‡½æ•°å®šä¹‰
        pattern = r"(?:export\s+(?:async\s+)?(?:const|function)\s+(\w+)|const\s+(\w+)\s*=.*?(?:export\s+))\s*=\s*(?:async\s+)?\([^)]*\)\s*=>\s*\{"
        matches = re.finditer(pattern, content)

        for match in matches:
            func_name = match.group(1) or match.group(2)
            if func_name:
                # æå–å‡½æ•°ä½“ä¸­çš„APIè°ƒç”¨
                func_start = match.end()
                func_content = self._extract_function_body(content, func_start)

                # æå–endpoint
                endpoint_match = re.search(r'["\']([^"\']+)["\']', func_content[:200])
                endpoint = endpoint_match.group(1) if endpoint_match else ""

                functions.append(
                    {
                        "name": func_name,
                        "endpoint": endpoint,
                        "line": content[: match.start()].count("\n") + 1,
                    }
                )

        return functions

    def _extract_function_body(self, content: str, start_pos: int) -> str:
        """æå–å‡½æ•°ä½“"""
        depth = 1
        pos = start_pos

        while pos < len(content) and depth > 0:
            if content[pos] == "{":
                depth += 1
            elif content[pos] == "}":
                depth -= 1
            pos += 1

        return content[start_pos:pos]


class ReportGenerator:
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""

    def __init__(self, api_endpoints: List[Dict], frontend_pages: List[Dict], frontend_api_calls: List[Dict]):
        self.api_endpoints = api_endpoints
        self.frontend_pages = frontend_pages
        self.frontend_api_calls = frontend_api_calls

    def generate_json_reports(self, output_dir: Path):
        """ç”ŸæˆJSONæ ¼å¼çš„æ¸…å•"""
        output_dir.mkdir(parents=True, exist_ok=True)

        # APIæ•°æ®æ¸…å•
        api_inventory = {
            "generated_at": datetime.now().isoformat(),
            "total_endpoints": len(self.api_endpoints),
            "endpoints": self.api_endpoints,
        }

        with open(output_dir / "api_data_inventory.json", "w", encoding="utf-8") as f:
            json.dump(api_inventory, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“„ ç”Ÿæˆ APIæ•°æ®æ¸…å•: {output_dir / 'api_data_inventory.json'}")

        # Web APIè°ƒç”¨æ¸…å•
        web_api_calls = {
            "generated_at": datetime.now().isoformat(),
            "total_pages": len(self.frontend_pages),
            "total_api_calls": len(self.frontend_api_calls),
            "pages": self.frontend_pages,
            "api_calls": self.frontend_api_calls,
        }

        with open(output_dir / "web_api_calls.json", "w", encoding="utf-8") as f:
            json.dump(web_api_calls, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“„ ç”Ÿæˆ Web APIè°ƒç”¨æ¸…å•: {output_dir / 'web_api_calls.json'}")

    def generate_markdown_report(self, output_file: Path):
        """ç”ŸæˆMarkdownæ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š"""
        print("ğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")

        # æ„å»ºæ˜ å°„å…³ç³»
        api_by_path = {ep["path"]: ep for ep in self.api_endpoints}
        api_usage_count = defaultdict(int)
        api_unused = set(api_by_path.keys())

        # ç»Ÿè®¡APIä½¿ç”¨æƒ…å†µï¼ˆä»HTTPè°ƒç”¨å’Œendpointå­—æ®µæå–ï¼‰
        for call in self.frontend_api_calls:
            endpoint = None
            if call["type"] == "http" and "endpoint" in call:
                endpoint = call["endpoint"]
            elif call["type"] == "api_function" and "endpoint" in call and call["endpoint"]:
                endpoint = call["endpoint"]

            if endpoint:
                # å°è¯•åŒ¹é…APIè·¯å¾„
                matched_path = self._match_api_path(endpoint, api_by_path.keys())
                if matched_path:
                    api_usage_count[matched_path] += 1
                    if matched_path in api_unused:
                        api_unused.remove(matched_path)

        # æŸ¥æ‰¾å‰ç«¯è¯·æ±‚ä½†æœªå®ç°çš„API
        frontend_requests = defaultdict(set)
        for call in self.frontend_api_calls:
            if call["type"] == "http" and "endpoint" in call:
                matched_path = self._match_api_path(call["endpoint"], api_by_path.keys())
                if matched_path and matched_path not in api_by_path:
                    frontend_requests[matched_path].add(call["source_file"])

        unimplemented = list(frontend_requests.keys())

        with open(output_file, "w", encoding="utf-8") as f:
            # å†™å…¥æŠ¥å‘Šå¤´éƒ¨
            f.write("# APIä¸Webå‰ç«¯æ•°æ®ä½¿ç”¨åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            # å†™å…¥æ¦‚è§ˆ
            self._write_overview(f, api_usage_count, unimplemented)

            # å†™å…¥APIç«¯ç‚¹ç»Ÿè®¡
            self._write_api_endpoints(f)

            # å†™å…¥é¡µé¢APIè°ƒç”¨æ¸…å•
            self._write_page_api_calls(f)

            # å†™å…¥æ•°æ®ä½¿ç”¨åˆ†æ
            self._write_data_usage_analysis(f, api_unused, unimplemented)

            # å†™å…¥æ•°æ®åº“ä¾èµ–åˆ†æ
            self._write_database_analysis(f)

            # å†™å…¥æ•°æ®æºç±»å‹ç»Ÿè®¡
            self._write_source_type_analysis(f)

            # å†™å…¥æ¨èæ”¹è¿›
            self._write_recommendations(f)

        print(f"ğŸ“„ ç”Ÿæˆåˆ†ææŠ¥å‘Š: {output_file}")

    def _match_api_path(self, frontend_path: str, backend_paths: Set[str]) -> str:
        """å°è¯•åŒ¹é…å‰ç«¯è·¯å¾„åˆ°åç«¯API"""
        # ç›´æ¥åŒ¹é…
        if frontend_path in backend_paths:
            return frontend_path

        # å»é™¤å‰å¯¼/åçš„åŒ¹é…
        normalized = frontend_path.lstrip("/")
        if normalized in backend_paths:
            return normalized

        # æ¨¡ç³ŠåŒ¹é…ï¼ˆè·¯å¾„å‚æ•°æ›¿æ¢ï¼‰
        frontend_parts = frontend_path.split("/")
        for backend_path in backend_paths:
            backend_parts = backend_path.split("/")
            if len(frontend_parts) == len(backend_parts):
                match = True
                for fp, bp in zip(frontend_parts, backend_parts):
                    if fp != bp and not (bp.startswith("{") and bp.endswith("}")):
                        match = False
                        break
                if match:
                    return backend_path

        return None

    def _write_overview(self, f, api_usage_count: Dict[str, int], unimplemented: List[str]):
        """å†™å…¥æ¦‚è§ˆ"""
        f.write("## æ¦‚è§ˆ\n\n")
        f.write(f"- **APIç«¯ç‚¹æ€»æ•°**: {len(self.api_endpoints)}\n")
        f.write(f"- **å‰ç«¯é¡µé¢æ€»æ•°**: {len(self.frontend_pages)}\n")
        f.write(f"- **APIè°ƒç”¨æ€»æ•°**: {len(self.frontend_api_calls)}\n")
        f.write(f"- **å·²ä½¿ç”¨çš„API**: {len(api_usage_count)}\n")
        f.write(f"- **æœªä½¿ç”¨çš„API**: {len(self.api_endpoints) - len(api_usage_count)}\n")
        f.write(f"- **å‰ç«¯è¯·æ±‚ä½†æœªå®ç°çš„API**: {len(unimplemented)}\n\n")

        # æ·»åŠ å¯è§†åŒ–æ¡å½¢å›¾
        f.write("### APIä½¿ç”¨æƒ…å†µå¯è§†åŒ–\n\n")
        total = len(self.api_endpoints)

        if total > 0:
            used = len(api_usage_count)
            unused = total - used

            f.write(f"```\n")
            f.write(f"å·²ä½¿ç”¨: {'â–ˆ' * int(used / total * 50)} {used} ({used / total * 100:.1f}%)\n")
            f.write(f"æœªä½¿ç”¨: {'â–‘' * int(unused / total * 50)} {unused} ({unused / total * 100:.1f}%)\n")
            f.write(f"```\n\n")
        else:
            f.write("```\n")
            f.write("æ— APIç«¯ç‚¹æ•°æ®\n")
            f.write("```\n\n")

    def _write_api_endpoints(self, f):
        """å†™å…¥APIç«¯ç‚¹ç»Ÿè®¡"""
        f.write("## APIç«¯ç‚¹ç»Ÿè®¡\n\n")
        f.write("### æŒ‰HTTPæ–¹æ³•åˆ†ç±»\n\n")
        f.write("| æ–¹æ³• | æ•°é‡ | å æ¯” |\n")
        f.write("|------|------|------|\n")

        method_count = defaultdict(int)
        for ep in self.api_endpoints:
            method_count[ep["method"]] += 1

        total = len(self.api_endpoints)
        for method, count in sorted(method_count.items()):
            percentage = (count / total * 100) if total > 0 else 0
            f.write(f"| {method} | {count} | {percentage:.1f}% |\n")

        f.write("\n### APIç«¯ç‚¹è¯¦æƒ…ï¼ˆæŒ‰è·¯å¾„åˆ†ç»„ï¼‰\n\n")

        # æŒ‰è·¯å¾„åˆ†ç»„
        api_by_prefix = defaultdict(list)
        for ep in self.api_endpoints:
            parts = ep["path"].split("/")
            if len(parts) > 1:
                prefix = f"/{parts[1]}"
            else:
                prefix = "å…¶ä»–"
            api_by_prefix[prefix].append(ep)

        for prefix, endpoints in sorted(api_by_prefix.items()):
            f.write(f"#### {prefix} ({len(endpoints)}ä¸ªç«¯ç‚¹)\n\n")
            f.write("| è·¯å¾„ | æ–¹æ³• | è¿”å›æ¨¡å‹ | æ•°æ®æº | æ–‡ä»¶:è¡Œå· |\n")
            f.write("|------|------|----------|--------|-----------|\n")

            for ep in endpoints:
                f.write(
                    f"| {ep['path']} | {ep['method']} | {ep['return_model']} | {ep['source_type']} | {ep['file']}:{ep['line_number']} |\n"
                )

            f.write("\n")

    def _write_page_api_calls(self, f):
        """å†™å…¥é¡µé¢APIè°ƒç”¨æ¸…å•"""
        f.write("## å‰ç«¯é¡µé¢APIè°ƒç”¨æ¸…å•\n\n")

        # åªæ˜¾ç¤ºæœ‰APIè°ƒç”¨çš„é¡µé¢
        pages_with_calls = [p for p in self.frontend_pages if p["api_calls"]]

        # æŒ‰APIè°ƒç”¨æ•°é‡æ’åº
        pages_with_calls.sort(key=lambda x: x["api_count"], reverse=True)

        f.write(f"### Top 10 APIè°ƒç”¨æœ€å¤šçš„é¡µé¢\n\n")
        f.write("| é¡µé¢ | ç±»å‹ | APIè°ƒç”¨æ•° |\n")
        f.write("|------|------|-----------|\n")
        for page in pages_with_calls[:10]:
            f.write(f"| {page['path']} | {page['type']} | {page['api_count']} |\n")

        f.write("\n### è¯¦ç»†APIè°ƒç”¨æ¸…å•\n\n")

        for page in pages_with_calls:
            f.write(f"#### {page['path']}\n\n")
            f.write(f"**ç±»å‹**: {page['type']}  \n")
            f.write(f"**APIè°ƒç”¨æ•°**: {page['api_count']}  \n\n")

            # æŒ‰ç±»å‹åˆ†ç»„æ˜¾ç¤º
            http_calls = [c for c in page["api_calls"] if c["type"] == "http"]
            api_object_calls = [c for c in page["api_calls"] if c["type"] == "api_object"]
            other_calls = [c for c in page["api_calls"] if c["type"] not in ["http", "api_object"]]

            if http_calls:
                f.write("##### HTTPè°ƒç”¨\n\n")
                f.write("| æ–¹æ³• | ç«¯ç‚¹ | è¡Œå· |\n")
                f.write("|------|------|------|\n")
                for call in http_calls[:10]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                    f.write(f"| {call['method']} | {call['endpoint']} | {call['line']} |\n")
                if len(http_calls) > 10:
                    f.write(f"| ... | è¿˜æœ‰ {len(http_calls) - 10} ä¸ª | ... |\n")
                f.write("\n")

            if api_object_calls:
                f.write("##### APIå¯¹è±¡è°ƒç”¨\n\n")
                f.write("| APIå¯¹è±¡ | æ–¹æ³• | è¡Œå· |\n")
                f.write("|---------|------|------|\n")
                for call in api_object_calls[:10]:
                    f.write(f"| {call['api_name']} | {call['method']} | {call['line']} |\n")
                if len(api_object_calls) > 10:
                    f.write(f"| ... | è¿˜æœ‰ {len(api_object_calls) - 10} ä¸ª | ... |\n")
                f.write("\n")

            if other_calls:
                f.write("##### å…¶ä»–è°ƒç”¨\n\n")
                f.write(f"å…± {len(other_calls)} ä¸ªå…¶ä»–è°ƒç”¨ï¼ˆç±»å‹: {[c['type'] for c in other_calls[:5]]}...ï¼‰\n\n")

    def _write_data_usage_analysis(self, f, api_unused: Set[str], unimplemented: List[str]):
        """å†™å…¥æ•°æ®ä½¿ç”¨åˆ†æ"""
        f.write("## æ•°æ®ä½¿ç”¨åˆ†æ\n\n")

        # æœªä½¿ç”¨çš„API
        f.write("### APIè¿”å›ä½†å‰ç«¯æœªä½¿ç”¨\n\n")
        if api_unused:
            f.write(f"å…± {len(api_unused)} ä¸ªAPIç«¯ç‚¹æœªè¢«å‰ç«¯ä½¿ç”¨ï¼š\n\n")
            f.write("| è·¯å¾„ | æ–¹æ³• | è¿”å›æ¨¡å‹ | æ–‡ä»¶ |\n")
            f.write("|------|------|----------|------|\n")

            for path in sorted(api_unused)[:50]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                ep = next((e for e in self.api_endpoints if e["path"] == path), None)
                if ep:
                    f.write(f"| {ep['path']} | {ep['method']} | {ep['return_model']} | {ep['file']} |\n")

            if len(api_unused) > 50:
                f.write(f"| ... | ... | ... | ... (è¿˜æœ‰ {len(api_unused) - 50} ä¸ª) |\n")
        else:
            f.write("âœ… æ‰€æœ‰APIç«¯ç‚¹éƒ½å·²è¢«å‰ç«¯ä½¿ç”¨\n\n")

        f.write("\n### å‰ç«¯è¯·æ±‚ä½†APIæœªå®ç°\n\n")
        if unimplemented:
            f.write(f"å…± {len(unimplemented)} ä¸ªç«¯ç‚¹å‰ç«¯è¯·æ±‚ä½†åç«¯æœªå®ç°ï¼š\n\n")
            f.write("| ç«¯ç‚¹ | è¯·æ±‚é¡µé¢æ•° |\n")
            f.write("|------|-----------|\n")

            for endpoint in sorted(unimplemented)[:20]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                pages = len(frontend_requests.get(endpoint, set()))
                f.write(f"| {endpoint} | {pages} |\n")

            if len(unimplemented) > 20:
                f.write(f"| ... | ... (è¿˜æœ‰ {len(unimplemented) - 20} ä¸ª) |\n")
        else:
            f.write("âœ… æ‰€æœ‰å‰ç«¯è¯·æ±‚çš„APIéƒ½å·²å®ç°\n\n")

    def _write_database_analysis(self, f):
        """å†™å…¥æ•°æ®åº“ä¾èµ–åˆ†æ"""
        f.write("## æ•°æ®åº“ä¾èµ–åˆ†æ\n\n")

        # ç»Ÿè®¡æ•°æ®åº“è¡¨ä½¿ç”¨
        db_tables = defaultdict(list)
        for ep in self.api_endpoints:
            for table in ep["db_dependencies"]:
                db_tables[table].append(ep["path"])

        if db_tables:
            f.write("### APIä½¿ç”¨çš„æ•°æ®åº“è¡¨\n\n")
            f.write("| è¡¨å | è¢«APIç«¯ç‚¹ä½¿ç”¨æ¬¡æ•° | ç«¯ç‚¹ç¤ºä¾‹ |\n")
            f.write("|------|------------------|----------|\n")

            for table, endpoints in sorted(db_tables.items(), key=lambda x: len(x[1]), reverse=True):
                f.write(f"| {table} | {len(endpoints)} | {', '.join(endpoints[:3])} |\n")
        else:
            f.write("â„¹ï¸  æœªæ£€æµ‹åˆ°æ˜ç¡®çš„æ•°æ®åº“è¡¨ä¾èµ–\n\n")

    def _write_source_type_analysis(self, f):
        """å†™å…¥æ•°æ®æºç±»å‹ç»Ÿè®¡"""
        f.write("## æ•°æ®æºç±»å‹ç»Ÿè®¡\n\n")
        f.write("| æ•°æ®æºç±»å‹ | APIæ•°é‡ | å æ¯” |\n")
        f.write("|-----------|---------|------|\n")

        source_count = defaultdict(int)
        for ep in self.api_endpoints:
            source_count[ep["source_type"]] += 1

        total = len(self.api_endpoints)
        for source_type, count in sorted(source_count.items()):
            percentage = (count / total * 100) if total > 0 else 0
            f.write(f"| {source_type} | {count} | {percentage:.1f}% |\n")

        f.write("\n### Mockæ•°æ®APIæ¸…å•\n\n")
        mock_apis = [ep for ep in self.api_endpoints if ep["source_type"] == "mock"]
        if mock_apis:
            f.write("| è·¯å¾„ | æ–¹æ³• | æ–‡ä»¶ |\n")
            f.write("|------|------|------|\n")
            for ep in mock_apis:
                f.write(f"| {ep['path']} | {ep['method']} | {ep['file']} |\n")
        else:
            f.write("âœ… æ²¡æœ‰ä½¿ç”¨Mockæ•°æ®çš„API\n\n")

    def _write_recommendations(self, f):
        """å†™å…¥æ¨èæ”¹è¿›"""
        f.write("## æ¨èæ”¹è¿›\n\n")

        # ç»Ÿè®¡æ•°æ®
        api_by_path = {ep["path"]: ep for ep in self.api_endpoints}
        api_usage_count = defaultdict(int)
        for call in self.frontend_api_calls:
            if call["type"] == "http" and "endpoint" in call:
                matched = self._match_api_path(call["endpoint"], api_by_path.keys())
                if matched:
                    api_usage_count[matched] += 1

        api_unused = set(api_by_path.keys()) - set(api_usage_count.keys())

        recommendations = []

        # 1. æ¸…ç†æœªä½¿ç”¨çš„API
        if len(api_unused) > 10:
            recommendations.append(
                {
                    "priority": "é«˜",
                    "category": "ä»£ç æ¸…ç†",
                    "description": f"æœ‰ {len(api_unused)} ä¸ªAPIç«¯ç‚¹æœªè¢«å‰ç«¯ä½¿ç”¨ï¼Œå»ºè®®è¯„ä¼°æ˜¯å¦éœ€è¦åˆ é™¤æˆ–æ ‡è®°ä¸ºdeprecated",
                }
            )

        # 2. Mockæ•°æ®æ›¿æ¢
        mock_count = sum(1 for ep in self.api_endpoints if ep["source_type"] == "mock")
        if mock_count > 0:
            recommendations.append(
                {
                    "priority": "ä¸­",
                    "category": "æ•°æ®æº",
                    "description": f"æœ‰ {mock_count} ä¸ªAPIä»åœ¨ä½¿ç”¨Mockæ•°æ®ï¼Œå»ºè®®æ›¿æ¢ä¸ºçœŸå®æ•°æ®æº",
                }
            )

        # 3. APIè°ƒç”¨ä¼˜åŒ–
        if len(self.frontend_api_calls) > 2000:
            recommendations.append(
                {
                    "priority": "ä½",
                    "category": "æ€§èƒ½ä¼˜åŒ–",
                    "description": f"å‰ç«¯å…± {len(self.frontend_api_calls)} ä¸ªAPIè°ƒç”¨ï¼Œå»ºè®®åˆ†ææ˜¯å¦æœ‰é‡å¤æˆ–å†—ä½™è°ƒç”¨",
                }
            )

        if recommendations:
            f.write("| ä¼˜å…ˆçº§ | ç±»åˆ« | å»ºè®® |\n")
            f.write("|--------|------|------|\n")
            for rec in recommendations:
                f.write(f"| {rec['priority']} | {rec['category']} | {rec['description']} |\n")
        else:
            f.write("âœ… æœªå‘ç°æ˜æ˜¾æ”¹è¿›ç‚¹\n\n")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="MyStocks APIä¸Webå‰ç«¯æ•°æ®ä½¿ç”¨åˆ†æå·¥å…·")
    parser.add_argument("--incremental", "-i", action="store_true", help="å¢é‡åˆ†ææ¨¡å¼ï¼Œåªåˆ†æä¿®æ”¹çš„æ–‡ä»¶")
    args = parser.parse_args()

    print("=" * 60)
    print("MyStocks APIä¸Webå‰ç«¯æ•°æ®ä½¿ç”¨åˆ†æå·¥å…·")
    if args.incremental:
        print("æ¨¡å¼: å¢é‡åˆ†æ")
    print("=" * 60)

    # è·¯å¾„é…ç½®
    backend_dir = Path("web/backend/app/api")
    frontend_dir = Path("web/frontend/src")
    output_dir = Path("docs/reports")
    report_file = output_dir / "API_WEB_DATA_USAGE_REPORT.md"

    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not backend_dir.exists():
        print(f"âŒ åç«¯APIç›®å½•ä¸å­˜åœ¨: {backend_dir}")
        return

    if not frontend_dir.exists():
        print(f"âŒ å‰ç«¯ç›®å½•ä¸å­˜åœ¨: {frontend_dir}")
        return

    # åˆ†æAPI
    api_analyzer = APIAnalyzer(str(backend_dir))
    api_endpoints = api_analyzer.analyze(incremental=args.incremental)

    # åˆ†æå‰ç«¯
    frontend_analyzer = FrontendAnalyzer(str(frontend_dir))
    frontend_pages, frontend_api_calls = frontend_analyzer.analyze(incremental=args.incremental)

    # ç”ŸæˆæŠ¥å‘Š
    report_generator = ReportGenerator(api_endpoints, frontend_pages, frontend_api_calls)
    report_generator.generate_json_reports(output_dir)
    report_generator.generate_markdown_report(report_file)

    print("\n" + "=" * 60)
    print("âœ… åˆ†æå®Œæˆï¼")
    print(f"   - APIç«¯ç‚¹: {len(api_endpoints)}")
    print(f"   - å‰ç«¯é¡µé¢: {len(frontend_pages)}")
    print(f"   - APIè°ƒç”¨: {len(frontend_api_calls)}")
    print(f"\n   æŠ¥å‘Šä½ç½®: {report_file}")
    print(f"   JSONæ¸…å•: {output_dir / 'api_data_inventory.json'}")
    print(f"   JSONæ¸…å•: {output_dir / 'web_api_calls.json'}")
    if args.incremental:
        print("\nğŸ’¡ æç¤º: ä½¿ç”¨ --incremental å‚æ•°å¯ä»¥åŠ é€Ÿåç»­åˆ†æ")
    print("=" * 60)


if __name__ == "__main__":
    main()
