#!/bin/bash

# MyStocks CI/CD æœˆåº¦ä¼˜åŒ–å®¡æŸ¥è„šæœ¬
# åŸºäºç›‘æ§æ•°æ®ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Šå’Œå»ºè®®

set -e

# é…ç½®
MONTH=$(date +%Y-%m)
REPORT_DIR="reports/cicd-optimization/${MONTH}"
GRAFANA_URL="${GRAFANA_URL:-http://localhost:3000}"
PROMETHEUS_URL="${PROMETHEUS_URL:-http://localhost:9090}"

# åˆ›å»ºæŠ¥å‘Šç›®å½•
mkdir -p "$REPORT_DIR"

log_info() {
    echo "[INFO] $(date '+%Y-%m-%d %H:%M:%S') - $1"
}

log_warning() {
    echo "[WARNING] $(date '+%Y-%m-%d %H:%M:%S') - $1"
}

log_error() {
    echo "[ERROR] $(date '+%Y-%m-%d %H:%M:%S') - $1"
}

# è·å–GitHub Actionsç»Ÿè®¡æ•°æ®
collect_github_actions_stats() {
    log_info "æ”¶é›†GitHub Actionsç»Ÿè®¡æ•°æ®..."

    # è·å–æœ€è¿‘30å¤©çš„workflowè¿è¡Œæ•°æ®
    if command -v gh &> /dev/null; then
        # å·¥ä½œæµæˆåŠŸç‡
        gh run list --limit 100 --json conclusion,createdAt > "$REPORT_DIR/gh_runs.json"

        # å·¥ä½œæµè¿è¡Œæ—¶é—´
        gh run list --limit 50 --json name,runNumber,duration > "$REPORT_DIR/gh_durations.json"
    else
        log_warning "GitHub CLIæœªå®‰è£…ï¼Œè·³è¿‡GitHub Actionsæ•°æ®æ”¶é›†"
        echo '{"workflows": []}' > "$REPORT_DIR/gh_runs.json"
        echo '{"durations": []}' > "$REPORT_DIR/gh_durations.json"
    fi
}

# è·å–Prometheusç›‘æ§æ•°æ®
collect_prometheus_metrics() {
    log_info "æ”¶é›†Prometheusç›‘æ§æ•°æ®..."

    python3 -c "
import requests
import json
import sys
from datetime import datetime, timedelta

PROMETHEUS_URL = '${PROMETHEUS_URL}'

def query_metric(query, days=30):
    '''æŸ¥è¯¢æŒ‡æ ‡æ•°æ®'''
    end_time = datetime.now()
    start_time = end_time - timedelta(days=days)

    try:
        response = requests.get(f'{PROMETHEUS_URL}/api/v1/query_range', params={
            'query': query,
            'start': start_time.timestamp(),
            'end': end_time.timestamp(),
            'step': '3600'  # 1å°æ—¶
        }, timeout=30)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f'æŸ¥è¯¢å¤±è´¥ {query}: {e}', file=sys.stderr)
        return {'data': {'result': []}}

# æ”¶é›†å…³é”®æŒ‡æ ‡
metrics = {}

# CI/CDç›¸å…³æŒ‡æ ‡
try:
    # å‡è®¾æœ‰è‡ªå®šä¹‰æŒ‡æ ‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™è·³è¿‡
    metrics['build_duration'] = query_metric('cicd_build_duration_seconds', 30)
    metrics['test_duration'] = query_metric('cicd_test_duration_seconds', 30)
    metrics['deployment_success'] = query_metric('cicd_deployment_success_total', 30)
except:
    pass

# ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
metrics['api_response_time'] = query_metric('histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))', 30)
metrics['error_rate'] = query_metric('rate(http_requests_total{status_code=~\"5..\"}[5m]) / rate(http_requests_total[5m]) * 100', 30)
metrics['cpu_usage'] = query_metric('system_cpu_usage_percent', 30)
metrics['memory_usage'] = query_metric('system_memory_usage_percent', 30)

# ä¿å­˜æŒ‡æ ‡æ•°æ®
with open('${REPORT_DIR}/prometheus_metrics.json', 'w') as f:
    json.dump(metrics, f, indent=2, default=str)

print('Prometheusæ•°æ®æ”¶é›†å®Œæˆ')
" 2>/dev/null || log_warning "Prometheusæ•°æ®æ”¶é›†å¤±è´¥"
}

# åˆ†ææ€§èƒ½è¶‹åŠ¿
analyze_performance_trends() {
    log_info "åˆ†ææ€§èƒ½è¶‹åŠ¿..."

    python3 -c "
import json
import statistics
from datetime import datetime

# è¯»å–æ•°æ®
try:
    with open('${REPORT_DIR}/prometheus_metrics.json', 'r') as f:
        metrics = json.load(f)
except FileNotFoundError:
    print('æ— ç›‘æ§æ•°æ®ï¼Œè·³è¿‡è¶‹åŠ¿åˆ†æ')
    exit(0)

analysis = {}

# åˆ†æAPIå“åº”æ—¶é—´è¶‹åŠ¿
if metrics.get('api_response_time', {}).get('data', {}).get('result'):
    response_times = []
    for result in metrics['api_response_time']['data']['result']:
        if result.get('values'):
            response_times.extend([float(v[1]) for v in result['values'] if v[1] and str(v[1]) != 'nan'])

    if response_times:
        avg_response = statistics.mean(response_times)
        max_response = max(response_times)
        p95_response = statistics.quantiles(response_times, n=20)[18] if len(response_times) >= 20 else max_response

        analysis['api_performance'] = {
            'avg_response_time': avg_response,
            'max_response_time': max_response,
            'p95_response_time': p95_response,
            'degradation_detected': avg_response > 2.0  # è¶…è¿‡2ç§’ç®—æ€§èƒ½ä¸‹é™
        }

# åˆ†æé”™è¯¯ç‡è¶‹åŠ¿
if metrics.get('error_rate', {}).get('data', {}).get('result'):
    error_rates = []
    for result in metrics['error_rate']['data']['result']:
        if result.get('values'):
            error_rates.extend([float(v[1]) for v in result['values'] if v[1] and str(v[1]) != 'nan'])

    if error_rates:
        avg_error_rate = statistics.mean(error_rates)
        max_error_rate = max(error_rates)

        analysis['error_trends'] = {
            'avg_error_rate': avg_error_rate,
            'max_error_rate': max_error_rate,
            'high_error_detected': avg_error_rate > 5.0  # è¶…è¿‡5%ç®—é«˜é”™è¯¯ç‡
        }

# ä¿å­˜åˆ†æç»“æœ
with open('${REPORT_DIR}/performance_analysis.json', 'w') as f:
    json.dump(analysis, f, indent=2)

print('æ€§èƒ½è¶‹åŠ¿åˆ†æå®Œæˆ')
" 2>/dev/null || log_warning "æ€§èƒ½åˆ†æå¤±è´¥"
}

# ç”Ÿæˆä¼˜åŒ–å»ºè®®
generate_optimization_recommendations() {
    log_info "ç”Ÿæˆä¼˜åŒ–å»ºè®®..."

    python3 -c "
import json
import os

recommendations = []

# è¯»å–åˆ†æç»“æœ
analysis_file = '${REPORT_DIR}/performance_analysis.json'
if os.path.exists(analysis_file):
    try:
        with open(analysis_file, 'r') as f:
            analysis = json.load(f)

        # åŸºäºAPIæ€§èƒ½ç”Ÿæˆå»ºè®®
        if 'api_performance' in analysis:
            perf = analysis['api_performance']
            if perf.get('degradation_detected', False):
                recommendations.append({
                    'category': 'æ€§èƒ½ä¼˜åŒ–',
                    'priority': 'high',
                    'title': 'APIå“åº”æ—¶é—´éœ€è¦ä¼˜åŒ–',
                    'description': f'å¹³å‡å“åº”æ—¶é—´ {perf[\"avg_response_time\"]:.2f}s è¶…è¿‡2ç§’ç›®æ ‡',
                    'actions': [
                        'ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢ï¼Œæ·»åŠ é€‚å½“ç´¢å¼•',
                        'å®ç°APIå“åº”ç¼“å­˜',
                        'æ£€æŸ¥ç½‘ç»œå»¶è¿Ÿå’Œå¸¦å®½',
                        'è€ƒè™‘ä½¿ç”¨CDNåŠ é€Ÿé™æ€èµ„æº'
                    ]
                })

        # åŸºäºé”™è¯¯ç‡ç”Ÿæˆå»ºè®®
        if 'error_trends' in analysis:
            errors = analysis['error_trends']
            if errors.get('high_error_detected', False):
                recommendations.append({
                    'category': 'ç¨³å®šæ€§ä¼˜åŒ–',
                    'priority': 'high',
                    'title': 'ç³»ç»Ÿé”™è¯¯ç‡éœ€è¦é™ä½',
                    'description': f'å¹³å‡é”™è¯¯ç‡ {errors[\"avg_error_rate\"]:.2f}% è¶…è¿‡5%é˜ˆå€¼',
                    'actions': [
                        'åŠ å¼ºé”™è¯¯å¤„ç†å’Œå¼‚å¸¸æ•è·',
                        'ä¼˜åŒ–æ•°æ®åº“è¿æ¥æ± é…ç½®',
                        'å¢åŠ é‡è¯•æœºåˆ¶å’Œç†”æ–­å™¨',
                        'è¿›è¡Œå‹åŠ›æµ‹è¯•éªŒè¯ç³»ç»Ÿæ‰¿è½½èƒ½åŠ›'
                    ]
                })

    except Exception as e:
        print(f'è¯»å–åˆ†æç»“æœå¤±è´¥: {e}')

# CI/CDæµç¨‹ä¼˜åŒ–å»ºè®®
recommendations.extend([
    {
        'category': 'CI/CDä¼˜åŒ–',
        'priority': 'medium',
        'title': 'è€ƒè™‘å¹¶è¡ŒåŒ–æµ‹è¯•æ‰§è¡Œ',
        'description': 'å½“å‰æµ‹è¯•é¡ºåºæ‰§è¡Œå¯èƒ½è€—æ—¶è¾ƒé•¿',
        'actions': [
            'å°†å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•å¹¶è¡Œæ‰§è¡Œ',
            'ä½¿ç”¨çŸ©é˜µç­–ç•¥åˆ†å‘åˆ°å¤šä¸ªrunner',
            'ä¼˜åŒ–æµ‹è¯•æ•°æ®å‡†å¤‡å’Œæ¸…ç†',
            'è€ƒè™‘ä½¿ç”¨æµ‹è¯•åˆ†ç‰‡åŠ é€Ÿæ‰§è¡Œ'
        ]
    },
    {
        'category': 'èµ„æºä¼˜åŒ–',
        'priority': 'low',
        'title': 'å®šæœŸæ¸…ç†CI/CDç¼“å­˜',
        'description': 'Dockerå±‚ç¼“å­˜å’Œä¾èµ–ç¼“å­˜å¯èƒ½å ç”¨è¿‡å¤šç©ºé—´',
        'actions': [
            'è®¾ç½®è‡ªåŠ¨ç¼“å­˜æ¸…ç†ç­–ç•¥',
            'å®šæœŸæ¸…ç†æœªä½¿ç”¨çš„Dockeré•œåƒ',
            'ä¼˜åŒ–GitHub Actionsç¼“å­˜é…ç½®'
        ]
    }
])

# ä¿å­˜å»ºè®®
with open('${REPORT_DIR}/optimization_recommendations.json', 'w') as f:
    json.dump(recommendations, f, indent=2)

print(f'ç”Ÿæˆäº† {len(recommendations)} æ¡ä¼˜åŒ–å»ºè®®')
" 2>/dev/null || log_warning "ç”Ÿæˆä¼˜åŒ–å»ºè®®å¤±è´¥"
}

# ç”Ÿæˆæœˆåº¦æŠ¥å‘Š
generate_monthly_report() {
    log_info "ç”Ÿæˆæœˆåº¦ä¼˜åŒ–æŠ¥å‘Š..."

    python3 -c "
import json
import os
from datetime import datetime

report_data = {
    'report_month': '${MONTH}',
    'generated_at': datetime.now().isoformat(),
    'summary': {},
    'recommendations': [],
    'metrics': {}
}

# è¯»å–å„ç§æ•°æ®
try:
    # æ€§èƒ½åˆ†æ
    if os.path.exists('${REPORT_DIR}/performance_analysis.json'):
        with open('${REPORT_DIR}/performance_analysis.json', 'r') as f:
            report_data['metrics']['performance'] = json.load(f)

    # ä¼˜åŒ–å»ºè®®
    if os.path.exists('${REPORT_DIR}/optimization_recommendations.json'):
        with open('${REPORT_DIR}/optimization_recommendations.json', 'r') as f:
            report_data['recommendations'] = json.load(f)

    # è®¡ç®—æ±‡æ€»ä¿¡æ¯
    high_priority = len([r for r in report_data['recommendations'] if r.get('priority') == 'high'])
    medium_priority = len([r for r in report_data['recommendations'] if r.get('priority') == 'medium'])
    low_priority = len([r for r in report_data['recommendations'] if r.get('priority') == 'low'])

    report_data['summary'] = {
        'total_recommendations': len(report_data['recommendations']),
        'high_priority': high_priority,
        'medium_priority': medium_priority,
        'low_priority': low_priority,
        'performance_issues': len([r for r in report_data['recommendations'] if r.get('category') == 'æ€§èƒ½ä¼˜åŒ–']),
        'stability_issues': len([r for r in report_data['recommendations'] if r.get('category') == 'ç¨³å®šæ€§ä¼˜åŒ–'])
    }

except Exception as e:
    print(f'ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {e}')

# ç”ŸæˆMarkdownæŠ¥å‘Š
markdown_report = f'''# MyStocks CI/CD æœˆåº¦ä¼˜åŒ–æŠ¥å‘Š

**æŠ¥å‘Šæœˆä»½**: {report_data['report_month']}
**ç”Ÿæˆæ—¶é—´**: {report_data['generated_at'][:19].replace('T', ' ')}

## ğŸ“Š æ‰§è¡Œæ€»ç»“

- **æ€»å»ºè®®æ•°**: {report_data['summary'].get('total_recommendations', 0)}
- **é«˜ä¼˜å…ˆçº§**: {report_data['summary'].get('high_priority', 0)}
- **ä¸­ä¼˜å…ˆçº§**: {report_data['summary'].get('medium_priority', 0)}
- **ä½ä¼˜å…ˆçº§**: {report_data['summary'].get('low_priority', 0)}

## ğŸ” æ€§èƒ½åˆ†æ

'''

# æ·»åŠ æ€§èƒ½æŒ‡æ ‡
if 'performance' in report_data.get('metrics', {}):
    perf = report_data['metrics']['performance']
    if 'api_performance' in perf:
        api_perf = perf['api_performance']
        markdown_report += f'''### APIæ€§èƒ½æŒ‡æ ‡
- å¹³å‡å“åº”æ—¶é—´: {api_perf.get('avg_response_time', 0):.2f}ç§’
- 95thç™¾åˆ†ä½: {api_perf.get('p95_response_time', 0):.2f}ç§’
- æœ€å¤§å“åº”æ—¶é—´: {api_perf.get('max_response_time', 0):.2f}ç§’
- æ€§èƒ½çŠ¶æ€: {'âš ï¸ éœ€è¦ä¼˜åŒ–' if api_perf.get('degradation_detected', False) else 'âœ… æ­£å¸¸'}

'''

    if 'error_trends' in perf:
        err_trends = perf['error_trends']
        markdown_report += f'''### é”™è¯¯ç‡åˆ†æ
- å¹³å‡é”™è¯¯ç‡: {err_trends.get('avg_error_rate', 0):.2f}%
- æœ€é«˜é”™è¯¯ç‡: {err_trends.get('max_error_rate', 0):.2f}%
- ç¨³å®šæ€§çŠ¶æ€: {'âš ï¸ éœ€è¦æ”¹è¿›' if err_trends.get('high_error_detected', False) else 'âœ… ç¨³å®š'}

'''

# æ·»åŠ ä¼˜åŒ–å»ºè®®
markdown_report += '''## ğŸ’¡ ä¼˜åŒ–å»ºè®®

'''

for i, rec in enumerate(report_data.get('recommendations', []), 1):
    priority_icon = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}.get(rec.get('priority'), 'âšª')
    markdown_report += f'''### {i}. {priority_icon} {rec.get('title', '')}

**ä¼˜å…ˆçº§**: {rec.get('priority', '').upper()}
**ç±»åˆ«**: {rec.get('category', '')}

{rec.get('description', '')}

**å»ºè®®æªæ–½**:
'''
    for action in rec.get('actions', []):
        markdown_report += f'''- {action}
'''

    markdown_report += '''
'''

# æ·»åŠ æ‰§è¡Œè®¡åˆ’
markdown_report += '''## ğŸ“… æ‰§è¡Œè®¡åˆ’

### æœ¬æœˆé‡ç‚¹ä»»åŠ¡
1. **é«˜ä¼˜å…ˆçº§å»ºè®®**: ç«‹å³æ‰§è¡Œï¼Œå®‰æ’ä¸“äººè´Ÿè´£
2. **ä¸­ä¼˜å…ˆçº§å»ºè®®**: æœ¬æœˆå†…å®Œæˆï¼Œçº³å…¥Sprintè®¡åˆ’
3. **ä½ä¼˜å…ˆçº§å»ºè®®**: è§†æƒ…å†µæ’æœŸï¼ŒæŒç»­æ”¹è¿›

### è·Ÿè¸ªæœºåˆ¶
- å»ºç«‹ä¼˜åŒ–ä»»åŠ¡è·Ÿè¸ªè¡¨
- æ¯å‘¨reviewè¿›å±•
- æœˆæœ«è¯„ä¼°ä¼˜åŒ–æ•ˆæœ

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜è¯·è”ç³»ï¼š
- **DevOpså›¢é˜Ÿ**: devops@mystocks.local
- **æŠ€æœ¯è´Ÿè´£äºº**: tech-lead@mystocks.local

---
*æ­¤æŠ¥å‘Šç”±è‡ªåŠ¨åŒ–è„šæœ¬ç”Ÿæˆï¼Œå¦‚æœ‰ç–‘é—®è¯·æŸ¥çœ‹è¯¦ç»†æ•°æ®æ–‡ä»¶*
'''

# ä¿å­˜MarkdownæŠ¥å‘Š
with open('${REPORT_DIR}/monthly_optimization_report.md', 'w', encoding='utf-8') as f:
    f.write(markdown_report)

# ä¿å­˜å®Œæ•´JSONæ•°æ®
with open('${REPORT_DIR}/monthly_report_data.json', 'w', encoding='utf-8') as f:
    json.dump(report_data, f, indent=2, ensure_ascii=False)

print('æœˆåº¦ä¼˜åŒ–æŠ¥å‘Šç”Ÿæˆå®Œæˆ')
" 2>/dev/null || log_warning "ç”Ÿæˆæœˆåº¦æŠ¥å‘Šå¤±è´¥"
}

# å‘é€æŠ¥å‘Šé€šçŸ¥
send_report_notification() {
    log_info "å‘é€æŠ¥å‘Šé€šçŸ¥..."

    if [ -f "${REPORT_DIR}/monthly_optimization_report.md" ]; then
        # è¿™é‡Œå¯ä»¥é›†æˆå„ç§é€šçŸ¥æ–¹å¼

        # ç¤ºä¾‹ï¼šå‘é€åˆ°Slackæˆ–ä¼ä¸šå¾®ä¿¡
        # curl -X POST "$WEBHOOK_URL" \
        #      -H "Content-Type: application/json" \
        #      -d "{\"text\": \"MyStocks ${MONTH} CI/CDä¼˜åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ\"}"

        log_success "æŠ¥å‘Šé€šçŸ¥å‘é€å®Œæˆ"
    else
        log_warning "æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡é€šçŸ¥"
    fi
}

# ä¸»å‡½æ•°
main() {
    log_info "å¼€å§‹MyStocks CI/CDæœˆåº¦ä¼˜åŒ–å®¡æŸ¥..."
    log_info "æŠ¥å‘Šæœˆä»½: $MONTH"
    log_info "æŠ¥å‘Šç›®å½•: $REPORT_DIR"

    # æ‰§è¡Œå„é˜¶æ®µ
    collect_github_actions_stats
    collect_prometheus_metrics
    analyze_performance_trends
    generate_optimization_recommendations
    generate_monthly_report
    send_report_notification

    log_success "CI/CDæœˆåº¦ä¼˜åŒ–å®¡æŸ¥å®Œæˆ"
    log_info "æŠ¥å‘Šä½ç½®: $REPORT_DIR"
    log_info "æŸ¥çœ‹æŠ¥å‘Š: cat $REPORT_DIR/monthly_optimization_report.md"
}

# å¦‚æœè„šæœ¬è¢«ç›´æ¥è¿è¡Œï¼Œåˆ™æ‰§è¡Œä¸»å‡½æ•°
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main
fi