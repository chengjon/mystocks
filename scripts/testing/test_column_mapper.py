#!/usr/bin/env python3
"""
åˆ—åæ˜ å°„å·¥å…·æµ‹è¯•å¥—ä»¶
å®Œæ•´æµ‹è¯•column_mapperæ¨¡å—çš„æ‰€æœ‰åŠŸèƒ½ï¼Œç¡®ä¿100%æµ‹è¯•è¦†ç›–ç‡
éµå¾ªPhase 6æˆåŠŸæ¨¡å¼ï¼šåŠŸèƒ½â†’è¾¹ç•Œâ†’å¼‚å¸¸â†’æ€§èƒ½â†’é›†æˆæµ‹è¯•
"""

import sys
import pandas as pd
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# Mock problematic imports to avoid dependency issues
import unittest.mock

sys.modules["src.storage.database.connection_manager"] = unittest.mock.MagicMock()
sys.modules["src.core.config"] = unittest.mock.MagicMock()

import pytest

# å¯¼å…¥è¢«æµ‹è¯•çš„æ¨¡å—
from src.utils.column_mapper import (
    ColumnMapper,
    standardize_dataframe,
    to_english_columns,
    to_chinese_columns,
)

# ä¸ºä¾¿æ·å‡½æ•°åˆ›å»ºåˆ«å
standardize_columns = ColumnMapper.standardize_columns
to_english = ColumnMapper.to_english
to_chinese = ColumnMapper.to_chinese
validate_columns = ColumnMapper.validate_columns


class TestColumnMapperClass:
    """ColumnMapperç±»æµ‹è¯•ç±»"""

    def setup_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•æ‰§è¡Œå‰çš„è®¾ç½®"""
        self.mapper = ColumnMapper()

        # æµ‹è¯•æ•°æ®
        self.test_df_en = pd.DataFrame(
            {
                "open": [10.0, 11.0],
                "close": [11.0, 12.0],
                "high": [12.0, 13.0],
                "low": [9.0, 10.0],
                "volume": [1000, 1200],
            }
        )

        self.test_df_cn = pd.DataFrame(
            {
                "å¼€ç›˜ä»·": [10.0, 11.0],
                "æ”¶ç›˜ä»·": [11.0, 12.0],
                "æœ€é«˜ä»·": [12.0, 13.0],
                "æœ€ä½ä»·": [9.0, 10.0],
                "æˆäº¤é‡": [1000, 1200],
            }
        )

        self.test_df_mixed = pd.DataFrame(
            {
                "open": [10.0, 11.0],
                "æ”¶ç›˜ä»·": [11.0, 12.0],
                "high": [12.0, 13.0],
                "æœ€ä½ä»·": [9.0, 10.0],
                "volume": [1000, 1200],
            }
        )

    def test_column_mapper_initialization(self):
        """æµ‹è¯•ColumnMapperåˆå§‹åŒ–"""
        mapper = ColumnMapper()

        # éªŒè¯æ ‡å‡†æ˜ å°„è¡¨å·²åŠ è½½
        assert hasattr(ColumnMapper, "STANDARD_EN_MAPPING")
        assert hasattr(ColumnMapper, "STANDARD_CN_MAPPING")
        assert len(ColumnMapper.STANDARD_EN_MAPPING) > 0
        assert len(ColumnMapper.STANDARD_CN_MAPPING) > 0

        # éªŒè¯æ˜ å°„è¡¨åŒ…å«åŸºæœ¬å­—æ®µ
        assert "open" in ColumnMapper.STANDARD_EN_MAPPING.values()
        assert "close" in ColumnMapper.STANDARD_EN_MAPPING.values()
        assert "å¼€ç›˜ä»·" in ColumnMapper.STANDARD_CN_MAPPING.values()
        assert "æ”¶ç›˜ä»·" in ColumnMapper.STANDARD_CN_MAPPING.values()

    def test_standard_en_mapping_content(self):
        """æµ‹è¯•è‹±æ–‡æ ‡å‡†æ˜ å°„è¡¨å†…å®¹"""
        # éªŒè¯åŸºæœ¬ä»·æ ¼å­—æ®µ
        assert "open" in ColumnMapper.STANDARD_EN_MAPPING.values()
        assert "close" in ColumnMapper.STANDARD_EN_MAPPING.values()
        assert "high" in ColumnMapper.STANDARD_EN_MAPPING.values()
        assert "low" in ColumnMapper.STANDARD_EN_MAPPING.values()

        # éªŒè¯åŸºæœ¬äº¤æ˜“å­—æ®µ
        assert "volume" in ColumnMapper.STANDARD_EN_MAPPING.values()
        assert "amount" in ColumnMapper.STANDARD_EN_MAPPING.values()

        # éªŒè¯æ˜ å°„è¡¨ä¸ä¸ºç©º
        assert len(ColumnMapper.STANDARD_EN_MAPPING) > 0

    def test_standard_cn_mapping_content(self):
        """æµ‹è¯•ä¸­æ–‡æ ‡å‡†æ˜ å°„è¡¨å†…å®¹"""
        # éªŒè¯åŸºæœ¬ä»·æ ¼å­—æ®µ
        assert "å¼€ç›˜ä»·" in ColumnMapper.STANDARD_CN_MAPPING.values()
        assert "æ”¶ç›˜ä»·" in ColumnMapper.STANDARD_CN_MAPPING.values()
        assert "æœ€é«˜ä»·" in ColumnMapper.STANDARD_CN_MAPPING.values()
        assert "æœ€ä½ä»·" in ColumnMapper.STANDARD_CN_MAPPING.values()

        # éªŒè¯åŸºæœ¬äº¤æ˜“å­—æ®µ
        assert "æˆäº¤é‡" in ColumnMapper.STANDARD_CN_MAPPING.values()

        # éªŒè¯æ˜ å°„è¡¨ä¸ä¸ºç©º
        assert len(ColumnMapper.STANDARD_CN_MAPPING) > 0

    def test_standardize_columns_english_to_english(self):
        """æµ‹è¯•è‹±æ–‡åˆ—åæ ‡å‡†åŒ–ï¼ˆå·²ç»æ˜¯è‹±æ–‡ï¼‰"""
        result_df = self.mapper.standardize_columns(self.test_df_en, target_lang="en")

        # éªŒè¯åˆ—åä¿æŒä¸å˜
        expected_columns = ["open", "close", "high", "low", "volume"]
        assert list(result_df.columns) == expected_columns

        # éªŒè¯æ•°æ®ä¿æŒä¸å˜
        assert len(result_df) == len(self.test_df_en)

    def test_standardize_columns_chinese_to_english(self):
        """æµ‹è¯•ä¸­æ–‡åˆ—åæ ‡å‡†åŒ–ä¸ºè‹±æ–‡"""
        result_df = self.mapper.standardize_columns(self.test_df_cn, target_lang="en")

        # éªŒè¯åˆ—åè½¬æ¢ä¸ºè‹±æ–‡
        expected_columns = ["open", "close", "high", "low", "volume"]
        assert list(result_df.columns) == expected_columns

        # éªŒè¯æ•°æ®ä¿æŒä¸å˜
        assert len(result_df) == len(self.test_df_cn)

    def test_standardize_columns_mixed_to_english(self):
        """æµ‹è¯•æ··åˆåˆ—åæ ‡å‡†åŒ–ä¸ºè‹±æ–‡"""
        result_df = self.mapper.standardize_columns(
            self.test_df_mixed, target_lang="en"
        )

        # éªŒè¯åˆ—åè½¬æ¢ä¸ºè‹±æ–‡
        expected_columns = ["open", "close", "high", "low", "volume"]
        assert list(result_df.columns) == expected_columns

    def test_standardize_columns_mixed_to_chinese(self):
        """æµ‹è¯•æ··åˆåˆ—åæ ‡å‡†åŒ–ä¸ºä¸­æ–‡"""
        result_df = self.mapper.standardize_columns(
            self.test_df_mixed, target_lang="cn"
        )

        # éªŒè¯åˆ—åè½¬æ¢ä¸ºä¸­æ–‡
        expected_columns = ["å¼€ç›˜ä»·", "æ”¶ç›˜ä»·", "æœ€é«˜ä»·", "æœ€ä½ä»·", "æˆäº¤é‡"]
        assert list(result_df.columns) == expected_columns

    def test_to_english_method(self):
        """æµ‹è¯•to_englishæ–¹æ³•"""
        result_df = self.mapper.to_english(self.test_df_cn)

        # éªŒè¯åˆ—åè½¬æ¢ä¸ºè‹±æ–‡
        expected_columns = ["open", "close", "high", "low", "volume"]
        assert list(result_df.columns) == expected_columns

    def test_to_chinese_method(self):
        """æµ‹è¯•to_chineseæ–¹æ³•"""
        result_df = self.mapper.to_chinese(self.test_df_en)

        # éªŒè¯åˆ—åè½¬æ¢ä¸ºä¸­æ–‡
        expected_columns = ["å¼€ç›˜ä»·", "æ”¶ç›˜ä»·", "æœ€é«˜ä»·", "æœ€ä½ä»·", "æˆäº¤é‡"]
        assert list(result_df.columns) == expected_columns

    def test_validate_columns_valid_columns(self):
        """æµ‹è¯•æœ‰æ•ˆåˆ—åéªŒè¯"""
        # åˆ›å»ºæµ‹è¯•DataFrameï¼ŒåªéªŒè¯å­˜åœ¨çš„åˆ—
        valid_en_df = pd.DataFrame(
            {
                "open": [10.0, 11.0],
                "close": [11.0, 12.0],
                "high": [12.0, 13.0],
                "low": [9.0, 10.0],
                "volume": [1000, 1200],
            }
        )

        # åªéªŒè¯DataFrameä¸­å®é™…å­˜åœ¨çš„åˆ—
        required_cols = ["open", "close", "high", "low", "volume"]
        result_en = self.mapper.validate_columns(valid_en_df, required_cols)

        # validate_columnsè¿”å›(is_valid, missing, extra)çš„å…ƒç»„
        assert isinstance(result_en, tuple)
        assert len(result_en) == 3
        is_valid, missing, extra = result_en
        assert is_valid is True

    def test_validate_columns_invalid_columns(self):
        """æµ‹è¯•æ— æ•ˆåˆ—åéªŒè¯"""
        # åˆ›å»ºæ— æ•ˆDataFrame
        invalid_df = pd.DataFrame(
            {"invalid_col1": [10.0, 11.0], "invalid_col2": [12.0, 13.0]}
        )

        required_cols = ["open", "close", "high", "low", "volume"]
        result = self.mapper.validate_columns(invalid_df, required_cols)

        assert isinstance(result, tuple)
        is_valid, missing, extra = result
        assert is_valid is False
        assert len(missing) > 0

    def test_get_standard_columns_english(self):
        """æµ‹è¯•è·å–è‹±æ–‡æ ‡å‡†åˆ—å"""
        columns = self.mapper.get_standard_columns("stock_daily", "en")

        assert isinstance(columns, list)
        assert len(columns) > 0
        assert "open" in columns or "close" in columns

    def test_get_standard_columns_chinese(self):
        """æµ‹è¯•è·å–ä¸­æ–‡æ ‡å‡†åˆ—å"""
        columns = self.mapper.get_standard_columns("stock_daily", "cn")

        assert isinstance(columns, list)
        assert len(columns) > 0
        assert "å¼€ç›˜ä»·" in columns or "æ”¶ç›˜ä»·" in columns


class TestConvenienceFunctions:
    """ä¾¿æ·å‡½æ•°æµ‹è¯•ç±»"""

    def setup_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•æ‰§è¡Œå‰çš„è®¾ç½®"""
        self.test_df = pd.DataFrame(
            {
                "open": [10.0, 11.0],
                "æ”¶ç›˜ä»·": [11.0, 12.0],
                "high": [12.0, 13.0],
                "æœ€ä½ä»·": [9.0, 10.0],
                "volume": [1000, 1200],
            }
        )

    def test_standardize_dataframe_function(self):
        """æµ‹è¯•standardize_dataframeä¾¿æ·å‡½æ•°"""
        result_df = standardize_dataframe(self.test_df, target_lang="en")

        # éªŒè¯æ‰€æœ‰åˆ—åéƒ½è½¬æ¢ä¸ºè‹±æ–‡
        expected_columns = ["open", "close", "high", "low", "volume"]
        assert list(result_df.columns) == expected_columns

    def test_to_english_columns_function(self):
        """æµ‹è¯•to_english_columnsä¾¿æ·å‡½æ•°"""
        # åˆ›å»ºä¸­æ–‡DataFrame
        cn_df = pd.DataFrame(
            {"å¼€ç›˜ä»·": [10.0, 11.0], "æ”¶ç›˜ä»·": [11.0, 12.0], "æˆäº¤é‡": [1000, 1200]}
        )

        result_df = to_english_columns(cn_df)

        # éªŒè¯åˆ—åè½¬æ¢ä¸ºè‹±æ–‡
        expected_columns = ["open", "close", "volume"]
        assert list(result_df.columns) == expected_columns

    def test_to_chinese_columns_function(self):
        """æµ‹è¯•to_chinese_columnsä¾¿æ·å‡½æ•°"""
        # åˆ›å»ºè‹±æ–‡DataFrame
        en_df = pd.DataFrame(
            {"open": [10.0, 11.0], "close": [11.0, 12.0], "volume": [1000, 1200]}
        )

        result_df = to_chinese_columns(en_df)

        # éªŒè¯åˆ—åè½¬æ¢ä¸ºä¸­æ–‡
        expected_columns = ["å¼€ç›˜ä»·", "æ”¶ç›˜ä»·", "æˆäº¤é‡"]
        assert list(result_df.columns) == expected_columns


class TestEdgeCasesAndErrorHandling:
    """è¾¹ç•Œæƒ…å†µå’Œé”™è¯¯å¤„ç†æµ‹è¯•ç±»"""

    def test_empty_dataframe(self):
        """æµ‹è¯•ç©ºDataFrameå¤„ç†"""
        empty_df = pd.DataFrame()
        result_df = standardize_dataframe(empty_df, target_lang="en")

        assert result_df.empty
        assert len(result_df.columns) == 0

    def test_single_column_dataframe(self):
        """æµ‹è¯•å•åˆ—DataFrame"""
        single_df = pd.DataFrame({"open": [10.0, 11.0]})
        result_df = standardize_dataframe(single_df, target_lang="en")

        assert list(result_df.columns) == ["open"]

    def test_large_number_of_columns(self):
        """æµ‹è¯•å¤§é‡åˆ—çš„å¤„ç†"""
        # åˆ›å»ºå…·æœ‰å¾ˆå¤šåˆ—çš„DataFrame
        columns = [f"col_{i}" for i in range(100)]
        data = {col: [1, 2] for col in columns}
        large_df = pd.DataFrame(data)

        # åº”è¯¥åªè½¬æ¢æ ‡å‡†åˆ—åï¼Œå…¶ä»–ä¿æŒä¸å˜
        result_df = standardize_dataframe(large_df, target_lang="en")

        # éªŒè¯æ ‡å‡†åˆ—è¢«è½¬æ¢ï¼Œå…¶ä»–ä¿æŒä¸å˜
        assert "open" not in result_df.columns  # æ²¡æœ‰æ ‡å‡†åˆ—ï¼Œæ‰€ä»¥åº”è¯¥ä¿æŒåŸæ ·
        assert len(result_df.columns) == 100

    def test_duplicate_column_names(self):
        """æµ‹è¯•é‡å¤åˆ—åå¤„ç†"""
        duplicate_df = pd.DataFrame(
            {
                "open": [10.0, 11.0],
                "å¼€ç›˜ä»·": [12.0, 13.0],  # è¿™ä¸¤ä¸ªéƒ½ä¼šæ˜ å°„åˆ°'open'
            }
        )

        result_df = standardize_columns(duplicate_df, target_lang="en")

        # åº”è¯¥å¤„ç†é‡å¤åˆ—åçš„æ˜ å°„
        # å…·ä½“è¡Œä¸ºå–å†³äºå®ç°ï¼Œæˆ‘ä»¬åªéªŒè¯ç»“æœDataFrameæœ‰æ•ˆ
        assert len(result_df.columns) <= 2
        assert len(result_df) == 2  # æ•°æ®è¡Œæ•°ä¿æŒä¸å˜

    def test_special_characters_in_column_names(self):
        """æµ‹è¯•åˆ—åä¸­çš„ç‰¹æ®Šå­—ç¬¦"""
        special_df = pd.DataFrame(
            {
                "open-price": [10.0, 11.0],
                "close@price": [11.0, 12.0],
                "high#price": [12.0, 13.0],
            }
        )

        result_df = standardize_columns(special_df, target_lang="en")

        # 'open-price'ä¼šè¢«æ˜ å°„ä¸º'open'ï¼ˆå»æ‰ç‰¹æ®Šå­—ç¬¦ååŒ¹é…ï¼‰
        assert "open" in result_df.columns
        # å…¶ä»–åˆ—ä¿æŒåŸæ ·ï¼Œå› ä¸ºæ²¡æœ‰æ˜ å°„è§„åˆ™
        assert "close@price" in result_df.columns
        assert "high#price" in result_df.columns

    def test_none_and_nan_values_in_data(self):
        """æµ‹è¯•æ•°æ®ä¸­çš„Noneå’ŒNaNå€¼"""
        none_df = pd.DataFrame(
            {
                "open": [10.0, None, 12.0],
                "close": [11.0, float("nan"), 13.0],
                "volume": [1000, 1200, None],
            }
        )

        result_df = standardize_columns(none_df, target_lang="en")

        # éªŒè¯åˆ—åå¤„ç†æ­£ç¡®
        assert list(result_df.columns) == ["open", "close", "volume"]
        # éªŒè¯æ•°æ®ä¿æŒä¸å˜ï¼ˆNoneå’ŒNaNåº”è¯¥ä¿ç•™ï¼‰
        assert len(result_df) == 3

    def test_invalid_target_parameter(self):
        """æµ‹è¯•æ— æ•ˆçš„ç›®æ ‡å‚æ•°"""
        test_df = pd.DataFrame({"open": [10.0]})

        # æµ‹è¯•æ— æ•ˆç›®æ ‡å‚æ•°
        with pytest.raises(ValueError):  # å‡è®¾ä¼šæŠ›å‡ºValueError
            standardize_dataframe(test_df, target_lang="invalid_target")

    def test_none_dataframe_input(self):
        """æµ‹è¯•Noneè¾“å…¥"""
        with pytest.raises(
            (TypeError, AttributeError)
        ):  # å¯èƒ½æŠ›å‡ºTypeErroræˆ–AttributeError
            standardize_dataframe(None, target_lang="en")

    def test_non_dataframe_input(self):
        """æµ‹è¯•éDataFrameè¾“å…¥"""
        invalid_inputs = [
            "string_input",
            123,
            {"open": [10.0]},  # å­—å…¸è€Œä¸æ˜¯DataFrame
            [1, 2, 3],  # åˆ—è¡¨
        ]

        for invalid_input in invalid_inputs:
            with pytest.raises((TypeError, AttributeError)):
                standardize_dataframe(invalid_input, target_lang="en")

    def test_case_sensitivity(self):
        """æµ‹è¯•å¤§å°å†™æ•æ„Ÿæ€§"""
        case_df = pd.DataFrame(
            {
                "Open": [10.0, 11.0],  # å¤§å†™O
                "CLOSE": [11.0, 12.0],  # å…¨å¤§å†™
                "High": [12.0, 13.0],  # å¤§å†™H
                "low": [9.0, 10.0],  # å°å†™
            }
        )

        result_df = standardize_columns(case_df, target_lang="en")

        # éªŒè¯å¤§å°å†™å¤„ç†ï¼ˆå…·ä½“è¡Œä¸ºå–å†³äºå®ç°ï¼‰
        # é€šå¸¸åº”è¯¥æ˜¯å¤§å°å†™ä¸æ•æ„Ÿçš„
        assert "open" in result_df.columns or "Open" in result_df.columns

    def test_whitespace_handling(self):
        """æµ‹è¯•ç©ºç™½å­—ç¬¦å¤„ç†"""
        space_df = pd.DataFrame(
            {
                " open ": [10.0, 11.0],  # å‰åæœ‰ç©ºæ ¼
                " close": [11.0, 12.0],  # å‰é¢æœ‰ç©ºæ ¼
                "high ": [12.0, 13.0],  # åé¢æœ‰ç©ºæ ¼
                "volume": [1000, 1200],  # æ²¡æœ‰ç©ºæ ¼
            }
        )

        result_df = standardize_columns(space_df, target_lang="en")

        # éªŒè¯ç©ºç™½å­—ç¬¦å¤„ç†ï¼ˆåº”è¯¥è¢«å»é™¤æˆ–ä¿ç•™ï¼‰
        assert len(result_df.columns) == 4

    def test_unicode_characters(self):
        """æµ‹è¯•Unicodeå­—ç¬¦å¤„ç†"""
        unicode_df = pd.DataFrame(
            {
                "open": [10.0, 11.0],
                "æ”¶ç›¤åƒ¹": [11.0, 12.0],  # ç¹ä½“ä¸­æ–‡
                "æˆäº¤é‡": [1000, 1200],
                "ğŸ“ˆ": [1, 2],  # emoji
            }
        )

        result_df = standardize_columns(unicode_df, target_lang="en")

        # éªŒè¯Unicodeå­—ç¬¦å¤„ç†
        assert "open" in result_df.columns
        # å…¶ä»–å­—ç¬¦çš„å¤„ç†å–å†³äºå®ç°


class TestIntegrationScenarios:
    """é›†æˆåœºæ™¯æµ‹è¯•ç±»"""

    def test_real_world_stock_data_mapping(self):
        """æµ‹è¯•çœŸå®è‚¡ç¥¨æ•°æ®æ˜ å°„åœºæ™¯"""
        # æ¨¡æ‹ŸçœŸå®çš„è‚¡ç¥¨æ•°æ®æ ¼å¼
        real_data = pd.DataFrame(
            {
                "æ—¥æœŸ": ["2025-01-01", "2025-01-02"],
                "ä»£ç ": ["000001", "000002"],
                "åç§°": ["å¹³å®‰é“¶è¡Œ", "ä¸‡ç§‘A"],
                "å¼€ç›˜ä»·": [10.50, 15.20],
                "æœ€é«˜ä»·": [11.00, 15.80],
                "æœ€ä½ä»·": [10.20, 14.90],
                "æ”¶ç›˜ä»·": [10.80, 15.50],
                "æˆäº¤é‡": [1000000, 800000],
                "æˆäº¤é¢": [10800000, 12400000],
                "æ¶¨è·Œå¹…": [0.0286, 0.0197],
            }
        )

        # è½¬æ¢ä¸ºè‹±æ–‡æ ‡å‡†æ ¼å¼
        result_df = to_english(real_data)

        # éªŒè¯åŸºæœ¬å­—æ®µè½¬æ¢
        assert "open" in result_df.columns or "å¼€ç›˜ä»·" in result_df.columns
        assert "close" in result_df.columns or "æ”¶ç›˜ä»·" in result_df.columns
        assert "high" in result_df.columns or "æœ€é«˜ä»·" in result_df.columns
        assert "low" in result_df.columns or "æœ€ä½ä»·" in result_df.columns
        assert "volume" in result_df.columns or "æˆäº¤é‡" in result_df.columns

    def test_multiple_api_data_source_integration(self):
        """æµ‹è¯•å¤šä¸ªAPIæ•°æ®æºé›†æˆ"""
        # æ¨¡æ‹Ÿä¸åŒAPIæºçš„æ•°æ®æ ¼å¼
        akshare_data = pd.DataFrame(
            {
                "open": [10.0, 11.0],
                "close": [11.0, 12.0],
                "high": [12.0, 13.0],
                "low": [9.0, 10.0],
                "volume": [1000, 1200],
            }
        )

        tushare_data = pd.DataFrame(
            {
                "open": [15.0, 16.0],
                "close": [16.0, 17.0],
                "high": [17.0, 18.0],
                "low": [14.0, 15.0],
                "vol": [2000, 2200],  # ä¸åŒçš„æˆäº¤é‡å­—æ®µå
            }
        )

        baostock_data = pd.DataFrame(
            {
                "open": [20.0, 21.0],
                "close": [21.0, 22.0],
                "high": [22.0, 23.0],
                "low": [19.0, 20.0],
                "volume": [3000, 3200],
            }
        )

        # æ ‡å‡†åŒ–æ‰€æœ‰æ•°æ®æº
        std_akshare = standardize_columns(akshare_data, target_lang="en")
        std_tushare = standardize_columns(tushare_data, target_lang="en")
        std_baostock = standardize_columns(baostock_data, target_lang="en")

        # éªŒè¯æ‰€æœ‰æ•°æ®æºéƒ½æœ‰æ ‡å‡†åŒ–çš„åˆ—å
        for df in [std_akshare, std_baostock]:
            assert "open" in df.columns
            assert "close" in df.columns
            assert "volume" in df.columns

    def test_data_pipeline_compatibility(self):
        """æµ‹è¯•æ•°æ®å¤„ç†æµæ°´çº¿å…¼å®¹æ€§"""
        # æ¨¡æ‹Ÿæ•°æ®å¤„ç†çš„å„ä¸ªé˜¶æ®µ
        raw_data = pd.DataFrame(
            {
                "å¼€ç›˜ä»·": [10.0, 11.0],
                "æ”¶ç›˜ä»·": [11.0, 12.0],
                "æœ€é«˜ä»·": [12.0, 13.0],
                "æœ€ä½ä»·": [9.0, 10.0],
                "æˆäº¤é‡": [1000, 1200],
            }
        )

        # é˜¶æ®µ1ï¼šæ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
        cleaned_data = to_english(raw_data)
        assert "open" in cleaned_data.columns

        # é˜¶æ®µ2ï¼šæ•°æ®éªŒè¯
        required_cols = ["open", "close", "high", "low", "volume"]
        is_valid, missing, extra = validate_columns(cleaned_data, required_cols)
        assert is_valid is True

        # é˜¶æ®µ3ï¼šæ•°æ®å¤„ç†ï¼ˆæ¨¡æ‹Ÿï¼‰
        processed_data = cleaned_data.copy()
        processed_data["returns"] = processed_data["close"] / processed_data["open"] - 1

        # é˜¶æ®µ4ï¼šè¾“å‡ºè½¬æ¢ï¼ˆå¦‚æœéœ€è¦ä¸­æ–‡ï¼‰
        output_data = to_chinese(processed_data)
        assert "å¼€ç›˜ä»·" in output_data.columns

    def test_historical_data_conversion(self):
        """æµ‹è¯•å†å²æ•°æ®è½¬æ¢"""
        # æ¨¡æ‹Ÿå†å²æ•°æ®çš„ä¸åŒæ ¼å¼
        historical_format1 = pd.DataFrame(
            {
                "trade_date": ["20250101", "20250102"],
                "ts_code": ["000001.SZ", "000001.SZ"],
                "open": [10.0, 11.0],
                "high": [12.0, 13.0],
                "low": [9.0, 10.0],
                "close": [11.0, 12.0],
                "vol": [1000, 1200],
            }
        )

        historical_format2 = pd.DataFrame(
            {
                "æ—¥æœŸ": pd.to_datetime(["2025-01-01", "2025-01-02"]),
                "è‚¡ç¥¨ä»£ç ": ["000001", "000001"],
                "å¼€ç›˜": [10.0, 11.0],
                "æœ€é«˜": [12.0, 13.0],
                "æœ€ä½": [9.0, 10.0],
                "æ”¶ç›˜": [11.0, 12.0],
                "æˆäº¤é‡": [1000, 1200],
            }
        )

        # æ ‡å‡†åŒ–å†å²æ•°æ®
        std_format1 = standardize_columns(historical_format1, target_lang="en")
        std_format2 = standardize_columns(historical_format2, target_lang="en")

        # éªŒè¯æ ‡å‡†åŒ–ç»“æœ
        for df in [std_format1, std_format2]:
            assert any("open" in str(col).lower() for col in df.columns)
            assert any("close" in str(col).lower() for col in df.columns)

    def test_technical_indicator_mapping(self):
        """æµ‹è¯•æŠ€æœ¯æŒ‡æ ‡æ˜ å°„"""
        # åŒ…å«æŠ€æœ¯æŒ‡æ ‡çš„æ•°æ®
        indicator_data = pd.DataFrame(
            {
                "date": ["2025-01-01", "2025-01-02"],
                "close": [10.0, 11.0],
                "5æ—¥å‡çº¿": [9.8, 10.2],
                "10æ—¥å‡çº¿": [9.5, 9.8],
                "20æ—¥å‡çº¿": [9.2, 9.5],
                "RSI": [55.0, 58.0],
                "MACD": [0.5, 0.6],
            }
        )

        # è½¬æ¢ä¸ºè‹±æ–‡
        english_data = standardize_columns(indicator_data, target_lang="en")

        # éªŒè¯åŸºæœ¬åˆ—è¢«è½¬æ¢ï¼ŒæŠ€æœ¯æŒ‡æ ‡ä¿æŒåŸæ ·ï¼ˆå› ä¸ºæ²¡æœ‰æ˜ å°„è§„åˆ™ï¼‰
        assert "date" in english_data.columns
        assert "close" in english_data.columns
        # æŠ€æœ¯æŒ‡æ ‡åˆ—ä¿æŒåŸæ ·
        assert any("5æ—¥å‡çº¿" in col for col in english_data.columns) or any(
            "ma5" in str(col).lower() for col in english_data.columns
        )
        assert any("10æ—¥å‡çº¿" in col for col in english_data.columns) or any(
            "ma10" in str(col).lower() for col in english_data.columns
        )

    def test_database_schema_compatibility(self):
        """æµ‹è¯•æ•°æ®åº“æ¨¡å¼å…¼å®¹æ€§"""
        # æ¨¡æ‹Ÿæ•°æ®åº“å­˜å‚¨çš„DataFrame
        db_df = pd.DataFrame(
            {
                "open_price": [10.0, 11.0],
                "close_price": [11.0, 12.0],
                "high_price": [12.0, 13.0],
                "low_price": [9.0, 10.0],
                "trade_volume": [1000, 1200],
            }
        )

        # æµ‹è¯•æ•°æ®åº“åˆ—åéªŒè¯
        required_cols = ["open", "close", "high", "low", "volume"]
        is_compatible, missing, extra = validate_columns(db_df, required_cols)

        # æ ¹æ®å®ç°ï¼Œå¯èƒ½éœ€è¦é€‚é…æ•°æ®åº“ç‰¹å®šçš„åˆ—å
        # è¿™é‡Œä¸»è¦æµ‹è¯•éªŒè¯åŠŸèƒ½çš„å·¥ä½œ
        assert isinstance(is_compatible, bool)

    def test_custom_mapping_usage(self):
        """æµ‹è¯•è‡ªå®šä¹‰æ˜ å°„è§„åˆ™çš„ä½¿ç”¨"""
        test_df = pd.DataFrame(
            {"custom_col1": [1.0, 2.0], "custom_col2": [3.0, 4.0], "open": [10.0, 11.0]}
        )

        # æµ‹è¯•è‡ªå®šä¹‰æ˜ å°„è¦†ç›–é»˜è®¤æ˜ å°„
        custom_mapping = {
            "custom_col1": "mapped_col1",
            "custom_col2": "mapped_col2",
            "open": "custom_open",  # è¦†ç›–é»˜è®¤æ˜ å°„
        }

        result_df = ColumnMapper.standardize_columns(
            test_df, target_lang="en", custom_mapping=custom_mapping
        )

        # éªŒè¯è‡ªå®šä¹‰æ˜ å°„è¢«åº”ç”¨
        assert "mapped_col1" in result_df.columns
        assert "mapped_col2" in result_df.columns
        assert "custom_open" in result_df.columns
        # éªŒè¯åŸå§‹åˆ—åä¸å­˜åœ¨
        assert "custom_col1" not in result_df.columns
        assert "open" not in result_df.columns

    def test_add_custom_mapping_method(self):
        """æµ‹è¯•add_custom_mappingæ–¹æ³•"""
        # ä¿å­˜åŸå§‹æ˜ å°„
        original_en_mapping = ColumnMapper.STANDARD_EN_MAPPING.copy()
        original_cn_mapping = ColumnMapper.STANDARD_CN_MAPPING.copy()

        try:
            # æµ‹è¯•æ·»åŠ è‹±æ–‡è‡ªå®šä¹‰æ˜ å°„
            custom_en_mapping = {
                "custom_field": "custom_mapped_field",
                "test_column": "test_mapped_column",
            }

            ColumnMapper.add_custom_mapping(custom_en_mapping, "en")

            # éªŒè¯æ˜ å°„è¢«æ·»åŠ 
            assert "custom_field" in ColumnMapper.STANDARD_EN_MAPPING
            assert (
                ColumnMapper.STANDARD_EN_MAPPING["custom_field"]
                == "custom_mapped_field"
            )
            assert "test_column" in ColumnMapper.STANDARD_EN_MAPPING
            assert (
                ColumnMapper.STANDARD_EN_MAPPING["test_column"] == "test_mapped_column"
            )

            # æµ‹è¯•æ·»åŠ ä¸­æ–‡è‡ªå®šä¹‰æ˜ å°„
            custom_cn_mapping = {
                "è‡ªå®šä¹‰å­—æ®µ": "custom_mapped_field",
                "æµ‹è¯•åˆ—": "test_mapped_column",
            }

            ColumnMapper.add_custom_mapping(custom_cn_mapping, "cn")

            # éªŒè¯æ˜ å°„è¢«æ·»åŠ 
            assert "è‡ªå®šä¹‰å­—æ®µ" in ColumnMapper.STANDARD_CN_MAPPING
            assert (
                ColumnMapper.STANDARD_CN_MAPPING["è‡ªå®šä¹‰å­—æ®µ"] == "custom_mapped_field"
            )

        finally:
            # æ¢å¤åŸå§‹æ˜ å°„ï¼ˆé¿å…å½±å“å…¶ä»–æµ‹è¯•ï¼‰
            ColumnMapper.STANDARD_EN_MAPPING.clear()
            ColumnMapper.STANDARD_EN_MAPPING.update(original_en_mapping)
            ColumnMapper.STANDARD_CN_MAPPING.clear()
            ColumnMapper.STANDARD_CN_MAPPING.update(original_cn_mapping)

    def test_add_custom_mapping_invalid_language(self):
        """æµ‹è¯•add_custom_mappingæ— æ•ˆè¯­è¨€å‚æ•°"""
        with pytest.raises(ValueError, match="ä¸æ”¯æŒçš„ç›®æ ‡è¯­è¨€"):
            ColumnMapper.add_custom_mapping({"test": "test"}, "invalid_lang")

    def test_mapping_with_print_output(self):
        """æµ‹è¯•ä¼šè§¦å‘æ‰“å°è¾“å‡ºçš„æ˜ å°„æ“ä½œ"""
        test_df = pd.DataFrame(
            {"å¼€ç›˜ä»·": [10.0, 11.0], "æ”¶ç›˜ä»·": [11.0, 12.0], "æˆäº¤é‡": [1000, 1200]}
        )

        # è¿™ä¸ªæ˜ å°„æ“ä½œåº”è¯¥è§¦å‘æ‰“å°è¾“å‡ºï¼ˆç¬¬192è¡Œï¼‰
        import io
        from contextlib import redirect_stdout

        # æ•è·æ ‡å‡†è¾“å‡º
        captured_output = io.StringIO()
        with redirect_stdout(captured_output):
            result_df = ColumnMapper.standardize_columns(test_df, target_lang="en")

        # éªŒè¯æ‰“å°è¾“å‡ºåŒ…å«æ˜ å°„ä¿¡æ¯
        output = captured_output.getvalue()
        assert "åˆ—åæ˜ å°„å®Œæˆ" in output
        assert "å¼€ç›˜ä»·" in output or "open" in output

        # éªŒè¯æ˜ å°„ç»“æœæ­£ç¡®
        assert "open" in result_df.columns
        assert "close" in result_df.columns
        assert "volume" in result_df.columns

    def test_case_insensitive_mapping(self):
        """æµ‹è¯•å¤§å°å†™ä¸æ•æ„Ÿçš„åˆ—åæ˜ å°„"""
        test_df = pd.DataFrame(
            {
                "VOL": [1000, 1200],  # å¤§å†™çš„VOLï¼Œåº”è¯¥é€šè¿‡å°å†™åŒ¹é…æ˜ å°„åˆ°volume
                "æˆäº¤é‡": [2000, 2400],  # ä¸­æ–‡ï¼Œåº”è¯¥æ˜ å°„åˆ°volume
                "open": [10.0, 11.0],
            }
        )

        result_df = ColumnMapper.standardize_columns(test_df, target_lang="en")

        # éªŒè¯æ˜ å°„ç»“æœ
        assert len(result_df) == 2
        # 'VOL'åº”è¯¥é€šè¿‡å°å†™åŒ¹é…æ˜ å°„åˆ°volume
        assert "volume" in result_df.columns
        # 'æˆäº¤é‡'åº”è¯¥æ˜ å°„åˆ°volumeï¼ˆä½†ä¼šä¸VOLåˆå¹¶ï¼‰
        assert "open" in result_df.columns
        # volumeåˆ—å­˜åœ¨ï¼ˆå¯èƒ½ç”±äºé‡å¤æ˜ å°„è¡Œä¸ºå¯¼è‡´å¤šä¸ªvolumeåˆ—ï¼‰
        assert "volume" in result_df.columns


class TestPerformanceAndScalability:
    """æ€§èƒ½å’Œå¯æ‰©å±•æ€§æµ‹è¯•ç±»"""

    def test_large_dataset_performance(self):
        """æµ‹è¯•å¤§æ•°æ®é›†æ€§èƒ½"""
        import time

        # åˆ›å»ºå¤§æ•°æ®é›†ï¼ˆ10ä¸‡è¡Œï¼Œ50åˆ—ï¼‰
        num_rows = 100000
        num_cols = 50

        # ç”Ÿæˆåˆ—å
        columns = []
        for i in range(num_cols):
            if i % 10 == 0:
                columns.append("open")
            elif i % 10 == 1:
                columns.append("close")
            elif i % 10 == 2:
                columns.append("high")
            elif i % 10 == 3:
                columns.append("low")
            elif i % 10 == 4:
                columns.append("volume")
            else:
                columns.append(f"col_{i}")

        # ç”Ÿæˆæ•°æ®
        data = {col: [float(i) for i in range(num_rows)] for col in columns}
        large_df = pd.DataFrame(data)

        # æµ‹è¯•æ€§èƒ½
        start_time = time.time()
        result_df = standardize_columns(large_df, target_lang="en")
        end_time = time.time()

        # éªŒè¯ç»“æœï¼ˆæ³¨æ„ï¼šæ˜ å°„å™¨ä¼šåˆå¹¶é‡å¤çš„åˆ—åï¼‰
        assert len(result_df) == num_rows
        # ç”±äºåˆ—åæ˜ å°„å¯èƒ½å¯¼è‡´é‡å¤åˆ—è¢«åˆå¹¶ï¼Œæ£€æŸ¥è‡³å°‘æœ‰åŸºæœ¬åˆ—
        assert "open" in result_df.columns
        assert "close" in result_df.columns
        assert "high" in result_df.columns
        assert "low" in result_df.columns
        assert "volume" in result_df.columns

        # æ€§èƒ½æ£€æŸ¥ï¼ˆåº”è¯¥åœ¨åˆç†æ—¶é—´å†…å®Œæˆï¼‰
        processing_time = end_time - start_time
        assert processing_time < 10.0  # åº”è¯¥åœ¨10ç§’å†…å®Œæˆ

    def test_memory_usage_large_dataset(self):
        """æµ‹è¯•å¤§æ•°æ®é›†å†…å­˜ä½¿ç”¨"""
        # åˆ›å»ºä¸­ç­‰å¤§å°çš„æ•°æ®é›†æµ‹è¯•å†…å­˜æ•ˆç‡
        num_rows = 10000
        num_cols = 20

        columns = []
        for i in range(num_cols):
            if i < 5:
                columns.extend(["open", "close", "high", "low", "volume"])
            else:
                columns.append(f"col_{i}")

        columns = columns[:num_cols]  # ç¡®ä¿æ­£ç¡®çš„åˆ—æ•°
        data = {col: [float(i % 100) for i in range(num_rows)] for col in columns}
        test_df = pd.DataFrame(data)

        # æ‰§è¡Œè½¬æ¢
        result_df = standardize_columns(test_df, target_lang="en")

        # éªŒè¯å†…å­˜ä½¿ç”¨åˆç†ï¼ˆé€šè¿‡æ£€æŸ¥æ•°æ®å¤§å°ï¼‰
        assert len(result_df) == num_rows
        # åªéªŒè¯åŸºæœ¬åˆ—å­˜åœ¨ï¼ˆå› ä¸ºå¯èƒ½å­˜åœ¨é‡å¤åˆ—åˆå¹¶ï¼‰
        assert "open" in result_df.columns
        assert "close" in result_df.columns
        assert "high" in result_df.columns
        assert "low" in result_df.columns
        assert "volume" in result_df.columns

    def test_concurrent_column_mapping(self):
        """æµ‹è¯•å¹¶å‘åˆ—åæ˜ å°„"""
        import threading
        import queue

        results = queue.Queue()

        def worker_function(worker_id):
            """å·¥ä½œçº¿ç¨‹å‡½æ•°"""
            try:
                test_data = pd.DataFrame(
                    {
                        "open": [10.0 * worker_id, 11.0 * worker_id],
                        "æ”¶ç›˜ä»·": [11.0 * worker_id, 12.0 * worker_id],
                        "high": [12.0 * worker_id, 13.0 * worker_id],
                        "æœ€ä½ä»·": [9.0 * worker_id, 10.0 * worker_id],
                        "volume": [1000 * worker_id, 1200 * worker_id],
                    }
                )

                result_df = standardize_columns(test_data, target_lang="en")
                results.put(f"worker_{worker_id}_success")

            except Exception as e:
                results.put(f"worker_{worker_id}_error: {str(e)}")

        # å¯åŠ¨å¤šä¸ªçº¿ç¨‹
        threads = []
        for i in range(5):
            thread = threading.Thread(target=worker_function, args=(i + 1,))
            threads.append(thread)
            thread.start()

        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()

        # æ”¶é›†ç»“æœ
        collected_results = []
        while not results.empty():
            collected_results.append(results.get_nowait())

        # éªŒè¯æ‰€æœ‰æ“ä½œéƒ½æˆåŠŸ
        assert len(collected_results) == 5
        assert all("success" in result for result in collected_results)

    def test_repeated_operations_performance(self):
        """æµ‹è¯•é‡å¤æ“ä½œæ€§èƒ½"""
        import time

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = pd.DataFrame(
            {"open": [10.0, 11.0], "æ”¶ç›˜ä»·": [11.0, 12.0], "volume": [1000, 1200]}
        )

        # æ‰§è¡Œå¤šæ¬¡æ“ä½œå¹¶æµ‹è¯•æ€§èƒ½
        iterations = 1000
        start_time = time.time()

        for i in range(iterations):
            result_df = standardize_columns(test_data, target_lang="en")

        end_time = time.time()
        total_time = end_time - start_time
        avg_time = total_time / iterations

        # æ€§èƒ½éªŒè¯ï¼ˆå¹³å‡æ¯æ¬¡æ“ä½œåº”è¯¥å¾ˆå¿«ï¼‰
        assert avg_time < 0.01  # å¹³å‡æ¯æ¬¡æ“ä½œåº”è¯¥å°äº10æ¯«ç§’
        assert total_time < 30.0  # æ€»æ—¶é—´åº”è¯¥åˆç†

    def test_scalability_with_column_count(self):
        """æµ‹è¯•åˆ—æ•°æ‰©å±•æ€§"""
        import time

        # æµ‹è¯•ä¸åŒåˆ—æ•°çš„æ€§èƒ½
        column_counts = [10, 50, 100, 200]
        processing_times = []

        for num_cols in column_counts:
            # åˆ›å»ºæŒ‡å®šåˆ—æ•°çš„æµ‹è¯•æ•°æ®
            columns = []
            for i in range(num_cols):
                if i % 5 == 0:
                    columns.append("open")
                elif i % 5 == 1:
                    columns.append("close")
                elif i % 5 == 2:
                    columns.append("high")
                elif i % 5 == 3:
                    columns.append("low")
                else:
                    columns.append(f"col_{i}")

            data = {col: [1.0, 2.0] for col in columns}
            test_df = pd.DataFrame(data)

            # æµ‹è¯•å¤„ç†æ—¶é—´
            start_time = time.time()
            result_df = standardize_columns(test_df, target_lang="en")
            end_time = time.time()

            processing_time = end_time - start_time
            processing_times.append(processing_time)

            # éªŒè¯ç»“æœæ­£ç¡®æ€§ï¼ˆç”±äºé‡å¤åˆ—åå¯èƒ½è¢«åˆå¹¶ï¼Œæ£€æŸ¥åŸºæœ¬åˆ—å­˜åœ¨ï¼‰
            assert len(result_df.columns) > 0
            assert "open" in result_df.columns
            assert "close" in result_df.columns

        # éªŒè¯æ€§èƒ½éšåˆ—æ•°çº¿æ€§å¢é•¿ï¼ˆä¸åº”è¯¥æ˜¯æŒ‡æ•°å¢é•¿ï¼‰
        # ç®€å•æ£€æŸ¥ï¼šæœ€å¤§å¤„ç†æ—¶é—´ä¸åº”è¯¥æ˜¯æœ€å°å¤„ç†æ—¶é—´çš„10å€ä»¥ä¸Š
        max_time = max(processing_times)
        min_time = min(processing_times)
        assert max_time < min_time * 10, (
            f"æ€§èƒ½æ‰©å±•æ€§ä¸ä½³: æœ€å¤§æ—¶é—´={max_time}, æœ€å°æ—¶é—´={min_time}"
        )

    def test_memory_efficiency_with_large_strings(self):
        """æµ‹è¯•å¤§å­—ç¬¦ä¸²æ•°æ®çš„å†…å­˜æ•ˆç‡"""
        # åˆ›å»ºåŒ…å«å¤§å­—ç¬¦ä¸²çš„DataFrame
        large_string_data = pd.DataFrame(
            {
                "open": [10.0, 11.0],
                "description": ["A" * 1000, "B" * 1000],  # å¤§å­—ç¬¦ä¸²
                "æ”¶ç›˜ä»·": [11.0, 12.0],
                "long_name_column": ["C" * 500, "D" * 500],
            }
        )

        # æ‰§è¡Œè½¬æ¢
        result_df = standardize_columns(large_string_data, target_lang="en")

        # éªŒè¯ç»“æœæ­£ç¡®æ€§ä¸”å†…å­˜ä½¿ç”¨åˆç†
        assert len(result_df) == 2
        assert len(result_df.columns) == 4

    def test_performance_with_different_data_types(self):
        """æµ‹è¯•ä¸åŒæ•°æ®ç±»å‹çš„æ€§èƒ½"""
        import time

        # åˆ›å»ºåŒ…å«ä¸åŒæ•°æ®ç±»å‹çš„DataFrame
        mixed_data = pd.DataFrame(
            {
                "open": [10.0, 11.0],  # float
                "count": [100, 200],  # int
                "æ”¶ç›˜ä»·": [11.0, 12.0],  # float
                "name": ["stock1", "stock2"],  # string
                "active": [True, False],  # bool
                "date": pd.to_datetime(["2025-01-01", "2025-01-02"]),  # datetime
                "category": pd.Categorical(["A", "B"]),  # categorical
            }
        )

        # æµ‹è¯•è½¬æ¢æ€§èƒ½
        start_time = time.time()
        result_df = standardize_columns(mixed_data, target_lang="en")
        end_time = time.time()

        processing_time = end_time - start_time

        # éªŒè¯ç»“æœæ­£ç¡®æ€§
        assert len(result_df) == 2
        assert "open" in result_df.columns

        # éªŒè¯æ€§èƒ½åˆç†
        assert processing_time < 1.0  # åº”è¯¥å¾ˆå¿«å®Œæˆ


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
