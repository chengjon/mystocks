#!/usr/bin/env python3
"""
ä»£ç é‡å¤æ£€æµ‹å™¨ - è¯†åˆ«é‡å¤å’Œç›¸ä¼¼ä»£ç 

åˆ†æä»£ç åº“ä¸­çš„å‡½æ•°å’Œä»£ç å—ï¼Œæ‰¾å‡ºé«˜åº¦ç›¸ä¼¼çš„éƒ¨åˆ†ã€‚

ä½œè€…: MyStocks Team
æ—¥æœŸ: 2025-10-19

ä½¿ç”¨æ–¹æ³•:
    python scripts/analysis/detect_duplicates.py
"""

import sys
import json
from pathlib import Path
from typing import List, Tuple, Dict
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(Path(__file__).parent))

from models import (
    DuplicationCase, DuplicationIndex, SeverityEnum,
    CodeBlock, ModuleInventory
)
from utils.similarity import SimilarityDetector
from utils.ast_parser import extract_code_block, tokenize_code
from generate_docs import load_inventory


def detect_function_duplicates(inventory: ModuleInventory) -> DuplicationIndex:
    """
    æ£€æµ‹å‡½æ•°çº§åˆ«çš„é‡å¤

    Args:
        inventory: æ¨¡å—æ¸…å•

    Returns:
        é‡å¤ç´¢å¼•
    """
    print("\næ­£åœ¨æ£€æµ‹å‡½æ•°çº§åˆ«é‡å¤...")

    detector = SimilarityDetector(min_token_similarity=0.6, min_ast_similarity=0.5)
    dup_index = DuplicationIndex()

    # æ”¶é›†æ‰€æœ‰å‡½æ•°
    all_functions = []
    for module in inventory.modules:
        # æ¨¡å—çº§å‡½æ•°
        for func in module.functions:
            all_functions.append((module.file_path, func, None))

        # ç±»æ–¹æ³•
        for cls in module.classes:
            for method in cls.methods:
                all_functions.append((module.file_path, method, cls.name))

    print(f"  æ€»å‡½æ•°æ•°: {len(all_functions)}")

    # æŒ‰å‡½æ•°ååˆ†ç»„ï¼ˆç›¸åŒåç§°çš„å‡½æ•°æ›´å¯èƒ½é‡å¤ï¼‰
    functions_by_name = defaultdict(list)
    for file_path, func, class_name in all_functions:
        # è·³è¿‡å¤ªçŸ­çš„å‡½æ•°
        if func.body_lines < 5:
            continue
        functions_by_name[func.name].append((file_path, func, class_name))

    # æ£€æµ‹æ¯ç»„ä¸­çš„é‡å¤
    checked_pairs = set()
    duplicates_found = 0

    for func_name, func_list in functions_by_name.items():
        if len(func_list) < 2:
            continue

        # æ¯”è¾ƒåŒåå‡½æ•°
        for i in range(len(func_list)):
            for j in range(i + 1, len(func_list)):
                file1, func1, cls1 = func_list[i]
                file2, func2, cls2 = func_list[j]

                pair_key = tuple(sorted([
                    f"{file1}:{func1.line_number}",
                    f"{file2}:{func2.line_number}"
                ]))

                if pair_key in checked_pairs:
                    continue
                checked_pairs.add(pair_key)

                # æå–ä»£ç 
                full_path1 = str(PROJECT_ROOT / file1)
                full_path2 = str(PROJECT_ROOT / file2)

                code1 = extract_code_block(full_path1, func1.line_number,
                                          func1.line_number + func1.body_lines)
                code2 = extract_code_block(full_path2, func2.line_number,
                                          func2.line_number + func2.body_lines)

                if not code1 or not code2:
                    continue

                # è®¡ç®—ç›¸ä¼¼åº¦
                token_sim = detector.calculate_token_similarity(code1, code2)
                ast_sim = detector.calculate_ast_similarity(code1, code2)

                # å¦‚æœç›¸ä¼¼åº¦è¶³å¤Ÿé«˜
                if token_sim >= 0.6 and ast_sim >= 0.5:
                    func1_name = f"{cls1}.{func1.name}" if cls1 else func1.name
                    func2_name = f"{cls2}.{func2.name}" if cls2 else func2.name

                    description = f"å‡½æ•° '{func_name}' åœ¨å¤šå¤„å®ç°ä¸­é«˜åº¦ç›¸ä¼¼"

                    dup = detector.create_duplication_case(
                        [
                            (file1, func1.line_number, func1.line_number + func1.body_lines, code1),
                            (file2, func2.line_number, func2.line_number + func2.body_lines, code2)
                        ],
                        token_sim,
                        ast_sim,
                        description
                    )

                    dup_index.add_duplication(dup)
                    duplicates_found += 1

    print(f"  âœ“ å‘ç° {duplicates_found} ä¸ªé‡å¤æ¡ˆä¾‹")

    return dup_index


def detect_pattern_duplicates(inventory: ModuleInventory, dup_index: DuplicationIndex):
    """
    æ£€æµ‹å¸¸è§æ¨¡å¼é‡å¤ï¼ˆå¦‚è¿æ¥æ•°æ®åº“çš„ä»£ç ï¼‰

    Args:
        inventory: æ¨¡å—æ¸…å•
        dup_index: é‡å¤ç´¢å¼•ï¼ˆå°†æ·»åŠ æ–°å‘ç°ï¼‰
    """
    print("\næ­£åœ¨æ£€æµ‹å¸¸è§æ¨¡å¼é‡å¤...")

    # å®šä¹‰è¦æ£€æµ‹çš„æ¨¡å¼
    patterns = {
        'database_connection': [
            'get_connection', 'connect', 'cursor', 'execute', 'commit', 'close'
        ],
        'error_handling': [
            'try', 'except', 'finally', 'raise', 'Exception'
        ],
        'data_validation': [
            'if', 'is None', 'raise ValueError', 'assert'
        ],
        'logging': [
            'logger', 'log', 'info', 'error', 'warning', 'debug'
        ]
    }

    detector = SimilarityDetector(min_token_similarity=0.7, min_ast_similarity=0.6)

    # æŒ‰æ¨¡å¼åˆ†ç»„å‡½æ•°
    for pattern_name, keywords in patterns.items():
        matching_functions = []

        for module in inventory.modules:
            for func in module.functions:
                # æ£€æŸ¥å‡½æ•°åæˆ– docstring æ˜¯å¦åŒ…å«å…³é”®è¯
                func_text = f"{func.name} {func.docstring or ''}".lower()
                if any(kw.lower() in func_text for kw in keywords):
                    matching_functions.append((module.file_path, func))

            for cls in module.classes:
                for method in cls.methods:
                    method_text = f"{method.name} {method.docstring or ''}".lower()
                    if any(kw.lower() in method_text for kw in keywords):
                        matching_functions.append((module.file_path, method))

        if len(matching_functions) >= 3:
            print(f"  æ¨¡å¼ '{pattern_name}': {len(matching_functions)} ä¸ªåŒ¹é…å‡½æ•°")

    print(f"  âœ“ æ¨¡å¼åˆ†æå®Œæˆ")


def analyze_duplicate_clusters(dup_index: DuplicationIndex) -> Dict:
    """
    åˆ†æé‡å¤é›†ç¾¤ï¼ˆå¤šä¸ªæ–‡ä»¶ä¸­çš„é‡å¤ï¼‰

    Args:
        dup_index: é‡å¤ç´¢å¼•

    Returns:
        é›†ç¾¤åˆ†æç»“æœ
    """
    print("\næ­£åœ¨åˆ†æé‡å¤é›†ç¾¤...")

    # æ‰¾å‡ºå½±å“å¤šä¸ªæ–‡ä»¶çš„é‡å¤
    clusters = defaultdict(list)

    for dup in dup_index.duplications:
        if len(dup.affected_files) >= 2:
            cluster_key = tuple(sorted(dup.affected_files))
            clusters[cluster_key].append(dup)

    # ç»Ÿè®¡
    multi_file_clusters = {k: v for k, v in clusters.items() if len(k) >= 2}

    print(f"  é‡å¤é›†ç¾¤æ•°: {len(multi_file_clusters)}")
    print(f"  æœ€å¤§é›†ç¾¤: {max((len(files) for files in multi_file_clusters.keys()), default=0)} ä¸ªæ–‡ä»¶")

    return {
        'total_clusters': len(multi_file_clusters),
        'clusters': [
            {
                'files': list(files),
                'duplication_count': len(dups),
                'severity_breakdown': {
                    'critical': sum(1 for d in dups if d.severity == SeverityEnum.CRITICAL),
                    'high': sum(1 for d in dups if d.severity == SeverityEnum.HIGH),
                    'medium': sum(1 for d in dups if d.severity == SeverityEnum.MEDIUM),
                    'low': sum(1 for d in dups if d.severity == SeverityEnum.LOW)
                }
            }
            for files, dups in sorted(multi_file_clusters.items(),
                                     key=lambda x: len(x[1]), reverse=True)[:10]
        ]
    }


def generate_duplication_summary(dup_index: DuplicationIndex) -> str:
    """ç”Ÿæˆé‡å¤åˆ†ææ‘˜è¦"""
    lines = []

    lines.append("# ä»£ç é‡å¤åˆ†ææ‘˜è¦\n")
    lines.append(f"**æ€»é‡å¤æ¡ˆä¾‹**: {dup_index.total_cases}\n")

    # ä¸¥é‡æ€§åˆ†å¸ƒ
    lines.append("## ä¸¥é‡æ€§åˆ†å¸ƒ\n")
    lines.append(f"- CRITICAL: {len(dup_index.critical)} æ¡ˆä¾‹")
    lines.append(f"- HIGH: {len(dup_index.high)} æ¡ˆä¾‹")
    lines.append(f"- MEDIUM: {len(dup_index.medium)} æ¡ˆä¾‹")
    lines.append(f"- LOW: {len(dup_index.low)} æ¡ˆä¾‹\n")

    # Top 5 å…³é”®é—®é¢˜
    if dup_index.critical or dup_index.high:
        lines.append("## ğŸ”´ éœ€è¦ç«‹å³å¤„ç†çš„é‡å¤\n")

        critical_and_high = dup_index.critical + dup_index.high
        for i, dup in enumerate(critical_and_high[:5], 1):
            lines.append(f"### {i}. {dup.id}")
            lines.append(f"- **ä¸¥é‡æ€§**: {dup.severity.value.upper()}")
            lines.append(f"- **ç›¸ä¼¼åº¦**: Token {dup.token_similarity:.0%}, AST {dup.ast_similarity:.0%}")
            lines.append(f"- **ä½ç½®**: {len(dup.blocks)} å¤„")
            for block in dup.blocks[:3]:  # åªæ˜¾ç¤ºå‰3å¤„
                lines.append(f"  - `{block.file_path}:{block.start_line}`")
            lines.append("")

    return '\n'.join(lines)


def save_duplication_index(dup_index: DuplicationIndex, output_path: str):
    """ä¿å­˜é‡å¤ç´¢å¼•åˆ° JSON"""

    data = {
        'total_cases': dup_index.total_cases,
        'summary': {
            'critical': len(dup_index.critical),
            'high': len(dup_index.high),
            'medium': len(dup_index.medium),
            'low': len(dup_index.low)
        },
        'duplications': []
    }

    for dup in dup_index.duplications:
        dup_data = {
            'id': dup.id,
            'severity': dup.severity.value,
            'token_similarity': dup.token_similarity,
            'ast_similarity': dup.ast_similarity,
            'description': dup.description,
            'merge_suggestion': dup.merge_suggestion,
            'affected_files': dup.affected_files,
            'blocks': [
                {
                    'file_path': block.file_path,
                    'start_line': block.start_line,
                    'end_line': block.end_line,
                    'function_name': block.function_name,
                    'class_name': block.class_name
                }
                for block in dup.blocks
            ]
        }
        data['duplications'].append(dup_data)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

    print(f"\nâœ“ é‡å¤ç´¢å¼•å·²ä¿å­˜: {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("MyStocks ä»£ç é‡å¤æ£€æµ‹å™¨")
    print("=" * 60)

    # åŠ è½½æ¸…å•
    inventory_path = PROJECT_ROOT / "docs/function-classification-manual/metadata/module-inventory.json"

    if not inventory_path.exists():
        print(f"\nâœ— é”™è¯¯: æ¸…å•æ–‡ä»¶ä¸å­˜åœ¨: {inventory_path}")
        print("  è¯·å…ˆè¿è¡Œ: python scripts/analysis/scan_codebase.py")
        return

    print(f"\nåŠ è½½æ¸…å•: {inventory_path}")
    inventory = load_inventory(str(inventory_path))
    print(f"âœ“ åŠ è½½å®Œæˆï¼Œå…± {len(inventory.modules)} ä¸ªæ¨¡å—")

    # æ£€æµ‹å‡½æ•°é‡å¤
    dup_index = detect_function_duplicates(inventory)

    # æ£€æµ‹æ¨¡å¼é‡å¤
    detect_pattern_duplicates(inventory, dup_index)

    # åˆ†æé›†ç¾¤
    cluster_analysis = analyze_duplicate_clusters(dup_index)

    # ç”Ÿæˆæ‘˜è¦
    print("\n" + "=" * 60)
    print("é‡å¤åˆ†æç»“æœ")
    print("=" * 60)
    print(f"\næ€»é‡å¤æ¡ˆä¾‹: {dup_index.total_cases}")
    print(f"  CRITICAL: {len(dup_index.critical)}")
    print(f"  HIGH: {len(dup_index.high)}")
    print(f"  MEDIUM: {len(dup_index.medium)}")
    print(f"  LOW: {len(dup_index.low)}")
    print(f"\né‡å¤é›†ç¾¤: {cluster_analysis['total_clusters']}")

    # ä¿å­˜ç»“æœ
    output_dir = PROJECT_ROOT / "docs/function-classification-manual/metadata"
    output_path = output_dir / "duplication-index.json"
    save_duplication_index(dup_index, str(output_path))

    # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
    summary = generate_duplication_summary(dup_index)
    summary_path = PROJECT_ROOT / "docs/function-classification-manual/duplication-summary.md"
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write(summary)
    print(f"âœ“ æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")

    # ä½¿ç”¨ markdown_writer ç”Ÿæˆå®Œæ•´æ–‡æ¡£
    from utils.markdown_writer import MarkdownWriter
    writer = MarkdownWriter(str(PROJECT_ROOT / "docs/function-classification-manual"))
    doc_path = writer.generate_duplication_analysis(dup_index)
    print(f"âœ“ å®Œæ•´æ–‡æ¡£å·²ç”Ÿæˆ: {doc_path}")

    print("\nâœ“ é‡å¤æ£€æµ‹å®Œæˆ!")

    return dup_index, cluster_analysis


if __name__ == "__main__":
    main()
