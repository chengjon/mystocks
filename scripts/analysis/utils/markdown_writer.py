"""
Markdown æ–‡æ¡£ç”Ÿæˆå™¨ - ç”Ÿæˆæ‰‹å†Œæ–‡æ¡£

ç”Ÿæˆæ ¼å¼è‰¯å¥½çš„ Markdown æ–‡æ¡£ï¼ŒåŒ…æ‹¬ï¼š
- æ¨¡å—åˆ†ç±»æ–‡æ¡£
- é‡å¤åˆ†ææŠ¥å‘Š
- ä¼˜åŒ–è·¯çº¿å›¾
- åˆå¹¶æŒ‡å—
- æ•°æ®æµå›¾

ä½œè€…: MyStocks Team
æ—¥æœŸ: 2025-10-19
"""

from typing import List, Dict, Optional
from pathlib import Path
from datetime import datetime

import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from models import (
    ModuleMetadata, CategoryEnum, DuplicationCase,
    OptimizationOpportunity, MergeRecommendation,
    DataFlow, ManualMetadata, ModuleInventory,
    DuplicationIndex, OptimizationRoadmap, ConsolidationGuide
)


class MarkdownWriter:
    """Markdown æ–‡æ¡£ç”Ÿæˆå™¨"""

    def __init__(self, output_dir: str):
        """
        åˆå§‹åŒ–æ–‡æ¡£ç”Ÿæˆå™¨

        Args:
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def generate_category_document(
        self,
        category: CategoryEnum,
        modules: List[ModuleMetadata],
        category_name_cn: str,
        category_description: str
    ) -> str:
        """
        ç”ŸæˆåŠŸèƒ½ç±»åˆ«æ–‡æ¡£

        Args:
            category: ç±»åˆ«æšä¸¾
            modules: è¯¥ç±»åˆ«çš„æ¨¡å—åˆ—è¡¨
            category_name_cn: ä¸­æ–‡ç±»åˆ«åç§°
            category_description: ç±»åˆ«æè¿°

        Returns:
            æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        """
        category_files = {
            CategoryEnum.CORE: "01-core-functions.md",
            CategoryEnum.AUXILIARY: "02-auxiliary-functions.md",
            CategoryEnum.INFRASTRUCTURE: "03-infrastructure-functions.md",
            CategoryEnum.MONITORING: "04-monitoring-functions.md",
            CategoryEnum.UTILITY: "05-utility-functions.md"
        }

        filename = category_files.get(category, f"{category.value}-functions.md")
        filepath = self.output_dir / filename

        lines = []

        # æ ‡é¢˜
        lines.append(f"# {category_name_cn}\n")
        lines.append(f"**ç±»åˆ«**: {category.value}")
        lines.append(f"**æ¨¡å—æ•°**: {len(modules)}")

        # ç»Ÿè®¡
        total_classes = sum(len(m.classes) for m in modules)
        total_functions = sum(
            len(m.functions) + sum(len(c.methods) for c in m.classes)
            for m in modules
        )
        total_lines = sum(m.lines_of_code for m in modules)

        lines.append(f"**ç±»æ•°**: {total_classes}")
        lines.append(f"**å‡½æ•°æ•°**: {total_functions}")
        lines.append(f"**ä»£ç è¡Œæ•°**: {total_lines}\n")

        # æè¿°
        lines.append("## æ¦‚è¿°\n")
        lines.append(f"{category_description}\n")

        # æ¨¡å—åˆ—è¡¨
        lines.append("## æ¨¡å—åˆ—è¡¨\n")

        for module in sorted(modules, key=lambda m: m.file_path):
            lines.append(f"### {module.module_name}\n")
            lines.append(f"**æ–‡ä»¶**: `{module.file_path}`\n")

            if module.docstring:
                lines.append("**è¯´æ˜**:\n")
                lines.append(f"{module.docstring}\n")

            # ç±»åˆ—è¡¨
            if module.classes:
                lines.append("#### ç±»\n")
                for cls in module.classes:
                    lines.append(f"##### `{cls.name}`\n")

                    if cls.docstring:
                        lines.append(f"{cls.docstring}\n")

                    if cls.base_classes:
                        lines.append(f"**ç»§æ‰¿**: {', '.join(f'`{b}`' for b in cls.base_classes)}\n")

                    # æ–¹æ³•åˆ—è¡¨
                    if cls.methods:
                        lines.append("**æ–¹æ³•**:\n")
                        for method in cls.methods:
                            params = self._format_parameters(method.parameters)
                            return_type = method.return_type or "None"
                            lines.append(
                                f"- `{method.name}({params})` â†’ `{return_type}` "
                                f"[{module.file_path}:{method.line_number}]"
                            )

                            if method.docstring:
                                first_line = method.docstring.split('\n')[0]
                                lines.append(f"  - {first_line}")

                        lines.append("")

            # å‡½æ•°åˆ—è¡¨
            if module.functions:
                lines.append("#### å‡½æ•°\n")
                for func in module.functions:
                    params = self._format_parameters(func.parameters)
                    return_type = func.return_type or "None"
                    lines.append(
                        f"##### `{func.name}({params})` â†’ `{return_type}`\n"
                    )
                    lines.append(f"**ä½ç½®**: [{module.file_path}:{func.line_number}]\n")

                    if func.docstring:
                        lines.append(f"{func.docstring}\n")

                    if func.decorators:
                        lines.append(f"**è£…é¥°å™¨**: {', '.join(f'`@{d}`' for d in func.decorators)}\n")

            lines.append("---\n")

        # å†™å…¥æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))

        return str(filepath)

    def generate_duplication_analysis(
        self,
        duplication_index: DuplicationIndex
    ) -> str:
        """
        ç”Ÿæˆé‡å¤åˆ†ææ–‡æ¡£

        Args:
            duplication_index: é‡å¤ç´¢å¼•

        Returns:
            æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        """
        filepath = self.output_dir / "06-duplication-analysis.md"
        lines = []

        lines.append("# ä»£ç é‡å¤åˆ†æ\n")
        lines.append(f"**æ€»é‡å¤æ¡ˆä¾‹æ•°**: {duplication_index.total_cases}\n")

        # ç»Ÿè®¡è¡¨
        lines.append("## ä¸¥é‡æ€§åˆ†å¸ƒ\n")
        lines.append("| ä¸¥é‡æ€§ | æ¡ˆä¾‹æ•° | æè¿° |")
        lines.append("|--------|--------|------|")
        lines.append(f"| CRITICAL | {len(duplication_index.critical)} | å‡ ä¹å®Œå…¨ç›¸åŒï¼Œéœ€ç«‹å³å¤„ç† |")
        lines.append(f"| HIGH | {len(duplication_index.high)} | é«˜åº¦ç›¸ä¼¼ï¼Œå»ºè®®ä¼˜å…ˆå¤„ç† |")
        lines.append(f"| MEDIUM | {len(duplication_index.medium)} | æ˜¾è‘—ç›¸ä¼¼ï¼Œå»ºè®®è€ƒè™‘åˆå¹¶ |")
        lines.append(f"| LOW | {len(duplication_index.low)} | éƒ¨åˆ†ç›¸ä¼¼ï¼Œå¯é€‰ä¼˜åŒ– |\n")

        # CRITICAL æ¡ˆä¾‹
        if duplication_index.critical:
            lines.append("## CRITICAL - ç«‹å³å¤„ç†\n")
            for dup in duplication_index.critical:
                lines.extend(self._format_duplication(dup))

        # HIGH æ¡ˆä¾‹
        if duplication_index.high:
            lines.append("## HIGH - ä¼˜å…ˆå¤„ç†\n")
            for dup in duplication_index.high:
                lines.extend(self._format_duplication(dup))

        # MEDIUM æ¡ˆä¾‹
        if duplication_index.medium:
            lines.append("## MEDIUM - å»ºè®®å¤„ç†\n")
            for dup in duplication_index.medium:
                lines.extend(self._format_duplication(dup))

        # LOW æ¡ˆä¾‹ï¼ˆæŠ˜å ï¼‰
        if duplication_index.low:
            lines.append("## LOW - å¯é€‰ä¼˜åŒ–\n")
            lines.append("<details>")
            lines.append("<summary>ç‚¹å‡»å±•å¼€ LOW ä¼˜å…ˆçº§æ¡ˆä¾‹</summary>\n")
            for dup in duplication_index.low:
                lines.extend(self._format_duplication(dup))
            lines.append("</details>\n")

        # å†™å…¥æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))

        return str(filepath)

    def generate_optimization_roadmap(
        self,
        roadmap: OptimizationRoadmap
    ) -> str:
        """
        ç”Ÿæˆä¼˜åŒ–è·¯çº¿å›¾æ–‡æ¡£

        Args:
            roadmap: ä¼˜åŒ–è·¯çº¿å›¾

        Returns:
            æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        """
        filepath = self.output_dir / "07-optimization-roadmap.md"
        lines = []

        lines.append("# ä¼˜åŒ–è·¯çº¿å›¾\n")
        lines.append(f"**æ€»ä¼˜åŒ–æœºä¼šæ•°**: {len(roadmap.opportunities)}\n")

        # å¿«é€Ÿèƒœåˆ©
        quick_wins = roadmap.get_quick_wins()
        if quick_wins:
            lines.append("## å¿«é€Ÿèƒœåˆ© âš¡\n")
            lines.append("*é«˜ä¼˜å…ˆçº§ + ä½å·¥ä½œé‡*\n")
            for opp in quick_wins:
                lines.extend(self._format_optimization(opp))

        # æ€§èƒ½ä¼˜åŒ–
        if roadmap.performance:
            lines.append("## æ€§èƒ½ä¼˜åŒ– ğŸš€\n")
            for opp in sorted(roadmap.performance, key=lambda o: o.priority.value):
                lines.extend(self._format_optimization(opp))

        # æ¶æ„ä¼˜åŒ–
        if roadmap.architecture:
            lines.append("## æ¶æ„ä¼˜åŒ– ğŸ—ï¸\n")
            for opp in sorted(roadmap.architecture, key=lambda o: o.priority.value):
                lines.extend(self._format_optimization(opp))

        # ä»£ç è´¨é‡
        if roadmap.code_quality:
            lines.append("## ä»£ç è´¨é‡ âœ¨\n")
            for opp in sorted(roadmap.code_quality, key=lambda o: o.priority.value):
                lines.extend(self._format_optimization(opp))

        # å†™å…¥æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))

        return str(filepath)

    def generate_consolidation_guide(
        self,
        guide: ConsolidationGuide
    ) -> str:
        """
        ç”Ÿæˆåˆå¹¶æŒ‡å—æ–‡æ¡£

        Args:
            guide: åˆå¹¶æŒ‡å—

        Returns:
            æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        """
        filepath = self.output_dir / "08-consolidation-guide.md"
        lines = []

        lines.append("# æ¨¡å—åˆå¹¶æŒ‡å—\n")
        lines.append(f"**æ€»åˆå¹¶å»ºè®®æ•°**: {len(guide.recommendations)}\n")

        # é«˜å½±å“åˆå¹¶
        high_impact = guide.get_high_impact()
        if high_impact:
            lines.append("## é«˜å½±å“åˆå¹¶ ğŸ’¥\n")
            lines.append("*åˆå¹¶ 3+ æ¨¡å—*\n")
            for rec in high_impact:
                lines.extend(self._format_merge_recommendation(rec))

        # æŒ‰é£é™©çº§åˆ«åˆ†ç»„
        for risk_level in ["low", "medium", "high"]:
            recs = guide.get_by_risk_level(risk_level)
            if recs:
                risk_name = {"low": "ä½é£é™©", "medium": "ä¸­é£é™©", "high": "é«˜é£é™©"}[risk_level]
                lines.append(f"## {risk_name}åˆå¹¶\n")
                for rec in recs:
                    lines.extend(self._format_merge_recommendation(rec))

        # å†™å…¥æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))

        return str(filepath)

    def generate_data_flow_maps(
        self,
        data_flows: List[DataFlow]
    ) -> str:
        """
        ç”Ÿæˆæ•°æ®æµå›¾æ–‡æ¡£

        Args:
            data_flows: æ•°æ®æµåˆ—è¡¨

        Returns:
            æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        """
        filepath = self.output_dir / "09-data-flow-maps.md"
        lines = []

        lines.append("# æ•°æ®æµå›¾\n")
        lines.append(f"**æ•°æ®æµæ•°é‡**: {len(data_flows)}\n")

        lines.append("## ç³»ç»Ÿæ¦‚è§ˆ\n")
        lines.append("```mermaid")
        lines.append("graph TB")
        lines.append("  subgraph å¤–éƒ¨æ•°æ®æº")
        lines.append("    AKSHARE[AKShare]")
        lines.append("    BAOSTOCK[Baostock]")
        lines.append("    TDX[é€šè¾¾ä¿¡]")
        lines.append("  end")
        lines.append("")
        lines.append("  subgraph é€‚é…å™¨å±‚")
        lines.append("    AK_ADAPTER[AKShare Adapter]")
        lines.append("    BS_ADAPTER[Baostock Adapter]")
        lines.append("    TDX_ADAPTER[TDX Adapter]")
        lines.append("  end")
        lines.append("")
        lines.append("  subgraph æ ¸å¿ƒå±‚")
        lines.append("    UNIFIED[Unified Manager]")
        lines.append("    ROUTER[Data Router]")
        lines.append("  end")
        lines.append("")
        lines.append("  subgraph æ•°æ®åº“å±‚")
        lines.append("    TDENGINE[(TDengine<br/>å¸‚åœºæ•°æ®)]")
        lines.append("    POSTGRES[(PostgreSQL<br/>è¡ç”Ÿæ•°æ®)]")
        lines.append("    MYSQL[(MySQL<br/>å‚è€ƒæ•°æ®)]")
        lines.append("    REDIS[(Redis<br/>äº¤æ˜“æ•°æ®)]")
        lines.append("  end")
        lines.append("")
        lines.append("  AKSHARE --> AK_ADAPTER")
        lines.append("  BAOSTOCK --> BS_ADAPTER")
        lines.append("  TDX --> TDX_ADAPTER")
        lines.append("")
        lines.append("  AK_ADAPTER --> UNIFIED")
        lines.append("  BS_ADAPTER --> UNIFIED")
        lines.append("  TDX_ADAPTER --> UNIFIED")
        lines.append("")
        lines.append("  UNIFIED --> ROUTER")
        lines.append("")
        lines.append("  ROUTER --> TDENGINE")
        lines.append("  ROUTER --> POSTGRES")
        lines.append("  ROUTER --> MYSQL")
        lines.append("  ROUTER --> REDIS")
        lines.append("```\n")

        # è¯¦ç»†æ•°æ®æµ
        for flow in data_flows:
            lines.append(f"## {flow.name}\n")
            lines.append(f"**ID**: {flow.id}")
            lines.append(f"**æè¿°**: {flow.description}\n")

            if flow.data_classification:
                lines.append(f"**æ•°æ®åˆ†ç±»**: {flow.data_classification}")

            if flow.database_target:
                lines.append(f"**ç›®æ ‡æ•°æ®åº“**: {flow.database_target}\n")

            lines.append("### æµç¨‹æ­¥éª¤\n")
            for i, step in enumerate(flow.steps, 1):
                module = step.get('module', 'Unknown')
                function = step.get('function', 'Unknown')
                action = step.get('action', '')
                lines.append(f"{i}. **{module}**.`{function}()` - {action}")

            lines.append("\n---\n")

        # å†™å…¥æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))

        return str(filepath)

    def _format_parameters(self, parameters: List) -> str:
        """æ ¼å¼åŒ–å‚æ•°åˆ—è¡¨"""
        if not parameters:
            return ""

        param_strs = []
        for param in parameters:
            if hasattr(param, 'name'):
                if param.type_annotation:
                    if param.default_value:
                        param_strs.append(f"{param.name}: {param.type_annotation} = {param.default_value}")
                    else:
                        param_strs.append(f"{param.name}: {param.type_annotation}")
                else:
                    if param.default_value:
                        param_strs.append(f"{param.name}={param.default_value}")
                    else:
                        param_strs.append(param.name)

        return ", ".join(param_strs)

    def _format_duplication(self, dup: DuplicationCase) -> List[str]:
        """æ ¼å¼åŒ–é‡å¤æ¡ˆä¾‹"""
        lines = []

        lines.append(f"### {dup.id}\n")
        lines.append(f"**æè¿°**: {dup.description}")
        lines.append(f"**Token ç›¸ä¼¼åº¦**: {dup.token_similarity:.1%}")
        lines.append(f"**AST ç›¸ä¼¼åº¦**: {dup.ast_similarity:.1%}\n")

        lines.append("**é‡å¤ä½ç½®**:\n")
        for block in dup.blocks:
            lines.append(f"- `{block.file_path}:{block.start_line}-{block.end_line}`")

        lines.append("")
        lines.append("**åˆå¹¶å»ºè®®**:\n")
        lines.append(f"{dup.merge_suggestion}\n")
        lines.append("---\n")

        return lines

    def _format_optimization(self, opp: OptimizationOpportunity) -> List[str]:
        """æ ¼å¼åŒ–ä¼˜åŒ–æœºä¼š"""
        lines = []

        priority_emoji = {
            "p0": "ğŸ”´",
            "p1": "ğŸŸ ",
            "p2": "ğŸŸ¡",
            "p3": "ğŸŸ¢"
        }
        emoji = priority_emoji.get(opp.priority.value, "âšª")

        lines.append(f"### {emoji} {opp.title}\n")
        lines.append(f"**ID**: {opp.id}")
        lines.append(f"**ä¼˜å…ˆçº§**: {opp.priority.value.upper()}")
        lines.append(f"**å·¥ä½œé‡**: {opp.effort_estimate}\n")

        lines.append(f"**å½“å‰çŠ¶æ€**:\n{opp.current_state}\n")
        lines.append(f"**å»ºè®®æ”¹è¿›**:\n{opp.proposed_change}\n")
        lines.append(f"**é¢„æœŸå½±å“**:\n{opp.expected_impact}\n")

        if opp.affected_modules:
            lines.append("**å—å½±å“æ¨¡å—**:")
            for module in opp.affected_modules:
                lines.append(f"- `{module}`")
            lines.append("")

        lines.append("---\n")

        return lines

    def _format_merge_recommendation(self, rec: MergeRecommendation) -> List[str]:
        """æ ¼å¼åŒ–åˆå¹¶å»ºè®®"""
        lines = []

        risk_emoji = {
            "low": "ğŸŸ¢",
            "medium": "ğŸŸ¡",
            "high": "ğŸ”´"
        }
        emoji = risk_emoji.get(rec.risk_level, "âšª")

        lines.append(f"### {emoji} {rec.title}\n")
        lines.append(f"**ID**: {rec.id}")
        lines.append(f"**é£é™©çº§åˆ«**: {rec.risk_level.upper()}")
        lines.append(f"**å·¥ä½œé‡**: {rec.effort_estimate}\n")

        lines.append("**å¾…åˆå¹¶æ¨¡å—**:")
        for module in rec.modules_to_merge:
            lines.append(f"- `{module}`")
        lines.append("")

        if rec.new_module_name:
            lines.append(f"**æ–°æ¨¡å—å**: `{rec.new_module_name}`\n")

        lines.append(f"**é€šç”¨åŠŸèƒ½**:\n{rec.common_functionality}\n")
        lines.append(f"**åˆå¹¶ç­–ç•¥**:\n{rec.merge_strategy}\n")

        if rec.migration_steps:
            lines.append("**è¿ç§»æ­¥éª¤**:")
            for i, step in enumerate(rec.migration_steps, 1):
                lines.append(f"{i}. {step}")
            lines.append("")

        lines.append("---\n")

        return lines


if __name__ == "__main__":
    # æµ‹è¯•
    output_dir = "/opt/claude/mystocks_spec/docs/function-classification-manual"
    writer = MarkdownWriter(output_dir)
    print(f"Markdown writer initialized with output dir: {output_dir}")
