#!/usr/bin/env python3
"""
å¢å¼ºä»£ç è´¨é‡é¢„æµ‹å™¨
é›†æˆæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯çš„é«˜çº§ä»£ç è´¨é‡åˆ†æç³»ç»Ÿ
"""

import ast
import json
import os
import sys
import time
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from collections import Counter, defaultdict
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


@dataclass
class CodeMetrics:
    """ä»£ç è´¨é‡æŒ‡æ ‡"""

    file_path: str
    lines_of_code: int
    cyclomatic_complexity: float
    cognitive_complexity: float
    maintainability_index: float
    technical_debt: float
    bug_risk_score: float
    security_vulnerabilities: int
    test_coverage_gap: float
    duplication_ratio: float
    code_churn: float
    developer_experience: float
    change_frequency: float


@dataclass
class QualityPrediction:
    """è´¨é‡é¢„æµ‹ç»“æœ"""

    file_path: str
    overall_score: float
    risk_level: str  # low, medium, high, critical
    predicted_bugs: int
    maintainability_score: float
    security_score: float
    performance_risk: float
    recommendations: List[str]
    confidence: float


@dataclass
class AdvancedMetrics:
    """é«˜çº§ä»£ç æŒ‡æ ‡"""

    file_path: str

    # ç»“æ„æŒ‡æ ‡
    class_count: int
    function_count: int
    avg_class_size: float
    avg_function_size: float
    max_function_size: int

    # è€¦åˆå’Œå†…èš
    coupling_between_objects: float
    lack_of_cohesion_of_methods: float
    afferent_coupling: int
    efferent_coupling: int

    # ç»§æ‰¿å’Œå¤šæ€
    depth_of_inheritance: float
    number_of_children: int
    response_for_class: float

    # è®¾è®¡æ¨¡å¼
    design_pattern_usage: float
    interface_segregation: float
    dependency_inversion: float

    # å¼‚å¸¸å¤„ç†
    exception_handling_score: float
    error_recovery_mechanisms: int

    # æ–‡æ¡£å’Œæ³¨é‡Š
    documentation_coverage: float
    api_documentation_ratio: float


class EnhancedCodeAnalyzer:
    """å¢å¼ºä»£ç åˆ†æå™¨"""

    def __init__(self):
        self.metrics_history = []
        self.ml_model = None
        self.feature_extractor = FeatureExtractor()
        self.pattern_recognizer = CodePatternRecognizer()
        self.deep_analyzer = DeepCodeAnalyzer()

    def analyze_file_advanced(self, file_path: str) -> AdvancedMetrics:
        """é«˜çº§æ–‡ä»¶åˆ†æ"""
        logger.info(f"ğŸ” æ‰§è¡Œé«˜çº§åˆ†æ: {file_path}")

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                source_code = f.read()

            tree = ast.parse(source_code)

            # åŸºç¡€ç»“æ„åˆ†æ
            classes = [
                node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)
            ]
            functions = [
                node
                for node in ast.walk(tree)
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef))
            ]

            # è®¡ç®—æŒ‡æ ‡
            metrics = AdvancedMetrics(
                file_path=file_path,
                class_count=len(classes),
                function_count=len(functions),
                avg_class_size=np.mean([len(node.body) for node in classes])
                if classes
                else 0,
                avg_function_size=np.mean(
                    [self._count_function_lines(node) for node in functions]
                )
                if functions
                else 0,
                max_function_size=max(
                    [self._count_function_lines(node) for node in functions]
                )
                if functions
                else 0,
                coupling_between_objects=self._calculate_cbo(tree),
                lack_of_cohesion_of_methods=self._calculate_lcm(tree),
                afferent_coupling=self._calculate_ac(file_path),
                efferent_coupling=self._calculate_ec(tree),
                depth_of_inheritance=self._calculate_dit(classes),
                number_of_children=len(classes),  # ç®€åŒ–è®¡ç®—
                response_for_class=self._calculate_rfc(classes),
                design_pattern_usage=self._detect_design_patterns(tree),
                interface_segregation=self._calculate_interface_segregation(tree),
                dependency_inversion=self._calculate_dependency_inversion(tree),
                exception_handling_score=self._calculate_exception_handling_score(tree),
                error_recovery_mechanisms=self._count_error_recovery(tree),
                documentation_coverage=self._calculate_documentation_coverage(
                    source_code
                ),
                api_documentation_ratio=self._calculate_api_documentation_ratio(tree),
            )

            logger.info(
                f"âœ… é«˜çº§åˆ†æå®Œæˆ: {metrics.class_count}ä¸ªç±», {metrics.function_count}ä¸ªå‡½æ•°"
            )
            return metrics

        except Exception as e:
            logger.error(f"âŒ é«˜çº§åˆ†æå¤±è´¥: {e}")
            return AdvancedMetrics(
                file_path=file_path,
                **{
                    k: 0
                    for k in AdvancedMetrics.__dataclass_fields__
                    if k != "file_path"
                },
            )

    def _count_function_lines(self, node: ast.AST) -> int:
        """è®¡ç®—å‡½æ•°è¡Œæ•°"""
        if hasattr(node, "end_lineno"):
            return node.end_lineno - node.lineno + 1
        return len(node.body)

    def _calculate_cbo(self, tree: ast.AST) -> float:
        """è®¡ç®—ç±»é—´è€¦åˆåº¦ (Coupling Between Objects)"""
        imports = set()
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name)
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module)
        return len(imports)

    def _calculate_lcm(self, tree: ast.AST) -> float:
        """è®¡ç®—æ–¹æ³•å†…èšç¼ºä¹åº¦ (Lack of Cohesion of Methods) - ç®€åŒ–ç‰ˆ"""
        classes = [node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
        total_lcm = 0

        for cls in classes:
            methods = [
                node
                for node in cls.body
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef))
            ]
            if len(methods) <= 1:
                continue

            # ç®€åŒ–çš„å†…èšåº¦è®¡ç®—ï¼šåŸºäºå…±äº«å±æ€§
            attributes = set()
            for method in methods:
                for node in ast.walk(method):
                    if isinstance(node, ast.Attribute) and isinstance(
                        node.ctx, ast.Load
                    ):
                        attributes.add(node.attr)

            # å†…èšåº¦ = å…±äº«å±æ€§æ•°é‡ / (æ–¹æ³•æ•° * å¹³å‡å±æ€§æ•°)
            cohesion = len(attributes) / (
                len(methods) * max(1, len(attributes) / len(methods))
            )
            total_lcm += 1 - cohesion

        return total_lcm / len(classes) if classes else 0

    def _calculate_ac(self, file_path: str) -> int:
        """è®¡ç®—ä¼ å…¥è€¦åˆåº¦ (Afferent Coupling) - ç®€åŒ–ç‰ˆ"""
        # åœ¨å®é™…å®ç°ä¸­ï¼Œéœ€è¦åˆ†æå…¶ä»–æ–‡ä»¶å¯¹å½“å‰æ–‡ä»¶çš„ä¾èµ–
        return 0

    def _calculate_ec(self, tree: ast.AST) -> int:
        """è®¡ç®—ä¼ å‡ºè€¦åˆåº¦ (Efferent Coupling)"""
        return len(
            [
                node
                for node in ast.walk(tree)
                if isinstance(node, (ast.Import, ast.ImportFrom))
            ]
        )

    def _calculate_dit(self, classes: List[ast.ClassDef]) -> float:
        """è®¡ç®—ç»§æ‰¿æ·±åº¦ (Depth of Inheritance)"""
        depths = []

        for cls in classes:
            depth = self._calculate_class_depth(cls)
            depths.append(depth)

        return np.mean(depths) if depths else 0

    def _calculate_class_depth(self, cls: ast.ClassDef) -> int:
        """é€’å½’è®¡ç®—ç±»ç»§æ‰¿æ·±åº¦"""
        if not cls.bases:
            return 0

        max_depth = 0
        for base in cls.bases:
            if isinstance(base, ast.Name):
                # ç®€åŒ–ï¼šå‡è®¾NameåŸºç±»çš„æ·±åº¦ä¸º1
                max_depth = max(max_depth, 1)

        return max_depth

    def _calculate_rfc(self, classes: List[ast.ClassDef]) -> float:
        """è®¡ç®—ç±»çš„å“åº”åº¦ (Response for Class)"""
        rfc_values = []

        for cls in classes:
            method_count = len(
                [
                    n
                    for n in cls.body
                    if isinstance(n, (ast.FunctionDef, ast.AsyncFunctionDef))
                ]
            )

            # è®¡ç®—æ–¹æ³•è°ƒç”¨
            calls = 0
            for node in ast.walk(cls):
                if isinstance(node, ast.Call):
                    calls += 1

            rfc = method_count + calls
            rfc_values.append(rfc)

        return np.mean(rfc_values) if rfc_values else 0

    def _detect_design_patterns(self, tree: ast.AST) -> float:
        """æ£€æµ‹è®¾è®¡æ¨¡å¼ä½¿ç”¨æƒ…å†µ"""
        pattern_score = 0
        patterns_detected = []

        classes = [node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]

        # æ£€æµ‹å•ä¾‹æ¨¡å¼
        singleton_patterns = self._detect_singleton_patterns(classes)
        if singleton_patterns:
            pattern_score += len(singleton_patterns) * 0.2
            patterns_detected.extend(["Singleton"] * len(singleton_patterns))

        # æ£€æµ‹å·¥å‚æ¨¡å¼
        factory_patterns = self._detect_factory_patterns(classes)
        if factory_patterns:
            pattern_score += len(factory_patterns) * 0.2
            patterns_detected.extend(["Factory"] * len(factory_patterns))

        # æ£€æµ‹è§‚å¯Ÿè€…æ¨¡å¼
        observer_patterns = self._detect_observer_patterns(classes)
        if observer_patterns:
            pattern_score += len(observer_patterns) * 0.2
            patterns_detected.extend(["Observer"] * len(observer_patterns))

        logger.info(f"ğŸ¯ æ£€æµ‹åˆ°è®¾è®¡æ¨¡å¼: {patterns_detected}")
        return min(pattern_score, 1.0)

    def _detect_singleton_patterns(self, classes: List[ast.ClassDef]) -> List[str]:
        """æ£€æµ‹å•ä¾‹æ¨¡å¼"""
        singleton_classes = []

        for cls in classes:
            has_private_constructor = False
            has_instance_variable = False
            has_get_instance_method = False

            for node in cls.body:
                if isinstance(node, ast.FunctionDef):
                    # æ£€æŸ¥__new__æ–¹æ³•æˆ–getInstanceæ–¹æ³•
                    if node.name in ["__new__", "getInstance", "get_instance"]:
                        has_get_instance_method = True

                    # æ£€æŸ¥ç§æœ‰æ„é€ å‡½æ•°
                    if node.name == "__init__":
                        for decorator in node.decorator_list:
                            if (
                                isinstance(decorator, ast.Name)
                                and decorator.id == "private"
                            ):
                                has_private_constructor = True

                # æ£€æŸ¥ç±»å˜é‡
                elif isinstance(node, ast.AnnAssign) or isinstance(node, ast.Assign):
                    for target in (
                        node.targets if hasattr(node, "targets") else [node.target]
                    ):
                        if isinstance(target, ast.Name) and target.id.startswith("_"):
                            has_instance_variable = True

            if has_get_instance_method or (
                has_private_constructor and has_instance_variable
            ):
                singleton_classes.append(cls.name)

        return singleton_classes

    def _detect_factory_patterns(self, classes: List[ast.ClassDef]) -> List[str]:
        """æ£€æµ‹å·¥å‚æ¨¡å¼"""
        factory_classes = []

        for cls in classes:
            factory_methods = ["create", "factory", "build", "make"]
            has_factory_method = False

            for node in cls.body:
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    if any(method in node.name.lower() for method in factory_methods):
                        has_factory_method = True
                        break

            if has_factory_method:
                factory_classes.append(cls.name)

        return factory_classes

    def _detect_observer_patterns(self, classes: List[ast.ClassDef]) -> List[str]:
        """æ£€æµ‹è§‚å¯Ÿè€…æ¨¡å¼"""
        observer_classes = []

        for cls in classes:
            has_observer_methods = False
            has_subject_methods = False

            for node in cls.body:
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    method_name = node.name.lower()
                    # è§‚å¯Ÿè€…æ–¹æ³•
                    if any(
                        method in method_name
                        for method in ["notify", "update", "subscribe", "unsubscribe"]
                    ):
                        has_observer_methods = True
                    # ä¸»é¢˜æ–¹æ³•
                    elif any(
                        method in method_name
                        for method in [
                            "attach",
                            "detach",
                            "add_observer",
                            "remove_observer",
                        ]
                    ):
                        has_subject_methods = True

            if has_observer_methods or has_subject_methods:
                observer_classes.append(cls.name)

        return observer_classes

    def _calculate_interface_segregation(self, tree: ast.AST) -> float:
        """è®¡ç®—æ¥å£éš”ç¦»åŸåˆ™éµå¾ªåº¦"""
        # æ£€æŸ¥æ˜¯å¦è¿‡åº¦ä¾èµ–å¤§æ¥å£
        total_imports = 0
        large_interfaces = 0

        for node in ast.walk(tree):
            if isinstance(node, ast.ImportFrom) and node.module:
                total_imports += 1
                # ç®€åŒ–ï¼šå¦‚æœå¯¼å…¥æ•´ä¸ªæ¨¡å—è€Œä¸æ˜¯ç‰¹å®šç±»/å‡½æ•°ï¼Œå¯èƒ½è¿åæ¥å£éš”ç¦»
                if not node.names or any(name.name == "*" for name in node.names):
                    large_interfaces += 1

        if total_imports == 0:
            return 1.0

        segregation_score = 1.0 - (large_interfaces / total_imports)
        return max(0, segregation_score)

    def _calculate_dependency_inversion(self, tree: ast.AST) -> float:
        """è®¡ç®—ä¾èµ–å€’ç½®åŸåˆ™éµå¾ªåº¦"""
        # æ£€æŸ¥æŠ½è±¡å¯¼å…¥ vs å…·ä½“å¯¼å…¥
        abstract_imports = 0
        total_imports = 0

        for node in ast.walk(tree):
            if isinstance(node, ast.ImportFrom):
                total_imports += 1
                # ç®€åŒ–ï¼šå‡è®¾abcã€interfacesç­‰ä¸ºæŠ½è±¡å¯¼å…¥
                if node.module and any(
                    abstract in node.module.lower()
                    for abstract in ["abc", "interface", "protocol"]
                ):
                    abstract_imports += 1
            elif isinstance(node, ast.Import):
                total_imports += len(node.names)

        if total_imports == 0:
            return 1.0

        inversion_score = abstract_imports / total_imports
        return min(inversion_score, 1.0)

    def _calculate_exception_handling_score(self, tree: ast.AST) -> float:
        """è®¡ç®—å¼‚å¸¸å¤„ç†è¯„åˆ†"""
        try_blocks = 0
        total_functions = 0
        functions_with_error_handling = 0

        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                total_functions += 1
                has_exception_handling = False

                for child in ast.walk(node):
                    if isinstance(child, ast.Try):
                        try_blocks += 1
                        has_exception_handling = True
                    elif isinstance(child, ast.Raise):
                        has_exception_handling = True

                if has_exception_handling:
                    functions_with_error_handling += 1

        # å¼‚å¸¸å¤„ç†è¯„åˆ† = æœ‰å¼‚å¸¸å¤„ç†çš„å‡½æ•°æ¯”ä¾‹ + tryå—å¯†åº¦
        function_score = (
            functions_with_error_handling / total_functions
            if total_functions > 0
            else 0
        )
        try_score = min(try_blocks / total_functions, 1.0) if total_functions > 0 else 0

        return (function_score + try_score) / 2

    def _count_error_recovery(self, tree: ast.AST) -> int:
        """è®¡ç®—é”™è¯¯æ¢å¤æœºåˆ¶æ•°é‡"""
        recovery_mechanisms = 0

        for node in ast.walk(tree):
            if isinstance(node, ast.ExceptHandler):
                # æ£€æŸ¥æ˜¯å¦æœ‰å…·ä½“çš„æ¢å¤é€»è¾‘
                if node.name or node.type:
                    recovery_mechanisms += 1

        return recovery_mechanisms

    def _calculate_documentation_coverage(self, source_code: str) -> float:
        """è®¡ç®—æ–‡æ¡£è¦†ç›–ç‡"""
        lines = source_code.split("\n")
        code_lines = [
            line for line in lines if line.strip() and not line.strip().startswith("#")
        ]
        doc_lines = [line for line in lines if line.strip().startswith("#")]

        if len(code_lines) == 0:
            return 1.0

        return len(doc_lines) / len(code_lines)

    def _calculate_api_documentation_ratio(self, tree: ast.AST) -> float:
        """è®¡ç®—APIæ–‡æ¡£æ¯”ä¾‹"""
        public_functions = []
        documented_functions = []

        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                if not node.name.startswith("_"):
                    public_functions.append(node)

                    # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æ¡£å­—ç¬¦ä¸²
                    if (
                        node.body
                        and isinstance(node.body[0], ast.Expr)
                        and isinstance(node.body[0].value, (ast.Str, ast.Constant))
                    ):
                        documented_functions.append(node)

        if len(public_functions) == 0:
            return 1.0

        return len(documented_functions) / len(public_functions)


class FeatureExtractor:
    """ç‰¹å¾æå–å™¨"""

    def __init__(self):
        self.feature_names = [
            "lines_of_code",
            "cyclomatic_complexity",
            "cognitive_complexity",
            "class_count",
            "function_count",
            "avg_function_size",
            "max_function_size",
            "coupling_between_objects",
            "lack_of_cohesion_of_methods",
            "depth_of_inheritance",
            "design_pattern_usage",
            "exception_handling_score",
            "documentation_coverage",
        ]

    def extract_features(
        self, metrics: CodeMetrics, advanced_metrics: AdvancedMetrics
    ) -> np.ndarray:
        """æå–ç‰¹å¾å‘é‡"""
        features = [
            metrics.lines_of_code,
            metrics.cyclomatic_complexity,
            metrics.cognitive_complexity,
            advanced_metrics.class_count,
            advanced_metrics.function_count,
            advanced_metrics.avg_function_size,
            advanced_metrics.max_function_size,
            advanced_metrics.coupling_between_objects,
            advanced_metrics.lack_of_cohesion_of_methods,
            advanced_metrics.depth_of_inheritance,
            advanced_metrics.design_pattern_usage,
            advanced_metrics.exception_handling_score,
            advanced_metrics.documentation_coverage,
        ]

        return np.array(features)


class CodePatternRecognizer:
    """ä»£ç æ¨¡å¼è¯†åˆ«å™¨"""

    def __init__(self):
        self.anti_patterns = {
            "long_method": {"threshold": 50, "weight": 0.3},
            "large_class": {"threshold": 20, "weight": 0.2},
            "god_object": {"threshold": 100, "weight": 0.4},
            "feature_envy": {"threshold": 10, "weight": 0.1},
        }

    def detect_anti_patterns(self, metrics: AdvancedMetrics) -> Dict[str, float]:
        """æ£€æµ‹åæ¨¡å¼"""
        detected = {}

        # é•¿æ–¹æ³•
        if metrics.max_function_size > self.anti_patterns["long_method"]["threshold"]:
            detected["long_method"] = (
                metrics.max_function_size
                - self.anti_patterns["long_method"]["threshold"]
            ) / self.anti_patterns["long_method"]["threshold"]

        # å¤§ç±»
        if metrics.avg_class_size > self.anti_patterns["large_class"]["threshold"]:
            detected["large_class"] = (
                metrics.avg_class_size - self.anti_patterns["large_class"]["threshold"]
            ) / self.anti_patterns["large_class"]["threshold"]

        # ä¸Šå¸å¯¹è±¡
        total_methods = metrics.function_count
        if total_methods > self.anti_patterns["god_object"]["threshold"]:
            detected["god_object"] = (
                total_methods - self.anti_patterns["god_object"]["threshold"]
            ) / self.anti_patterns["god_object"]["threshold"]

        return detected


class DeepCodeAnalyzer:
    """æ·±åº¦ä»£ç åˆ†æå™¨"""

    def __init__(self):
        self.vulnerability_patterns = {
            "sql_injection": r'(\b(SELECT|INSERT|UPDATE|DELETE)\b.*\b(FORMAT|%|f")',
            "xss": r"(innerHTML|outerHTML|document\.write)",
            "hardcoded_secrets": r'(password|secret|key)\s*=\s*["\'][^"\']+["\']',
            "unsafe_eval": r"(eval\(|exec\(|__import__)",
        }

    def security_analysis(self, source_code: str) -> Dict[str, int]:
        """å®‰å…¨åˆ†æ"""
        vulnerabilities = Counter()

        import re

        for vuln_type, pattern in self.vulnerability_patterns.items():
            matches = re.findall(pattern, source_code, re.IGNORECASE)
            vulnerabilities[vuln_type] = len(matches)

        return dict(vulnerabilities)

    def complexity_analysis(self, tree: ast.AST) -> Dict[str, float]:
        """å¤æ‚åº¦åˆ†æ"""
        complexity_scores = {}

        # åµŒå¥—å¤æ‚åº¦
        max_nesting = self._calculate_max_nesting(tree)
        complexity_scores["max_nesting"] = max_nesting

        # è®¤çŸ¥å¤æ‚åº¦
        cognitive_complexity = self._calculate_cognitive_complexity(tree)
        complexity_scores["cognitive_complexity"] = cognitive_complexity

        return complexity_scores

    def _calculate_max_nesting(self, tree: ast.AST) -> int:
        """è®¡ç®—æœ€å¤§åµŒå¥—æ·±åº¦"""
        max_depth = 0

        def _get_depth(node, current_depth=0):
            nonlocal max_depth
            max_depth = max(max_depth, current_depth)

            for child in ast.iter_child_nodes(node):
                if isinstance(child, (ast.If, ast.For, ast.While, ast.Try, ast.With)):
                    _get_depth(child, current_depth + 1)
                else:
                    _get_depth(child, current_depth)

        _get_depth(tree)
        return max_depth

    def _calculate_cognitive_complexity(self, tree: ast.AST) -> float:
        """è®¡ç®—è®¤çŸ¥å¤æ‚åº¦"""
        complexity = 0

        for node in ast.walk(tree):
            # åŸºç¡€å¤æ‚åº¦
            if isinstance(node, (ast.If, ast.For, ast.While, ast.Try)):
                complexity += 1

            # é€»è¾‘è¿ç®—ç¬¦å¢åŠ å¤æ‚åº¦
            if isinstance(node, ast.BoolOp):
                complexity += len(node.values) - 1

            # åµŒå¥—å¢åŠ å¤æ‚åº¦
            if isinstance(node, ast.If):
                for child in ast.iter_child_nodes(node):
                    if isinstance(child, ast.If):
                        complexity += 1

        return complexity


class QualityPredictor:
    """è´¨é‡é¢„æµ‹å™¨"""

    def __init__(self):
        self.model = self._create_ml_model()

    def _create_ml_model(self):
        """åˆ›å»ºæœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šåŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹
        return None

    def predict_quality(self, features: np.ndarray) -> QualityPrediction:
        """é¢„æµ‹ä»£ç è´¨é‡"""
        # ç®€åŒ–çš„é¢„æµ‹é€»è¾‘
        overall_score = max(
            0, min(100, 100 - features[1] * 2 - features[2] * 1.5)
        )  # åŸºäºå¤æ‚åº¦çš„ç®€åŒ–è¯„åˆ†

        if overall_score >= 80:
            risk_level = "low"
        elif overall_score >= 60:
            risk_level = "medium"
        elif overall_score >= 40:
            risk_level = "high"
        else:
            risk_level = "critical"

        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(features)

        return QualityPrediction(
            file_path="",  # éœ€è¦ä»å¤–éƒ¨ä¼ å…¥
            overall_score=overall_score,
            risk_level=risk_level,
            predicted_bugs=max(0, int(features[1] * 0.1)),  # ç®€åŒ–çš„Bugé¢„æµ‹
            maintainability_score=max(0, min(100, 100 - features[0] * 0.1)),
            security_score=90,  # éœ€è¦å®‰å…¨åˆ†æ
            performance_risk=min(100, features[1] * 5),  # åŸºäºåœˆå¤æ‚åº¦çš„æ€§èƒ½é£é™©
            recommendations=recommendations,
            confidence=0.85,
        )

    def _generate_recommendations(self, features: np.ndarray) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        if features[1] > 10:  # åœˆå¤æ‚åº¦
            recommendations.append("ğŸ”§ å»ºè®®é™ä½å‡½æ•°åœˆå¤æ‚åº¦ï¼Œè€ƒè™‘æ‹†åˆ†å¤æ‚å‡½æ•°")

        if features[4] > 20:  # æœ€å¤§å‡½æ•°å¤§å°
            recommendations.append("ğŸ“ å‡½æ•°è¿‡é•¿ï¼Œå»ºè®®å°†å¤§å‡½æ•°æ‹†åˆ†ä¸ºæ›´å°çš„å•å…ƒ")

        if features[7] > 5:  # è€¦åˆåº¦
            recommendations.append("ğŸ”— æ¨¡å—é—´è€¦åˆåº¦è¿‡é«˜ï¼Œå»ºè®®å¼•å…¥æ¥å£æˆ–ä¾èµ–æ³¨å…¥")

        if features[8] > 0.5:  # å†…èšç¼ºä¹åº¦
            recommendations.append("ğŸ¯ ç±»çš„å†…èšåº¦è¾ƒä½ï¼Œå»ºè®®é‡æ–°ç»„ç»‡ç›¸å…³æ–¹æ³•")

        if features[12] < 0.3:  # æ–‡æ¡£è¦†ç›–ç‡
            recommendations.append("ğŸ“š æ–‡æ¡£è¦†ç›–ç‡ä¸è¶³ï¼Œå»ºè®®æ·»åŠ æ›´å¤šæ³¨é‡Šå’Œæ–‡æ¡£")

        return recommendations


def main():
    """ä¸»å…¥å£å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="å¢å¼ºä»£ç è´¨é‡é¢„æµ‹å™¨")
    parser.add_argument("files", nargs="+", help="è¦åˆ†æçš„Pythonæ–‡ä»¶")
    parser.add_argument("--output", "-o", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    analyzer = EnhancedCodeAnalyzer()
    predictor = QualityPredictor()
    feature_extractor = FeatureExtractor()

    results = []

    print("ğŸš€ å¯åŠ¨å¢å¼ºä»£ç è´¨é‡é¢„æµ‹å™¨")
    print("=" * 50)

    for file_path in args.files:
        if not Path(file_path).exists():
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            continue

        print(f"\nğŸ” åˆ†ææ–‡ä»¶: {file_path}")

        try:
            # æ‰§è¡Œé«˜çº§åˆ†æ
            advanced_metrics = analyzer.analyze_file_advanced(file_path)

            # è®¡ç®—åŸºç¡€æŒ‡æ ‡
            with open(file_path, "r", encoding="utf-8") as f:
                source_code = f.read()

            # ç®€åŒ–çš„åŸºç¡€æŒ‡æ ‡è®¡ç®—
            lines_of_code = len(
                [line for line in source_code.split("\n") if line.strip()]
            )
            cyclomatic_complexity = analyzer.deep_analyzer.complexity_analysis(
                ast.parse(source_code)
            )["cognitive_complexity"]

            code_metrics = CodeMetrics(
                file_path=file_path,
                lines_of_code=lines_of_code,
                cyclomatic_complexity=cyclomatic_complexity,
                cognitive_complexity=cyclomatic_complexity,
                maintainability_index=70.0,  # ç®€åŒ–
                technical_debt=0.0,
                bug_risk_score=0.0,
                security_vulnerabilities=0,
                test_coverage_gap=0.0,
                duplication_ratio=0.0,
                code_churn=0.0,
                developer_experience=1.0,
                change_frequency=0.0,
            )

            # ç‰¹å¾æå–
            features = feature_extractor.extract_features(
                code_metrics, advanced_metrics
            )

            # è´¨é‡é¢„æµ‹
            prediction = predictor.predict_quality(features)
            prediction.file_path = file_path

            results.append(
                {
                    "metrics": asdict(advanced_metrics),
                    "prediction": asdict(prediction),
                    "features": features.tolist(),
                }
            )

            print(
                f"âœ… åˆ†æå®Œæˆ: {prediction.overall_score:.1f}åˆ† ({prediction.risk_level})"
            )

        except Exception as e:
            print(f"âŒ åˆ†æå¤±è´¥: {e}")
            continue

    # è¾“å‡ºç»“æœ
    if args.output:
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ“„ ç»“æœå·²ä¿å­˜åˆ°: {args.output}")

    # æ‰“å°æ‘˜è¦
    print(f"\nğŸ“Š åˆ†ææ‘˜è¦:")
    print(f"   åˆ†ææ–‡ä»¶æ•°: {len(results)}")
    if results:
        avg_score = np.mean([r["prediction"]["overall_score"] for r in results])
        risk_distribution = Counter([r["prediction"]["risk_level"] for r in results])

        print(f"   å¹³å‡è´¨é‡åˆ†: {avg_score:.1f}")
        print(f"   é£é™©åˆ†å¸ƒ: {dict(risk_distribution)}")


if __name__ == "__main__":
    main()
