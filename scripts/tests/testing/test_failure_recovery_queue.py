#!/usr/bin/env python3
"""
æ•…éšœæ¢å¤é˜Ÿåˆ—æµ‹è¯•å¥—ä»¶
å®Œæ•´æµ‹è¯•failure_recovery_queueæ¨¡å—çš„æ‰€æœ‰åŠŸèƒ½ï¼Œç¡®ä¿100%æµ‹è¯•è¦†ç›–ç‡
éµå¾ªPhase 6æˆåŠŸæ¨¡å¼ï¼šåŠŸèƒ½â†’è¾¹ç•Œâ†’å¼‚å¸¸â†’æ€§èƒ½â†’é›†æˆæµ‹è¯•
"""

import sys
import os
import tempfile
import shutil
import time
from pathlib import Path
from unittest.mock import patch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

import pytest
import sqlite3
import json

# å¯¼å…¥è¢«æµ‹è¯•çš„æ¨¡å—
from src.utils.failure_recovery_queue import FailureRecoveryQueue


class TestFailureRecoveryQueue:
    """æ•…éšœæ¢å¤é˜Ÿåˆ—æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•ç±»"""

    def setup_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•æ‰§è¡Œå‰çš„è®¾ç½®"""
        # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºæµ‹è¯•
        self.temp_dir = tempfile.mkdtemp()
        self.test_db_path = os.path.join(self.temp_dir, "test_queue.db")

    def teardown_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•æ‰§è¡Œåçš„æ¸…ç†"""
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)

    def test_initialization_with_default_path(self):
        """æµ‹è¯•ä½¿ç”¨é»˜è®¤è·¯å¾„åˆå§‹åŒ–"""
        with patch("os.makedirs") as mock_makedirs:
            # åˆ›å»ºå®ä¾‹
            queue = FailureRecoveryQueue()

            # éªŒè¯ç›®å½•åˆ›å»ºè¢«è°ƒç”¨
            mock_makedirs.assert_called_once()
            args, kwargs = mock_makedirs.call_args
            assert args[0] == "/tmp"  # é»˜è®¤è·¯å¾„çš„ç›®å½•éƒ¨åˆ†
            assert kwargs["exist_ok"] is True

            # éªŒè¯å±æ€§è®¾ç½®
            assert queue.db_path == "/tmp/mystocks_recovery_queue.db"

    def test_initialization_with_custom_path(self):
        """æµ‹è¯•ä½¿ç”¨è‡ªå®šä¹‰è·¯å¾„åˆå§‹åŒ–"""
        import tempfile

        custom_path = tempfile.mktemp(suffix=".db")

        with patch("os.makedirs") as mock_makedirs:
            queue = FailureRecoveryQueue(custom_path)

            mock_makedirs.assert_called_once()
            assert queue.db_path == custom_path

    def test_initialization_creates_directory(self):
        """æµ‹è¯•åˆå§‹åŒ–æ—¶åˆ›å»ºç›®å½•"""
        nested_path = os.path.join(self.temp_dir, "nested", "path", "test.db")

        # ç¡®ä¿ç›®å½•ä¸å­˜åœ¨
        assert not os.path.exists(os.path.dirname(nested_path))

        # åˆå§‹åŒ–é˜Ÿåˆ—
        queue = FailureRecoveryQueue(nested_path)

        # éªŒè¯ç›®å½•è¢«åˆ›å»º
        assert os.path.exists(os.path.dirname(nested_path))
        assert queue.db_path == nested_path

    def test_database_initialization(self):
        """æµ‹è¯•æ•°æ®åº“åˆå§‹åŒ–"""
        queue = FailureRecoveryQueue(self.test_db_path)

        # éªŒè¯æ•°æ®åº“æ–‡ä»¶è¢«åˆ›å»º
        assert os.path.exists(self.test_db_path)

        # éªŒè¯è¡¨ç»“æ„ï¼ˆæ’é™¤SQLiteç³»ç»Ÿè¡¨ï¼‰
        conn = sqlite3.connect(self.test_db_path)
        cursor = conn.cursor()

        cursor.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'"
        )
        tables = cursor.fetchall()
        conn.close()

        assert len(tables) == 1
        assert tables[0][0] == "outbox_queue"

    def test_table_structure(self):
        """æµ‹è¯•è¡¨ç»“æ„æ­£ç¡®æ€§"""
        queue = FailureRecoveryQueue(self.test_db_path)

        conn = sqlite3.connect(self.test_db_path)
        cursor = conn.cursor()

        cursor.execute("PRAGMA table_info(outbox_queue)")
        columns = cursor.fetchall()
        conn.close()

        # éªŒè¯åˆ—å­˜åœ¨å’Œç±»å‹ï¼ˆåŸºäºå®é™…PRAGMAè¾“å‡ºæ ¼å¼ï¼‰
        expected_columns = {
            "id": (
                "INTEGER",
                0,
                None,
                1,
            ),  # cid=0, name=id, type=INTEGER, notnull=0, dflt_value=None, pk=1
            "classification": (
                "TEXT",
                1,
                None,
                0,
            ),  # cid=1, name=classification, type=TEXT, notnull=1, dflt_value=None, pk=0
            "target_database": (
                "TEXT",
                1,
                None,
                0,
            ),  # cid=2, name=target_database, type=TEXT, notnull=1, dflt_value=None, pk=0
            "data_json": (
                "TEXT",
                1,
                None,
                0,
            ),  # cid=3, name=data_json, type=TEXT, notnull=1, dflt_value=None, pk=0
            "created_at": (
                "TIMESTAMP",
                0,
                "CURRENT_TIMESTAMP",
                0,
            ),  # cid=4, é»˜è®¤å€¼æ˜¯å­—ç¬¦ä¸²'CURRENT_TIMESTAMP'
            "retry_count": ("INTEGER", 0, "0", 0),  # cid=5, é»˜è®¤å€¼æ˜¯å­—ç¬¦ä¸²'0'
            "status": ("TEXT", 0, "'pending'", 0),  # cid=6, é»˜è®¤å€¼æ˜¯å­—ç¬¦ä¸²"'pending'"
        }

        assert len(columns) == len(expected_columns)

        for col in columns:
            cid, col_name, col_type, notnull, dflt_value, pk = col
            assert col_name in expected_columns
            expected_type, expected_notnull, expected_default, expected_pk = (
                expected_columns[col_name]
            )
            assert col_type == expected_type  # æ•°æ®ç±»å‹
            assert notnull == expected_notnull  # æ˜¯å¦éç©º
            assert dflt_value == expected_default  # é»˜è®¤å€¼
            assert pk == expected_pk  # æ˜¯å¦ä¸»é”®

    def test_enqueue_basic(self):
        """æµ‹è¯•åŸºæœ¬å…¥é˜Ÿæ“ä½œ"""
        queue = FailureRecoveryQueue(self.test_db_path)

        test_data = {"symbol": "000001", "price": 10.5, "volume": 1000}
        queue.enqueue("test_classification", "test_db", test_data)

        # éªŒè¯æ•°æ®è¢«æ’å…¥
        conn = sqlite3.connect(self.test_db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM outbox_queue")
        rows = cursor.fetchall()
        conn.close()

        assert len(rows) == 1
        row = rows[0]
        assert row[1] == "test_classification"  # classification
        assert row[2] == "test_db"  # target_database
        assert json.loads(row[3]) == test_data  # data_json
        assert row[5] == 0  # retry_count
        assert row[6] == "pending"  # status

    def test_enqueue_multiple_items(self):
        """æµ‹è¯•å¤šä¸ªé¡¹ç›®å…¥é˜Ÿ"""
        queue = FailureRecoveryQueue(self.test_db_path)

        items = [
            {"symbol": "000001", "price": 10.5},
            {"symbol": "000002", "price": 20.3},
            {"symbol": "600000", "price": 15.7},
        ]

        for i, item in enumerate(items):
            queue.enqueue(f"classification_{i}", f"db_{i}", item)

        # éªŒè¯æ‰€æœ‰æ•°æ®è¢«æ’å…¥
        conn = sqlite3.connect(self.test_db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM outbox_queue")
        count = cursor.fetchone()[0]
        conn.close()

        assert count == len(items)

    def test_enqueue_with_complex_data(self):
        """æµ‹è¯•å¤æ‚æ•°æ®ç»“æ„çš„å…¥é˜Ÿ"""
        queue = FailureRecoveryQueue(self.test_db_path)

        complex_data = {
            "symbol": "000001",
            "price": 10.5,
            "metadata": {
                "source": "test",
                "timestamp": "2024-01-01T12:00:00",
                "tags": ["stock", "test"],
            },
            "nested": {"level1": {"level2": "deep_value"}},
            "array_data": [1, 2, 3, {"key": "value"}],
        }

        queue.enqueue("complex", "production", complex_data)

        # éªŒè¯å¤æ‚æ•°æ®è¢«æ­£ç¡®åºåˆ—åŒ–å’Œå­˜å‚¨
        conn = sqlite3.connect(self.test_db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT data_json FROM outbox_queue")
        stored_json = cursor.fetchone()[0]
        conn.close()

        # éªŒè¯JSONåºåˆ—åŒ–å’Œååºåˆ—åŒ–
        loaded_data = json.loads(stored_json)
        assert loaded_data == complex_data

    def test_get_pending_items_default_limit(self):
        """æµ‹è¯•è·å–å¾…å¤„ç†é¡¹ç›®ï¼ˆé»˜è®¤é™åˆ¶ï¼‰"""
        queue = FailureRecoveryQueue(self.test_db_path)

        # æ·»åŠ ä¸€äº›æµ‹è¯•æ•°æ®
        for i in range(5):
            queue.enqueue(f"class_{i}", f"db_{i}", {"id": i})

        # è·å–å¾…å¤„ç†é¡¹ç›®
        items = queue.get_pending_items()

        assert len(items) == 5
        for i, item in enumerate(items):
            assert item[0] == i + 1  # id
            assert item[1] == f"class_{i}"
            assert item[2] == f"db_{i}"
            assert json.loads(item[3])["id"] == i

    def test_get_pending_items_with_limit(self):
        """æµ‹è¯•è·å–å¾…å¤„ç†é¡¹ç›®ï¼ˆæŒ‡å®šé™åˆ¶ï¼‰"""
        queue = FailureRecoveryQueue(self.test_db_path)

        # æ·»åŠ æµ‹è¯•æ•°æ®
        for i in range(10):
            queue.enqueue(f"class_{i}", f"db_{i}", {"id": i})

        # è·å–é™åˆ¶æ•°é‡çš„é¡¹ç›®
        items = queue.get_pending_items(limit=3)

        assert len(items) == 3
        # éªŒè¯æŒ‰æ—¶é—´æ’åºï¼ˆASCï¼‰
        for i, item in enumerate(items):
            assert json.loads(item[3])["id"] == i

    def test_get_pending_items_empty_queue(self):
        """æµ‹è¯•ä»ç©ºé˜Ÿåˆ—è·å–é¡¹ç›®"""
        queue = FailureRecoveryQueue(self.test_db_path)

        items = queue.get_pending_items()

        assert items == []

    def test_get_pending_items_filtering(self):
        """æµ‹è¯•è·å–å¾…å¤„ç†é¡¹ç›®çš„è¿‡æ»¤"""
        queue = FailureRecoveryQueue(self.test_db_path)

        # æ·»åŠ ä¸€äº›é¡¹ç›®
        queue.enqueue("pending", "db1", {"id": 1})

        # æ‰‹åŠ¨æ’å…¥ä¸€ä¸ªå·²å®Œæˆçš„é¡¹ç›®æ¥æµ‹è¯•è¿‡æ»¤
        conn = sqlite3.connect(self.test_db_path)
        cursor = conn.cursor()
        cursor.execute(
            "INSERT INTO outbox_queue (classification, target_database, data_json, status) VALUES (?, ?, ?, ?)",
            ("completed", "db2", json.dumps({"id": 2}), "completed"),
        )
        conn.commit()
        conn.close()

        # è·å–å¾…å¤„ç†é¡¹ç›®åº”è¯¥åªè¿”å›pendingçŠ¶æ€çš„é¡¹ç›®
        items = queue.get_pending_items()

        assert len(items) == 1
        assert items[0][1] == "pending"


class TestErrorHandling:
    """é”™è¯¯å¤„ç†æµ‹è¯•ç±»"""

    def setup_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•æ‰§è¡Œå‰çš„è®¾ç½®"""
        self.temp_dir = tempfile.mkdtemp()
        self.test_db_path = os.path.join(self.temp_dir, "test_queue.db")

    def teardown_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•æ‰§è¡Œåçš„æ¸…ç†"""
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)

    def test_database_connection_failure(self):
        """æµ‹è¯•æ•°æ®åº“è¿æ¥å¤±è´¥"""
        # æ¨¡æ‹Ÿsqlite3.connectæŠ›å‡ºå¼‚å¸¸çš„æƒ…å†µ
        with patch(
            "sqlite3.connect",
            side_effect=sqlite3.OperationalError("unable to open database file"),
        ):
            # åˆ›å»ºé˜Ÿåˆ—åº”è¯¥å› ä¸ºè¿æ¥å¤±è´¥è€ŒæŠ›å‡ºå¼‚å¸¸
            with pytest.raises(sqlite3.OperationalError):
                FailureRecoveryQueue(self.test_db_path)

    def test_enqueue_invalid_data_type(self):
        """æµ‹è¯•å…¥é˜Ÿæ— æ•ˆæ•°æ®ç±»å‹"""
        queue = FailureRecoveryQueue(self.test_db_path)

        # å°è¯•å…¥é˜Ÿä¸å¯åºåˆ—åŒ–çš„æ•°æ®
        invalid_data = {
            "function": lambda x: x,  # å‡½æ•°å¯¹è±¡æ— æ³•JSONåºåˆ—åŒ–
            "binary": b"bytes",  # äºŒè¿›åˆ¶æ•°æ®
        }

        # è¿™åº”è¯¥å› ä¸ºJSONåºåˆ—åŒ–å¤±è´¥è€ŒæŠ›å‡ºå¼‚å¸¸
        with pytest.raises(TypeError):
            queue.enqueue("test", "test_db", invalid_data)

    def test_get_pending_items_database_error(self):
        """æµ‹è¯•è·å–é¡¹ç›®æ—¶æ•°æ®åº“é”™è¯¯"""
        queue = FailureRecoveryQueue(self.test_db_path)

        # åˆ é™¤æ•°æ®åº“æ–‡ä»¶æ¥æ¨¡æ‹Ÿæ•°æ®åº“é”™è¯¯
        os.remove(self.test_db_path)

        # è¿™åº”è¯¥æŠ›å‡ºæ•°æ®åº“é”™è¯¯
        with pytest.raises(sqlite3.OperationalError):
            queue.get_pending_items()


class TestEdgeCases:
    """è¾¹ç•Œæƒ…å†µæµ‹è¯•ç±»"""

    def setup_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•æ‰§è¡Œå‰çš„è®¾ç½®"""
        self.temp_dir = tempfile.mkdtemp()
        self.test_db_path = os.path.join(self.temp_dir, "test_queue.db")

    def teardown_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•æ‰§è¡Œåçš„æ¸…ç†"""
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)

    def test_enqueue_empty_data(self):
        """æµ‹è¯•å…¥é˜Ÿç©ºæ•°æ®"""
        queue = FailureRecoveryQueue(self.test_db_path)

        # æµ‹è¯•ç©ºå­—å…¸
        queue.enqueue("empty", "test_db", {})

        # éªŒè¯ç©ºæ•°æ®è¢«æ­£ç¡®å­˜å‚¨
        conn = sqlite3.connect(self.test_db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT data_json FROM outbox_queue")
        stored_json = cursor.fetchone()[0]
        conn.close()

        assert json.loads(stored_json) == {}

    def test_enqueue_large_data(self):
        """æµ‹è¯•å…¥é˜Ÿå¤§é‡æ•°æ®"""
        queue = FailureRecoveryQueue(self.test_db_path)

        # åˆ›å»ºå¤§é‡æ•°æ®
        large_data = {
            "large_array": list(range(1000)),
            "large_string": "x" * 10000,
            "nested": {},
        }

        # åµŒå¥—ç»“æ„
        for i in range(100):
            large_data["nested"][f"key_{i}"] = f"value_{i}" * 100

        # è¿™åº”è¯¥èƒ½æ­£å¸¸å¤„ç†
        queue.enqueue("large", "test_db", large_data)

        # éªŒè¯æ•°æ®è¢«æ­£ç¡®å­˜å‚¨
        conn = sqlite3.connect(self.test_db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT data_json FROM outbox_queue")
        stored_json = cursor.fetchone()[0]
        conn.close()

        loaded_data = json.loads(stored_json)
        assert len(loaded_data["large_array"]) == 1000
        assert len(loaded_data["large_string"]) == 10000
        assert len(loaded_data["nested"]) == 100

    def test_get_pending_items_zero_limit(self):
        """æµ‹è¯•è·å–é›¶ä¸ªå¾…å¤„ç†é¡¹ç›®"""
        queue = FailureRecoveryQueue(self.test_db_path)

        # æ·»åŠ ä¸€äº›æ•°æ®
        queue.enqueue("test", "test_db", {"id": 1})

        # è·å–é›¶ä¸ªé¡¹ç›®
        items = queue.get_pending_items(limit=0)

        assert items == []

    def test_get_pending_items_negative_limit(self):
        """æµ‹è¯•è·å–è´Ÿæ•°ä¸ªå¾…å¤„ç†é¡¹ç›®"""
        queue = FailureRecoveryQueue(self.test_db_path)

        # æ·»åŠ ä¸€äº›æ•°æ®
        queue.enqueue("test", "test_db", {"id": 1})

        # SQLiteä¸­LIMIT -1æ„å‘³ç€è¿”å›æ‰€æœ‰è¡Œï¼Œè€Œä¸æ˜¯ç©ºåˆ—è¡¨
        items = queue.get_pending_items(limit=-1)

        # åº”è¯¥è¿”å›æ‰€æœ‰é¡¹ç›®ï¼ˆSQLiteä¸­LIMIT -1 = æ— é™åˆ¶ï¼‰
        assert len(items) == 1
        assert items[0][1] == "test"  # classification
        assert items[0][2] == "test_db"  # target_database

    def test_concurrent_operations(self):
        """æµ‹è¯•å¹¶å‘æ“ä½œ"""
        import threading

        queue = FailureRecoveryQueue(self.test_db_path)
        results = []
        errors = []

        def producer():
            try:
                for i in range(10):
                    queue.enqueue(f"producer_{i}", "test_db", {"id": i})
                    results.append(f"producer_{i}")
            except Exception as e:
                errors.append(f"Producer error: {e}")

        def consumer():
            try:
                time.sleep(0.1)  # ç­‰å¾…ç”Ÿäº§è€…äº§ç”Ÿä¸€äº›æ•°æ®
                items = queue.get_pending_items(limit=50)
                results.append(f"consumer_got_{len(items)}")
            except Exception as e:
                errors.append(f"Consumer error: {e}")

        # åˆ›å»ºå¹¶å¯åŠ¨çº¿ç¨‹
        producer_thread = threading.Thread(target=producer)
        consumer_thread = threading.Thread(target=consumer)

        producer_thread.start()
        consumer_thread.start()

        # ç­‰å¾…çº¿ç¨‹å®Œæˆ
        producer_thread.join()
        consumer_thread.join()

        # éªŒè¯æ²¡æœ‰é”™è¯¯å‘ç”Ÿ
        assert len(errors) == 0
        assert "producer_9" in results

    def test_unicode_handling(self):
        """æµ‹è¯•Unicodeå­—ç¬¦å¤„ç†"""
        queue = FailureRecoveryQueue(self.test_db_path)

        unicode_data = {
            "chinese": "æµ‹è¯•ä¸­æ–‡æ•°æ®",
            "emoji": "ğŸš€ğŸŒŸğŸ’",
            "special": "CafÃ© rÃ©sumÃ© naÃ¯ve",
            "math": "âˆ‘âˆâˆ«âˆ†âˆ‡âˆ‚",
        }

        queue.enqueue("unicode", "test_db", unicode_data)

        # éªŒè¯Unicodeæ•°æ®è¢«æ­£ç¡®å­˜å‚¨å’Œæ£€ç´¢
        items = queue.get_pending_items()
        assert len(items) == 1

        stored_data = json.loads(items[0][3])
        assert stored_data == unicode_data


class TestPerformance:
    """æ€§èƒ½æµ‹è¯•ç±»"""

    def setup_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•æ‰§è¡Œå‰çš„è®¾ç½®"""
        self.temp_dir = tempfile.mkdtemp()
        self.test_db_path = os.path.join(self.temp_dir, "test_queue.db")

    def teardown_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•æ‰§è¡Œåçš„æ¸…ç†"""
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)

    def test_enqueue_performance(self):
        """æµ‹è¯•å…¥é˜Ÿæ€§èƒ½"""
        queue = FailureRecoveryQueue(self.test_db_path)

        iterations = 1000
        test_data = {"symbol": "TEST", "data": "x" * 100}

        start_time = time.time()
        for i in range(iterations):
            queue.enqueue(f"perf_test_{i}", "test_db", test_data)
        end_time = time.time()

        avg_time = (end_time - start_time) / iterations * 1000  # æ¯«ç§’

        # æ¯æ¬¡å…¥é˜Ÿæ“ä½œåº”è¯¥åœ¨åˆç†æ—¶é—´å†…å®Œæˆï¼ˆå°äº10æ¯«ç§’ï¼‰
        assert avg_time < 10, f"å…¥é˜Ÿæ“ä½œå¹³å‡è€—æ—¶ {avg_time:.2f} æ¯«ç§’ï¼Œè¶…è¿‡é¢„æœŸ"

        # éªŒè¯æ‰€æœ‰æ•°æ®éƒ½è¢«æ­£ç¡®å­˜å‚¨
        items = queue.get_pending_items(limit=iterations * 2)
        assert len(items) == iterations

    def test_get_pending_items_performance(self):
        """æµ‹è¯•è·å–å¾…å¤„ç†é¡¹ç›®æ€§èƒ½"""
        queue = FailureRecoveryQueue(self.test_db_path)

        # å…ˆæ·»åŠ å¤§é‡æ•°æ®
        for i in range(1000):
            queue.enqueue(f"perf_{i}", "test_db", {"id": i})

        iterations = 100

        start_time = time.time()
        for _ in range(iterations):
            items = queue.get_pending_items(limit=100)
        end_time = time.time()

        avg_time = (end_time - start_time) / iterations * 1000  # æ¯«ç§’

        # è·å–æ“ä½œåº”è¯¥å¾ˆå¿«ï¼ˆå°äº5æ¯«ç§’ï¼‰
        assert avg_time < 5, f"è·å–æ“ä½œå¹³å‡è€—æ—¶ {avg_time:.2f} æ¯«ç§’ï¼Œè¶…è¿‡é¢„æœŸ"

    def test_database_size_impact(self):
        """æµ‹è¯•æ•°æ®åº“å¤§å°å¯¹æ€§èƒ½çš„å½±å“"""
        queue = FailureRecoveryQueue(self.test_db_path)

        # æ·»åŠ ä¸åŒå¤§å°çš„æ•°æ®
        small_data = {"small": "test"}
        medium_data = {"medium": "x" * 1000}
        large_data = {"large": "x" * 10000}

        data_sizes = [small_data, medium_data, large_data]

        for data in data_sizes:
            start_time = time.time()

            # æ·»åŠ 100ä¸ªè¿™ç§å¤§å°çš„æ•°æ®
            for i in range(100):
                queue.enqueue("size_test", "test_db", data)

            end_time = time.time()

            # æµ‹é‡è·å–æ€§èƒ½
            get_start = time.time()
            items = queue.get_pending_items(limit=200)
            get_end = time.time()

            enqueue_time = (end_time - start_time) * 1000
            get_time = (get_end - get_start) * 1000

            # éªŒè¯æ€§èƒ½åœ¨å¯æ¥å—èŒƒå›´å†…
            assert enqueue_time < 1000, (
                f"æ•°æ®å¤§å° {len(str(data))} å…¥é˜Ÿè€—æ—¶ {enqueue_time:.2f} æ¯«ç§’"
            )
            assert get_time < 50, (
                f"æ•°æ®å¤§å° {len(str(data))} è·å–è€—æ—¶ {get_time:.2f} æ¯«ç§’"
            )


class TestIntegration:
    """é›†æˆæµ‹è¯•ç±»"""

    def setup_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•æ‰§è¡Œå‰çš„è®¾ç½®"""
        self.temp_dir = tempfile.mkdtemp()
        self.test_db_path = os.path.join(self.temp_dir, "test_queue.db")

    def teardown_method(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•æ‰§è¡Œåçš„æ¸…ç†"""
        if os.path.exists(self.temp_dir):
            shutil.rmtree(self.temp_dir)

    def test_full_workflow(self):
        """æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹"""
        queue = FailureRecoveryQueue(self.test_db_path)

        # 1. æ¨¡æ‹Ÿæ•°æ®åº“æ•…éšœæ—¶çš„æ•°æ®å…¥é˜Ÿ
        failed_operations = [
            {
                "classification": "stock_data",
                "target": "postgresql",
                "data": {"symbol": "000001", "price": 10.5},
            },
            {
                "classification": "stock_data",
                "target": "postgresql",
                "data": {"symbol": "000002", "price": 20.3},
            },
            {
                "classification": "market_data",
                "target": "tdengine",
                "data": {"symbol": "600000", "volume": 1000},
            },
            {
                "classification": "user_data",
                "target": "postgresql",
                "data": {"user_id": 1, "action": "buy"},
            },
        ]

        # 2. å…¥é˜Ÿæ‰€æœ‰å¤±è´¥çš„æ“ä½œ
        for op in failed_operations:
            queue.enqueue(op["classification"], op["target"], op["data"])

        # 3. æ¨¡æ‹Ÿæ•°æ®åº“æ¢å¤åè·å–å¾…å¤„ç†é¡¹ç›®
        pending_items = queue.get_pending_items(limit=50)

        # 4. éªŒè¯æ•°æ®å®Œæ•´æ€§
        assert len(pending_items) == len(failed_operations)

        # 5. æ¨¡æ‹Ÿå¤„ç†æ¯ä¸ªé¡¹ç›®
        for i, item in enumerate(pending_items):
            classification, target_db, data_json = item[1], item[2], json.loads(item[3])

            # éªŒè¯æ•°æ®åŒ¹é…
            original_op = failed_operations[i]
            assert classification == original_op["classification"]
            assert target_db == original_op["target"]
            assert data_json == original_op["data"]

    def test_data_persistence_across_instances(self):
        """æµ‹è¯•è·¨å®ä¾‹çš„æ•°æ®æŒä¹…æ€§"""
        # ç¬¬ä¸€ä¸ªå®ä¾‹å…¥é˜Ÿæ•°æ®
        queue1 = FailureRecoveryQueue(self.test_db_path)
        test_data = {"persistent": True, "timestamp": "2024-01-01"}
        queue1.enqueue("persistence_test", "test_db", test_data)

        # ç¬¬äºŒä¸ªå®ä¾‹åº”è¯¥èƒ½è®¿é—®ç›¸åŒçš„æ•°æ®
        queue2 = FailureRecoveryQueue(self.test_db_path)
        items = queue2.get_pending_items()

        assert len(items) == 1
        stored_data = json.loads(items[0][3])
        assert stored_data == test_data

    def test_multiple_queues_isolation(self):
        """æµ‹è¯•å¤šä¸ªé˜Ÿåˆ—ä¹‹é—´çš„éš”ç¦»"""
        # åˆ›å»ºä¸¤ä¸ªä¸åŒçš„é˜Ÿåˆ—
        queue1_path = os.path.join(self.temp_dir, "queue1.db")
        queue2_path = os.path.join(self.temp_dir, "queue2.db")

        queue1 = FailureRecoveryQueue(queue1_path)
        queue2 = FailureRecoveryQueue(queue2_path)

        # åœ¨ä¸åŒé˜Ÿåˆ—ä¸­æ·»åŠ æ•°æ®
        queue1.enqueue("queue1", "db1", {"source": "first"})
        queue2.enqueue("queue2", "db2", {"source": "second"})

        # éªŒè¯éš”ç¦»æ€§
        items1 = queue1.get_pending_items()
        items2 = queue2.get_pending_items()

        assert len(items1) == 1
        assert len(items2) == 1
        assert json.loads(items1[0][3])["source"] == "first"
        assert json.loads(items2[0][3])["source"] == "second"

    def test_realistic_failure_recovery_scenario(self):
        """æµ‹è¯•çœŸå®çš„æ•…éšœæ¢å¤åœºæ™¯"""
        queue = FailureRecoveryQueue(self.test_db_path)

        # æ¨¡æ‹Ÿé«˜é¢‘äº¤æ˜“æ—¶çš„æ•…éšœ
        trade_operations = []

        # ç”Ÿæˆ1000ä¸ªäº¤æ˜“æ“ä½œ
        for i in range(100):
            trade = {
                "classification": "trade_execution",
                "target_database": "postgresql",
                "data": {
                    "order_id": f"ORDER_{i}_{int(time.time() * 1000)}",
                    "symbol": f"00000{i % 10}",
                    "price": 10.0 + (i % 100) * 0.01,
                    "quantity": 100 + i,
                    "timestamp": time.time(),
                    "side": "buy" if i % 2 == 0 else "sell",
                },
            }
            trade_operations.append(trade)

        # ç³»ç»Ÿæ•…éšœæ—¶æ‰¹é‡å…¥é˜Ÿ
        enqueue_start = time.time()
        for trade in trade_operations:
            queue.enqueue(
                trade["classification"], trade["target_database"], trade["data"]
            )
        enqueue_time = time.time() - enqueue_start

        # æ¨¡æ‹Ÿç³»ç»Ÿæ¢å¤åæ‰¹é‡å¤„ç†
        recovery_start = time.time()
        batch_size = 50
        processed_count = 0

        while processed_count < len(trade_operations):
            batch = queue.get_pending_items(limit=batch_size)
            if not batch:
                break

            # æ¨¡æ‹Ÿå¤„ç†æ¯ä¸ªäº¤æ˜“
            for item in batch:
                trade_data = json.loads(item[3])
                # è¿™é‡Œä¼šåŒ…å«å®é™…çš„ä¸šåŠ¡é€»è¾‘å¤„ç†
                processed_count += 1

            # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
            time.sleep(0.001)  # 1ms

        recovery_time = time.time() - recovery_start

        # éªŒè¯æ€§èƒ½æŒ‡æ ‡
        assert enqueue_time < 10.0, f"å…¥é˜Ÿ1000ä¸ªæ“ä½œè€—æ—¶ {enqueue_time:.2f}ç§’"
        assert recovery_time < 5.0, f"æ¢å¤å¤„ç†è€—æ—¶ {recovery_time:.2f}ç§’"
        assert processed_count == len(trade_operations)

    def test_database_schema_migration(self):
        """æµ‹è¯•æ•°æ®åº“æ¨¡å¼è¿ç§»åœºæ™¯"""
        # åˆ›å»ºåˆå§‹é˜Ÿåˆ—
        queue = FailureRecoveryQueue(self.test_db_path)

        # æ·»åŠ ä¸€äº›æ•°æ®
        queue.enqueue("migration_test", "db", {"version": 1})

        # éªŒè¯åˆå§‹è¡¨ç»“æ„
        conn = sqlite3.connect(self.test_db_path)
        cursor = conn.cursor()
        cursor.execute("PRAGMA table_info(outbox_queue)")
        original_columns = [col[1] for col in cursor.fetchall()]
        conn.close()

        # æ¨¡æ‹Ÿæ·»åŠ æ–°åˆ—ï¼ˆå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦ä¿®æ”¹ä»£ç ï¼‰
        conn = sqlite3.connect(self.test_db_path)
        cursor = conn.cursor()
        cursor.execute("ALTER TABLE outbox_queue ADD COLUMN processed_at TIMESTAMP")
        conn.commit()
        conn.close()

        # åˆ›å»ºæ–°å®ä¾‹åº”è¯¥ä»èƒ½å·¥ä½œ
        queue2 = FailureRecoveryQueue(self.test_db_path)

        # éªŒè¯æ•°æ®ä»ç„¶å¯è®¿é—®
        items = queue2.get_pending_items()
        assert len(items) == 1
        assert json.loads(items[0][3])["version"] == 1


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
