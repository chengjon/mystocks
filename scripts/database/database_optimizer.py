"""
æ•°æ®åº“ä¼˜åŒ–å’Œç»´æŠ¤è„šæœ¬
Database Optimization and Maintenance Script

æä¾›æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–ã€ç´¢å¼•ç»´æŠ¤ã€æŸ¥è¯¢ä¼˜åŒ–ã€æ•°æ®æ¸…ç†ç­‰åŠŸèƒ½ã€‚
Provides database performance optimization, index maintenance, query optimization, data cleanup, etc.
"""

import asyncio
import logging
import sys
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import argparse

# Setup project path
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from src.core.database import DatabaseManager

logger = logging.getLogger(__name__)


class DatabaseOptimizer:
    """æ•°æ®åº“ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.db_manager = DatabaseManager()

    async def analyze_table_statistics(self) -> Dict[str, Any]:
        """åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("ğŸ“Š åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯...")

        stats = {"tables": {}, "indexes": {}, "performance": {}, "recommendations": []}

        try:
            # è·å–æ‰€æœ‰è¡¨ä¿¡æ¯
            tables_query = """
                SELECT
                    schemaname,
                    tablename,
                    n_tup_ins as inserts,
                    n_tup_upd as updates,
                    n_tup_del as deletes,
                    n_live_tup as live_rows,
                    n_dead_tup as dead_rows,
                    last_vacuum,
                    last_autovacuum,
                    last_analyze,
                    last_autoanalyze
                FROM pg_stat_user_tables
                ORDER BY tablename;
            """

            async with self.db_manager.get_connection() as conn:
                tables = await conn.fetch(tables_query)

                for table in tables:
                    table_name = table["tablename"]
                    stats["tables"][table_name] = {
                        "schema": table["schemaname"],
                        "live_rows": table["live_rows"],
                        "dead_rows": table["dead_rows"],
                        "inserts": table["inserts"],
                        "updates": table["updates"],
                        "deletes": table["deletes"],
                        "last_vacuum": table["last_vacuum"],
                        "last_analyze": table["last_analyze"],
                        "bloat_ratio": (table["dead_rows"] / max(table["live_rows"], 1))
                        if table["live_rows"] > 0
                        else 0,
                    }

                    # è†¨èƒ€æ£€æŸ¥
                    if stats["tables"][table_name]["bloat_ratio"] > 0.2:
                        stats["recommendations"].append(
                            {
                                "type": "vacuum",
                                "table": table_name,
                                "issue": ".2%",
                                "solution": "è¿è¡ŒVACUUMæˆ–VACUUM FULL",
                            }
                        )

            # åˆ†æç´¢å¼•ä½¿ç”¨æƒ…å†µ
            indexes_query = """
                SELECT
                    schemaname,
                    tablename,
                    indexname,
                    idx_scan as index_scans,
                    idx_tup_read as tuples_read,
                    idx_tup_fetch as tuples_fetched
                FROM pg_stat_user_indexes
                ORDER BY tablename, indexname;
            """

            async with self.db_manager.get_connection() as conn:
                indexes = await conn.fetch(indexes_query)

                for index in indexes:
                    index_name = index["indexname"]
                    table_name = index["tablename"]

                    stats["indexes"][index_name] = {
                        "table": table_name,
                        "scans": index["index_scans"],
                        "tuples_read": index["tuples_read"],
                        "tuples_fetched": index["tuples_fetched"],
                        "efficiency": (index["tuples_fetched"] / max(index["tuples_read"], 1))
                        if index["tuples_read"] > 0
                        else 0,
                    }

                    # æœªä½¿ç”¨ç´¢å¼•æ£€æŸ¥
                    if index["index_scans"] == 0:
                        stats["recommendations"].append(
                            {
                                "type": "unused_index",
                                "index": index_name,
                                "table": table_name,
                                "issue": "ç´¢å¼•æœªè¢«ä½¿ç”¨",
                                "solution": "è€ƒè™‘åˆ é™¤æœªä½¿ç”¨çš„ç´¢å¼•",
                            }
                        )

        except Exception as e:
            logger.error(f"åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            stats["error"] = str(e)

        return stats

    async def optimize_indexes(self) -> Dict[str, Any]:
        """ç´¢å¼•ä¼˜åŒ–"""
        logger.info("ğŸ”§ ä¼˜åŒ–ç´¢å¼•...")

        result = {"created_indexes": [], "dropped_indexes": [], "reindexed_indexes": [], "errors": []}

        try:
            # åˆ†ææ½œåœ¨çš„ç´¢å¼•ä¼˜åŒ–æœºä¼š
            analyze_query = """
                SELECT
                    schemaname,
                    tablename,
                    attname as column_name,
                    n_distinct,
                    correlation
                FROM pg_stats
                WHERE schemaname = 'public'
                AND n_distinct > 100  -- é«˜åŸºæ•°åˆ—
                ORDER BY n_distinct DESC;
            """

            async with self.db_manager.get_connection() as conn:
                candidates = await conn.fetch(analyze_query)

                for candidate in candidates:
                    table = candidate["tablename"]
                    column = candidate["column_name"]

                    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç´¢å¼•
                    check_index_query = f"""
                        SELECT 1 FROM pg_indexes
                        WHERE schemaname = 'public'
                        AND tablename = '{table}'
                        AND indexdef LIKE '%{column}%';
                    """

                    existing = await conn.fetchval(check_index_query)

                    if not existing:
                        # åˆ›å»ºç´¢å¼•
                        try:
                            create_index_query = f"""
                                CREATE INDEX CONCURRENTLY idx_{table}_{column}
                                ON {table} ({column});
                            """

                            await conn.execute(create_index_query)
                            result["created_indexes"].append(
                                {"table": table, "column": column, "index_name": f"idx_{table}_{column}"}
                            )

                            logger.info(f"âœ… åˆ›å»ºç´¢å¼•: {table}.{column}")

                        except Exception as e:
                            result["errors"].append(
                                {"operation": "create_index", "table": table, "column": column, "error": str(e)}
                            )

        except Exception as e:
            logger.error(f"ç´¢å¼•ä¼˜åŒ–å¤±è´¥: {e}")
            result["errors"].append({"operation": "analyze", "error": str(e)})

        return result

    async def vacuum_analyze_tables(self) -> Dict[str, Any]:
        """æ¸…ç†å’Œåˆ†æè¡¨"""
        logger.info("ğŸ§¹ æ¸…ç†å’Œåˆ†æè¡¨...")

        result = {"vacuumed_tables": [], "analyzed_tables": [], "errors": []}

        try:
            # è·å–éœ€è¦æ¸…ç†çš„è¡¨
            tables_query = """
                SELECT tablename
                FROM pg_stat_user_tables
                WHERE n_dead_tup > 1000  -- æ­»å…ƒç»„è¶…è¿‡1000
                OR last_vacuum IS NULL
                OR last_vacuum < NOW() - INTERVAL '7 days';
            """

            async with self.db_manager.get_connection() as conn:
                tables = await conn.fetch(tables_query)

                for table in tables:
                    table_name = table["tablename"]

                    try:
                        # VACUUM ANALYZE
                        await conn.execute(f"VACUUM ANALYZE {table_name}")

                        result["vacuumed_tables"].append(table_name)
                        result["analyzed_tables"].append(table_name)

                        logger.info(f"âœ… VACUUM ANALYZE: {table_name}")

                    except Exception as e:
                        result["errors"].append({"operation": "vacuum_analyze", "table": table_name, "error": str(e)})

        except Exception as e:
            logger.error(f"æ¸…ç†å’Œåˆ†æè¡¨å¤±è´¥: {e}")
            result["errors"].append({"operation": "query_tables", "error": str(e)})

        return result

    async def optimize_queries(self) -> Dict[str, Any]:
        """æŸ¥è¯¢ä¼˜åŒ–åˆ†æ"""
        logger.info("ğŸ” åˆ†ææŸ¥è¯¢æ€§èƒ½...")

        result = {"slow_queries": [], "frequent_queries": [], "recommendations": []}

        try:
            # åˆ†ææ…¢æŸ¥è¯¢
            slow_queries_query = """
                SELECT
                    query,
                    calls,
                    total_time,
                    mean_time,
                    rows
                FROM pg_stat_statements
                WHERE mean_time > 1000  -- å¹³å‡æ‰§è¡Œæ—¶é—´è¶…è¿‡1ç§’
                ORDER BY mean_time DESC
                LIMIT 10;
            """

            async with self.db_manager.get_connection() as conn:
                slow_queries = await conn.fetch(slow_queries_query)

                for query in slow_queries:
                    result["slow_queries"].append(
                        {
                            "query": query["query"][:200] + "..." if len(query["query"]) > 200 else query["query"],
                            "calls": query["calls"],
                            "total_time_ms": query["total_time"],
                            "mean_time_ms": query["mean_time"],
                            "rows_affected": query["rows"],
                        }
                    )

                    result["recommendations"].append(
                        {
                            "type": "slow_query",
                            "query": query["query"][:100] + "...",
                            "issue": ".2f",
                            "solution": "è€ƒè™‘æ·»åŠ ç´¢å¼•æˆ–ä¼˜åŒ–æŸ¥è¯¢ç»“æ„",
                        }
                    )

            # åˆ†æé¢‘ç¹æŸ¥è¯¢
            frequent_queries_query = """
                SELECT
                    query,
                    calls,
                    total_time,
                    mean_time
                FROM pg_stat_statements
                WHERE calls > 1000  -- è°ƒç”¨æ¬¡æ•°è¶…è¿‡1000
                ORDER BY calls DESC
                LIMIT 10;
            """

            async with self.db_manager.get_connection() as conn:
                frequent_queries = await conn.fetch(frequent_queries_query)

                for query in frequent_queries:
                    result["frequent_queries"].append(
                        {
                            "query": query["query"][:200] + "..." if len(query["query"]) > 200 else query["query"],
                            "calls": query["calls"],
                            "total_time_ms": query["total_time"],
                            "mean_time_ms": query["mean_time"],
                        }
                    )

        except Exception as e:
            logger.error(f"æŸ¥è¯¢ä¼˜åŒ–åˆ†æå¤±è´¥: {e}")
            result["error"] = str(e)

        return result

    async def cleanup_old_data(self, days_to_keep: int = 90) -> Dict[str, Any]:
        """æ¸…ç†æ—§æ•°æ®"""
        logger.info(f"ğŸ—‘ï¸ æ¸…ç† {days_to_keep} å¤©å‰çš„æ—§æ•°æ®...")

        result = {"deleted_records": {}, "errors": []}

        try:
            cutoff_date = datetime.now() - timedelta(days=days_to_keep)

            # æ¸…ç†æ—§çš„å®¡è®¡æ—¥å¿—
            audit_cleanup_query = """
                DELETE FROM audit_logs
                WHERE created_at < $1;
            """

            # æ¸…ç†æ—§çš„æ€§èƒ½æŒ‡æ ‡
            metrics_cleanup_query = """
                DELETE FROM performance_metrics
                WHERE timestamp < $1;
            """

            # æ¸…ç†æ—§çš„ç¼“å­˜æ•°æ®
            cache_cleanup_query = """
                DELETE FROM cache_entries
                WHERE expires_at < NOW() OR (last_accessed < $1 AND expires_at IS NULL);
            """

            async with self.db_manager.get_connection() as conn:
                # æ¸…ç†å®¡è®¡æ—¥å¿—
                try:
                    deleted = await conn.execute(audit_cleanup_query, cutoff_date)
                    result["deleted_records"]["audit_logs"] = deleted
                except Exception as e:
                    result["errors"].append({"table": "audit_logs", "error": str(e)})

                # æ¸…ç†æ€§èƒ½æŒ‡æ ‡
                try:
                    deleted = await conn.execute(metrics_cleanup_query, cutoff_date)
                    result["deleted_records"]["performance_metrics"] = deleted
                except Exception as e:
                    result["errors"].append({"table": "performance_metrics", "error": str(e)})

                # æ¸…ç†ç¼“å­˜
                try:
                    deleted = await conn.execute(cache_cleanup_query, cutoff_date)
                    result["deleted_records"]["cache_entries"] = deleted
                except Exception as e:
                    result["errors"].append({"table": "cache_entries", "error": str(e)})

            logger.info(f"âœ… æ•°æ®æ¸…ç†å®Œæˆ: {result['deleted_records']}")

        except Exception as e:
            logger.error(f"æ•°æ®æ¸…ç†å¤±è´¥: {e}")
            result["errors"].append({"operation": "cleanup", "error": str(e)})

        return result

    async def generate_optimization_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        logger.info("ğŸ“‹ ç”Ÿæˆæ•°æ®åº“ä¼˜åŒ–æŠ¥å‘Š...")

        report = {
            "timestamp": datetime.now().isoformat(),
            "database_stats": {},
            "optimization_results": {},
            "recommendations": [],
        }

        try:
            # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
            report["database_stats"] = await self.analyze_table_statistics()

            # æ‰§è¡Œä¼˜åŒ–
            report["optimization_results"]["index_optimization"] = await self.optimize_indexes()
            report["optimization_results"]["table_maintenance"] = await self.vacuum_analyze_tables()
            report["optimization_results"]["query_analysis"] = await self.optimize_queries()

            # æ±‡æ€»å»ºè®®
            all_recommendations = []
            for section in report["database_stats"].get("recommendations", []):
                all_recommendations.append(section)

            for section in report["optimization_results"]["query_analysis"].get("recommendations", []):
                all_recommendations.append(section)

            report["recommendations"] = all_recommendations

        except Exception as e:
            logger.error(f"ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Šå¤±è´¥: {e}")
            report["error"] = str(e)

        return report


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ•°æ®åº“ä¼˜åŒ–å’Œç»´æŠ¤å·¥å…·")
    parser.add_argument(
        "--action",
        choices=["analyze", "optimize_indexes", "vacuum", "query_analysis", "cleanup", "full_report"],
        default="full_report",
        help="è¦æ‰§è¡Œçš„æ“ä½œ",
    )
    parser.add_argument("--days-to-keep", type=int, default=90, help="æ¸…ç†æ•°æ®æ—¶ä¿ç•™çš„å¤©æ•°")

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")

    optimizer = DatabaseOptimizer()

    try:
        if args.action == "analyze":
            result = await optimizer.analyze_table_statistics()
            print("ğŸ“Š è¡¨ç»Ÿè®¡åˆ†æç»“æœ:")
            print(f"è¡¨æ•°é‡: {len(result.get('tables', {}))}")
            print(f"ç´¢å¼•æ•°é‡: {len(result.get('indexes', {}))}")
            print(f"ä¼˜åŒ–å»ºè®®: {len(result.get('recommendations', []))}")

        elif args.action == "optimize_indexes":
            result = await optimizer.optimize_indexes()
            print("ğŸ”§ ç´¢å¼•ä¼˜åŒ–ç»“æœ:")
            print(f"åˆ›å»ºç´¢å¼•: {len(result.get('created_indexes', []))}")
            print(f"åˆ é™¤ç´¢å¼•: {len(result.get('dropped_indexes', []))}")
            print(f"é‡å»ºç´¢å¼•: {len(result.get('reindexed_indexes', []))}")

        elif args.action == "vacuum":
            result = await optimizer.vacuum_analyze_tables()
            print("ğŸ§¹ è¡¨ç»´æŠ¤ç»“æœ:")
            print(f"æ¸…ç†è¡¨æ•°: {len(result.get('vacuumed_tables', []))}")
            print(f"åˆ†æè¡¨æ•°: {len(result.get('analyzed_tables', []))}")

        elif args.action == "query_analysis":
            result = await optimizer.optimize_queries()
            print("ğŸ” æŸ¥è¯¢åˆ†æç»“æœ:")
            print(f"æ…¢æŸ¥è¯¢æ•°é‡: {len(result.get('slow_queries', []))}")
            print(f"é¢‘ç¹æŸ¥è¯¢æ•°é‡: {len(result.get('frequent_queries', []))}")

        elif args.action == "cleanup":
            result = await optimizer.cleanup_old_data(args.days_to_keep)
            print("ğŸ—‘ï¸ æ•°æ®æ¸…ç†ç»“æœ:")
            for table, count in result.get("deleted_records", {}).items():
                print(f"{table}: åˆ é™¤ {count} æ¡è®°å½•")

        elif args.action == "full_report":
            report = await optimizer.generate_optimization_report()
            print("ğŸ“‹ å®Œæ•´æ•°æ®åº“ä¼˜åŒ–æŠ¥å‘Š")
            print("=" * 50)
            print(f"ç”Ÿæˆæ—¶é—´: {report['timestamp']}")
            print(f"è¡¨æ•°é‡: {len(report['database_stats'].get('tables', {}))}")
            print(f"ç´¢å¼•æ•°é‡: {len(report['database_stats'].get('indexes', {}))}")

            print(f"\nä¼˜åŒ–ç»“æœ:")
            opt_results = report.get("optimization_results", {})
            for operation, result in opt_results.items():
                if isinstance(result, dict):
                    success_count = len([k for k in result.keys() if k.endswith("indexes") or k.endswith("tables")])
                    print(f"  {operation}: {success_count} é¡¹æˆåŠŸ")

            print(f"\nä¼˜åŒ–å»ºè®®æ•°é‡: {len(report.get('recommendations', []))}")

            # æ˜¾ç¤ºå‰5ä¸ªå»ºè®®
            recommendations = report.get("recommendations", [])[:5]
            if recommendations:
                print("\nå‰5ä¸ªä¼˜åŒ–å»ºè®®:")
                for i, rec in enumerate(recommendations, 1):
                    print(f"  {i}. {rec.get('type', 'unknown')}: {rec.get('solution', 'N/A')}")

        print("\nâœ… æ•°æ®åº“ä¼˜åŒ–ä»»åŠ¡å®Œæˆ!")

    except Exception as e:
        logger.error(f"æ•°æ®åº“ä¼˜åŒ–ä»»åŠ¡å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
