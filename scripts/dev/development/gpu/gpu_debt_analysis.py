#!/usr/bin/env python3
"""
GPUåŠ é€Ÿå¼•æ“æŠ€æœ¯å€ºåŠ¡åˆ†æå·¥å…·
åˆ†æç°æœ‰GPUä»£ç åº“çš„æŠ€æœ¯å€ºåŠ¡é—®é¢˜ï¼Œç¡®å®šé‡æ„ä¼˜å…ˆçº§
"""

import os
import sys
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime
import ast
import re

# æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


def analyze_gpu_codebase_structure():
    """åˆ†æGPUä»£ç åº“ç»“æ„"""
    print("ğŸ” åˆ†æGPUä»£ç åº“ç»“æ„...")

    gpu_directories = [
        "src/gpu",
        "src/gpu/accelerated",
        "src/gpu/api_system",
        "src/gpu/api_system/services",
        "src/gpu/api_system/tests",
    ]

    structure_analysis = {}

    for gpu_dir in gpu_directories:
        if os.path.exists(gpu_dir):
            files = []
            for root, dirs, filenames in os.walk(gpu_dir):
                for filename in filenames:
                    if filename.endswith(".py"):
                        filepath = os.path.join(root, filename)
                        relative_path = os.path.relpath(filepath, gpu_dir)
                        files.append(relative_path)

            structure_analysis[gpu_dir] = {
                "exists": True,
                "file_count": len(files),
                "files": files,
            }
        else:
            structure_analysis[gpu_dir] = {
                "exists": False,
                "file_count": 0,
                "files": [],
            }

    return structure_analysis


def analyze_technical_debt_issues(file_path: str) -> Dict[str, Any]:
    """åˆ†æå•ä¸ªæ–‡ä»¶çš„æŠ€æœ¯å€ºåŠ¡é—®é¢˜"""
    issues = {
        "file_path": file_path,
        "lines": 0,
        "complexity_issues": [],
        "maintainability_issues": [],
        "performance_issues": [],
        "security_issues": [],
        "documentation_issues": [],
        "error_handling_issues": [],
    }

    try:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
            lines = content.split("\n")
            issues["lines"] = len(lines)

            # åˆ†ææŠ€æœ¯å€ºåŠ¡é—®é¢˜
            issues.update(identify_issues_by_type(content, lines, file_path))

    except Exception as e:
        issues["read_error"] = str(e)

    return issues


def identify_issues_by_type(
    content: str, lines: List[str], file_path: str
) -> Dict[str, Any]:
    """è¯†åˆ«å„ç±»æŠ€æœ¯å€ºåŠ¡é—®é¢˜"""
    issues = {
        "complexity_issues": [],
        "maintainability_issues": [],
        "performance_issues": [],
        "security_issues": [],
        "documentation_issues": [],
        "error_handling_issues": [],
    }

    # 1. å¤æ‚åº¦é—®é¢˜
    issues["complexity_issues"].extend(
        analyze_complexity_issues(content, lines, file_path)
    )

    # 2. å¯ç»´æŠ¤æ€§é—®é¢˜
    issues["maintainability_issues"].extend(
        analyze_maintainability_issues(content, lines, file_path)
    )

    # 3. æ€§èƒ½é—®é¢˜
    issues["performance_issues"].extend(
        analyze_performance_issues(content, lines, file_path)
    )

    # 4. å®‰å…¨é—®é¢˜
    issues["security_issues"].extend(analyze_security_issues(content, lines, file_path))

    # 5. æ–‡æ¡£é—®é¢˜
    issues["documentation_issues"].extend(
        analyze_documentation_issues(content, lines, file_path)
    )

    # 6. é”™è¯¯å¤„ç†é—®é¢˜
    issues["error_handling_issues"].extend(
        analyze_error_handling_issues(content, lines, file_path)
    )

    return issues


def analyze_complexity_issues(
    content: str, lines: List[str], file_path: str
) -> List[Dict[str, Any]]:
    """åˆ†æä»£ç å¤æ‚åº¦é—®é¢˜"""
    complexity_issues = []

    # é•¿å‡½æ•°
    for i, line in enumerate(lines, 1):
        if len(line.strip()) > 150:
            complexity_issues.append(
                {
                    "type": "long_line",
                    "severity": "medium",
                    "line": i,
                    "description": f"ä»£ç è¡Œè¿‡é•¿ ({len(line)} å­—ç¬¦)",
                    "suggestion": "å°†é•¿è¡Œæ‹†åˆ†ä¸ºå¤šè¡Œ",
                }
            )

    # å¤æ‚å‡½æ•°å®šä¹‰
    if "def " in content:
        complexity_issues.extend(analyze_function_complexity(content, file_path))

    # æ·±å±‚åµŒå¥—
    complexity_issues.extend(analyze_nesting_complexity(content, lines, file_path))

    # å¤æ‚è¡¨è¾¾å¼
    complexity_issues.extend(analyze_expression_complexity(content, lines, file_path))

    return complexity_issues


def analyze_function_complexity(content: str, file_path: str) -> List[Dict[str, Any]]:
    """åˆ†æå‡½æ•°å¤æ‚åº¦"""
    function_issues = []

    try:
        tree = ast.parse(content)
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                # è®¡ç®—å‡½æ•°å¤æ‚åº¦
                complexity = calculate_cyclomatic_complexity(node)

                if complexity > 10:
                    severity = "high" if complexity > 20 else "medium"
                    function_issues.append(
                        {
                            "type": "complex_function",
                            "severity": severity,
                            "function": node.name,
                            "line": node.lineno,
                            "complexity": complexity,
                            "description": f"å‡½æ•°å¤æ‚åº¦è¿‡é«˜ ({complexity})",
                            "suggestion": "è€ƒè™‘æ‹†åˆ†å‡½æ•°æˆ–ç®€åŒ–é€»è¾‘",
                        }
                    )

                # æ£€æŸ¥å‚æ•°æ•°é‡
                args_count = len(node.args.args) + len(node.args.defaults)
                if args_count > 7:
                    function_issues.append(
                        {
                            "type": "too_many_parameters",
                            "severity": "medium",
                            "function": node.name,
                            "line": node.lineno,
                            "parameters": args_count,
                            "description": f"å‡½æ•°å‚æ•°è¿‡å¤š ({args_count})",
                            "suggestion": "è€ƒè™‘ä½¿ç”¨é…ç½®å¯¹è±¡æˆ–æ•°æ®ç±»",
                        }
                    )

    except Exception:
        # å¦‚æœASTè§£æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ–¹æ³•
        function_count = content.count("def ")
        if function_count > 10:
            function_issues.append(
                {
                    "type": "too_many_functions",
                    "severity": "medium",
                    "file": file_path,
                    "count": function_count,
                    "description": f"æ–‡ä»¶ä¸­å‡½æ•°è¿‡å¤š ({function_count})",
                    "suggestion": "è€ƒè™‘å°†ç›¸å…³åŠŸèƒ½ç»„ç»‡åˆ°ç±»æˆ–æ¨¡å—ä¸­",
                }
            )

    return function_issues


def analyze_nesting_complexity(
    content: str, lines: List[str], file_path: str
) -> List[Dict[str, Any]]:
    """åˆ†æåµŒå¥—å¤æ‚åº¦"""
    nesting_issues = []

    max_nesting = 0
    current_nesting = 0

    for i, line in enumerate(lines, 1):
        # ç®€å•çš„åµŒå¥—æ£€æµ‹
        open_brackets = (
            line.count("{")
            + line.count("if ")
            + line.count("for ")
            + line.count("while ")
        )
        close_brackets = (
            line.count("}") + line.count("elif ") + line.count(":") * 2
        )  # ç®€åŒ–å¤„ç†

        current_nesting += open_brackets - close_brackets
        max_nesting = max(max_nesting, current_nesting)

        if current_nesting > 5:
            nesting_issues.append(
                {
                    "type": "deep_nesting",
                    "severity": "high",
                    "line": i,
                    "nesting_level": current_nesting,
                    "description": f"åµŒå¥—å±‚æ¬¡è¿‡æ·± ({current_nesting} å±‚)",
                    "suggestion": "è€ƒè™‘ä½¿ç”¨æ—©æœŸè¿”å›æˆ–æå–å‡½æ•°",
                }
            )

    if max_nesting > 3:
        nesting_issues.append(
            {
                "type": "max_nesting_too_deep",
                "severity": "medium",
                "file": file_path,
                "max_nesting": max_nesting,
                "description": f"æœ€å¤§åµŒå¥—å±‚æ¬¡è¿‡æ·± ({max_nesting} å±‚)",
                "suggestion": "ä¼˜åŒ–ä»£ç ç»“æ„ï¼Œå‡å°‘åµŒå¥—å±‚æ¬¡",
            }
        )

    return nesting_issues


def analyze_expression_complexity(
    content: str, lines: List[str], file_path: str
) -> List[Dict[str, Any]]:
    """åˆ†æè¡¨è¾¾å¼å¤æ‚åº¦"""
    expression_issues = []

    for i, line in enumerate(lines, 1):
        # æ£€æŸ¥é•¿è¡¨è¾¾å¼
        if len(line) > 120:
            expression_issues.append(
                {
                    "type": "long_expression",
                    "severity": "medium",
                    "line": i,
                    "description": f"è¡¨è¾¾å¼è¿‡é•¿ ({len(line)} å­—ç¬¦)",
                    "suggestion": "å°†é•¿è¡¨è¾¾å¼æ‹†åˆ†ä¸ºå¤šè¡Œæˆ–ä½¿ç”¨ä¸­é—´å˜é‡",
                }
            )

        # æ£€æŸ¥å¤æ‚çš„é“¾å¼è°ƒç”¨
        if line.count(".") > 5:
            expression_issues.append(
                {
                    "type": "complex_chaining",
                    "severity": "medium",
                    "line": i,
                    "description": f"æ–¹æ³•é“¾è¿‡é•¿ ({line.count('.')} ä¸ªè°ƒç”¨)",
                    "suggestion": "è€ƒè™‘ä½¿ç”¨ä¸­é—´å˜é‡å­˜å‚¨ä¸­é—´ç»“æœ",
                }
            )

    return expression_issues


def calculate_cyclomatic_complexity(node: ast.FunctionDef) -> int:
    """è®¡ç®—åœˆå¤æ‚åº¦"""
    complexity = 1  # åŸºç¡€å¤æ‚åº¦

    for child in ast.walk(node):
        if isinstance(child, (ast.If, ast.For, ast.While)):
            complexity += 1
        elif isinstance(child, ast.BoolOp):
            complexity += len(child.values) - 1

    return complexity


def analyze_maintainability_issues(
    content: str, lines: List[str], file_path: str
) -> List[Dict[str, Any]]:
    """åˆ†æå¯ç»´æŠ¤æ€§é—®é¢˜"""
    maintainability_issues = []

    # ç¡¬ç¼–ç å€¼
    hardcode_patterns = [
        (r"\b[0-9]+", "ç¡¬ç¼–ç äºŒè¿›åˆ¶æ•°æ®"),
        (r'"\d{1,2}[/-]\d{2}[/-]\d{4}', "ç¡¬ç¼–ç æ—¥æœŸ"),
        (r"https?://[^\s\)]+", "ç¡¬ç¼–ç URL"),
        (r"\d+\.\d+\.\d+\.\d+", "ç¡¬ç¼–ç IPåœ°å€"),
        (r'password[^"\']*', "ç¡¬ç¼–ç å¯†ç "),
        (r"mysql://[^\s\)]+", "ç¡¬ç¼–ç æ•°æ®åº“è¿æ¥"),
    ]

    for pattern, description in hardcode_patterns:
        if re.search(pattern, content, re.IGNORECASE):
            maintainability_issues.append(
                {
                    "type": "hardcoded_value",
                    "severity": "high",
                    "pattern": description,
                    "description": f"å‘ç°ç¡¬ç¼–ç å€¼: {description}",
                    "suggestion": "ä½¿ç”¨é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡",
                }
            )

    # ç¼ºå°‘æ³¨é‡Š
    comment_lines = sum(1 for line in lines if line.strip().startswith("#"))
    code_lines = sum(
        1 for line in lines if line.strip() and not line.strip().startswith("#")
    )

    if code_lines > 0 and comment_lines / code_lines < 0.1:
        maintainability_issues.append(
            {
                "type": "insufficient_comments",
                "severity": "medium",
                "ratio": comment_lines / code_lines,
                "description": f"æ³¨é‡Šæ¯”ä¾‹è¿‡ä½ ({comment_lines}/{code_lines})",
                "suggestion": "å¢åŠ ä»£ç æ³¨é‡Šå’Œæ–‡æ¡£è¯´æ˜",
            }
        )

    # é•¿ç±»
    class_count = content.count("class ")
    if class_count > 10:
        maintainability_issues.append(
            {
                "type": "too_many_classes",
                "severity": "medium",
                "file": file_path,
                "count": class_count,
                "description": f"ç±»æ•°é‡è¿‡å¤š ({class_count})",
                "suggestion": "è€ƒè™‘æ‹†åˆ†æ¨¡å—æˆ–ç»„ç»‡ç›¸å…³åŠŸèƒ½",
            }
        )

    return maintainability_issues


def analyze_performance_issues(
    content: str, lines: List[str], file_path: str
) -> List[Dict[str, Any]]:
    """åˆ†ææ€§èƒ½é—®é¢˜"""
    performance_issues = []

    # GPUç›¸å…³æ€§èƒ½é—®é¢˜
    gpu_patterns = [
        (r"cuda\.synchronize\(\)", "åŒæ­¥GPUæ“ä½œå¯èƒ½é˜»å¡"),
        (r"torch\.cuda\.empty_cache\(\)", "æ¸…ç©ºç¼“å­˜å¯èƒ½å½±å“æ€§èƒ½"),
        (r"torch\.no_grad\(\)", "ç¦ç”¨æ¢¯åº¦å¯èƒ½å½±å“ä¼˜åŒ–"),
        (r"\.cpu\(\)", "é¢‘ç¹çš„CPU-GPUæ•°æ®ä¼ è¾“"),
        (r"\.cuda\(\)", "é¢‘ç¹çš„CPU-GPUæ•°æ®ä¼ è¾“"),
        (r"np\.array\(.*\.cuda\(\)\)", "numpyæ•°ç»„GPUè½¬æ¢"),
    ]

    for i, line in enumerate(lines, 1):
        for pattern, description in gpu_patterns:
            if re.search(pattern, line):
                performance_issues.append(
                    {
                        "type": "gpu_performance_issue",
                        "severity": "high",
                        "line": i,
                        "pattern": description,
                        "description": f"GPUæ€§èƒ½é—®é¢˜: {description}",
                        "suggestion": "ä¼˜åŒ–GPUæ“ä½œå’Œå†…å­˜ç®¡ç†",
                    }
                )

    # å†…å­˜æ³„æ¼é£é™©
    memory_patterns = [
        (r"global\s+\w+\s*=", "å…¨å±€å˜é‡å¯èƒ½å¯¼è‡´å†…å­˜æ³„æ¼"),
        (r"while\s+True:", "æ— é™å¾ªç¯å¯èƒ½å¯¼è‡´å†…å­˜æ³„æ¼"),
        (r"\.append\(.*\)", "é¢‘ç¹çš„appendæ“ä½œ"),
    ]

    for i, line in enumerate(lines, 1):
        for pattern, description in memory_patterns:
            if re.search(pattern, line):
                performance_issues.append(
                    {
                        "type": "memory_leak_risk",
                        "severity": "high",
                        "line": i,
                        "pattern": description,
                        "description": f"å†…å­˜æ³„æ¼é£é™©: {description}",
                        "suggestion": "ä½¿ç”¨å†…å­˜ç®¡ç†å·¥å…·æˆ–é‡æ–°è®¾è®¡",
                    }
                )

    return performance_issues


def analyze_security_issues(
    content: str, lines: List[str], file_path: str
) -> List[Dict[str, Any]]:
    """åˆ†æå®‰å…¨é—®é¢˜"""
    security_issues = []

    # SQLæ³¨å…¥é£é™©
    if "SELECT" in content or "INSERT" in content or "UPDATE" in content:
        if "execute(" in content and "%" in content:
            security_issues.append(
                {
                    "type": "sql_injection_risk",
                    "severity": "critical",
                    "description": "å¯èƒ½å­˜åœ¨SQLæ³¨å…¥é£é™©",
                    "suggestion": "ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢",
                }
            )

    # ç¡¬ç¼–ç å‡­è¯
    credential_patterns = [
        (r'password\s*=\s*[\'"][^\'\']*["\']*', "ç¡¬ç¼–ç å¯†ç "),
        (r'api_key\s*=\s*[\'"][^\'\']*["\']*', "ç¡¬ç¼–ç APIå¯†é’¥"),
        (r'token\s*=\s*[\'"][^\'\']*["\']*', "ç¡¬ç¼–ç ä»¤ç‰Œ"),
        (r'secret\s*=\s*[\'"][^\'\']*["\']*', "ç¡¬ç¼–ç å¯†é’¥"),
    ]

    for pattern, description in credential_patterns:
        if re.search(pattern, content):
            security_issues.append(
                {
                    "type": "hardcoded_credentials",
                    "severity": "critical",
                    "pattern": description,
                    "description": f"å®‰å…¨é—®é¢˜: {description}",
                    "suggestion": "ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶",
                }
            )

    # ä¸å®‰å…¨çš„æ–‡ä»¶æ“ä½œ
    if "open(" in content and "w" in content:
        security_issues.append(
            {
                "type": "unsafe_file_operation",
                "severity": "medium",
                "description": "å¯èƒ½å­˜åœ¨ä¸å®‰å…¨çš„æ–‡ä»¶å†™å…¥æ“ä½œ",
                "suggestion": "éªŒè¯æ–‡ä»¶è·¯å¾„å¹¶ä½¿ç”¨é€‚å½“çš„æƒé™",
            }
        )

    return security_issues


def analyze_documentation_issues(
    content: str, lines: List[str], file_path: str
) -> List[Dict[str, Any]]:
    """åˆ†ææ–‡æ¡£é—®é¢˜"""
    documentation_issues = []

    # ç¼ºå°‘æ¨¡å—æ–‡æ¡£å­—ç¬¦ä¸²
    if not content.startswith('"""'):
        documentation_issues.append(
            {
                "type": "missing_module_docstring",
                "severity": "medium",
                "file": file_path,
                "description": "ç¼ºå°‘æ¨¡å—çº§åˆ«çš„æ–‡æ¡£å­—ç¬¦ä¸²",
                "suggestion": "æ·»åŠ æ¨¡å—è¯´æ˜å’ŒåŠŸèƒ½æ¦‚è¿°",
            }
        )

    # æ£€æŸ¥ç±»å’Œå‡½æ•°æ–‡æ¡£
    docstring_count = content.count('"""') + content.count("'''")
    class_count = content.count("class ")
    function_count = content.count("def ")

    if class_count > 0:
        if docstring_count < class_count:
            documentation_issues.append(
                {
                    "type": "insufficient_class_documentation",
                    "severity": "medium",
                    "file": file_path,
                    "classes": class_count,
                    "docstrings": docstring_count,
                    "description": f"ç±»æ–‡æ¡£ä¸è¶³ ({docstring_count}/{class_count})",
                    "suggestion": "ä¸ºæ‰€æœ‰ç±»æ·»åŠ è¯¦ç»†çš„æ–‡æ¡£å­—ç¬¦ä¸²",
                }
            )

    return documentation_issues


def analyze_error_handling_issues(
    content: str, lines: List[str], file_path: str
) -> List[Dict[str, Any]]:
    """åˆ†æé”™è¯¯å¤„ç†é—®é¢˜"""
    error_handling_issues = []

    # ç¼ºå°‘å¼‚å¸¸å¤„ç†
    try_blocks = content.count("try:")
    except_blocks = content.count("except:")

    if try_blocks > 0 and except_blocks == 0:
        error_handling_issues.append(
            {
                "type": "missing_exception_handling",
                "severity": "high",
                "file": file_path,
                "try_blocks": try_blocks,
                "except_blocks": except_blocks,
                "description": f"tryå—æ²¡æœ‰å¯¹åº”çš„exceptå¤„ç† ({try_blocks} ä¸ªtryå—)",
                "suggestion": "æ·»åŠ é€‚å½“çš„å¼‚å¸¸å¤„ç†é€»è¾‘",
            }
        )

    # å®½æ³›çš„å¼‚å¸¸å¤„ç†
    broad_except_patterns = [
        (r"except:\s*pass\s*$", "ç©ºçš„å¼‚å¸¸å¤„ç†"),
        (r"except\s*Exception\s*as\s*e:\s*print", "ä»…æ‰“å°å¼‚å¸¸"),
        (r"except:\s*except\s*Exception:", "è¿‡äºå®½æ³›çš„å¼‚å¸¸æ•è·"),
    ]

    for pattern, description in broad_except_patterns:
        if re.search(pattern, content):
            error_handling_issues.append(
                {
                    "type": "broad_exception_handling",
                    "severity": "medium",
                    "pattern": description,
                    "description": f"å¼‚å¸¸å¤„ç†è¿‡äºå®½æ³›: {description}",
                    "suggestion": "ä½¿ç”¨ç‰¹å®šçš„å¼‚å¸¸ç±»å‹å’Œé€‚å½“çš„å¤„ç†é€»è¾‘",
                }
            )

    return error_handling_issues


def generate_debt_prioritization(analysis_results: Dict[str, Any]) -> Dict[str, Any]:
    """ç”ŸæˆæŠ€æœ¯å€ºåŠ¡ä¼˜å…ˆçº§"""
    priorities = {"high_priority": [], "medium_priority": [], "low_priority": []}

    total_issues = 0
    critical_count = 0
    high_count = 0
    medium_count = 0

    for file_path, file_analysis in analysis_results.items():
        if "read_error" in file_analysis:
            continue

        file_total = 0
        file_critical = 0
        file_high = 0
        file_medium = 0

        for issue_type in [
            "security_issues",
            "performance_issues",
            "complexity_issues",
            "maintainability_issues",
            "error_handling_issues",
        ]:
            issues = file_analysis.get(issue_type, [])
            for issue in issues:
                file_total += 1
                severity = issue.get("severity", "medium")
                if severity == "critical":
                    file_critical += 1
                    critical_count += 1
                elif severity == "high":
                    file_high += 1
                    high_count += 1
                elif severity == "medium":
                    file_medium += 1
                    medium_count += 1

        # æ·»åŠ åˆ°ä¼˜å…ˆçº§åˆ—è¡¨
        if file_critical > 0:
            priorities["high_priority"].append(
                {
                    "file": file_path,
                    "severity": "critical",
                    "count": file_critical,
                    "total_issues": file_total,
                    "details": file_analysis,
                }
            )
        elif file_high > 0 or file_total > 10:
            priorities["medium_priority"].append(
                {
                    "file": file_path,
                    "severity": "high",
                    "count": file_high,
                    "total_issues": file_total,
                    "details": file_analysis,
                }
            )
        elif file_medium > 0 or file_total > 5:
            priorities["low_priority"].append(
                {
                    "file": file_path,
                    "severity": "medium",
                    "count": file_medium,
                    "total_issues": file_total,
                    "details": file_analysis,
                }
            )

        total_issues += file_total

    return {
        "total_issues": total_issues,
        "critical_count": critical_count,
        "high_count": high_count,
        "medium_count": medium_count,
        "priorities": priorities,
        "analysis_results": analysis_results,
    }


def main():
    """ä¸»åˆ†æå‡½æ•°"""
    print("=" * 80)
    print("ğŸš€ Phase 6: GPUåŠ é€Ÿå¼•æ“æŠ€æœ¯å€ºåŠ¡è¯„ä¼°")
    print("=" * 80)

    # 1. åˆ†æGPUä»£ç åº“ç»“æ„
    print("\nğŸ“Š GPUä»£ç åº“ç»“æ„åˆ†æ...")
    structure_analysis = analyze_gpu_codebase_structure()

    for directory, info in structure_analysis.items():
        if info["exists"]:
            print(f"   {directory}:")
            print(f"     - æ–‡ä»¶æ•°é‡: {info['file_count']}")
            print(f"     - æ ¸å¿ƒæ–‡ä»¶: {info['files'][:5]}")  # æ˜¾ç¤ºå‰5ä¸ªæ–‡ä»¶
            if len(info["files"]) > 5:
                print(f"     - å…¶ä»–æ–‡ä»¶: {len(info['files']) - 5} ä¸ªæ–‡ä»¶")
        else:
            print(f"   {directory}: âŒ ç›®å½•ä¸å­˜åœ¨")

    # 2. åˆ†ææŠ€æœ¯å€ºåŠ¡
    print("\nğŸ” åˆ†æGPUæŠ€æœ¯å€ºåŠ¡...")
    analysis_results = {}

    for directory, info in structure_analysis.items():
        if info["exists"]:
            for file in info["files"]:
                file_path = os.path.join(directory, file)
                if os.path.isfile(file_path):
                    analysis_results[file_path] = analyze_technical_debt_issues(
                        file_path
                    )

    # 3. ç”Ÿæˆä¼˜å…ˆçº§æŠ¥å‘Š
    print("\nğŸ“ˆ ç”ŸæˆæŠ€æœ¯å€ºåŠ¡ä¼˜å…ˆçº§æŠ¥å‘Š...")
    prioritization = generate_debt_prioritization(analysis_results)

    # 4. è¾“å‡ºåˆ†æç»“æœ
    priorities = prioritization["priorities"]
    print("\nğŸ“Š GPUæŠ€æœ¯å€ºåŠ¡ç»Ÿè®¡:")
    print(f"   æ€»é—®é¢˜æ•°: {prioritization['total_issues']}")
    print(f"   ä¸¥é‡é—®é¢˜: {prioritization['critical_count']}")
    print(f"   é«˜ä¼˜å…ˆçº§: {len(priorities['high_priority'])} ä¸ªæ–‡ä»¶")
    print(f"   ä¸­ä¼˜å…ˆçº§: {len(priorities['medium_priority'])} ä¸ªæ–‡ä»¶")
    print(f"   ä½ä¼˜å…ˆçº§: {len(priorities['low_priority'])} ä¸ªæ–‡ä»¶")

    print("\nğŸ¯ é«˜ä¼˜å…ˆçº§é—®é¢˜:")
    for item in priorities["high_priority"][:5]:
        print(f"   - {item['file']}: {item['count']} ä¸ªé—®é¢˜")

    print("\nğŸ”§ å»ºè®®é‡æ„é¡ºåº:")
    print("   1. ä¿®å¤å®‰å…¨é—®é¢˜å’Œæ€§èƒ½é—®é¢˜ (Critical + High)")
    print("   2. é‡æ„å¤æ‚åº¦è¿‡é«˜çš„æ¨¡å— (High)")
    print("   3. ä¼˜åŒ–GPUå†…å­˜ç®¡ç†å’Œæ•°æ®ä¼ è¾“")
    print("   4. æ”¹å–„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•")
    print("   5. æ·»åŠ å’Œå®Œå–„æ–‡æ¡£å’Œæ³¨é‡Š")

    # ä¿å­˜åˆ†ææŠ¥å‘Š
    import json

    report_data = {
        "timestamp": datetime.now().isoformat(),
        "phase": "6.1",
        "title": "GPUåŠ é€Ÿå¼•æ“æŠ€æœ¯å€ºåŠ¡è¯„ä¼°",
        "structure_analysis": structure_analysis,
        "prioritization": prioritization,
        "recommendations": [
            "ä¼˜å…ˆå¤„ç†å®‰å…¨é—®é¢˜å’Œæ€§èƒ½ç“¶é¢ˆ",
            "é‡æ„å¤æ‚çš„GPUè®¡ç®—æ¨¡å—",
            "ä¼˜åŒ–GPUå†…å­˜ç®¡ç†å’Œæ•°æ®ä¼ è¾“",
            "å»ºç«‹ç»Ÿä¸€çš„é”™è¯¯å¤„ç†æœºåˆ¶",
            "å®Œå–„GPUä»£ç çš„æ–‡æ¡£å’Œæ³¨é‡Š",
            "å®æ–½GPUæ€§èƒ½ç›‘æ§å’ŒæŒ‡æ ‡æ”¶é›†",
        ],
    }

    report_path = "docs/reports/gpu_debt_analysis.json"
    os.makedirs(os.path.dirname(report_path), exist_ok=True)

    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report_data, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

    print("\nâœ… Phase 6.1 GPUåŠ é€Ÿå¼•æ“æŠ€æœ¯å€ºåŠ¡è¯„ä¼°å®Œæˆ!")
    print("\nğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨:")
    print("   1. å¼€å§‹ Phase 6.2: GPUåŠ é€Ÿå¼•æ“æ¶æ„é‡æ„")
    print("   2. æ ¹æ®ä¼˜å…ˆçº§å¼€å§‹é‡æ„é«˜æŠ€æœ¯å€ºåŠ¡æ¨¡å—")
    print("   3. å®æ–½GPUæ€§èƒ½ä¼˜åŒ–å’Œå†…å­˜ç®¡ç†æ”¹è¿›")
    print("   4. å»ºç«‹GPUç›‘æ§å’ŒæŒ‡æ ‡æ”¶é›†ä½“ç³»")


if __name__ == "__main__":
    main()
