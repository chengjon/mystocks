#!/usr/bin/env python3
"""
Phase 6.2.4 GPUå€ºåŠ¡è¿ç§»åˆ†æå™¨
åˆ†æGPUå€ºåŠ¡æ–‡ä»¶ï¼Œåˆ¶å®šè¿ç§»ç­–ç•¥ï¼Œä¸ºæ–°HALå’Œå†…æ ¸æ¥å£é›†æˆåšå‡†å¤‡
"""

import os
import re
from pathlib import Path
from typing import List, Dict, Any, Tuple
import json
from dataclasses import dataclass, field
from collections import defaultdict


@dataclass
class GPUDebtFile:
    """GPUå€ºåŠ¡æ–‡ä»¶ä¿¡æ¯"""

    path: str
    name: str
    size: int
    gpu_issues: List[str] = field(default_factory=list)
    migration_priority: str = "MEDIUM"
    complexity: str = "MEDIUM"
    dependencies: List[str] = field(default_factory=list)
    recommended_hal_layer: str = ""
    recommended_kernel_type: str = ""


@dataclass
class MigrationPattern:
    """è¿ç§»æ¨¡å¼"""

    old_pattern: str
    new_pattern: str
    description: str
    hal_layer: str
    complexity: str = "LOW"


class GPUDebtMigrationAnalyzer:
    """GPUå€ºåŠ¡è¿ç§»åˆ†æå™¨"""

    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root) if project_root else Path(".")
        self.gpu_files: List[GPUDebtFile] = []
        self.migration_patterns: List[MigrationPattern] = []
        self.analysis_results = {}

    def analyze_gpu_debt_files(self, gpu_debt_report: str = None) -> Dict[str, Any]:
        """åˆ†æGPUå€ºåŠ¡æ–‡ä»¶"""
        print("ğŸ” åˆ†æGPUå€ºåŠ¡æ–‡ä»¶...")

        # 1. åŠ è½½GPUå€ºåŠ¡æŠ¥å‘Š
        if gpu_debt_report and os.path.exists(gpu_debt_report):
            print(f"   åŠ è½½GPUå€ºåŠ¡æŠ¥å‘Š: {gpu_debt_report}")
            debt_data = self._load_debt_report(gpu_debt_report)
        else:
            print("   ä½¿ç”¨ç°æœ‰å€ºåŠ¡ä¿¡æ¯è¿›è¡Œåˆ†æ")
            debt_data = self._get_default_debt_files()

        # 2. åˆ†ææ¯ä¸ªæ–‡ä»¶
        analyzed_files = []
        for file_info in debt_data.get("files", []):
            gpu_file = self._analyze_single_file(file_info)
            analyzed_files.append(gpu_file)

        self.gpu_files = analyzed_files

        # 3. ç”Ÿæˆè¿ç§»æ¨¡å¼
        self._generate_migration_patterns()

        # 4. åˆ›å»ºè¿ç§»ç­–ç•¥
        migration_strategy = self._create_migration_strategy()

        self.analysis_results = {
            "total_files": len(analyzed_files),
            "analyzed_files": analyzed_files,
            "migration_patterns": self.migration_patterns,
            "migration_strategy": migration_strategy,
            "hal_layers": self._analyze_hal_layer_usage(),
            "kernel_types": self._analyze_kernel_usage(),
            "complexity_breakdown": self._analyze_complexity(),
        }

        return self.analysis_results

    def _load_debt_report(self, report_path: str) -> Dict[str, Any]:
        """åŠ è½½å€ºåŠ¡æŠ¥å‘Š"""
        try:
            with open(report_path, "r", encoding="utf-8") as f:
                if report_path.endswith(".json"):
                    return json.load(f)
                else:
                    # ç®€å•çš„æ–‡æœ¬è§£æ
                    content = f.read()
                    return self._parse_text_debt_report(content)
        except Exception as e:
            print(f"   è­¦å‘Š: æ— æ³•åŠ è½½å€ºåŠ¡æŠ¥å‘Š {e}")
            return self._get_default_debt_files()

    def _parse_text_debt_report(self, content: str) -> Dict[str, Any]:
        """è§£ææ–‡æœ¬æ ¼å¼çš„å€ºåŠ¡æŠ¥å‘Š"""
        files = []
        current_file = None

        for line in content.split("\n"):
            if line.startswith("File:"):
                if current_file:
                    files.append(current_file)
                current_file = {
                    "path": line.replace("File:", "").strip(),
                    "issues": [],
                    "priority": "MEDIUM",
                    "complexity": "MEDIUM",
                }
            elif line.startswith("Issues:") and current_file:
                issues_str = line.replace("Issues:", "").strip()
                current_file["issues"] = [
                    issue.strip() for issue in issues_str.split(",")
                ]

        if current_file:
            files.append(current_file)

        return {"files": files}

    def _get_default_debt_files(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤çš„GPUå€ºåŠ¡æ–‡ä»¶åˆ—è¡¨"""
        # åŸºäºPhase 6.1çš„è¯„ä¼°ç»“æœ
        default_files = [
            {
                "path": "src/gpu/data_processor.py",
                "issues": [
                    "Direct GPU calls without error handling",
                    "Memory leak potential",
                ],
                "priority": "HIGH",
                "complexity": "HIGH",
            },
            {
                "path": "src/gpu/feature_generator.py",
                "issues": [
                    "Inconsistent GPU initialization",
                    "Missing resource cleanup",
                ],
                "priority": "HIGH",
                "complexity": "MEDIUM",
            },
            {
                "path": "src/gpu/matrix_operations.py",
                "issues": ["Hardcoded GPU device selection", "No fallback mechanism"],
                "priority": "MEDIUM",
                "complexity": "LOW",
            },
            {
                "path": "src/gpu/ml_inference.py",
                "issues": ["Tensor management issues", "GPU memory fragmentation"],
                "priority": "HIGH",
                "complexity": "HIGH",
            },
            {
                "path": "src/gpu/price_predictor.py",
                "issues": ["Direct CUDA calls", "No performance monitoring"],
                "priority": "MEDIUM",
                "complexity": "MEDIUM",
            },
            {
                "path": "src/gpu/risk_calculator.py",
                "issues": ["Inefficient memory usage", "Missing error recovery"],
                "priority": "MEDIUM",
                "complexity": "MEDIUM",
            },
            {
                "path": "src/gpu/strategy_optimizer.py",
                "issues": ["Resource contention", "No load balancing"],
                "priority": "LOW",
                "complexity": "HIGH",
            },
            {
                "path": "src/gpu/volatility_analyzer.py",
                "issues": ["Synchronous GPU operations", "No async support"],
                "priority": "MEDIUM",
                "complexity": "LOW",
            },
        ]

        # æ·»åŠ æ›´å¤šæ–‡ä»¶åˆ°38ä¸ª
        for i in range(8, 38):
            default_files.append(
                {
                    "path": f"src/gpu/debt_file_{i}.py",
                    "issues": [f"GPU issue {i}"],
                    "priority": "MEDIUM",
                    "complexity": "MEDIUM",
                }
            )

        return {"files": default_files}

    def _analyze_single_file(self, file_info: Dict[str, Any]) -> GPUDebtFile:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        file_path = file_info["path"]
        full_path = self.project_root / file_path

        if not full_path.exists():
            print(f"   è­¦å‘Š: æ–‡ä»¶ä¸å­˜åœ¨ {file_path}")
            return GPUDebtFile(
                path=file_path,
                name=os.path.basename(file_path),
                size=0,
                gpu_issues=file_info.get("issues", []),
                migration_priority=file_info.get("priority", "MEDIUM"),
                complexity=file_info.get("complexity", "MEDIUM"),
            )

        # è¯»å–æ–‡ä»¶å†…å®¹
        try:
            with open(full_path, "r", encoding="utf-8") as f:
                content = f.read()
        except Exception as e:
            print(f"   é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
            return GPUDebtFile(
                path=file_path,
                name=os.path.basename(file_path),
                size=0,
                gpu_issues=["File read error"] + file_info.get("issues", []),
            )

        file_size = len(content)

        # åˆ†æGPUä½¿ç”¨æ¨¡å¼
        gpu_patterns = self._find_gpu_patterns(content)

        # ç¡®å®šæ¨èçš„HALå±‚å’Œå†…æ ¸ç±»å‹
        hal_layer, kernel_type = self._recommend_hal_and_kernel(
            file_path, content, gpu_patterns
        )

        # åˆ†æä¾èµ–å…³ç³»
        dependencies = self._analyze_dependencies(content)

        return GPUDebtFile(
            path=file_path,
            name=os.path.basename(file_path),
            size=file_size,
            gpu_issues=file_info.get("issues", []) + gpu_patterns,
            migration_priority=file_info.get("priority", "MEDIUM"),
            complexity=file_info.get("complexity", "MEDIUM"),
            dependencies=dependencies,
            recommended_hal_layer=hal_layer,
            recommended_kernel_type=kernel_type,
        )

    def _find_gpu_patterns(self, content: str) -> List[str]:
        """æŸ¥æ‰¾GPUä½¿ç”¨æ¨¡å¼"""
        patterns = []

        # GPUåº“å¯¼å…¥
        gpu_imports = [
            r"import\s+cupy",
            r"import\s+torch",
            r"import\s+nvidia",
            r"from\s+cupy",
            r"from\s+torch",
            r"cuda\.",
            r"\.cuda\(\)",
        ]

        for pattern in gpu_imports:
            if re.search(pattern, content, re.IGNORECASE):
                patterns.append(f"GPU library usage: {pattern}")

        # ç›´æ¥GPUè°ƒç”¨
        direct_calls = [
            r"cp\.array",
            r"torch\.tensor",
            r"\.cuda\(\)",
            r"\.to\('cuda'\)",
            r"torch\.no_grad\(\)",
        ]

        for pattern in direct_calls:
            if re.search(pattern, content):
                patterns.append(f"Direct GPU call: {pattern}")

        # å†…å­˜ç®¡ç†é—®é¢˜
        memory_issues = [
            r"del\s+gpu_",
            r"cuda\.empty_cache",
            r"torch\.cuda\.synchronize",
        ]

        for pattern in memory_issues:
            if re.search(pattern, content):
                patterns.append(f"Memory management: {pattern}")

        return patterns

    def _recommend_hal_and_kernel(
        self, file_path: str, content: str, patterns: List[str]
    ) -> Tuple[str, str]:
        """æ¨èHALå±‚å’Œå†…æ ¸ç±»å‹"""
        file_name = file_path.lower()

        # æ ¹æ®æ–‡ä»¶åå’Œå†…å®¹æ¨è
        if "matrix" in file_name or any("matrix" in p for p in patterns):
            return "HardwareAbstractionLayer", "MatrixKernel"
        elif "transform" in file_name or "feature" in file_name:
            return "HardwareAbstractionLayer", "TransformKernel"
        elif "inference" in file_name or "ml" in file_name or "predict" in file_name:
            return "HardwareAbstractionLayer", "InferenceKernel"
        elif "resource" in file_name or "manager" in file_name:
            return "GPUResourceManager", "ResourceKernel"
        elif "strategy" in file_name or "context" in file_name:
            return "StrategyGPUContext", "StrategyKernel"
        elif "memory" in file_name or "pool" in file_name:
            return "MemoryPool", "MemoryKernel"
        else:
            return "HardwareAbstractionLayer", "GeneralKernel"

    def _analyze_dependencies(self, content: str) -> List[str]:
        """åˆ†ææ–‡ä»¶ä¾èµ–å…³ç³»"""
        dependencies = []

        # æŸ¥æ‰¾importè¯­å¥
        import_patterns = [
            r"import\s+(\w+)",
            r"from\s+(\w+)",
        ]

        for pattern in import_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                if match not in ["os", "sys", "time", "json", "typing"]:
                    dependencies.append(match)

        return list(set(dependencies))

    def _generate_migration_patterns(self):
        """ç”Ÿæˆè¿ç§»æ¨¡å¼"""
        self.migration_patterns = [
            MigrationPattern(
                old_pattern="import cupy as cp",
                new_pattern="from src.gpu.core.hardware_abstraction import get_gpu_resource_manager",
                description="Replace direct CuPy import with HAL resource manager",
                hal_layer="HardwareAbstractionLayer",
                complexity="MEDIUM",
            ),
            MigrationPattern(
                old_pattern="cp.array(data)",
                new_pattern="await gpu_manager.allocate_array(data)",
                description="Replace direct CuPy array creation with HAL allocation",
                hal_layer="GPUResourceManager",
                complexity="LOW",
            ),
            MigrationPattern(
                old_pattern="result = matrix_a @ matrix_b",
                new_pattern="result = await executor.execute_matrix_operation(matrix_a, matrix_b, config)",
                description="Replace direct matrix multiplication with kernel executor",
                hal_layer="KernelExecutor",
                complexity="LOW",
            ),
            MigrationPattern(
                old_pattern="torch.tensor(data, device='cuda')",
                new_pattern="await gpu_manager.allocate_tensor(data)",
                description="Replace direct PyTorch tensor creation with HAL allocation",
                hal_layer="GPUResourceManager",
                complexity="MEDIUM",
            ),
            MigrationPattern(
                old_pattern="del gpu_array",
                new_pattern="await gpu_manager.deallocate_array(gpu_array)",
                description="Replace manual memory cleanup with HAL deallocation",
                hal_layer="GPUResourceManager",
                complexity="LOW",
            ),
            MigrationPattern(
                old_pattern="cp.cuda.Device(0).use()",
                new_pattern="await gpu_manager.initialize_device()",
                description="Replace direct device selection with HAL device management",
                hal_layer="GPUResourceManager",
                complexity="HIGH",
            ),
            MigrationPattern(
                old_pattern="model.to('cuda')",
                new_pattern="await gpu_manager.allocate_model(model)",
                description="Replace direct model movement with HAL model allocation",
                hal_layer="GPUResourceManager",
                complexity="MEDIUM",
            ),
        ]

    def _create_migration_strategy(self) -> Dict[str, Any]:
        """åˆ›å»ºè¿ç§»ç­–ç•¥"""
        # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„æ–‡ä»¶
        priority_groups = defaultdict(list)
        for gpu_file in self.gpu_files:
            priority_groups[gpu_file.migration_priority].append(gpu_file)

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "high_priority": len(priority_groups.get("HIGH", [])),
            "medium_priority": len(priority_groups.get("MEDIUM", [])),
            "low_priority": len(priority_groups.get("LOW", [])),
            "total_files": len(self.gpu_files),
            "total_lines": sum(f.size for f in self.gpu_files),
            "hal_layer_distribution": defaultdict(int),
            "kernel_type_distribution": defaultdict(int),
        }

        # ç»Ÿè®¡HALå±‚åˆ†å¸ƒ
        for gpu_file in self.gpu_files:
            stats["hal_layer_distribution"][gpu_file.recommended_hal_layer] += 1
            stats["kernel_type_distribution"][gpu_file.recommended_kernel_type] += 1

        # ç”Ÿæˆè¿ç§»è®¡åˆ’
        migration_plan = [
            {
                "phase": 1,
                "name": "High Priority Files",
                "files": priority_groups.get("HIGH", []),
                "description": "Critical GPU files with high impact issues",
                "estimated_effort": "3-5 days",
            },
            {
                "phase": 2,
                "name": "Medium Priority Files",
                "files": priority_groups.get("MEDIUM", []),
                "description": "Standard GPU files requiring migration",
                "estimated_effort": "5-7 days",
            },
            {
                "phase": 3,
                "name": "Low Priority Files",
                "files": priority_groups.get("LOW", []),
                "description": "Non-critical GPU files for final cleanup",
                "estimated_effort": "2-3 days",
            },
        ]

        return {
            "statistics": stats,
            "migration_plan": migration_plan,
            "migration_patterns": self.migration_patterns,
            "success_criteria": {
                "all_files_migrated": "100% of GPU files use HAL interfaces",
                "no_direct_gpu_calls": "Zero direct CUDA/CuPy/PyTorch calls",
                "proper_error_handling": "All GPU operations have fallback mechanisms",
                "performance_maintained": "No performance regression",
                "tests_pass": "All existing tests continue to pass",
            },
        }

    def _analyze_hal_layer_usage(self) -> Dict[str, Any]:
        """åˆ†æHALå±‚ä½¿ç”¨æƒ…å†µ"""
        hal_distribution = defaultdict(int)
        for gpu_file in self.gpu_files:
            hal_distribution[gpu_file.recommended_hal_layer] += 1

        return dict(hal_distribution)

    def _analyze_kernel_usage(self) -> Dict[str, Any]:
        """åˆ†æå†…æ ¸ä½¿ç”¨æƒ…å†µ"""
        kernel_distribution = defaultdict(int)
        for gpu_file in self.gpu_files:
            kernel_distribution[gpu_file.recommended_kernel_type] += 1

        return dict(kernel_distribution)

    def _analyze_complexity(self) -> Dict[str, Any]:
        """åˆ†æå¤æ‚åº¦åˆ†å¸ƒ"""
        complexity_distribution = defaultdict(int)
        total_complexity = 0

        for gpu_file in self.gpu_files:
            complexity_distribution[gpu_file.complexity] += 1
            # ç®€å•çš„å¤æ‚åº¦è¯„åˆ†
            if gpu_file.complexity == "HIGH":
                total_complexity += 3
            elif gpu_file.complexity == "MEDIUM":
                total_complexity += 2
            else:
                total_complexity += 1

        return {
            "distribution": dict(complexity_distribution),
            "total_complexity_score": total_complexity,
            "average_complexity": total_complexity / max(1, len(self.gpu_files)),
        }

    def generate_migration_report(self, output_path: str = None) -> str:
        """ç”Ÿæˆè¿ç§»æŠ¥å‘Š"""
        report_path = output_path or "gpu_migration_analysis_report.json"

        try:
            with open(report_path, "w", encoding="utf-8") as f:
                json.dump(self.analysis_results, f, indent=2, ensure_ascii=False)

            print(f"âœ… è¿ç§»åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
            return report_path

        except Exception as e:
            print(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
            return ""

    def print_summary(self):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        if not self.analysis_results:
            print("âŒ å°šæœªè¿›è¡Œåˆ†æ")
            return

        print("\n" + "=" * 60)
        print("ğŸ“Š GPUå€ºåŠ¡è¿ç§»åˆ†ææ‘˜è¦")
        print("=" * 60)

        results = self.analysis_results

        # åŸºæœ¬ä¿¡æ¯
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {results['total_files']}")
        print(f"ğŸ“ æ€»ä»£ç è¡Œæ•°: {sum(f.size for f in results['analyzed_files']):,}")

        # ä¼˜å…ˆçº§åˆ†å¸ƒ
        strategy = results["migration_strategy"]
        stats = strategy["statistics"]
        print("\nğŸ”¥ ä¼˜å…ˆçº§åˆ†å¸ƒ:")
        print(f"   é«˜ä¼˜å…ˆçº§: {stats['high_priority']} æ–‡ä»¶")
        print(f"   ä¸­ä¼˜å…ˆçº§: {stats['medium_priority']} æ–‡ä»¶")
        print(f"   ä½ä¼˜å…ˆçº§: {stats['low_priority']} æ–‡ä»¶")

        # HALå±‚åˆ†å¸ƒ
        print("\nğŸ—ï¸ HALå±‚æ¨è:")
        for hal_layer, count in stats["hal_layer_distribution"].items():
            print(f"   {hal_layer}: {count} æ–‡ä»¶")

        # å†…æ ¸ç±»å‹åˆ†å¸ƒ
        print("\nğŸ§® å†…æ ¸ç±»å‹æ¨è:")
        for kernel_type, count in stats["kernel_type_distribution"].items():
            print(f"   {kernel_type}: {count} æ–‡ä»¶")

        # è¿ç§»è®¡åˆ’
        print("\nğŸ“‹ è¿ç§»è®¡åˆ’:")
        for phase in strategy["migration_plan"]:
            print(
                f"   é˜¶æ®µ{phase['phase']}: {phase['name']} ({len(phase['files'])}æ–‡ä»¶)"
            )
            print(f"      é¢„ä¼°å·¥ä½œé‡: {phase['estimated_effort']}")

        print("\n" + "=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    analyzer = GPUDebtMigrationAnalyzer()

    # æ£€æŸ¥æ˜¯å¦æœ‰GPUå€ºåŠ¡æŠ¥å‘Š
    debt_report = "gpu_debt_analysis_2025.json"

    print("ğŸš€ å¼€å§‹GPUå€ºåŠ¡è¿ç§»åˆ†æ...")

    # æ‰§è¡Œåˆ†æ
    results = analyzer.analyze_gpu_debt_files(debt_report)

    # ç”ŸæˆæŠ¥å‘Š
    report_path = analyzer.generate_migration_report()

    # æ‰“å°æ‘˜è¦
    analyzer.print_summary()

    return results, report_path


if __name__ == "__main__":
    main()
