#!/usr/bin/env python3
"""
åˆ†æGPUæ ¸å¿ƒè®¡ç®—æ¨¡å—
Phase 6.3.1 - åˆ†æç°æœ‰GPUæ ¸å¿ƒè®¡ç®—æ¨¡å—

è¯†åˆ«é‡æ„ç›®æ ‡ã€ä¼˜åŒ–æœºä¼šå’ŒæŠ€æœ¯å€ºåŠ¡
"""

import os
import re
from pathlib import Path
from typing import Dict, List, Any
import json
from datetime import datetime


class GPUCoreAnalyzer:
    """GPUæ ¸å¿ƒæ¨¡å—åˆ†æå™¨"""

    def __init__(self):
        self.project_root = Path(".")
        self.analysis_results = {}

        # GPUæ ¸å¿ƒæ¨¡å—è·¯å¾„
        self.core_module_paths = [
            "src/gpu/accelerated/",
            "src/gpu/core/",
            "src/gpu/api_system/",
        ]

    def analyze_gpu_core_modules(self) -> Dict[str, Any]:
        """åˆ†æGPUæ ¸å¿ƒæ¨¡å—"""
        print("ğŸ” åˆ†æGPUæ ¸å¿ƒè®¡ç®—æ¨¡å—...")

        analysis_sections = [
            ("æ¨¡å—è§„æ¨¡åˆ†æ", self.analyze_module_scale),
            ("ä»£ç è´¨é‡åˆ†æ", self.analyze_code_quality),
            ("æ€§èƒ½çƒ­ç‚¹åˆ†æ", self.analyze_performance_hotspots),
            ("ä¾èµ–å…³ç³»åˆ†æ", self.analyze_dependencies),
            ("é‡æ„ä¼˜å…ˆçº§åˆ†æ", self.analyze_refactoring_priorities),
        ]

        for section_name, analysis_func in analysis_sections:
            print(f"   ğŸ“Š {section_name}...")
            try:
                result = analysis_func()
                self.analysis_results[section_name] = result
                print(f"   âœ… {section_name}å®Œæˆ")
            except Exception as e:
                print(f"   âŒ {section_name}å¤±è´¥: {e}")
                self.analysis_results[section_name] = {
                    "success": False,
                    "error": str(e),
                }

        return self.generate_comprehensive_analysis()

    def analyze_module_scale(self) -> Dict[str, Any]:
        """åˆ†ææ¨¡å—è§„æ¨¡"""
        module_stats = {}
        total_files = 0
        total_lines = 0
        total_size = 0

        for module_path in self.core_module_paths:
            if not os.path.exists(module_path):
                continue

            stats = self._calculate_directory_stats(module_path)
            module_stats[module_path] = stats

            total_files += stats["file_count"]
            total_lines += stats["line_count"]
            total_size += stats["total_size_bytes"]

        return {
            "success": True,
            "module_statistics": module_stats,
            "summary": {
                "total_files": total_files,
                "total_lines": total_lines,
                "total_size_kb": total_size / 1024,
                "average_lines_per_file": total_lines / total_files
                if total_files > 0
                else 0,
            },
        }

    def analyze_code_quality(self) -> Dict[str, Any]:
        """åˆ†æä»£ç è´¨é‡"""
        quality_issues = {
            "complex_functions": [],
            "large_files": [],
            "missing_documentation": [],
            "potential_bugs": [],
            "performance_issues": [],
        }

        total_files_analyzed = 0

        for module_path in self.core_module_paths:
            if not os.path.exists(module_path):
                continue

            for root, dirs, files in os.walk(module_path):
                for file in files:
                    if file.endswith(".py"):
                        file_path = os.path.join(root, file)
                        file_analysis = self._analyze_file_quality(file_path)

                        total_files_analyzed += 1
                        for issue_type, issues in file_analysis.items():
                            if issues:
                                quality_issues[issue_type].extend(issues)

        return {
            "success": True,
            "files_analyzed": total_files_analyzed,
            "quality_issues": quality_issues,
            "total_issues": sum(len(issues) for issues in quality_issues.values()),
        }

    def analyze_performance_hotspots(self) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½çƒ­ç‚¹"""
        performance_patterns = {
            "nested_loops": [],
            "large_array_operations": [],
            "synchronous_gpu_calls": [],
            "memory_allocations": [],
            "file_io_operations": [],
        }

        total_files_analyzed = 0

        for module_path in self.core_module_paths:
            if not os.path.exists(module_path):
                continue

            for root, dirs, files in os.walk(module_path):
                for file in files:
                    if file.endswith(".py"):
                        file_path = os.path.join(root, file)
                        file_analysis = self._analyze_performance_patterns(file_path)

                        total_files_analyzed += 1
                        for pattern_type, occurrences in file_analysis.items():
                            if occurrences:
                                performance_patterns[pattern_type].extend(occurrences)

        return {
            "success": True,
            "files_analyzed": total_files_analyzed,
            "performance_hotspots": performance_patterns,
            "total_hotspots": sum(
                len(occurrences) for occurrences in performance_patterns.values()
            ),
        }

    def analyze_dependencies(self) -> Dict[str, Any]:
        """åˆ†æä¾èµ–å…³ç³»"""
        dependencies = {
            "external_libraries": set(),
            "internal_dependencies": set(),
            "circular_dependencies": [],
            "missing_dependencies": [],
        }

        total_files_analyzed = 0

        for module_path in self.core_module_paths:
            if not os.path.exists(module_path):
                continue

            for root, dirs, files in os.walk(module_path):
                for file in files:
                    if file.endswith(".py"):
                        file_path = os.path.join(root, file)
                        file_deps = self._analyze_file_dependencies(file_path)

                        total_files_analyzed += 1
                        dependencies["external_libraries"].update(file_deps["external"])
                        dependencies["internal_dependencies"].update(
                            file_deps["internal"]
                        )

        # Convert sets to lists for JSON serialization
        dependencies["external_libraries"] = list(dependencies["external_libraries"])
        dependencies["internal_dependencies"] = list(
            dependencies["internal_dependencies"]
        )

        return {
            "success": True,
            "files_analyzed": total_files_analyzed,
            "dependencies": dependencies,
        }

    def analyze_refactoring_priorities(self) -> Dict[str, Any]:
        """åˆ†æé‡æ„ä¼˜å…ˆçº§"""
        priorities = {
            "high_priority": [],  # æ€§èƒ½å…³é”®æˆ–æœ‰ä¸¥é‡é—®é¢˜
            "medium_priority": [],  # ä»£ç è´¨é‡é—®é¢˜
            "low_priority": [],  # ä¼˜åŒ–æœºä¼š
        }

        # åŸºäºä¹‹å‰çš„åˆ†æç»“æœç¡®å®šä¼˜å…ˆçº§
        if "ä»£ç è´¨é‡åˆ†æ" in self.analysis_results:
            quality_issues = self.analysis_results["ä»£ç è´¨é‡åˆ†æ"]["quality_issues"]

            # é«˜ä¼˜å…ˆçº§ï¼šæ½œåœ¨bugå’Œæ€§èƒ½é—®é¢˜
            priorities["high_priority"].extend(quality_issues.get("potential_bugs", []))
            priorities["high_priority"].extend(
                quality_issues.get("performance_issues", [])
            )

            # ä¸­ä¼˜å…ˆçº§ï¼šå¤æ‚å‡½æ•°å’Œå¤§æ–‡ä»¶
            priorities["medium_priority"].extend(
                quality_issues.get("complex_functions", [])
            )
            priorities["medium_priority"].extend(quality_issues.get("large_files", []))

            # ä½ä¼˜å…ˆçº§ï¼šæ–‡æ¡£é—®é¢˜
            priorities["low_priority"].extend(
                quality_issues.get("missing_documentation", [])
            )

        if "æ€§èƒ½çƒ­ç‚¹åˆ†æ" in self.analysis_results:
            hotspots = self.analysis_results["æ€§èƒ½çƒ­ç‚¹åˆ†æ"]["performance_hotspots"]

            # é«˜ä¼˜å…ˆçº§ï¼šåµŒå¥—å¾ªç¯å’ŒåŒæ­¥GPUè°ƒç”¨
            priorities["high_priority"].extend(hotspots.get("nested_loops", []))
            priorities["high_priority"].extend(
                hotspots.get("synchronous_gpu_calls", [])
            )

            # ä¸­ä¼˜å…ˆçº§ï¼šå¤§æ•°ç»„æ“ä½œ
            priorities["medium_priority"].extend(
                hotspots.get("large_array_operations", [])
            )

            # ä½ä¼˜å…ˆçº§ï¼šå†…å­˜åˆ†é…å’Œæ–‡ä»¶IO
            priorities["low_priority"].extend(hotspots.get("memory_allocations", []))
            priorities["low_priority"].extend(hotspots.get("file_io_operations", []))

        return {
            "success": True,
            "refactoring_priorities": priorities,
            "summary": {
                "high_priority_count": len(priorities["high_priority"]),
                "medium_priority_count": len(priorities["medium_priority"]),
                "low_priority_count": len(priorities["low_priority"]),
            },
        }

    def _calculate_directory_stats(self, directory: str) -> Dict[str, Any]:
        """è®¡ç®—ç›®å½•ç»Ÿè®¡ä¿¡æ¯"""
        file_count = 0
        line_count = 0
        total_size = 0

        for root, dirs, files in os.walk(directory):
            for file in files:
                if file.endswith(".py"):
                    file_path = os.path.join(root, file)
                    try:
                        with open(file_path, "r", encoding="utf-8") as f:
                            lines = f.readlines()
                            file_count += 1
                            line_count += len(lines)
                            total_size += os.path.getsize(file_path)
                    except:
                        continue

        return {
            "file_count": file_count,
            "line_count": line_count,
            "total_size_bytes": total_size,
        }

    def _analyze_file_quality(self, file_path: str) -> Dict[str, List[str]]:
        """åˆ†æå•ä¸ªæ–‡ä»¶çš„ä»£ç è´¨é‡"""
        issues = {
            "complex_functions": [],
            "large_files": [],
            "missing_documentation": [],
            "potential_bugs": [],
            "performance_issues": [],
        }

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                lines = content.split("\n")

            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            if len(lines) > 500:  # å¤§äº500è¡Œè®¤ä¸ºæ˜¯å¤§æ–‡ä»¶
                issues["large_files"].append(f"{file_path}: {len(lines)} lines")

            # æ£€æŸ¥å‡½æ•°å¤æ‚åº¦
            function_pattern = r"def\s+(\w+)\s*\([^)]*\):"
            functions = re.finditer(function_pattern, content)

            for match in functions:
                func_start = match.start()
                func_name = match.group(1)

                # ç®€å•æ£€æŸ¥å‡½æ•°é•¿åº¦ï¼ˆåŸºäºç¼©è¿›ï¼‰
                func_lines = 0
                max_nested_level = 0
                current_nested = 0

                for i in range(match.end(), len(content)):
                    if content[i] == "\n":
                        continue

                    if content[i : i + 4] == "    ":
                        current_nested = 1
                    elif content[i : i + 8] == "        ":
                        current_nested = 2
                    elif content[i : i + 12] == "            ":
                        current_nested = 3

                    max_nested_level = max(max_nested_level, current_nested)

                    if content[i] == "\n":
                        if current_nested == 0 and i > match.end() + 10:
                            break
                        func_lines += 1

                if func_lines > 50:  # å¤§äº50è¡Œè®¤ä¸ºæ˜¯å¤æ‚å‡½æ•°
                    issues["complex_functions"].append(
                        f"{file_path}: {func_name} ({func_lines} lines, nested: {max_nested_level})"
                    )

                # æ£€æŸ¥æ½œåœ¨bug
                if "eval(" in content or "exec(" in content:
                    issues["potential_bugs"].append(
                        f"{file_path}: Contains eval/exec usage"
                    )

            # æ£€æŸ¥æ–‡æ¡£
            if not content.startswith('"""') and not content.startswith("'''"):
                issues["missing_documentation"].append(
                    f"{file_path}: Missing module docstring"
                )

            # æ£€æŸ¥æ€§èƒ½é—®é¢˜
            if content.count("for ") > 10:  # è¿‡å¤šçš„forå¾ªç¯
                issues["performance_issues"].append(f"{file_path}: High loop count")

            if ".cuda()" in content or ".to(device)" in content:
                issues["performance_issues"].append(
                    f"{file_path}: Direct GPU calls without HAL"
                )

        except Exception as e:
            issues["potential_bugs"].append(f"{file_path}: Analysis failed - {e}")

        return issues

    def _analyze_performance_patterns(self, file_path: str) -> Dict[str, List[str]]:
        """åˆ†ææ€§èƒ½æ¨¡å¼"""
        patterns = {
            "nested_loops": [],
            "large_array_operations": [],
            "synchronous_gpu_calls": [],
            "memory_allocations": [],
            "file_io_operations": [],
        }

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                lines = f.readlines()

            for i, line in enumerate(lines, 1):
                line_content = line.strip()

                # åµŒå¥—å¾ªç¯æ£€æµ‹
                if "for " in line_content and i > 1:
                    prev_lines = "".join(lines[max(0, i - 10) : i])
                    if prev_lines.count("for ") >= 1:
                        patterns["nested_loops"].append(
                            f"{file_path}:{i} - Nested loop detected"
                        )

                # å¤§æ•°ç»„æ“ä½œ
                if any(
                    keyword in line_content
                    for keyword in ["np.zeros(", "np.ones(", "np.empty(", "cupy."]
                ):
                    patterns["large_array_operations"].append(
                        f"{file_path}:{i} - Large array operation"
                    )

                # åŒæ­¥GPUè°ƒç”¨
                if any(
                    keyword in line_content
                    for keyword in [".cuda()", ".to(device)", "cuda.synchronize()"]
                ):
                    patterns["synchronous_gpu_calls"].append(
                        f"{file_path}:{i} - Synchronous GPU call"
                    )

                # å†…å­˜åˆ†é…
                if "allocate" in line_content or "malloc" in line_content:
                    patterns["memory_allocations"].append(
                        f"{file_path}:{i} - Memory allocation"
                    )

                # æ–‡ä»¶IOæ“ä½œ
                if any(
                    keyword in line_content
                    for keyword in ["open(", "with open", "read(", "write("]
                ):
                    patterns["file_io_operations"].append(
                        f"{file_path}:{i} - File I/O operation"
                    )

        except Exception:
            pass

        return patterns

    def _analyze_file_dependencies(self, file_path: str) -> Dict[str, List[str]]:
        """åˆ†ææ–‡ä»¶ä¾èµ–"""
        dependencies = {"external": [], "internal": []}

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # æŸ¥æ‰¾importè¯­å¥
            import_patterns = [
                r"import\s+([a-zA-Z_][a-zA-Z0-9_\.]*)",
                r"from\s+([a-zA-Z_][a-zA-Z0-9_\.]*)\s+import",
            ]

            for pattern in import_patterns:
                matches = re.finditer(pattern, content)
                for match in matches:
                    module_name = match.group(1)

                    if module_name.startswith("src.") or module_name.startswith(".."):
                        dependencies["internal"].append(f"{file_path}: {module_name}")
                    elif not module_name.startswith("."):
                        dependencies["external"].append(f"{file_path}: {module_name}")

        except Exception:
            pass

        return dependencies

    def generate_comprehensive_analysis(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        return {
            "analysis_timestamp": datetime.now().isoformat(),
            "analyzer_version": "1.0.0",
            "analysis_results": self.analysis_results,
            "summary": self._generate_summary(),
            "recommendations": self._generate_recommendations(),
        }

    def _generate_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        summary = {
            "total_modules_analyzed": len(
                [p for p in self.core_module_paths if os.path.exists(p)]
            ),
            "key_findings": [],
        }

        if "æ¨¡å—è§„æ¨¡åˆ†æ" in self.analysis_results:
            scale_summary = self.analysis_results["æ¨¡å—è§„æ¨¡åˆ†æ"]["summary"]
            summary["key_findings"].append(
                f"æ€»ä»£ç é‡: {scale_summary['total_lines']:,} è¡Œ ({scale_summary['total_files']} ä¸ªæ–‡ä»¶)"
            )

        if "ä»£ç è´¨é‡åˆ†æ" in self.analysis_results:
            quality_summary = self.analysis_results["ä»£ç è´¨é‡åˆ†æ"]
            summary["key_findings"].append(
                f"ä»£ç è´¨é‡é—®é¢˜: {quality_summary['total_issues']} ä¸ªé—®é¢˜åœ¨ {quality_summary['files_analyzed']} ä¸ªæ–‡ä»¶ä¸­"
            )

        if "æ€§èƒ½çƒ­ç‚¹åˆ†æ" in self.analysis_results:
            perf_summary = self.analysis_results["æ€§èƒ½çƒ­ç‚¹åˆ†æ"]
            summary["key_findings"].append(
                f"æ€§èƒ½çƒ­ç‚¹: {perf_summary['total_hotspots']} ä¸ªçƒ­ç‚¹éœ€è¦ä¼˜åŒ–"
            )

        if "é‡æ„ä¼˜å…ˆçº§åˆ†æ" in self.analysis_results:
            priority_summary = self.analysis_results["é‡æ„ä¼˜å…ˆçº§åˆ†æ"]["summary"]
            summary["key_findings"].append(
                f"é‡æ„ä¼˜å…ˆçº§: {priority_summary['high_priority_count']} é«˜ä¼˜å…ˆçº§, {priority_summary['medium_priority_count']} ä¸­ä¼˜å…ˆçº§"
            )

        return summary

    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆé‡æ„å»ºè®®"""
        recommendations = []

        recommendations.extend(
            [
                "Phase 6.3.2: ä¼˜åŒ–TransformKernelEngineå®ç° - ä¿®å¤æ•°æ®å˜æ¢å†…æ ¸çš„æ€§èƒ½é—®é¢˜",
                "Phase 6.3.3: ä¼˜åŒ–MemoryPoolå†…å­˜ç®¡ç† - æé«˜å†…å­˜åˆ†é…å’Œé‡Šæ”¾æ€§èƒ½",
                "Phase 6.3.4: GPUæ ¸å¿ƒç®—æ³•ä¼˜åŒ– - é’ˆå¯¹å¤§çŸ©é˜µæ“ä½œè¿›è¡Œç®—æ³•ä¼˜åŒ–",
                "Phase 6.3.5: æ ¸å¿ƒåŠŸèƒ½é‡æ„æµ‹è¯•éªŒè¯ - ç¡®ä¿é‡æ„åçš„æ€§èƒ½æå‡å’Œç¨³å®šæ€§",
            ]
        )

        if "é‡æ„ä¼˜å…ˆçº§åˆ†æ" in self.analysis_results:
            priority_data = self.analysis_results["é‡æ„ä¼˜å…ˆçº§åˆ†æ"]
            if "summary" in priority_data:
                high_priority_count = priority_data["summary"].get(
                    "high_priority_count", 0
                )
                if high_priority_count > 5:
                    recommendations.append("ä¼˜å…ˆå¤„ç†é«˜ä¼˜å…ˆçº§é‡æ„é¡¹ç›®ï¼Œå‡å°‘æŠ€æœ¯å€ºåŠ¡")

        return recommendations


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Phase 6.3.1 GPUæ ¸å¿ƒè®¡ç®—æ¨¡å—åˆ†æ")
    print("=" * 60)

    analyzer = GPUCoreAnalyzer()

    # æ‰§è¡Œåˆ†æ
    analysis = analyzer.analyze_gpu_core_modules()

    # ä¿å­˜åˆ†ææŠ¥å‘Š
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = f"gpu_core_modules_analysis_{timestamp}.json"

    try:
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    except Exception as e:
        print(f"\nâŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 60)
    print("ğŸ“Š GPUæ ¸å¿ƒæ¨¡å—åˆ†ææ‘˜è¦")
    print("=" * 60)

    if "summary" in analysis:
        for finding in analysis["summary"]["key_findings"]:
            print(f"ğŸ” {finding}")

    if "recommendations" in analysis:
        print("\nğŸ’¡ é‡æ„å»ºè®®:")
        for i, rec in enumerate(analysis["recommendations"], 1):
            print(f"   {i}. {rec}")

    print("\n" + "=" * 60)

    return analysis


if __name__ == "__main__":
    analysis = main()
