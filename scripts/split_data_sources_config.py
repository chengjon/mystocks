#!/usr/bin/env python3
"""
æ•°æ®æºé…ç½®æ–‡ä»¶è‡ªåŠ¨æ‹†åˆ†è„šæœ¬
Data Sources Configuration Auto-Split Script

å°†å¤§å‹çš„ data_sources_registry.yaml æ–‡ä»¶æŒ‰æ•°æ®æºç±»å‹è‡ªåŠ¨æ‹†åˆ†ä¸ºå¤šä¸ªå­æ–‡ä»¶ã€‚
"""

import os
import yaml
from pathlib import Path
from typing import Dict, Any, List
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DataSourcesSplitter:
    """æ•°æ®æºé…ç½®æ‹†åˆ†å™¨"""

    def __init__(self, config_dir: str = "config", config_file: str = None):
        self.config_dir = Path(config_dir)
        if config_file:
            self.main_config_file = Path(config_file)
        else:
            self.main_config_file = self.config_dir / "data_sources_registry.yaml"
        self.sources_dir = self.config_dir / "data_sources"
        self.sources_dir.mkdir(exist_ok=True)

    def split_config(self):
        """æ‰§è¡Œé…ç½®æ–‡ä»¶æ‹†åˆ†"""
        logger.info("å¼€å§‹æ‹†åˆ†æ•°æ®æºé…ç½®æ–‡ä»¶...")

        # 1. åŠ è½½åŸå§‹é…ç½®
        original_config = self._load_yaml_file(self.main_config_file)
        if not original_config or "data_sources" not in original_config:
            logger.error("åŸå§‹é…ç½®æ–‡ä»¶æ— æ•ˆ")
            return False

        data_sources = original_config["data_sources"]
        logger.info(f"å‘ç° {len(data_sources)} ä¸ªæ•°æ®æºé…ç½®")

        # 2. æŒ‰source_nameåˆ†ç»„
        grouped_sources = self._group_by_source_name(data_sources)

        # 3. åˆ›å»ºå­é…ç½®æ–‡ä»¶
        for source_name, sources in grouped_sources.items():
            self._create_sub_config(source_name, sources)

        # 4. åˆ›å»ºæ–°çš„ä¸»é…ç½®æ–‡ä»¶
        self._create_main_config(original_config, list(grouped_sources.keys()))

        # 5. åˆ›å»ºæ¨¡æ¿æ–‡ä»¶
        self._create_template()

        logger.info("âœ… é…ç½®æ–‡ä»¶æ‹†åˆ†å®Œæˆ")
        return True

    def _group_by_source_name(
        self, data_sources: Dict[str, Any]
    ) -> Dict[str, Dict[str, Any]]:
        """æŒ‰source_nameå¯¹æ•°æ®æºè¿›è¡Œåˆ†ç»„"""
        grouped = {}

        for endpoint_name, config in data_sources.items():
            source_name = config.get("source_name", "unknown")

            if source_name not in grouped:
                grouped[source_name] = {}

            grouped[source_name][endpoint_name] = config

        logger.info(f"æŒ‰source_nameåˆ†ç»„ç»“æœ: {list(grouped.keys())}")
        return grouped

    def _create_sub_config(self, source_name: str, sources: Dict[str, Any]):
        """åˆ›å»ºå­é…ç½®æ–‡ä»¶"""
        sub_config = {
            "#": f"{source_name.upper()} æ•°æ®æºé…ç½®",
            "#": f"åŒ…å«æ‰€æœ‰{source_name}ç›¸å…³çš„{len(sources)}ä¸ªæ•°æ®æº",
            "data_sources": sources,
        }

        filename = f"{source_name}.yaml"
        filepath = self.sources_dir / filename

        self._save_yaml_file(filepath, sub_config)
        logger.info(f"âœ… åˆ›å»ºå­é…ç½®æ–‡ä»¶: {filepath} ({len(sources)} ä¸ªæ•°æ®æº)")

    def _create_main_config(
        self, original_config: Dict[str, Any], source_names: List[str]
    ):
        """åˆ›å»ºæ–°çš„ä¸»é…ç½®æ–‡ä»¶"""
        main_config = {
            "#": "æ•°æ®æºæ³¨å†Œè¡¨ä¸»é…ç½®æ–‡ä»¶",
            "#": "æ§åˆ¶å­é…ç½®æ–‡ä»¶çš„åŠ è½½",
            "version": "2.1",
            "last_updated": "2026-01-15T10:00:00",
            "load_sources": source_names,  # åŠ¨æ€ç”ŸæˆåŠ è½½åˆ—è¡¨
            "global_config": {
                "default_target_db": "postgresql",
                "enable_caching": True,
                "health_check_interval": 300,
            },
            "aliases": {
                "akshare.stock_zh_a_hist": "akshare_daily_kline",
                "sina_finance.stock_ratings": "stock_ratings_sina",
            },
        }

        # å¤‡ä»½åŸæ–‡ä»¶
        backup_file = self.main_config_file.with_suffix(".yaml.backup")
        if backup_file.exists():
            backup_file.unlink()
        self.main_config_file.rename(backup_file)
        logger.info(f"âœ… åŸé…ç½®æ–‡ä»¶å·²å¤‡ä»½åˆ°: {backup_file}")

        # ä¿å­˜æ–°ä¸»é…ç½®æ–‡ä»¶
        self._save_yaml_file(self.main_config_file, main_config)
        logger.info(f"âœ… åˆ›å»ºæ–°çš„ä¸»é…ç½®æ–‡ä»¶: {self.main_config_file}")

    def _create_template(self):
        """åˆ›å»ºé…ç½®æ¨¡æ¿æ–‡ä»¶"""
        template = {
            "#": "æ•°æ®æºé…ç½®æ¨¡æ¿",
            "#": "å¤åˆ¶æ­¤æ¨¡æ¿åˆ›å»ºæ–°çš„æ•°æ®æºé…ç½®",
            "data_sources": {
                "your_source_name_endpoint": {
                    "source_name": "your_source_name",
                    "source_type": "api_library",  # api_library | crawler | database | file | mock
                    "endpoint_name": "your_source_name.your_endpoint",
                    "call_method": "function_call",
                    "#": "5å±‚æ•°æ®åˆ†ç±»ç»‘å®š",
                    "data_category": "DAILY_KLINE",  # æ•°æ®åˆ†ç±»
                    "data_classification": "market_data",  # market_data | reference_data | derived_data | transaction_data | metadata
                    "classification_level": 1,  # 1-5å±‚
                    "target_db": "postgresql",  # postgresql | tdengine
                    "table_name": "your_table_name",
                    "#": "å‚æ•°å®šä¹‰ (JSON Schemaæ ¼å¼)",
                    "parameters": {
                        "symbol": {
                            "type": "string",
                            "required": True,
                            "description": "è‚¡ç¥¨ä»£ç ",
                            "example": "600000",
                        }
                    },
                    "description": "æ•°æ®æºæè¿°",
                    "update_frequency": "daily",  # daily | weekly | realtime
                    "data_quality_score": 8.0,  # 1-10åˆ†
                    "priority": 1,  # 1-10, è¶Šé«˜ä¼˜å…ˆçº§è¶Šé«˜
                    "status": "active",  # active | maintenance | deprecated
                    "tags": ["tag1", "tag2"],
                    "#": "æµ‹è¯•å‚æ•°",
                    "test_parameters": {"symbol": "600000"},
                    "#": "æ•°æ®è´¨é‡è§„åˆ™",
                    "quality_rules": {
                        "min_record_count": 1,
                        "max_response_time": 10.0,
                        "required_columns": ["column1", "column2"],
                    },
                }
            },
        }

        template_file = self.sources_dir / "_template.yaml"
        self._save_yaml_file(template_file, template)
        logger.info(f"âœ… åˆ›å»ºé…ç½®æ¨¡æ¿: {template_file}")

    def _load_yaml_file(self, filepath: Path) -> Dict[str, Any]:
        """åŠ è½½YAMLæ–‡ä»¶"""
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                return yaml.safe_load(f) or {}
        except Exception as e:
            logger.error(f"åŠ è½½YAMLæ–‡ä»¶å¤±è´¥ {filepath}: {e}")
            return {}

    def _save_yaml_file(self, filepath: Path, data: Dict[str, Any]):
        """ä¿å­˜YAMLæ–‡ä»¶"""
        try:
            with open(filepath, "w", encoding="utf-8") as f:
                yaml.dump(
                    data,
                    f,
                    default_flow_style=False,
                    allow_unicode=True,
                    sort_keys=False,
                )
        except Exception as e:
            logger.error(f"ä¿å­˜YAMLæ–‡ä»¶å¤±è´¥ {filepath}: {e}")


def main():
    """ä¸»å‡½æ•°"""
    import sys

    config_file = sys.argv[1] if len(sys.argv) > 1 else None
    splitter = DataSourcesSplitter(config_file=config_file)

    if splitter.split_config():
        logger.info("\nğŸ‰ æ•°æ®æºé…ç½®æ‹†åˆ†å®Œæˆï¼")
        logger.info("\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
        logger.info("1. æ£€æŸ¥æ‹†åˆ†åçš„æ–‡ä»¶ç»“æ„")
        logger.info("2. è¿è¡Œæµ‹è¯•éªŒè¯åŠ è½½åŠŸèƒ½")
        logger.info("3. æ›´æ–°ç›¸å…³æ–‡æ¡£")
    else:
        logger.error("âŒ æ•°æ®æºé…ç½®æ‹†åˆ†å¤±è´¥")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
