#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–æ•°æ®æ¸…æ´—è°ƒåº¦å™¨

åŠŸèƒ½:
1. æ¯æ—¥æ”¶ç›˜åè‡ªåŠ¨éªŒè¯Kçº¿æ•°æ®
2. æ¯å‘¨æ£€æŸ¥è¡Œä¸šæ•°æ®è´¨é‡
3. è‡ªåŠ¨ä¿®å¤adj_factorç¼ºå¤±å€¼
4. ç”Ÿæˆæ¸…æ´—æŠ¥å‘Šå¹¶å‘Šè­¦

ç”¨æ³•:
    # å¯åŠ¨è°ƒåº¦å™¨ï¼ˆåå°è¿è¡Œï¼‰
    nohup python scripts/data_cleaning/auto_clean_scheduler.py > logs/auto_clean.log 2>&1 &

    # æµ‹è¯•å•æ¬¡æ‰§è¡Œ
    python scripts/data_cleaning/auto_clean_scheduler.py --test

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    python scripts/data_cleaning/auto_clean_scheduler.py --log-level DEBUG
"""

import argparse
import logging
import sys
import schedule
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

import pandas as pd

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from scripts.data_cleaning.verify_db_data import DatabaseVerifier


class AutoCleanScheduler:
    """è‡ªåŠ¨åŒ–æ•°æ®æ¸…æ´—è°ƒåº¦å™¨"""

    def __init__(self, log_level: str = "INFO"):
        """
        åˆå§‹åŒ–è°ƒåº¦å™¨

        å‚æ•°:
            log_level: æ—¥å¿—çº§åˆ«
        """
        # é…ç½®æ—¥å¿—
        log_format = "%(asctime)s - %(levelname)s - %(message)s"
        logging.basicConfig(
            level=getattr(logging, log_level),
            format=log_format,
            handlers=[logging.StreamHandler(), logging.FileHandler("logs/auto_clean.log", encoding="utf-8")],
        )

        self.logger = logging.getLogger(__name__)

        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        Path("logs").mkdir(exist_ok=True)
        Path("reports/data_cleaning").mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–éªŒè¯å™¨
        self.verifier = DatabaseVerifier()

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "start_time": datetime.now().isoformat(),
            "kline_checks": 0,
            "industry_checks": 0,
            "auto_fixes": 0,
            "alerts": 0,
        }

    def daily_kline_check(self) -> Dict:
        """
        æ¯æ—¥æ£€æŸ¥Kçº¿æ•°æ®

        è¿”å›:
            æ£€æŸ¥ç»“æœå­—å…¸
        """
        self.logger.info("=" * 60)
        self.logger.info("æ‰§è¡Œæ¯æ—¥Kçº¿æ•°æ®æ£€æŸ¥...")
        self.logger.info("=" * 60)

        result = {
            "timestamp": datetime.now().isoformat(),
            "check_type": "daily_kline",
            "tables_checked": [],
            "issues_found": [],
            "auto_fixes": 0,
        }

        tables_to_check = ["stocks_daily", "stocks_weekly", "stocks_monthly"]

        for table in tables_to_check:
            self.logger.info(f"\næ£€æŸ¥è¡¨: {table}")

            try:
                # æ£€æŸ¥adj_factor
                adj_result = self.verifier.check_adj_factor(table)
                result["tables_checked"].append({"table": table, "adj_factor": adj_result})

                # åˆ¤æ–­æ˜¯å¦éœ€è¦è‡ªåŠ¨ä¿®å¤
                if adj_result["valid_percent"] < 95:
                    self.logger.warning(
                        f"âš ï¸ {table} adj_factoræœ‰æ•ˆç‡ä¸º{adj_result['valid_percent']:.2f}%ï¼Œè‡ªåŠ¨ä¿®å¤ä¸­..."
                    )

                    fix_result = self.verifier.fix_adj_factor(table, default_value=1.0, dry_run=False)

                    result["auto_fixes"] += fix_result["fixed_count"]
                    result["issues_found"].append(
                        {
                            "table": table,
                            "issue": "adj_factor_incomplete",
                            "severity": "WARNING",
                            "fixed": fix_result["fixed_count"],
                        }
                    )

                    self.logger.info(f"âœ… å·²ä¿®å¤{fix_result['fixed_count']}æ¡è®°å½•")

                    # è®°å½•å‘Šè­¦
                    self._send_alert(
                        f"Kçº¿æ•°æ®è‡ªåŠ¨ä¿®å¤: {table}",
                        f"adj_factoræœ‰æ•ˆç‡{adj_result['valid_percent']:.2f}%ï¼Œ"
                        f"å·²è‡ªåŠ¨ä¿®å¤{fix_result['fixed_count']}æ¡è®°å½•",
                    )
                else:
                    self.logger.info(f"âœ… {table} adj_factorå®Œæ•´ç‡: {adj_result['valid_percent']:.2f}%")

                # æ£€æŸ¥Kçº¿ç»“æ„
                structure_result = self.verifier.check_kline_structure(table)
                result["tables_checked"][-1]["structure"] = structure_result

                if not structure_result["has_all_required"]:
                    self.logger.error(f"âŒ {table} ç¼ºå°‘å¿…éœ€åˆ—: {structure_result['missing_columns']}")

                    result["issues_found"].append(
                        {
                            "table": table,
                            "issue": "missing_columns",
                            "severity": "ERROR",
                            "details": structure_result["missing_columns"],
                        }
                    )

                    self._send_alert(f"Kçº¿æ•°æ®ç»“æ„å¼‚å¸¸: {table}", f"ç¼ºå°‘å¿…éœ€åˆ—: {structure_result['missing_columns']}")

            except Exception as e:
                self.logger.error(f"æ£€æŸ¥ {table} å¤±è´¥: {e}")
                result["issues_found"].append(
                    {"table": table, "issue": "check_failed", "severity": "ERROR", "error": str(e)}
                )

        # æ›´æ–°ç»Ÿè®¡
        self.stats["kline_checks"] += 1

        # ç”ŸæˆæŠ¥å‘Š
        self._save_daily_report(result)

        return result

    def weekly_industry_check(self) -> Dict:
        """
        æ¯å‘¨æ£€æŸ¥è¡Œä¸šæ•°æ®

        è¿”å›:
            æ£€æŸ¥ç»“æœå­—å…¸
        """
        self.logger.info("=" * 60)
        self.logger.info("æ‰§è¡Œæ¯å‘¨è¡Œä¸šæ•°æ®æ£€æŸ¥...")
        self.logger.info("=" * 60)

        result = {
            "timestamp": datetime.now().isoformat(),
            "check_type": "weekly_industry",
            "tables_checked": [],
            "issues_found": [],
            "auto_fixes": 0,
        }

        tables_to_check = ["stocks_basic"]

        for table in tables_to_check:
            self.logger.info(f"\næ£€æŸ¥è¡¨: {table}")

            try:
                # æ£€æŸ¥è¡Œä¸šæ•°æ®
                industry_result = self.verifier.check_industry_data(table)
                result["tables_checked"].append({"table": table, "industry_data": industry_result})

                dirty_percent = industry_result["dirty_percent"]

                if dirty_percent > 0:
                    self.logger.warning(
                        f"âš ï¸ {table} è„æ•°æ®ç‡: {dirty_percent:.2f}% "
                        f"({industry_result['dirty_rows']}/{industry_result['total_rows']})"
                    )

                    # å¦‚æœè„æ•°æ®è¶…è¿‡10%ï¼Œä»…å‘Šè­¦ä¸è‡ªåŠ¨ä¿®å¤
                    if dirty_percent > 10:
                        self.logger.error(f"âŒ è„æ•°æ®ç‡è¿‡é«˜({dirty_percent:.2f}%)ï¼Œéœ€è¦äººå·¥å®¡æ ¸")

                        result["issues_found"].append(
                            {
                                "table": table,
                                "issue": "high_dirty_rate",
                                "severity": "CRITICAL",
                                "dirty_rate": dirty_percent,
                                "dirty_rows": industry_result["dirty_rows"],
                            }
                        )

                        self._send_alert(
                            f"è¡Œä¸šæ•°æ®è´¨é‡é—®é¢˜: {table}",
                            f"è„æ•°æ®ç‡{dirty_percent:.2f}%ï¼Œéœ€è¦äººå·¥å®¡æ ¸ï¼ˆ{industry_result['dirty_rows']}æ¡è®°å½•ï¼‰",
                        )
                    else:
                        # è„æ•°æ®ç‡è¾ƒä½ï¼Œè‡ªåŠ¨æ¸…æ´—
                        self.logger.info("è„æ•°æ®ç‡è¾ƒä½ï¼Œè‡ªåŠ¨æ¸…æ´—ä¸­...")

                        clean_result = self.verifier.clean_industry_data(table, dry_run=False)

                        result["auto_fixes"] += clean_result["fixed_count"]
                        result["issues_found"].append(
                            {
                                "table": table,
                                "issue": "dirty_industry_data",
                                "severity": "WARNING",
                                "dirty_rate": dirty_percent,
                                "fixed": clean_result["fixed_count"],
                            }
                        )

                        self.logger.info(f"âœ… å·²æ¸…æ´—{clean_result['fixed_count']}æ¡è®°å½•")

                        self._send_alert(
                            f"è¡Œä¸šæ•°æ®è‡ªåŠ¨æ¸…æ´—: {table}",
                            f"è„æ•°æ®ç‡{dirty_percent:.2f}%ï¼Œå·²è‡ªåŠ¨æ¸…æ´—{clean_result['fixed_count']}æ¡è®°å½•",
                        )
                else:
                    self.logger.info("âœ… è¡Œä¸šæ•°æ®å®Œæ•´ï¼Œæ— éœ€æ¸…æ´—")

            except Exception as e:
                self.logger.error(f"æ£€æŸ¥ {table} å¤±è´¥: {e}")
                result["issues_found"].append(
                    {"table": table, "issue": "check_failed", "severity": "ERROR", "error": str(e)}
                )

        # æ›´æ–°ç»Ÿè®¡
        self.stats["industry_checks"] += 1

        # ç”ŸæˆæŠ¥å‘Š
        self._save_weekly_report(result)

        return result

    def _send_alert(self, title: str, message: str):
        """
        å‘é€å‘Šè­¦

        å‚æ•°:
            title: å‘Šè­¦æ ‡é¢˜
            message: å‘Šè­¦å†…å®¹
        """
        self.stats["alerts"] += 1

        # è¿™é‡Œå¯ä»¥é›†æˆé‚®ä»¶ã€é’‰é’‰ã€ä¼ä¸šå¾®ä¿¡ç­‰å‘Šè­¦æ–¹å¼
        # ç›®å‰ä»…è®°å½•åˆ°æ—¥å¿—
        self.logger.warning(f"ğŸ”” å‘Šè­¦: [{title}] {message}")

    def _save_daily_report(self, result: Dict):
        """ä¿å­˜æ¯æ—¥æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d")
        report_path = Path(f"reports/data_cleaning/daily_{timestamp}.json")

        try:
            import json

            with open(report_path, "w", encoding="utf-8") as f:
                json.dump(result, f, indent=2, ensure_ascii=False)

            self.logger.info(f"æ¯æ—¥æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        except Exception as e:
            self.logger.error(f"ä¿å­˜æ¯æ—¥æŠ¥å‘Šå¤±è´¥: {e}")

    def _save_weekly_report(self, result: Dict):
        """ä¿å­˜æ¯å‘¨æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%W")
        report_path = Path(f"reports/data_cleaning/weekly_{timestamp}.json")

        try:
            import json

            with open(report_path, "w", encoding="utf-8") as f:
                json.dump(result, f, indent=2, ensure_ascii=False)

            self.logger.info(f"æ¯å‘¨æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        except Exception as e:
            self.logger.error(f"ä¿å­˜æ¯å‘¨æŠ¥å‘Šå¤±è´¥: {e}")

    def generate_summary(self) -> Dict:
        """ç”Ÿæˆç»Ÿè®¡æ‘˜è¦"""
        summary = self.stats.copy()
        summary["end_time"] = datetime.now().isoformat()

        duration = datetime.fromisoformat(summary["end_time"]) - datetime.fromisoformat(summary["start_time"])

        summary["duration_seconds"] = duration.total_seconds()
        summary["duration_formatted"] = str(duration)

        return summary

    def run(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        self.logger.info("=" * 60)
        self.logger.info("è‡ªåŠ¨åŒ–æ¸…æ´—è°ƒåº¦å™¨å·²å¯åŠ¨")
        self.logger.info("=" * 60)

        # æ³¨å†Œå®šæ—¶ä»»åŠ¡
        schedule.every().day.at("16:00").do(self.daily_kline_check)
        schedule.every().monday.at("09:00").do(self.weekly_industry_check)

        self.logger.info("å·²æ³¨å†Œå®šæ—¶ä»»åŠ¡:")
        self.logger.info("  - æ¯æ—¥ 16:00: Kçº¿æ•°æ®æ£€æŸ¥")
        self.logger.info("  - æ¯å‘¨ä¸€ 09:00: è¡Œä¸šæ•°æ®æ£€æŸ¥")
        self.logger.info("")

        # å¯åŠ¨æ—¶æ‰§è¡Œä¸€æ¬¡æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰
        # self.daily_kline_check()

        self.logger.info("è°ƒåº¦å™¨è¿è¡Œä¸­ï¼ŒæŒ‰ Ctrl+C åœæ­¢...")

        try:
            while True:
                schedule.run_pending()
                time.sleep(60)
        except KeyboardInterrupt:
            self.logger.info("\n" + "=" * 60)
            self.logger.info("è°ƒåº¦å™¨å·²åœæ­¢")
            self.logger.info("=" * 60)

            # è¾“å‡ºç»Ÿè®¡æ‘˜è¦
            summary = self.generate_summary()
            self.logger.info("ç»Ÿè®¡æ‘˜è¦:")
            self.logger.info(f"  è¿è¡Œæ—¶é•¿: {summary['duration_formatted']}")
            self.logger.info(f"  Kçº¿æ£€æŸ¥æ¬¡æ•°: {summary['kline_checks']}")
            self.logger.info(f"  è¡Œä¸šæ£€æŸ¥æ¬¡æ•°: {summary['industry_checks']}")
            self.logger.info(f"  è‡ªåŠ¨ä¿®å¤æ¬¡æ•°: {summary['auto_fixes']}")
            self.logger.info(f"  å‘Šè­¦æ¬¡æ•°: {summary['alerts']}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="è‡ªåŠ¨åŒ–æ•°æ®æ¸…æ´—è°ƒåº¦å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å¯åŠ¨è°ƒåº¦å™¨ï¼ˆåå°è¿è¡Œï¼‰
  nohup python auto_clean_scheduler.py > logs/auto_clean.log 2>&1 &

  # æµ‹è¯•å•æ¬¡Kçº¿æ£€æŸ¥
  python auto_clean_scheduler.py --test-kline

  # æµ‹è¯•å•æ¬¡è¡Œä¸šæ£€æŸ¥
  python auto_clean_scheduler.py --test-industry

  # è®¾ç½®æ—¥å¿—çº§åˆ«
  python auto_clean_scheduler.py --log-level DEBUG
        """,
    )

    parser.add_argument("--test-kline", action="store_true", help="æµ‹è¯•å•æ¬¡Kçº¿æ£€æŸ¥")
    parser.add_argument("--test-industry", action="store_true", help="æµ‹è¯•å•æ¬¡è¡Œä¸šæ£€æŸ¥")
    parser.add_argument(
        "--log-level", type=str, default="INFO", choices=["DEBUG", "INFO", "WARNING", "ERROR"], help="æ—¥å¿—çº§åˆ«"
    )

    args = parser.parse_args()

    # åˆå§‹åŒ–è°ƒåº¦å™¨
    scheduler = AutoCleanScheduler(log_level=args.log_level)

    # æµ‹è¯•æ¨¡å¼
    if args.test_kline:
        result = scheduler.daily_kline_check()
        print("\n" + "=" * 60)
        print("æµ‹è¯•å®Œæˆ")
        print("=" * 60)
        return

    if args.test_industry:
        result = scheduler.weekly_industry_check()
        print("\n" + "=" * 60)
        print("æµ‹è¯•å®Œæˆ")
        print("=" * 60)
        return

    # æ­£å¸¸æ¨¡å¼ï¼šå¯åŠ¨è°ƒåº¦å™¨
    scheduler.run()


if __name__ == "__main__":
    main()
