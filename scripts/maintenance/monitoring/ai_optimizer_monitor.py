#!/usr/bin/env python3
"""
AIæµ‹è¯•ä¼˜åŒ–å™¨ç›‘æ§å’Œåé¦ˆæ”¶é›†ç³»ç»Ÿ
å®æ—¶ç›‘æ§å·¥å…·ä½¿ç”¨æƒ…å†µã€æ”¶é›†ç”¨æˆ·åé¦ˆã€åˆ†ææ€§èƒ½æŒ‡æ ‡

åŠŸèƒ½:
1. ä½¿ç”¨æƒ…å†µç›‘æ§
2. æ€§èƒ½æŒ‡æ ‡æ”¶é›†
3. ç”¨æˆ·åé¦ˆç®¡ç†
4. è¶‹åŠ¿åˆ†ææŠ¥å‘Š
5. å¼‚å¸¸æ£€æµ‹å’Œå‘Šè­¦

ä½œè€…: MyStocks AI Team
ç‰ˆæœ¬: 1.0
æ—¥æœŸ: 2025-01-22
"""

import json
import time
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
from dataclasses import dataclass
import argparse
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
MONITORING_DIR = PROJECT_ROOT / "monitoring_data"
MONITORING_DIR.mkdir(exist_ok=True)

# æ•°æ®åº“è·¯å¾„
DB_PATH = MONITORING_DIR / "ai_optimizer_monitor.db"


@dataclass
class UsageRecord:
    """ä½¿ç”¨è®°å½•"""

    timestamp: datetime
    command: str
    files_count: int
    execution_time: float
    exit_code: int
    user_id: Optional[str] = None
    session_id: Optional[str] = None
    parameters: Optional[Dict] = None


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""

    timestamp: datetime
    cpu_usage: float
    memory_usage_mb: float
    disk_io_mb: float
    network_io_kb: float
    files_processed: int
    avg_file_size_kb: float


@dataclass
class UserFeedback:
    """ç”¨æˆ·åé¦ˆ"""

    timestamp: datetime
    user_id: str
    feedback_type: str  # bug, suggestion, feature_request, general
    category: str  # performance, usability, accuracy, other
    rating: Optional[int]  # 1-5æ˜Ÿè¯„åˆ†
    comment: str
    module: Optional[str] = None
    environment: Optional[Dict] = None


class AIOptimizerMonitor:
    """AIæµ‹è¯•ä¼˜åŒ–å™¨ç›‘æ§ç³»ç»Ÿ"""

    def __init__(self, db_path: Optional[Path] = None):
        self.db_path = db_path or DB_PATH
        self.init_database()

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS usage_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    command TEXT NOT NULL,
                    files_count INTEGER,
                    execution_time REAL,
                    exit_code INTEGER,
                    user_id TEXT,
                    session_id TEXT,
                    parameters TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            conn.execute("""
                CREATE TABLE IF NOT EXISTS performance_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    cpu_usage REAL,
                    memory_usage_mb REAL,
                    disk_io_mb REAL,
                    network_io_kb REAL,
                    files_processed INTEGER,
                    avg_file_size_kb REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            conn.execute("""
                CREATE TABLE IF NOT EXISTS user_feedback (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    user_id TEXT NOT NULL,
                    feedback_type TEXT NOT NULL,
                    category TEXT NOT NULL,
                    rating INTEGER,
                    comment TEXT NOT NULL,
                    module TEXT,
                    environment TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            conn.execute("""
                CREATE TABLE IF NOT EXISTS system_alerts (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    alert_type TEXT NOT NULL,
                    severity TEXT NOT NULL,
                    message TEXT NOT NULL,
                    metadata TEXT,
                    resolved BOOLEAN DEFAULT FALSE,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # åˆ›å»ºç´¢å¼•
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_usage_timestamp ON usage_logs(timestamp)"
            )
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_perf_timestamp ON performance_metrics(timestamp)"
            )
            conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_feedback_timestamp ON user_feedback(timestamp)"
            )

    def record_usage(self, usage: UsageRecord):
        """è®°å½•ä½¿ç”¨æƒ…å†µ"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute(
                """
                INSERT INTO usage_logs
                (timestamp, command, files_count, execution_time, exit_code, user_id, session_id, parameters)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    usage.timestamp.isoformat(),
                    usage.command,
                    usage.files_count,
                    usage.execution_time,
                    usage.exit_code,
                    usage.user_id,
                    usage.session_id,
                    json.dumps(usage.parameters) if usage.parameters else None,
                ),
            )

    def record_performance(self, metrics: PerformanceMetrics):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute(
                """
                INSERT INTO performance_metrics
                (timestamp, cpu_usage, memory_usage_mb, disk_io_mb, network_io_kb, files_processed, avg_file_size_kb)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    metrics.timestamp.isoformat(),
                    metrics.cpu_usage,
                    metrics.memory_usage_mb,
                    metrics.disk_io_mb,
                    metrics.network_io_kb,
                    metrics.files_processed,
                    metrics.avg_file_size_kb,
                ),
            )

    def record_feedback(self, feedback: UserFeedback):
        """è®°å½•ç”¨æˆ·åé¦ˆ"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute(
                """
                INSERT INTO user_feedback
                (timestamp, user_id, feedback_type, category, rating, comment, module, environment)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    feedback.timestamp.isoformat(),
                    feedback.user_id,
                    feedback.feedback_type,
                    feedback.category,
                    feedback.rating,
                    feedback.comment,
                    feedback.module,
                    json.dumps(feedback.environment) if feedback.environment else None,
                ),
            )

    def create_alert(
        self,
        alert_type: str,
        severity: str,
        message: str,
        metadata: Optional[Dict] = None,
    ):
        """åˆ›å»ºå‘Šè­¦"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute(
                """
                INSERT INTO system_alerts
                (timestamp, alert_type, severity, message, metadata)
                VALUES (?, ?, ?, ?, ?)
            """,
                (
                    datetime.now().isoformat(),
                    alert_type,
                    severity,
                    message,
                    json.dumps(metadata) if metadata else None,
                ),
            )

    def get_usage_stats(self, days: int = 7) -> Dict:
        """è·å–ä½¿ç”¨ç»Ÿè®¡"""
        with sqlite3.connect(self.db_path) as conn:
            # åŸºç¡€ç»Ÿè®¡
            start_date = (datetime.now() - timedelta(days=days)).isoformat()

            # æ€»ä½¿ç”¨æ¬¡æ•°
            total_usage = conn.execute(
                "SELECT COUNT(*) FROM usage_logs WHERE timestamp >= ?", (start_date,)
            ).fetchone()[0]

            # æˆåŠŸç‡
            success_rate = (
                conn.execute(
                    "SELECT AVG(CASE WHEN exit_code = 0 THEN 1.0 ELSE 0.0 END) FROM usage_logs WHERE timestamp >= ?",
                    (start_date,),
                ).fetchone()[0]
                or 0
            )

            # å¹³å‡æ‰§è¡Œæ—¶é—´
            avg_execution_time = (
                conn.execute(
                    "SELECT AVG(execution_time) FROM usage_logs WHERE timestamp >= ?",
                    (start_date,),
                ).fetchone()[0]
                or 0
            )

            # æ¯æ—¥ä½¿ç”¨è¶‹åŠ¿
            daily_usage = conn.execute(
                """
                SELECT DATE(timestamp) as date, COUNT(*) as count
                FROM usage_logs
                WHERE timestamp >= ?
                GROUP BY DATE(timestamp)
                ORDER BY date
            """,
                (start_date,),
            ).fetchall()

            # çƒ­é—¨å‘½ä»¤
            popular_commands = conn.execute(
                """
                SELECT command, COUNT(*) as count
                FROM usage_logs
                WHERE timestamp >= ?
                GROUP BY command
                ORDER BY count DESC
                LIMIT 5
            """,
                (start_date,),
            ).fetchall()

            return {
                "total_usage": total_usage,
                "success_rate": success_rate * 100,
                "avg_execution_time": avg_execution_time,
                "daily_usage": dict(daily_usage),
                "popular_commands": dict(popular_commands),
            }

    def get_performance_stats(self, days: int = 7) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        with sqlite3.connect(self.db_path) as conn:
            start_date = (datetime.now() - timedelta(days=days)).isoformat()

            # å¹³å‡æ€§èƒ½æŒ‡æ ‡
            avg_metrics = conn.execute(
                """
                SELECT
                    AVG(cpu_usage) as avg_cpu,
                    AVG(memory_usage_mb) as avg_memory,
                    AVG(disk_io_mb) as avg_disk_io,
                    AVG(files_processed) as avg_files
                FROM performance_metrics
                WHERE timestamp >= ?
            """,
                (start_date,),
            ).fetchone()

            # æ€§èƒ½è¶‹åŠ¿
            performance_trend = conn.execute(
                """
                SELECT DATE(timestamp) as date,
                       AVG(cpu_usage) as avg_cpu,
                       AVG(memory_usage_mb) as avg_memory
                FROM performance_metrics
                WHERE timestamp >= ?
                GROUP BY DATE(timestamp)
                ORDER BY date
            """,
                (start_date,),
            ).fetchall()

            return {
                "avg_cpu_usage": avg_metrics[0] or 0,
                "avg_memory_usage": avg_metrics[1] or 0,
                "avg_disk_io": avg_metrics[2] or 0,
                "avg_files_processed": avg_metrics[3] or 0,
                "performance_trend": [
                    {"date": row[0], "avg_cpu": row[1], "avg_memory": row[2]}
                    for row in performance_trend
                ],
            }

    def get_feedback_summary(self, days: int = 30) -> Dict:
        """è·å–åé¦ˆæ‘˜è¦"""
        with sqlite3.connect(self.db_path) as conn:
            start_date = (datetime.now() - timedelta(days=days)).isoformat()

            # åé¦ˆç»Ÿè®¡
            feedback_stats = conn.execute(
                """
                SELECT
                    feedback_type,
                    category,
                    COUNT(*) as count,
                    AVG(rating) as avg_rating
                FROM user_feedback
                WHERE timestamp >= ?
                GROUP BY feedback_type, category
            """,
                (start_date,),
            ).fetchall()

            # è¯„åˆ†åˆ†å¸ƒ
            rating_distribution = conn.execute(
                """
                SELECT rating, COUNT(*) as count
                FROM user_feedback
                WHERE timestamp >= ? AND rating IS NOT NULL
                GROUP BY rating
            """,
                (start_date,),
            ).fetchall()

            return {
                "feedback_by_type": [
                    {
                        "type": row[0],
                        "category": row[1],
                        "count": row[2],
                        "avg_rating": row[3],
                    }
                    for row in feedback_stats
                ],
                "rating_distribution": dict(rating_distribution),
            }

    def detect_anomalies(self) -> List[Dict]:
        """æ£€æµ‹å¼‚å¸¸"""
        anomalies = []

        # æ£€æµ‹æœ€è¿‘çš„å¤±è´¥ç‡å¼‚å¸¸
        recent_failures = self._check_failure_rate_anomaly()
        if recent_failures:
            anomalies.extend(recent_failures)

        # æ£€æµ‹æ€§èƒ½å¼‚å¸¸
        performance_anomalies = self._check_performance_anomalies()
        if performance_anomalies:
            anomalies.extend(performance_anomalies)

        # æ£€æµ‹ä½¿ç”¨é‡å¼‚å¸¸
        usage_anomalies = self._check_usage_anomalies()
        if usage_anomalies:
            anomalies.extend(usage_anomalies)

        return anomalies

    def _check_failure_rate_anomaly(self) -> List[Dict]:
        """æ£€æŸ¥å¤±è´¥ç‡å¼‚å¸¸"""
        anomalies = []

        with sqlite3.connect(self.db_path) as conn:
            # æœ€è¿‘24å°æ—¶çš„å¤±è´¥ç‡
            recent_start = (datetime.now() - timedelta(hours=24)).isoformat()
            recent_stats = conn.execute(
                "SELECT COUNT(*) as total, SUM(CASE WHEN exit_code != 0 THEN 1 ELSE 0 END) as failures "
                "FROM usage_logs WHERE timestamp >= ?",
                (recent_start,),
            ).fetchone()

            # å‰24å°æ—¶çš„å¤±è´¥ç‡
            previous_start = (datetime.now() - timedelta(hours=48)).isoformat()
            previous_end = (datetime.now() - timedelta(hours=24)).isoformat()
            previous_stats = conn.execute(
                "SELECT COUNT(*) as total, SUM(CASE WHEN exit_code != 0 THEN 1 ELSE 0 END) as failures "
                "FROM usage_logs WHERE timestamp BETWEEN ? AND ?",
                (previous_start, previous_end),
            ).fetchone()

            if recent_stats[0] > 0:
                recent_failure_rate = recent_stats[1] / recent_stats[0]
            else:
                recent_failure_rate = 0

            if previous_stats[0] > 0:
                previous_failure_rate = previous_stats[1] / previous_stats[0]
            else:
                previous_failure_rate = 0

            # å¤±è´¥ç‡å¢åŠ è¶…è¿‡50%åˆ™å‘Šè­¦
            if (
                recent_failure_rate > 0.1
                and recent_failure_rate > previous_failure_rate * 1.5
            ):
                anomalies.append(
                    {
                        "type": "failure_rate_spike",
                        "severity": "high",
                        "message": f"å¤±è´¥ç‡å¼‚å¸¸ä¸Šå‡: {recent_failure_rate * 100:.1f}% (å‰24å°æ—¶) vs {previous_failure_rate * 100:.1f}% (ä¹‹å‰24å°æ—¶)",
                        "data": {
                            "recent_failure_rate": recent_failure_rate,
                            "previous_failure_rate": previous_failure_rate,
                            "recent_total": recent_stats[0],
                            "recent_failures": recent_stats[1],
                        },
                    }
                )

        return anomalies

    def _check_performance_anomalies(self) -> List[Dict]:
        """æ£€æŸ¥æ€§èƒ½å¼‚å¸¸"""
        anomalies = []

        with sqlite3.connect(self.db_path) as conn:
            # æœ€è¿‘24å°æ—¶çš„æ€§èƒ½
            recent_start = (datetime.now() - timedelta(hours=24)).isoformat()
            recent_perf = conn.execute(
                "SELECT AVG(execution_time) as avg_time, MAX(execution_time) as max_time "
                "FROM usage_logs WHERE timestamp >= ?",
                (recent_start,),
            ).fetchone()

            # æ£€æµ‹æ‰§è¡Œæ—¶é—´å¼‚å¸¸
            if recent_perf[0] and recent_perf[0] > 30:  # å¹³å‡æ‰§è¡Œæ—¶é—´è¶…è¿‡30ç§’
                anomalies.append(
                    {
                        "type": "slow_execution",
                        "severity": "medium",
                        "message": f"å¹³å‡æ‰§è¡Œæ—¶é—´è¿‡é•¿: {recent_perf[0]:.1f}ç§’",
                        "data": {
                            "avg_execution_time": recent_perf[0],
                            "max_execution_time": recent_perf[1],
                        },
                    }
                )

            # æ£€æµ‹å†…å­˜ä½¿ç”¨å¼‚å¸¸
            recent_memory = conn.execute(
                "SELECT AVG(memory_usage_mb) FROM performance_metrics WHERE timestamp >= ?",
                (recent_start,),
            ).fetchone()

            if recent_memory[0] and recent_memory[0] > 1000:  # å†…å­˜ä½¿ç”¨è¶…è¿‡1GB
                anomalies.append(
                    {
                        "type": "high_memory_usage",
                        "severity": "high",
                        "message": f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {recent_memory[0]:.1f}MB",
                        "data": {"avg_memory_usage": recent_memory[0]},
                    }
                )

        return anomalies

    def _check_usage_anomalies(self) -> List[Dict]:
        """æ£€æŸ¥ä½¿ç”¨é‡å¼‚å¸¸"""
        anomalies = []

        with sqlite3.connect(self.db_path) as conn:
            # æœ€è¿‘7å¤©çš„ä½¿ç”¨é‡
            recent_start = (datetime.now() - timedelta(days=7)).isoformat()
            recent_usage = conn.execute(
                "SELECT COUNT(*) FROM usage_logs WHERE timestamp >= ?", (recent_start,)
            ).fetchone()[0]

            # å‰7å¤©çš„ä½¿ç”¨é‡
            previous_start = (datetime.now() - timedelta(days=14)).isoformat()
            previous_end = recent_start
            previous_usage = conn.execute(
                "SELECT COUNT(*) FROM usage_logs WHERE timestamp BETWEEN ? AND ?",
                (previous_start, previous_end),
            ).fetchone()[0]

            # ä½¿ç”¨é‡çªç„¶ä¸‹é™è¶…è¿‡50%
            if previous_usage > 10 and recent_usage < previous_usage * 0.5:
                anomalies.append(
                    {
                        "type": "usage_drop",
                        "severity": "medium",
                        "message": f"ä½¿ç”¨é‡å¼‚å¸¸ä¸‹é™: æœ€è¿‘7å¤© {recent_usage} æ¬¡ vs ä¹‹å‰7å¤© {previous_usage} æ¬¡",
                        "data": {
                            "recent_usage": recent_usage,
                            "previous_usage": previous_usage,
                            "drop_percentage": (1 - recent_usage / previous_usage)
                            * 100,
                        },
                    }
                )

        return anomalies

    def generate_daily_report(self) -> str:
        """ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š"""
        today = datetime.now().strftime("%Y-%m-%d")

        usage_stats = self.get_usage_stats(1)
        performance_stats = self.get_performance_stats(1)
        feedback_summary = self.get_feedback_summary(7)  # æœ€è¿‘7å¤©çš„åé¦ˆ
        anomalies = self.detect_anomalies()

        report = f"""# AIæµ‹è¯•ä¼˜åŒ–å™¨æ¯æ—¥ç›‘æ§æŠ¥å‘Š

**æ—¥æœŸ**: {today}
**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## ğŸ“Š ä»Šæ—¥ä½¿ç”¨ç»Ÿè®¡

- **æ€»ä½¿ç”¨æ¬¡æ•°**: {usage_stats["total_usage"]}
- **æˆåŠŸç‡**: {usage_stats["success_rate"]:.1f}%
- **å¹³å‡æ‰§è¡Œæ—¶é—´**: {usage_stats["avg_execution_time"]:.2f}ç§’

## âš¡ æ€§èƒ½æŒ‡æ ‡

- **å¹³å‡CPUä½¿ç”¨**: {performance_stats["avg_cpu_usage"]:.1f}%
- **å¹³å‡å†…å­˜ä½¿ç”¨**: {performance_stats["avg_memory_usage"]:.1f}MB
- **å¹³å‡å¤„ç†æ–‡ä»¶æ•°**: {performance_stats["avg_files_processed"]:.1f}

## ğŸ—£ï¸ ç”¨æˆ·åé¦ˆ

æœ€è¿‘7å¤©åé¦ˆç»Ÿè®¡:
{self._format_feedback_summary(feedback_summary)}

## ğŸš¨ å¼‚å¸¸æ£€æµ‹

"""

        if anomalies:
            for anomaly in anomalies:
                report += f"### {anomaly['severity'].upper()}: {anomaly['type']}\n"
                report += f"{anomaly['message']}\n\n"
        else:
            report += "âœ… æœªæ£€æµ‹åˆ°å¼‚å¸¸\n"

        report += f"""
## ğŸ“ˆ è¶‹åŠ¿åˆ†æ

### ä½¿ç”¨è¶‹åŠ¿
{self._format_usage_trend(usage_stats.get("daily_usage", {}))}

### æ€§èƒ½è¶‹åŠ¿
{self._format_performance_trend(performance_stats.get("performance_trend", []))}

## ğŸ¯ å»ºè®®è¡ŒåŠ¨

{self._generate_recommendations(usage_stats, performance_stats, anomalies)}

---
*æŠ¥å‘Šç”±AIæµ‹è¯•ä¼˜åŒ–å™¨ç›‘æ§ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""

        return report

    def _format_feedback_summary(self, feedback_summary: Dict) -> str:
        """æ ¼å¼åŒ–åé¦ˆæ‘˜è¦"""
        if not feedback_summary["feedback_by_type"]:
            return "æš‚æ— åé¦ˆæ•°æ®"

        summary = ""
        for feedback in feedback_summary["feedback_by_type"]:
            summary += (
                f"- {feedback['type']} ({feedback['category']}): {feedback['count']} æ¡"
            )
            if feedback["avg_rating"]:
                summary += f", å¹³å‡è¯„åˆ†: {feedback['avg_rating']:.1f}â­"
            summary += "\n"

        return summary

    def _format_usage_trend(self, daily_usage: Dict) -> str:
        """æ ¼å¼åŒ–ä½¿ç”¨è¶‹åŠ¿"""
        if not daily_usage:
            return "æš‚æ— ä½¿ç”¨æ•°æ®"

        trend = "| æ—¥æœŸ | ä½¿ç”¨æ¬¡æ•° |\n"
        trend += "|------|----------|\n"

        for date, count in sorted(daily_usage.items()):
            trend += f"| {date} | {count} |\n"

        return trend

    def _format_performance_trend(self, performance_trend: List) -> str:
        """æ ¼å¼åŒ–æ€§èƒ½è¶‹åŠ¿"""
        if not performance_trend:
            return "æš‚æ— æ€§èƒ½æ•°æ®"

        trend = "| æ—¥æœŸ | å¹³å‡CPU | å¹³å‡å†…å­˜ |\n"
        trend += "|------|---------|----------|\n"

        for day_data in performance_trend:
            trend += f"| {day_data['date']} | {day_data['avg_cpu']:.1f}% | {day_data['avg_memory']:.1f}MB |\n"

        return trend

    def _generate_recommendations(
        self, usage_stats: Dict, performance_stats: Dict, anomalies: List
    ) -> str:
        """ç”Ÿæˆå»ºè®®"""
        recommendations = []

        # åŸºäºä½¿ç”¨æƒ…å†µçš„å»ºè®®
        if usage_stats["success_rate"] < 90:
            recommendations.append("ğŸ”§ æˆåŠŸç‡åä½ï¼Œå»ºè®®æ£€æŸ¥å¸¸è§å¤±è´¥åŸå› å¹¶ä¼˜åŒ–é”™è¯¯å¤„ç†")

        if usage_stats["avg_execution_time"] > 10:
            recommendations.append("âš¡ æ‰§è¡Œæ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–ç®—æ³•æ€§èƒ½æˆ–å¢åŠ ç¼“å­˜æœºåˆ¶")

        # åŸºäºæ€§èƒ½çš„å»ºè®®
        if performance_stats["avg_memory_usage"] > 500:
            recommendations.append("ğŸ’¾ å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–å†…å­˜ç®¡ç†æˆ–å¢åŠ å†…å­˜é™åˆ¶")

        # åŸºäºå¼‚å¸¸çš„å»ºè®®
        if anomalies:
            recommendations.append("ğŸš¨ æ£€æµ‹åˆ°ç³»ç»Ÿå¼‚å¸¸ï¼Œå»ºè®®ç«‹å³å…³æ³¨å¹¶é‡‡å–ç›¸åº”æªæ–½")

        if not recommendations:
            recommendations.append("âœ… ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œç»§ç»­ä¿æŒå½“å‰é…ç½®")

        return "\n".join(f"- {rec}" for rec in recommendations)

    def save_daily_report(self, report: str) -> Path:
        """ä¿å­˜æ¯æ—¥æŠ¥å‘Š"""
        today = datetime.now().strftime("%Y-%m-%d")
        report_path = MONITORING_DIR / f"daily_report_{today}.md"

        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report)

        return report_path


def run_monitoring_daemon():
    """è¿è¡Œç›‘æ§å®ˆæŠ¤è¿›ç¨‹"""
    monitor = AIOptimizerMonitor()

    logger.info("ğŸ¤– AIæµ‹è¯•ä¼˜åŒ–å™¨ç›‘æ§å®ˆæŠ¤è¿›ç¨‹å¯åŠ¨")
    logger.info(f"æ•°æ®åº“è·¯å¾„: {monitor.db_path}")
    logger.info(f"ç›‘æ§æ•°æ®ç›®å½•: {MONITORING_DIR}")

    try:
        while True:
            try:
                # æ£€æµ‹å¼‚å¸¸
                anomalies = monitor.detect_anomalies()

                if anomalies:
                    logger.warning(f"ğŸš¨ æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸")
                    for anomaly in anomalies:
                        logger.warning(
                            f"  - {anomaly['severity']}: {anomaly['message']}"
                        )

                        # åˆ›å»ºå‘Šè­¦è®°å½•
                        monitor.create_alert(
                            anomaly["type"],
                            anomaly["severity"],
                            anomaly["message"],
                            anomaly.get("data"),
                        )

                # ç”Ÿæˆæ¯æ—¥æŠ¥å‘Šï¼ˆæ¯å¤©æ—©ä¸Š9ç‚¹ï¼‰
                now = datetime.now()
                if now.hour == 9 and now.minute == 0:
                    logger.info("ğŸ“Š ç”Ÿæˆæ¯æ—¥ç›‘æ§æŠ¥å‘Š")
                    report = monitor.generate_daily_report()
                    report_path = monitor.save_daily_report(report)
                    logger.info(f"ğŸ“„ æ¯æ—¥æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

                # ç­‰å¾…1åˆ†é’Ÿåç»§ç»­ç›‘æ§
                time.sleep(60)

            except Exception as e:
                logger.error(f"âŒ ç›‘æ§è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                time.sleep(60)  # å‡ºé”™æ—¶ç­‰å¾…1åˆ†é’Ÿåç»§ç»­

    except KeyboardInterrupt:
        logger.info("â¹ï¸  ç›‘æ§å®ˆæŠ¤è¿›ç¨‹å·²åœæ­¢")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="AIæµ‹è¯•ä¼˜åŒ–å™¨ç›‘æ§å’Œåé¦ˆæ”¶é›†ç³»ç»Ÿ")
    parser.add_argument("--daemon", "-d", action="store_true", help="è¿è¡Œç›‘æ§å®ˆæŠ¤è¿›ç¨‹")
    parser.add_argument(
        "--report", "-r", action="store_true", help="ç”Ÿæˆå¹¶æ˜¾ç¤ºæ¯æ—¥æŠ¥å‘Š"
    )
    parser.add_argument(
        "--usage-stats", "-u", type=int, default=7, help="è·å–æœ€è¿‘Nå¤©çš„ä½¿ç”¨ç»Ÿè®¡"
    )
    parser.add_argument(
        "--performance-stats", "-p", type=int, default=7, help="è·å–æœ€è¿‘Nå¤©çš„æ€§èƒ½ç»Ÿè®¡"
    )
    parser.add_argument(
        "--check-anomalies", "-a", action="store_true", help="æ£€æŸ¥å¹¶æŠ¥å‘Šå¼‚å¸¸"
    )
    parser.add_argument(
        "--feedback", "-f", action="store_true", help="æ˜¾ç¤ºç”¨æˆ·åé¦ˆæ‘˜è¦"
    )

    args = parser.parse_args()

    try:
        monitor = AIOptimizerMonitor()

        if args.daemon:
            run_monitoring_daemon()
        elif args.report:
            logger.info("ğŸ“Š ç”Ÿæˆæ¯æ—¥ç›‘æ§æŠ¥å‘Š")
            report = monitor.generate_daily_report()
            print(report)

            # ä¿å­˜æŠ¥å‘Š
            report_path = monitor.save_daily_report(report)
            print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        elif args.usage_stats:
            stats = monitor.get_usage_stats(args.usage_stats)
            print(f"ğŸ“Š æœ€è¿‘{args.usage_stats}å¤©ä½¿ç”¨ç»Ÿè®¡:")
            print(json.dumps(stats, indent=2, ensure_ascii=False))
        elif args.performance_stats:
            stats = monitor.get_performance_stats(args.performance_stats)
            print(f"âš¡ æœ€è¿‘{args.performance_stats}å¤©æ€§èƒ½ç»Ÿè®¡:")
            print(json.dumps(stats, indent=2, ensure_ascii=False))
        elif args.check_anomalies:
            anomalies = monitor.detect_anomalies()
            if anomalies:
                print(f"ğŸš¨ æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸:")
                for anomaly in anomalies:
                    print(f"  [{anomaly['severity'].upper()}] {anomaly['message']}")
            else:
                print("âœ… æœªæ£€æµ‹åˆ°å¼‚å¸¸")
        elif args.feedback:
            feedback = monitor.get_feedback_summary()
            print("ğŸ—£ï¸ ç”¨æˆ·åé¦ˆæ‘˜è¦:")
            print(json.dumps(feedback, indent=2, ensure_ascii=False))
        else:
            print("è¯·ä½¿ç”¨ --help æŸ¥çœ‹å¯ç”¨é€‰é¡¹")
            return 1

    except Exception as e:
        logger.error(f"ğŸ’¥ ç›‘æ§ç³»ç»Ÿå‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback

        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    import sys

    sys.exit(main())
