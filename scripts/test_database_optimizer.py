"""
æµ‹è¯•æ•°æ®åº“ä¼˜åŒ–å™¨
Test Database Optimizer

éªŒè¯æ•°æ®åº“ä¼˜åŒ–ã€ç´¢å¼•ä¼˜åŒ–ã€æŸ¥è¯¢åˆ†æç­‰åŠŸèƒ½çš„æ­£ç¡®æ€§ã€‚
Validates database optimization, index optimization, query analysis functions.
"""

import asyncio
import logging
import sys
import os
from unittest.mock import patch, MagicMock

# Setup project path
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from scripts.database.database_optimizer import DatabaseOptimizer

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MockDatabaseConnection:
    """æ¨¡æ‹Ÿæ•°æ®åº“è¿æ¥"""

    def __init__(self):
        self.queries_executed = []
        self.fetch_results = {
            "table_stats": [
                {
                    "schemaname": "public",
                    "tablename": "test_table",
                    "n_tup_ins": 1000,
                    "n_tup_upd": 500,
                    "n_tup_del": 100,
                    "n_live_tup": 1400,
                    "n_dead_tup": 200,
                    "last_vacuum": None,
                    "last_autovacuum": None,
                    "last_analyze": None,
                    "last_autoanalyze": None,
                }
            ],
            "index_stats": [
                {
                    "schemaname": "public",
                    "tablename": "test_table",
                    "indexname": "idx_test_column",
                    "idx_scan": 100,
                    "idx_tup_read": 1000,
                    "idx_tup_fetch": 800,
                },
                {
                    "schemaname": "public",
                    "tablename": "test_table",
                    "indexname": "idx_unused",
                    "idx_scan": 0,
                    "idx_tup_read": 0,
                    "idx_tup_fetch": 0,
                },
            ],
            "index_candidates": [
                {
                    "schemaname": "public",
                    "tablename": "test_table",
                    "attname": "high_cardinality_column",
                    "n_distinct": 10000,
                    "correlation": 0.8,
                }
            ],
            "vacuum_candidates": [{"tablename": "test_table"}],
            "slow_queries": [
                {
                    "query": "SELECT * FROM large_table WHERE slow_column = ?",
                    "calls": 100,
                    "total_time": 50000,
                    "mean_time": 500,
                    "rows": 1000,
                }
            ],
            "frequent_queries": [
                {"query": "SELECT COUNT(*) FROM frequent_table", "calls": 5000, "total_time": 2500, "mean_time": 0.5}
            ],
        }

    async def fetch(self, query: str):
        """æ¨¡æ‹ŸæŸ¥è¯¢ç»“æœ"""
        if "pg_stat_user_tables" in query:
            return self.fetch_results["table_stats"]
        elif "pg_stat_user_indexes" in query:
            return self.fetch_results["index_stats"]
        elif "pg_stats" in query:
            return self.fetch_results["index_candidates"]
        elif "n_dead_tup > 1000" in query:
            return self.fetch_results["vacuum_candidates"]
        elif "pg_stat_statements" in query and "mean_time > 1000" in query:
            return self.fetch_results["slow_queries"]
        elif "pg_stat_statements" in query and "calls > 1000" in query:
            return self.fetch_results["frequent_queries"]
        return []

    async def fetchval(self, query: str):
        """æ¨¡æ‹Ÿå•ä¸ªå€¼æŸ¥è¯¢"""
        return None

    async def execute(self, query: str, *args):
        """æ¨¡æ‹Ÿæ‰§è¡ŒæŸ¥è¯¢"""
        self.queries_executed.append((query, args))
        return "1"  # æ¨¡æ‹Ÿå½±å“è¡Œæ•°

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        pass


async def test_table_statistics_analysis():
    """æµ‹è¯•è¡¨ç»Ÿè®¡åˆ†æ"""
    logger.info("ğŸ§ª æµ‹è¯•è¡¨ç»Ÿè®¡åˆ†æ...")

    optimizer = DatabaseOptimizer()

    # Mockæ•°æ®åº“è¿æ¥
    mock_conn = MockDatabaseConnection()

    with patch.object(optimizer.db_manager, "get_connection") as mock_get_conn:
        mock_get_conn.return_value.__aenter__.return_value = mock_conn
        mock_get_conn.return_value.__aexit__.return_value = None

        stats = await optimizer.analyze_table_statistics()

        # éªŒè¯ç»“æœ
        assert "tables" in stats
        assert "test_table" in stats["tables"]
        assert "indexes" in stats
        assert "recommendations" in stats

        # æ£€æŸ¥è†¨èƒ€æ£€æµ‹
        table_stats = stats["tables"]["test_table"]
        assert "bloat_ratio" in table_stats
        assert table_stats["bloat_ratio"] > 0.1  # 200/1400 = 0.142

        # æ£€æŸ¥æ¨èå»ºè®®
        recommendations = stats["recommendations"]
        assert len(recommendations) >= 2  # VACUUM + unused index

        logger.info("âœ… è¡¨ç»Ÿè®¡åˆ†ææµ‹è¯•é€šè¿‡")
        return True


async def test_index_optimization():
    """æµ‹è¯•ç´¢å¼•ä¼˜åŒ–"""
    logger.info("ğŸ§ª æµ‹è¯•ç´¢å¼•ä¼˜åŒ–...")

    optimizer = DatabaseOptimizer()
    mock_conn = MockDatabaseConnection()

    with patch.object(optimizer.db_manager, "get_connection") as mock_get_conn:
        mock_get_conn.return_value.__aenter__.return_value = mock_conn
        mock_get_conn.return_value.__aexit__.return_value = None

        result = await optimizer.optimize_indexes()

        # éªŒè¯ç»“æœ
        assert "created_indexes" in result
        assert "dropped_indexes" in result
        assert "errors" in result

        # æ£€æŸ¥æ˜¯å¦åˆ›å»ºäº†ç´¢å¼•
        created = result["created_indexes"]
        assert len(created) > 0

        # éªŒè¯ç´¢å¼•åˆ›å»ºå‚æ•°
        index_info = created[0]
        assert "table" in index_info
        assert "column" in index_info
        assert "index_name" in index_info

        logger.info("âœ… ç´¢å¼•ä¼˜åŒ–æµ‹è¯•é€šè¿‡")
        return True


async def test_table_maintenance():
    """æµ‹è¯•è¡¨ç»´æŠ¤"""
    logger.info("ğŸ§ª æµ‹è¯•è¡¨ç»´æŠ¤...")

    optimizer = DatabaseOptimizer()
    mock_conn = MockDatabaseConnection()

    with patch.object(optimizer.db_manager, "get_connection") as mock_get_conn:
        mock_get_conn.return_value.__aenter__.return_value = mock_conn
        mock_get_conn.return_value.__aexit__.return_value = None

        result = await optimizer.vacuum_analyze_tables()

        # éªŒè¯ç»“æœ
        assert "vacuumed_tables" in result
        assert "analyzed_tables" in result
        assert "errors" in result

        # æ£€æŸ¥VACUUM ANALYZEæ‰§è¡Œ
        assert len(result["vacuumed_tables"]) > 0
        assert result["vacuumed_tables"][0] == "test_table"

        logger.info("âœ… è¡¨ç»´æŠ¤æµ‹è¯•é€šè¿‡")
        return True


async def test_query_analysis():
    """æµ‹è¯•æŸ¥è¯¢åˆ†æ"""
    logger.info("ğŸ§ª æµ‹è¯•æŸ¥è¯¢åˆ†æ...")

    optimizer = DatabaseOptimizer()
    mock_conn = MockDatabaseConnection()

    with patch.object(optimizer.db_manager, "get_connection") as mock_get_conn:
        mock_get_conn.return_value.__aenter__.return_value = mock_conn
        mock_get_conn.return_value.__aexit__.return_value = None

        result = await optimizer.optimize_queries()

        # éªŒè¯ç»“æœ
        assert "slow_queries" in result
        assert "frequent_queries" in result
        assert "recommendations" in result

        # æ£€æŸ¥æ…¢æŸ¥è¯¢æ£€æµ‹
        slow_queries = result["slow_queries"]
        assert len(slow_queries) > 0

        slow_query = slow_queries[0]
        assert "query" in slow_query
        assert "calls" in slow_query
        assert "mean_time_ms" in slow_query

        # æ£€æŸ¥æ¨èå»ºè®®
        recommendations = result["recommendations"]
        assert len(recommendations) > 0

        logger.info("âœ… æŸ¥è¯¢åˆ†ææµ‹è¯•é€šè¿‡")
        return True


async def test_data_cleanup():
    """æµ‹è¯•æ•°æ®æ¸…ç†"""
    logger.info("ğŸ§ª æµ‹è¯•æ•°æ®æ¸…ç†...")

    optimizer = DatabaseOptimizer()
    mock_conn = MockDatabaseConnection()

    with patch.object(optimizer.db_manager, "get_connection") as mock_get_conn:
        mock_get_conn.return_value.__aenter__.return_value = mock_conn
        mock_get_conn.return_value.__aexit__.return_value = None

        result = await optimizer.cleanup_old_data(days_to_keep=90)

        # éªŒè¯ç»“æœ
        assert "deleted_records" in result
        assert "errors" in result

        # æ£€æŸ¥åˆ é™¤è®°å½•ç»Ÿè®¡
        deleted = result["deleted_records"]
        assert len(deleted) >= 2  # audit_logs and performance_metrics

        logger.info("âœ… æ•°æ®æ¸…ç†æµ‹è¯•é€šè¿‡")
        return True


async def test_full_optimization_report():
    """æµ‹è¯•å®Œæ•´ä¼˜åŒ–æŠ¥å‘Š"""
    logger.info("ğŸ§ª æµ‹è¯•å®Œæ•´ä¼˜åŒ–æŠ¥å‘Š...")

    optimizer = DatabaseOptimizer()
    mock_conn = MockDatabaseConnection()

    with patch.object(optimizer.db_manager, "get_connection") as mock_get_conn:
        mock_get_conn.return_value.__aenter__.return_value = mock_conn
        mock_get_conn.return_value.__aexit__.return_value = None

        report = await optimizer.generate_optimization_report()

        # éªŒè¯æŠ¥å‘Šç»“æ„
        assert "timestamp" in report
        assert "database_stats" in report
        assert "optimization_results" in report
        assert "recommendations" in report

        # éªŒè¯æ•°æ®åº“ç»Ÿè®¡
        db_stats = report["database_stats"]
        assert "tables" in db_stats
        assert "indexes" in db_stats

        # éªŒè¯ä¼˜åŒ–ç»“æœ
        opt_results = report["optimization_results"]
        assert "index_optimization" in opt_results
        assert "table_maintenance" in opt_results
        assert "query_analysis" in opt_results

        logger.info("âœ… å®Œæ•´ä¼˜åŒ–æŠ¥å‘Šæµ‹è¯•é€šè¿‡")
        return True


async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    logger.info("ğŸš€ è¿è¡Œæ•°æ®åº“ä¼˜åŒ–å™¨å®Œæ•´æµ‹è¯•å¥—ä»¶...")

    results = []

    # æµ‹è¯•1: è¡¨ç»Ÿè®¡åˆ†æ
    logger.info("\n" + "=" * 50)
    logger.info("TEST 1: è¡¨ç»Ÿè®¡åˆ†æ")
    logger.info("=" * 50)
    result1 = await test_table_statistics_analysis()
    results.append(("Table Statistics Analysis", result1))

    # æµ‹è¯•2: ç´¢å¼•ä¼˜åŒ–
    logger.info("\n" + "=" * 50)
    logger.info("TEST 2: ç´¢å¼•ä¼˜åŒ–")
    logger.info("=" * 50)
    result2 = await test_index_optimization()
    results.append(("Index Optimization", result2))

    # æµ‹è¯•3: è¡¨ç»´æŠ¤
    logger.info("\n" + "=" * 50)
    logger.info("TEST 3: è¡¨ç»´æŠ¤")
    logger.info("=" * 50)
    result3 = await test_table_maintenance()
    results.append(("Table Maintenance", result3))

    # æµ‹è¯•4: æŸ¥è¯¢åˆ†æ
    logger.info("\n" + "=" * 50)
    logger.info("TEST 4: æŸ¥è¯¢åˆ†æ")
    logger.info("=" * 50)
    result4 = await test_query_analysis()
    results.append(("Query Analysis", result4))

    # æµ‹è¯•5: æ•°æ®æ¸…ç†
    logger.info("\n" + "=" * 50)
    logger.info("TEST 5: æ•°æ®æ¸…ç†")
    logger.info("=" * 50)
    result5 = await test_data_cleanup()
    results.append(("Data Cleanup", result5))

    # æµ‹è¯•6: å®Œæ•´ä¼˜åŒ–æŠ¥å‘Š
    logger.info("\n" + "=" * 50)
    logger.info("TEST 6: å®Œæ•´ä¼˜åŒ–æŠ¥å‘Š")
    logger.info("=" * 50)
    result6 = await test_full_optimization_report()
    results.append(("Full Optimization Report", result6))

    # æ€»ç»“
    logger.info("\n" + "=" * 50)
    logger.info("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    logger.info("=" * 50)

    passed = 0
    total = len(results)

    for test_name, success in results:
        status = "âœ… PASSED" if success else "âŒ FAILED"
        logger.info("%s: %s", test_name, status)
        if success:
            passed += 1

    logger.info("æ€»ä½“: %d/%d æµ‹è¯•é€šè¿‡", passed, total)

    if passed == total:
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! æ•°æ®åº“ä¼˜åŒ–å™¨å·²å‡†å¤‡å°±ç»ªã€‚")
        logger.info("æ•°æ®åº“ä¼˜åŒ–åŠŸèƒ½åŒ…æ‹¬è¡¨åˆ†æã€ç´¢å¼•ä¼˜åŒ–ã€æŸ¥è¯¢åˆ†æã€æ•°æ®æ¸…ç†ç­‰ã€‚")
        return True
    else:
        logger.warning("âš ï¸ æŸäº›æµ‹è¯•å¤±è´¥ã€‚è¯·æ£€æŸ¥å®ç°ã€‚")
        return False


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    success = asyncio.run(run_all_tests())
    exit(0 if success else 1)
