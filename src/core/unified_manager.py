"""
MyStocksç»Ÿä¸€æ•°æ®ç®¡ç†å™¨ - é›†æˆç›‘æ§ç‰ˆæœ¬ (US1 + US3)

è¿™æ˜¯ç³»ç»Ÿçš„æ ¸å¿ƒå…¥å£,æä¾›ç»Ÿä¸€çš„æ•°æ®ä¿å­˜å’ŒåŠ è½½æ¥å£ã€‚
ç”¨æˆ·åªéœ€è°ƒç”¨save_data_by_classification()å’Œload_data_by_classification(),
ç³»ç»Ÿè‡ªåŠ¨æ ¹æ®æ•°æ®åˆ†ç±»è·¯ç”±åˆ°æœ€ä¼˜æ•°æ®åº“ã€‚

æ–°å¢åŠŸèƒ½ (US3):
- æ‰€æœ‰æ“ä½œè‡ªåŠ¨è®°å½•åˆ°ç›‘æ§æ•°æ®åº“
- æ€§èƒ½æŒ‡æ ‡è‡ªåŠ¨æ”¶é›†
- æ…¢æŸ¥è¯¢è‡ªåŠ¨å‘Šè­¦
- æ•°æ®è´¨é‡è‡ªåŠ¨æ£€æŸ¥

åˆ›å»ºæ—¥æœŸ: 2025-10-11
ç‰ˆæœ¬: 2.0.0 (MVP US1 + US3ç›‘æ§é›†æˆ)
"""

import pandas as pd
import time
import logging
from typing import Optional, Dict, Any, List, Union
from datetime import datetime

from src.core.data_classification import DataClassification, DatabaseTarget

logger = logging.getLogger(__name__)
# US3: å·²ç§»é™¤DataStorageStrategyï¼Œä½¿ç”¨DataManagerè¿›è¡Œè·¯ç”±
from src.core.batch_failure_strategy import (
    BatchFailureStrategy,
    BatchFailureHandler,
    BatchOperationResult,
)
from src.data_access import (
    TDengineDataAccess,
    PostgreSQLDataAccess,
)
# æ³¨é‡Šæ‰ä¸å­˜åœ¨çš„MySQLå¯¼å…¥ - ç³»ç»Ÿå·²ç®€åŒ–ä¸ºTDengine+PostgreSQLåŒæ•°æ®åº“æ¶æ„
# from src.storage.database.database_manager import MySQLDataAccess
# æ³¨é‡Šæ‰ä¸å­˜åœ¨çš„Rediså¯¼å…¥
# from src.db_manager.redis_manager import RedisDataAccess
from src.utils.failure_recovery_queue import FailureRecoveryQueue

# ç›‘æ§ç»„ä»¶ (US3)
from src.monitoring.monitoring_database import get_monitoring_database
from src.monitoring.performance_monitor import get_performance_monitor
from src.monitoring.data_quality_monitor import get_quality_monitor
from src.monitoring.alert_manager import get_alert_manager


class MyStocksUnifiedManager:
    """
    MyStocksç»Ÿä¸€æ•°æ®ç®¡ç†å™¨

    **æ ¸å¿ƒåŠŸèƒ½** (MVP US1):
    1. è‡ªåŠ¨è·¯ç”±: æ ¹æ®æ•°æ®åˆ†ç±»è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ•°æ®åº“
    2. ç»Ÿä¸€æ¥å£: 2è¡Œä»£ç å®Œæˆä¿å­˜/åŠ è½½æ“ä½œ
    3. æ•…éšœæ¢å¤: æ•°æ®åº“ä¸å¯ç”¨æ—¶è‡ªåŠ¨æ’é˜Ÿ,æ•°æ®ä¸ä¸¢å¤±
    4. æ‰¹é‡æ“ä½œ: æ”¯æŒ10ä¸‡æ¡è®°å½•çš„é«˜æ€§èƒ½æ‰¹é‡ä¿å­˜

    **ä½¿ç”¨ç¤ºä¾‹**:
        ```python
        manager = MyStocksUnifiedManager()

        # ä¿å­˜Tickæ•°æ® â†’ è‡ªåŠ¨è·¯ç”±åˆ°TDengine
        manager.save_data_by_classification(
            DataClassification.TICK_DATA,
            tick_df,
            table_name='tick_600000'
        )

        # åŠ è½½æ—¥çº¿æ•°æ® â†’ è‡ªåŠ¨è·¯ç”±åˆ°PostgreSQL
        kline_df = manager.load_data_by_classification(
            DataClassification.DAILY_KLINE,
            table_name='daily_kline',
            filters={'symbol': '600000.SH'}
        )
        ```
    """

    # Type hints for optional monitoring components
    monitoring_db: Optional[Any]
    performance_monitor: Optional[Any]
    quality_monitor: Optional[Any]
    alert_manager: Optional[Any]

    def __init__(self, enable_monitoring: bool = True) -> None:
        """
        åˆå§‹åŒ–ç»Ÿä¸€ç®¡ç†å™¨

        Args:
            enable_monitoring: æ˜¯å¦å¯ç”¨ç›‘æ§åŠŸèƒ½ (é»˜è®¤True)
        """
        # åˆå§‹åŒ–2ä¸ªæ•°æ®è®¿é—®å±‚ (ç³»ç»Ÿå·²ç®€åŒ–ä¸ºTDengine+PostgreSQLåŒæ•°æ®åº“æ¶æ„)
        self.tdengine = TDengineDataAccess()
        self.postgresql = PostgreSQLDataAccess()
        # æ³¨é‡Šæ‰ä¸å­˜åœ¨çš„Redisè®¿é—®å±‚
        # self.redis = RedisDataAccess()

        # åˆå§‹åŒ–æ•…éšœæ¢å¤é˜Ÿåˆ—
        self.recovery_queue = FailureRecoveryQueue()

        # åˆå§‹åŒ–ç›‘æ§ç»„ä»¶ (US3)
        self.enable_monitoring = enable_monitoring
        if enable_monitoring:
            try:
                self.monitoring_db = get_monitoring_database()
                self.performance_monitor = get_performance_monitor()
                self.quality_monitor = get_quality_monitor()
                self.alert_manager = get_alert_manager()
                print("   - ç›‘æ§ç»„ä»¶å·²å¯ç”¨ âœ…")
            except Exception as e:
                print(f"   - ç›‘æ§ç»„ä»¶åˆå§‹åŒ–å¤±è´¥,å·²ç¦ç”¨: {e}")
                self.enable_monitoring = False
                self.monitoring_db = None
                self.performance_monitor = None
                self.quality_monitor = None
                self.alert_manager = None
        else:
            self.monitoring_db = None
            self.performance_monitor = None
            self.quality_monitor = None
            self.alert_manager = None

        print("âœ… MyStocksUnifiedManager åˆå§‹åŒ–æˆåŠŸ")
        print("   - æ”¯æŒ34ä¸ªæ•°æ®åˆ†ç±»çš„è‡ªåŠ¨è·¯ç”±")
        print("   - 2ç§æ•°æ®åº“è¿æ¥å°±ç»ª (TDengine + PostgreSQL)")
        print("   - æ•…éšœæ¢å¤é˜Ÿåˆ—å·²å¯ç”¨")

    def _get_target_database(self, classification: DataClassification) -> DatabaseTarget:
        """
        æ ¹æ®æ•°æ®åˆ†ç±»è·å–ç›®æ ‡æ•°æ®åº“
        
        Args:
            classification: æ•°æ®åˆ†ç±»
            
        Returns:
            DatabaseTarget: ç›®æ ‡æ•°æ®åº“
        """
        # ç®€å•çš„è·¯ç”±è§„åˆ™ï¼Œæ ¹æ®æ•°æ®åˆ†ç±»é€‰æ‹©æ•°æ®åº“
        if classification in [
            DataClassification.TICK_DATA,
            DataClassification.MINUTE_KLINE,
            DataClassification.ORDER_BOOK_DEPTH,
        ]:
            return DatabaseTarget.TDENGINE
        elif classification in [
            DataClassification.DAILY_KLINE,
            DataClassification.FUNDAMENTAL_METRICS,
            DataClassification.SYMBOLS_INFO,
            DataClassification.INDEX_CONSTITUENTS,
            DataClassification.TRADE_CALENDAR,
            DataClassification.TRADE_RECORDS,
            DataClassification.POSITION_HISTORY,
            DataClassification.SYSTEM_CONFIG,
            DataClassification.TASK_SCHEDULE,
            DataClassification.DATA_QUALITY_METRICS,
        ]:
            return DatabaseTarget.POSTGRESQL
        else:
            # é»˜è®¤ä½¿ç”¨PostgreSQL
            return DatabaseTarget.POSTGRESQL

    def save_data_by_classification(
        self,
        classification: DataClassification,
        data: pd.DataFrame,
        table_name: str,
        **kwargs: Any,
    ) -> bool:
        """
        æŒ‰åˆ†ç±»ä¿å­˜æ•°æ® (æ ¸å¿ƒæ–¹æ³• #1)

        æ ¹æ®æ•°æ®åˆ†ç±»è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ•°æ®åº“å¹¶ä¿å­˜æ•°æ®ã€‚
        å¦‚æœç›®æ ‡æ•°æ®åº“ä¸å¯ç”¨,æ•°æ®è‡ªåŠ¨åŠ å…¥æ•…éšœæ¢å¤é˜Ÿåˆ—ã€‚

        Args:
            classification: æ•°æ®åˆ†ç±»æšä¸¾
            data: æ•°æ®DataFrame
            table_name: ç›®æ ‡è¡¨å
            **kwargs: é¢å¤–å‚æ•° (å¦‚ttl, timestamp_colç­‰)

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ

        Raises:
            ValueError: æœªçŸ¥çš„æ•°æ®åˆ†ç±»

        Example:
            # ä¿å­˜åˆ†é’Ÿçº¿æ•°æ®
            success = manager.save_data_by_classification(
                DataClassification.MINUTE_KLINE,
                kline_df,
                table_name='minute_kline_600000',
                timestamp_col='ts'
            )
        """
        if data.empty:
            print("âš ï¸  æ•°æ®ä¸ºç©º,è·³è¿‡ä¿å­˜")
            return True

        # US3: ä½¿ç”¨DataManagerè¿›è¡Œè·¯ç”±
        # from src.core.data_storage_strategy import DataManager
        # æš‚æ—¶ä½¿ç”¨ç®€å•çš„æ•°æ®ç®¡ç†å™¨æ›¿ä»£
        target_db = self._get_target_database(classification)
        operation_success = False
        rows_affected = 0

        # æ€§èƒ½ç›‘æ§ä¸Šä¸‹æ–‡ (US3)
        context_manager = (
            self.performance_monitor.track_operation(
                operation_name=f"save_{classification.value}",
                classification=classification.value,
                database_type=target_db.value,
                table_name=table_name,
            )
            if self.enable_monitoring and self.performance_monitor is not None
            else None
        )

        try:
            # ä½¿ç”¨æ€§èƒ½ç›‘æ§ä¸Šä¸‹æ–‡
            if context_manager:
                context_manager.__enter__()

            print(f"ğŸ“ è·¯ç”±: {classification.value} â†’ {target_db.value.upper()}")

            # æ ¹æ®ç›®æ ‡æ•°æ®åº“é€‰æ‹©è®¿é—®å±‚
            if target_db == DatabaseTarget.TDENGINE:
                rows_affected = self.tdengine.insert_dataframe(
                    table_name, data, **kwargs
                )
                print(f"âœ… TDengineä¿å­˜æˆåŠŸ: {rows_affected}è¡Œ")

            elif target_db == DatabaseTarget.POSTGRESQL:
                rows_affected = self.postgresql.insert_dataframe(table_name, data)
                print(f"âœ… PostgreSQLä¿å­˜æˆåŠŸ: {rows_affected}è¡Œ")

            operation_success = True

            # è®°å½•æ“ä½œæ—¥å¿— (US3)
            if self.enable_monitoring and self.monitoring_db:
                self.monitoring_db.log_operation(
                    operation_type="SAVE",
                    classification=classification.value,
                    target_database=target_db.value,
                    table_name=table_name,
                    record_count=rows_affected,
                    operation_status="SUCCESS",
                )

            return True

        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
            print("ğŸ“¥ æ•°æ®å·²åŠ å…¥æ•…éšœæ¢å¤é˜Ÿåˆ—")

            # è®°å½•å¤±è´¥æ“ä½œæ—¥å¿— (US3)
            if self.enable_monitoring and self.monitoring_db:
                self.monitoring_db.log_operation(
                    operation_type="SAVE",
                    classification=classification.value,
                    target_database=target_db.value,
                    table_name=table_name,
                    record_count=len(data),
                    operation_status="FAILED",
                    error_message=str(e),
                )

            # åŠ å…¥æ•…éšœæ¢å¤é˜Ÿåˆ—
            self.recovery_queue.enqueue(
                classification=classification.value,
                target_database=target_db.value,
                data={
                    "table_name": table_name,
                    "data": data.to_dict("records"),
                    "kwargs": kwargs,
                },
            )

            return False

        finally:
            # é€€å‡ºæ€§èƒ½ç›‘æ§ä¸Šä¸‹æ–‡
            if context_manager:
                try:
                    context_manager.__exit__(None, None, None)
                except Exception:
                    pass

    def load_data_by_classification(
        self,
        classification: DataClassification,
        table_name: str,
        filters: Optional[Dict[str, Any]] = None,
        columns: Optional[List[str]] = None,
        limit: Optional[int] = None,
        **kwargs: Any,
    ) -> pd.DataFrame:
        """
        æŒ‰åˆ†ç±»åŠ è½½æ•°æ® (æ ¸å¿ƒæ–¹æ³• #2)

        æ ¹æ®æ•°æ®åˆ†ç±»è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ•°æ®åº“å¹¶æŸ¥è¯¢æ•°æ®ã€‚

        Args:
            classification: æ•°æ®åˆ†ç±»æšä¸¾
            table_name: è¡¨å
            filters: è¿‡æ»¤æ¡ä»¶ {'symbol': '600000.SH', 'date >= ': '2025-01-01'}
            columns: æŸ¥è¯¢å­—æ®µåˆ—è¡¨
            limit: è¿”å›è¡Œæ•°é™åˆ¶
            **kwargs: é¢å¤–å‚æ•° (å¦‚start_time, end_timeç­‰)

        Returns:
            æŸ¥è¯¢ç»“æœDataFrame

        Example:
            # åŠ è½½æ—¥çº¿æ•°æ®
            df = manager.load_data_by_classification(
                DataClassification.DAILY_KLINE,
                table_name='daily_kline',
                filters={'symbol': '600000.SH'},
                start_time=datetime(2025, 1, 1),
                end_time=datetime(2025, 12, 31)
            )
        """
        # è·å–ç›®æ ‡æ•°æ®åº“
        # from src.core.data_storage_strategy import DataManager
        target_db = self._get_target_database(classification)

        # æ€§èƒ½ç›‘æ§ä¸Šä¸‹æ–‡ (US3)
        context_manager = (
            self.performance_monitor.track_operation(
                operation_name=f"load_{classification.value}",
                classification=classification.value,
                database_type=target_db.value,
                table_name=table_name,
            )
            if self.enable_monitoring and self.performance_monitor is not None
            else None
        )

        try:
            # ä½¿ç”¨æ€§èƒ½ç›‘æ§ä¸Šä¸‹æ–‡
            if context_manager:
                context_manager.__enter__()

            print(f"ğŸ“ è·¯ç”±: {classification.value} â†’ {target_db.value.upper()}")

            # æ„å»ºwhereå­å¥
            where = self._build_where_clause(filters) if filters else None

            # æ ¹æ®ç›®æ ‡æ•°æ®åº“æŸ¥è¯¢
            if target_db == DatabaseTarget.TDENGINE:
                # TDengineæ—¶é—´èŒƒå›´æŸ¥è¯¢
                if "start_time" in kwargs and "end_time" in kwargs:
                    df = self.tdengine.query_by_time_range(
                        table_name,
                        kwargs["start_time"],
                        kwargs["end_time"],
                        columns=columns,
                        limit=limit,
                    )
                else:
                    df = self.tdengine.query_latest(table_name, limit or 100)

            elif target_db == DatabaseTarget.POSTGRESQL:
                # PostgreSQLæŸ¥è¯¢
                if "start_time" in kwargs and "end_time" in kwargs:
                    time_column = kwargs.get("time_column", "time")
                    df = self.postgresql.query_by_time_range(
                        table_name,
                        time_column,
                        kwargs["start_time"],
                        kwargs["end_time"],
                        columns=columns,
                        filters=where,
                    )
                else:
                    df = self.postgresql.query(table_name, columns, where, limit=limit)

            print(f"âœ… æŸ¥è¯¢æˆåŠŸ: {len(df)}è¡Œ")

            # è®°å½•æ“ä½œæ—¥å¿— (US3)
            if self.enable_monitoring and self.monitoring_db:
                self.monitoring_db.log_operation(
                    operation_type="LOAD",
                    classification=classification.value,
                    target_database=target_db.value,
                    table_name=table_name,
                    record_count=len(df),
                    operation_status="SUCCESS",
                )

            return df

        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")

            # è®°å½•å¤±è´¥æ“ä½œæ—¥å¿— (US3)
            if self.enable_monitoring and self.monitoring_db:
                self.monitoring_db.log_operation(
                    operation_type="LOAD",
                    classification=classification.value,
                    target_database=target_db.value,
                    table_name=table_name,
                    record_count=0,
                    operation_status="FAILED",
                    error_message=str(e),
                )

            return pd.DataFrame()

        finally:
            # é€€å‡ºæ€§èƒ½ç›‘æ§ä¸Šä¸‹æ–‡
            if context_manager:
                try:
                    context_manager.__exit__(None, None, None)
                except Exception:
                    pass

    def _save_to_redis(self, key: str, data: pd.DataFrame, ttl: Optional[int] = None) -> None:
        """
        ä¿å­˜æ•°æ®åˆ°Redis (å·²æ³¨é‡Š)

        æ ¹æ®æ•°æ®ç»“æ„é€‰æ‹©æœ€ä¼˜Redisæ•°æ®ç±»å‹:
        - å•æ¡è®°å½• â†’ String
        - å¤šæ¡è®°å½• â†’ Hash (key-value pairs)
        """
        # æ³¨é‡Šæ‰Redisç›¸å…³ä»£ç ï¼Œå› ä¸ºæ¨¡å—ä¸å­˜åœ¨
        # if len(data) == 1:
        #     # å•æ¡è®°å½• â†’ String
        #     self.redis.set(key, data.iloc[0].to_dict(), ttl=ttl)
        # else:
        #     # å¤šæ¡è®°å½• â†’ Hash
        #     for idx, row in data.iterrows():
        #         field = str(row.get("symbol", idx))
        #         self.redis.hset(key, field, row.to_dict())
        #
        #     if ttl:
        #         self.redis.expire(key, ttl)
        pass

    def _load_from_redis(
        self, key: str, filters: Optional[Dict[str, Any]] = None
    ) -> pd.DataFrame:
        """
        ä»RedisåŠ è½½æ•°æ® (å·²æ³¨é‡Š)

        è‡ªåŠ¨æ£€æµ‹æ•°æ®ç±»å‹å¹¶è¿”å›DataFrame
        """
        # æ³¨é‡Šæ‰Redisç›¸å…³ä»£ç ï¼Œå› ä¸ºæ¨¡å—ä¸å­˜åœ¨
        # # å°è¯•Stringç±»å‹
        # value = self.redis.get(key)
        # if value:
        #     return pd.DataFrame([value])
        #
        # # å°è¯•Hashç±»å‹
        # data = self.redis.hgetall(key)
        # if data:
        #     df = pd.DataFrame.from_dict(data, orient="index")
        #
        #     # åº”ç”¨è¿‡æ»¤å™¨
        #     if filters:
        #         for col, val in filters.items():
        #             if col in df.columns:
        #                 df = df[df[col] == val]
        #
        #     return df
        #
        # return pd.DataFrame()
        return pd.DataFrame()

    def _build_where_clause(self, filters: Dict[str, Any]) -> str:
        """
        æ„å»ºWHEREå­å¥

        Args:
            filters: è¿‡æ»¤æ¡ä»¶å­—å…¸

        Returns:
            WHEREå­å¥å­—ç¬¦ä¸²

        Example:
            {'symbol': '600000.SH', 'date >= ': '2025-01-01'}
            â†’ "symbol = '600000.SH' AND date >= '2025-01-01'"
        """
        conditions = []

        for key, value in filters.items():
            # æ”¯æŒæ“ä½œç¬¦åç¼€ (å¦‚ 'date >= ')
            if key.endswith((" =", " >", " <", " >=", " <=", " !=")):
                operator = key.split()[-1]
                column = key.rsplit(operator, 1)[0].strip()
                if isinstance(value, str):
                    conditions.append(f"{column} {operator} '{value}'")
                else:
                    conditions.append(f"{column} {operator} {value}")
            else:
                # é»˜è®¤ä½¿ç”¨ = æ“ä½œç¬¦
                if isinstance(value, str):
                    conditions.append(f"{key} = '{value}'")
                else:
                    conditions.append(f"{key} = {value}")

        return " AND ".join(conditions)

    def get_routing_info(self, classification: DataClassification, **kwargs: Any) -> Dict[str, Any]:
        """
        è·å–æ•°æ®åˆ†ç±»çš„è·¯ç”±ä¿¡æ¯

        Args:
            classification: æ•°æ®åˆ†ç±»
            **kwargs: å¯é€‰å‚æ•°ï¼Œå¦‚ retention_days

        Returns:
            è·¯ç”±ä¿¡æ¯å­—å…¸

        Example:
            info = manager.get_routing_info(DataClassification.TICK_DATA)
            # {'target_db': 'tdengine', 'retention_days': 30, 'ttl': None}
        """
        # from src.core.data_storage_strategy import DataManager
        target_db = self._get_target_database(classification)
        # US3: ç§»é™¤DataStorageRulesï¼Œä½¿ç”¨ç®€åŒ–é…ç½®
        retention = kwargs.get("retention_days", None)  # ç®€åŒ–é…ç½®ï¼Œä»å‚æ•°è·å–ä¿ç•™å¤©æ•°
        ttl = None  # Rediså·²è¢«ç§»é™¤

        return {"target_db": target_db.value, "retention_days": retention, "ttl": ttl}

    def save_data_batch_with_strategy(
        self,
        classification: DataClassification,
        data: pd.DataFrame,
        table_name: str,
        strategy: BatchFailureStrategy = BatchFailureStrategy.CONTINUE,
        **kwargs: Any,
    ) -> BatchOperationResult:
        """
        ä½¿ç”¨æŒ‡å®šå¤±è´¥ç­–ç•¥ä¿å­˜æ‰¹é‡æ•°æ® (æ ¸å¿ƒæ–¹æ³• #3)

        æä¾›ä¸‰ç§å¤±è´¥å¤„ç†ç­–ç•¥:
        - ROLLBACK: ä»»ä½•å¤±è´¥éƒ½å›æ»šæ•´ä¸ªæ‰¹æ¬¡
        - CONTINUE: è·³è¿‡å¤±è´¥è®°å½•,ç»§ç»­å¤„ç†
        - RETRY: è‡ªåŠ¨é‡è¯•å¤±è´¥è®°å½•

        Args:
            classification: æ•°æ®åˆ†ç±»æšä¸¾
            data: æ•°æ®DataFrame
            table_name: ç›®æ ‡è¡¨å
            strategy: å¤±è´¥ç­–ç•¥ (é»˜è®¤CONTINUE)
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            BatchOperationResult: æ‰¹é‡æ“ä½œç»“æœ

        Example:
            # ä½¿ç”¨RETRYç­–ç•¥ä¿å­˜10ä¸‡æ¡Tickæ•°æ®
            result = manager.save_data_batch_with_strategy(
                DataClassification.TICK_DATA,
                tick_df,
                table_name='tick_600000',
                strategy=BatchFailureStrategy.RETRY
            )
            print(f"æˆåŠŸç‡: {result.success_rate:.2%}")
            print(f"å¤±è´¥è®°å½•: {result.failed_records}")
        """
        if data.empty:
            print("âš ï¸  æ•°æ®ä¸ºç©º,è·³è¿‡ä¿å­˜")
            return BatchOperationResult(
                total_records=0,
                successful_records=0,
                failed_records=0,
                strategy_used=strategy,
                execution_time_ms=0.0,
            )

        # è·å–ç›®æ ‡æ•°æ®åº“
        # from src.core.data_storage_strategy import DataManager
        target_db = self._get_target_database(classification)
        print(
            f"ğŸ“ è·¯ç”±: {classification.value} â†’ {target_db.value.upper()} (ç­–ç•¥: {strategy.value.upper()})"
        )

        # åˆ›å»ºå¤±è´¥å¤„ç†å™¨
        handler = BatchFailureHandler(
            strategy=strategy,
            max_retries=kwargs.get("max_retries", 3),
            retry_delay_base=kwargs.get("retry_delay_base", 1.0),
        )

        # å®šä¹‰æ“ä½œå‡½æ•°
        def operation(batch: pd.DataFrame) -> bool:
            try:
                if target_db == DatabaseTarget.TDENGINE:
                    self.tdengine.insert_dataframe(table_name, batch, **kwargs)
                elif target_db == DatabaseTarget.POSTGRESQL:
                    self.postgresql.insert_dataframe(table_name, batch)
                elif target_db == DatabaseTarget.REDIS:
                    ttl = kwargs.get("ttl") or 86400  # é»˜è®¤1å¤©
                    self._save_to_redis(table_name, batch, ttl)
                return True
            except Exception as e:
                print(f"âš ï¸  æ‰¹æ¬¡ä¿å­˜å¼‚å¸¸: {e}")
                return False

        # æ‰§è¡Œæ‰¹é‡æ“ä½œ
        result = handler.execute_batch(data, operation, f"save_{classification.value}")

        # å¦‚æœæœ‰å¤±è´¥è®°å½•,åŠ å…¥æ•…éšœæ¢å¤é˜Ÿåˆ—
        if result.failed_records > 0 and strategy != BatchFailureStrategy.ROLLBACK:
            failed_data = (
                data.iloc[result.failed_indices] if result.failed_indices else data
            )
            print(f"ğŸ“¥ {result.failed_records} æ¡å¤±è´¥è®°å½•å·²åŠ å…¥æ•…éšœæ¢å¤é˜Ÿåˆ—")

            self.recovery_queue.enqueue(
                classification=classification.value,
                target_database=target_db.value,
                data={
                    "table_name": table_name,
                    "data": failed_data.to_dict("records"),
                    "kwargs": kwargs,
                },
            )

        return result

    def get_monitoring_statistics(self) -> Dict[str, Any]:
        """
        è·å–ç›‘æ§ç»Ÿè®¡ä¿¡æ¯ (US3)

        Returns:
            dict: ç›‘æ§ç»Ÿè®¡ä¿¡æ¯ {
                'performance': {...},
                'alerts': {...},
                'enabled': bool
            }
        """
        if not self.enable_monitoring:
            return {"enabled": False, "message": "ç›‘æ§åŠŸèƒ½æœªå¯ç”¨"}

        try:
            stats: Dict[str, Any] = {
                "enabled": True,
                "performance": (
                    self.performance_monitor.get_performance_summary(hours=24)
                    if self.performance_monitor is not None
                    else {}
                ),
                "alerts": {},  # AlertManager.get_statistics() æ–¹æ³•ä¸å­˜åœ¨ï¼Œå¾…å®ç°
                "monitoring_db": {"connected": self.monitoring_db is not None},
            }
            return stats
        except Exception as e:
            logger.error(f"è·å–ç›‘æ§ç»Ÿè®¡å¤±è´¥: {e}")
            return {"enabled": True, "error": str(e)}

    def check_data_quality(
        self, classification: DataClassification, table_name: str, **kwargs: Any
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ•°æ®è´¨é‡æ£€æŸ¥ (US3)

        æ”¯æŒçš„æ£€æŸ¥ç»´åº¦:
        - completeness: å®Œæ•´æ€§æ£€æŸ¥ (éœ€è¦ total_records, null_records)
        - freshness: æ–°é²œåº¦æ£€æŸ¥ (éœ€è¦ latest_timestamp)
        - accuracy: å‡†ç¡®æ€§æ£€æŸ¥ (éœ€è¦ total_records, invalid_records)

        Args:
            classification: æ•°æ®åˆ†ç±»
            table_name: è¡¨å
            **kwargs: æ£€æŸ¥å‚æ•°

        Returns:
            dict: è´¨é‡æ£€æŸ¥ç»“æœ

        Example:
            # æ£€æŸ¥å®Œæ•´æ€§
            result = manager.check_data_quality(
                DataClassification.DAILY_KLINE,
                'daily_kline',
                check_type='completeness',
                total_records=10000,
                null_records=50
            )
        """
        if not self.enable_monitoring:
            return {"error": "ç›‘æ§åŠŸèƒ½æœªå¯ç”¨"}

        check_type = kwargs.get("check_type", "completeness")
        # from src.core.data_storage_strategy import DataManager
        target_db = self._get_target_database(classification)

        try:
            result: Dict[str, Any] = {"error": f"æœªçŸ¥çš„æ£€æŸ¥ç±»å‹: {check_type}"}

            if self.quality_monitor is None:
                return {"error": "è´¨é‡ç›‘æ§å™¨æœªåˆå§‹åŒ–"}

            if check_type == "completeness":
                result = self.quality_monitor.check_completeness(
                    classification=classification.value,
                    database_type=target_db.value,
                    table_name=table_name,
                    total_records=kwargs.get("total_records", 0),
                    null_records=kwargs.get("null_records", 0),
                    threshold=kwargs.get("threshold"),
                )
            elif check_type == "freshness":
                result = self.quality_monitor.check_freshness(
                    classification=classification.value,
                    database_type=target_db.value,
                    table_name=table_name,
                    latest_timestamp=kwargs.get("latest_timestamp"),
                    threshold_seconds=kwargs.get("threshold_seconds"),
                )
            elif check_type == "accuracy":
                result = self.quality_monitor.check_accuracy(
                    classification=classification.value,
                    database_type=target_db.value,
                    table_name=table_name,
                    total_records=kwargs.get("total_records", 0),
                    invalid_records=kwargs.get("invalid_records", 0),
                    validation_rules=kwargs.get("validation_rules"),
                    threshold=kwargs.get("threshold"),
                )

            logger.info(f"âœ“ æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆ: {table_name} - {check_type}")
            return result

        except Exception as e:
            logger.error(f"æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥: {e}")
            return {"error": str(e)}

    def close_all_connections(self) -> None:
        """å…³é—­æ‰€æœ‰æ•°æ®åº“è¿æ¥"""
        print("\næ­£åœ¨å…³é—­æ‰€æœ‰æ•°æ®åº“è¿æ¥...")
        self.tdengine.close()
        self.postgresql.close_all()
        # self.mysql.close()  # MySQLå·²ç§»é™¤ï¼Œç³»ç»Ÿä½¿ç”¨TDengine+PostgreSQLåŒæ•°æ®åº“æ¶æ„
        # self.redis.close()  # Rediså·²ç§»é™¤
        print("âœ… æ‰€æœ‰è¿æ¥å·²å…³é—­")


if __name__ == "__main__":
    """æµ‹è¯•ç»Ÿä¸€ç®¡ç†å™¨"""
    print("\n" + "=" * 80)
    print("MyStocksç»Ÿä¸€æ•°æ®ç®¡ç†å™¨ - MVPæµ‹è¯•")
    print("=" * 80 + "\n")

    # åˆå§‹åŒ–ç®¡ç†å™¨
    manager = MyStocksUnifiedManager()

    # æµ‹è¯•è·¯ç”±ä¿¡æ¯æŸ¥è¯¢
    print("\nğŸ“Š è·¯ç”±ä¿¡æ¯æµ‹è¯•:\n")
    test_classifications = [
        DataClassification.TICK_DATA,
        DataClassification.DAILY_KLINE,
        DataClassification.SYMBOLS_INFO,
        DataClassification.REALTIME_POSITIONS,
    ]

    for classification in test_classifications:
        info = manager.get_routing_info(classification)
        print(f"  {classification.value}")
        print(f"    â†’ ç›®æ ‡æ•°æ®åº“: {info['target_db'].upper()}")
        print(
            f"    â†’ ä¿ç•™å‘¨æœŸ: {info['retention_days']}å¤©"
            if info["retention_days"]
            else f"    â†’ ä¿ç•™å‘¨æœŸ: æ°¸ä¹…"
        )
        if info["ttl"]:
            print(f"    â†’ TTL: {info['ttl']}ç§’")

    print("\n" + "=" * 80)
    print("âœ… ç»Ÿä¸€ç®¡ç†å™¨åŸºç¡€åŠŸèƒ½éªŒè¯é€šè¿‡")
    print("=" * 80 + "\n")

    print("æ ¸å¿ƒåŠŸèƒ½:")
    print("  âœ… save_data_by_classification() - æŒ‰åˆ†ç±»ä¿å­˜")
    print("  âœ… load_data_by_classification() - æŒ‰åˆ†ç±»åŠ è½½")
    print("  âœ… save_data_batch_with_strategy() - æ‰¹é‡ä¿å­˜(å«å¤±è´¥ç­–ç•¥)")
    print("  âœ… è‡ªåŠ¨è·¯ç”±åˆ°æœ€ä¼˜æ•°æ®åº“")
    print("  âœ… æ•…éšœæ¢å¤é˜Ÿåˆ—")
    print("  âœ… è·¯ç”±ä¿¡æ¯æŸ¥è¯¢")
    print("  âœ… ä¸‰ç§æ‰¹é‡å¤±è´¥ç­–ç•¥ (ROLLBACK/CONTINUE/RETRY)")

    # å…³é—­è¿æ¥
    manager.close_all_connections()
