#!/usr/bin/env python3
"""
GPUåŠ é€Ÿç»Ÿä¸€ç®¡ç†å™¨
é›†æˆGPUç»„ä»¶åˆ°MyStocksé¡¹ç›®çš„ç»Ÿä¸€ç®¡ç†ç³»ç»Ÿä¸­
æ”¯æŒè‡ªåŠ¨æ£€æµ‹GPUç¯å¢ƒå¹¶æä¾›ç»Ÿä¸€çš„GPUåŠ é€Ÿæ¥å£
"""

import time
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
import pandas as pd

# GPUç»„ä»¶å¯¼å…¥
from .price_predictor_gpu import GPUPricePredictor
from .feature_generator_gpu import GPUFeatureGenerator
from .data_processor_gpu import GPUDataProcessor, ProcessingConfig


@dataclass
class GPUConfig:
    """GPUé…ç½®"""

    gpu_enabled: bool = True
    n_jobs: int = 1
    chunk_size: int = 10000
    memory_limit_gb: float = 8.0
    enable_parallel_processing: bool = True
    enable_streaming: bool = True
    enable_distributed: bool = False


@dataclass
class GPUProcessingResult:
    """GPUå¤„ç†ç»“æœ"""

    processing_time: float
    data_size: int
    gpu_memory_usage: float
    speedup_factor: float
    results: Any
    errors: List[str]
    performance_metrics: Dict[str, float]


class GPUUnifiedManager:
    """GPUåŠ é€Ÿç»Ÿä¸€ç®¡ç†å™¨"""

    def __init__(self, config: Optional[GPUConfig] = None):
        """åˆå§‹åŒ–GPUç»Ÿä¸€ç®¡ç†å™¨"""
        self.config = config or GPUConfig()
        self.logger = logging.getLogger(__name__)

        # åˆå§‹åŒ–GPUç»„ä»¶
        self.gpu_enabled = self._detect_gpu_environment()
        self.data_processor = GPUDataProcessor(
            gpu_enabled=self.gpu_enabled,
            n_jobs=self.config.n_jobs,
            chunk_size=self.config.chunk_size,
        )
        self.feature_generator = GPUFeatureGenerator(gpu_enabled=self.gpu_enabled)
        self.price_predictor = GPUPricePredictor(gpu_enabled=self.gpu_enabled)

        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            "total_processing_time": 0.0,
            "total_data_processed": 0,
            "gpu_enabled_operations": 0,
            "cpu_fallback_operations": 0,
            "average_speedup": 1.0,
        }

        self.logger.info(f"GPUç»Ÿä¸€ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ - GPUåŠ é€Ÿ: {self.gpu_enabled}")

    def _detect_gpu_environment(self) -> bool:
        """æ£€æµ‹GPUç¯å¢ƒæ˜¯å¦å¯ç”¨"""
        try:
            import cupy as cp

            # æ£€æŸ¥æ˜¯å¦æœ‰GPUè®¾å¤‡
            cp.cuda.Device(0)
            self.logger.info("âœ… GPUç¯å¢ƒæ£€æµ‹æˆåŠŸ")
            return True
        except Exception as e:
            self.logger.warning(f"âš ï¸  GPUç¯å¢ƒæ£€æµ‹å¤±è´¥: {e}, å°†ä½¿ç”¨CPUæ¨¡å¼")
            return False

    def process_data_with_gpu(
        self, data: pd.DataFrame, processing_config: Optional[ProcessingConfig] = None
    ) -> GPUProcessingResult:
        """ä½¿ç”¨GPUå¤„ç†æ•°æ®"""
        start_time = time.time()

        try:
            # æ•°æ®é¢„å¤„ç†
            processed_data = self.data_processor.preprocess(data=data, config=processing_config or ProcessingConfig())

            # ç‰¹å¾ç”Ÿæˆ
            feature_data = self.feature_generator.generate_features(processed_data)

            processing_time = time.time() - start_time

            # è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
            gpu_memory = 0.0
            if self.gpu_enabled:
                try:
                    import cupy as cp

                    gpu_memory = cp.cuda.get_default_memory_pool().used_bytes() / 1024 / 1024
                except Exception:
                    gpu_memory = 0.0

            result = GPUProcessingResult(
                processing_time=processing_time,
                data_size=len(data),
                gpu_memory_usage=gpu_memory,
                speedup_factor=self._calculate_speedup_factor(data, processing_time),
                results=feature_data,
                errors=[],
                performance_metrics=self._collect_performance_metrics(),
            )

            self._update_performance_stats(result)

            self.logger.info(f"GPUæ•°æ®å¤„ç†å®Œæˆ - è€—æ—¶: {processing_time:.4f}ç§’, æ•°æ®é‡: {len(data)}è¡Œ")
            return result

        except Exception as e:
            error_msg = f"GPUæ•°æ®å¤„ç†å¤±è´¥: {str(e)}"
            self.logger.error(error_msg)

            result = GPUProcessingResult(
                processing_time=time.time() - start_time,
                data_size=len(data),
                gpu_memory_usage=0.0,
                speedup_factor=0.0,
                results=None,
                errors=[error_msg],
                performance_metrics={},
            )

            return result

    def generate_predictions_with_gpu(
        self,
        data: pd.DataFrame,
        model_type: str = "ridge",
        prediction_horizon: int = 1,
        training_horizon: Optional[int] = None,
    ) -> GPUProcessingResult:
        """ä½¿ç”¨GPUç”Ÿæˆé¢„æµ‹"""
        start_time = time.time()

        try:
            # å¦‚æœæ²¡æœ‰è®­ç»ƒè¿‡ï¼Œå…ˆè®­ç»ƒæ¨¡å‹
            if not self.price_predictor.is_fitted:
                training_data = data if training_horizon is None else data[-training_horizon:]
                self.logger.info("å¼€å§‹è®­ç»ƒGPUé¢„æµ‹æ¨¡å‹...")
                self.price_predictor.train_models(training_data)
                self.logger.info("GPUæ¨¡å‹è®­ç»ƒå®Œæˆ")

            # è¿›è¡Œé¢„æµ‹
            prediction_result = self.price_predictor.predict_price(
                data=data, model_name=model_type, prediction_horizon=prediction_horizon
            )

            processing_time = time.time() - start_time

            result = GPUProcessingResult(
                processing_time=processing_time,
                data_size=len(data),
                gpu_memory_usage=self._get_gpu_memory_usage(),
                speedup_factor=self._calculate_speedup_factor(data, processing_time),
                results=prediction_result,
                errors=[],
                performance_metrics={
                    "model_used": prediction_result.model_used,
                    "confidence_score": prediction_result.confidence_score,
                    "mse": prediction_result.error_metrics.get("mse", 0),
                    "r2_score": prediction_result.error_metrics.get("r2_score", 0),
                },
            )

            self._update_performance_stats(result)

            self.logger.info(f"GPUé¢„æµ‹å®Œæˆ - æ¨¡å‹: {model_type}, é¢„æµ‹ä»·æ ¼: {prediction_result.predicted_price:.2f}")
            return result

        except Exception as e:
            error_msg = f"GPUé¢„æµ‹å¤±è´¥: {str(e)}"
            self.logger.error(error_msg)

            result = GPUProcessingResult(
                processing_time=time.time() - start_time,
                data_size=len(data),
                gpu_memory_usage=0.0,
                speedup_factor=0.0,
                results=None,
                errors=[error_msg],
                performance_metrics={},
            )

            return result

    def batch_process_with_gpu(
        self, data_list: List[pd.DataFrame], operation: str = "process"
    ) -> List[GPUProcessingResult]:
        """æ‰¹é‡GPUå¤„ç†"""
        results = []

        self.logger.info(f"å¼€å§‹æ‰¹é‡GPUå¤„ç† - æ•°æ®æ•°é‡: {len(data_list)}, æ“ä½œ: {operation}")

        for i, data in enumerate(data_list):
            try:
                if operation == "process":
                    result = self.process_data_with_gpu(data)
                elif operation == "predict":
                    result = self.generate_predictions_with_gpu(data)
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„GPUæ“ä½œ: {operation}")

                results.append(result)

                # è¿›åº¦æ—¥å¿—
                if (i + 1) % 10 == 0:
                    self.logger.info(f"æ‰¹é‡GPUå¤„ç†è¿›åº¦: {i + 1}/{len(data_list)}")

            except Exception as e:
                self.logger.error(f"æ‰¹é‡GPUå¤„ç†ä¸­ç¬¬{i + 1}ä¸ªæ•°æ®å¤±è´¥: {e}")

                error_result = GPUProcessingResult(
                    processing_time=0.0,
                    data_size=len(data),
                    gpu_memory_usage=0.0,
                    speedup_factor=0.0,
                    results=None,
                    errors=[f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}"],
                    performance_metrics={},
                )
                results.append(error_result)

        self.logger.info(f"æ‰¹é‡GPUå¤„ç†å®Œæˆ - æˆåŠŸ: {len([r for r in results if not r.errors])}/{len(data_list)}")
        return results

    def optimize_hyperparameters_with_gpu(self, data: pd.DataFrame, model_type: str = "ridge") -> GPUProcessingResult:
        """ä½¿ç”¨GPUä¼˜åŒ–è¶…å‚æ•°"""
        start_time = time.time()

        try:
            optimization_result = self.price_predictor.optimize_hyperparameters(data=data, model_type=model_type)

            processing_time = time.time() - start_time

            result = GPUProcessingResult(
                processing_time=processing_time,
                data_size=len(data),
                gpu_memory_usage=self._get_gpu_memory_usage(),
                speedup_factor=self._calculate_speedup_factor(data, processing_time),
                results=optimization_result,
                errors=[],
                performance_metrics={
                    "best_params": optimization_result.get("best_params", {}),
                    "best_score": optimization_result.get("best_score", 0),
                    "model_type": model_type,
                },
            )

            self._update_performance_stats(result)

            self.logger.info(
                f"GPUè¶…å‚æ•°ä¼˜åŒ–å®Œæˆ - æ¨¡å‹: {model_type}, æœ€ä½³åˆ†æ•°: {optimization_result.get('best_score', 0):.4f}"
            )
            return result

        except Exception as e:
            error_msg = f"GPUè¶…å‚æ•°ä¼˜åŒ–å¤±è´¥: {str(e)}"
            self.logger.error(error_msg)

            result = GPUProcessingResult(
                processing_time=time.time() - start_time,
                data_size=len(data),
                gpu_memory_usage=0.0,
                speedup_factor=0.0,
                results=None,
                errors=[error_msg],
                performance_metrics={},
            )

            return result

    def benchmark_gpu_vs_cpu(self, data: pd.DataFrame, operation: str = "process") -> Dict:
        """GPUä¸CPUæ€§èƒ½å¯¹æ¯”"""
        self.logger.info("å¼€å§‹GPU vs CPUæ€§èƒ½å¯¹æ¯”æµ‹è¯•")

        # GPUæµ‹è¯•
        gpu_start = time.time()
        if operation == "process":
            gpu_result = self.process_data_with_gpu(data)
        elif operation == "predict":
            gpu_result = self.generate_predictions_with_gpu(data)
        elif operation == "optimize":
            gpu_result = self.optimize_hyperparameters_with_gpu(data)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {operation}")

        gpu_time = time.time() - gpu_start

        # CPUæµ‹è¯•
        self.logger.info("å¼€å§‹CPUæ€§èƒ½æµ‹è¯•...")
        cpu_start = time.time()

        if operation == "process":
            # ä½¿ç”¨CPUç‰ˆæœ¬çš„æ•°æ®å¤„ç†å™¨
            from ..data_processor_gpu import DataProcessorCPU

            cpu_processor = DataProcessorCPU(gpu_enabled=False)
            cpu_result = cpu_processor.preprocess(data, ProcessingConfig())
        elif operation == "predict":
            # ä½¿ç”¨CPUç‰ˆæœ¬çš„ä»·æ ¼é¢„æµ‹å™¨
            from ..price_predictor_gpu import PricePredictorCPU

            cpu_predictor = PricePredictorCPU(gpu_enabled=False)
            cpu_result = cpu_predictor.predict_price(data)
        elif operation == "optimize":
            # ä½¿ç”¨CPUç‰ˆæœ¬çš„è¶…å‚æ•°ä¼˜åŒ–
            from ..price_predictor_gpu import PricePredictorCPU

            cpu_predictor = PricePredictorCPU(gpu_enabled=False)
            cpu_result = cpu_predictor.optimize_hyperparameters(data)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {operation}")

        cpu_time = time.time() - cpu_start

        # è®¡ç®—åŠ é€Ÿæ¯”
        speedup = cpu_time / gpu_time if gpu_time > 0 else 1.0

        benchmark_result = {
            "operation": operation,
            "gpu_time": gpu_time,
            "cpu_time": cpu_time,
            "speedup": speedup,
            "gpu_memory_usage": (gpu_result.gpu_memory_usage if hasattr(gpu_result, "gpu_memory_usage") else 0),
            "gpu_result": (gpu_result.results if hasattr(gpu_result, "results") else None),
            "cpu_result": cpu_result,
            "efficiency_metrics": {
                "gpu_efficiency": (1 / gpu_time) if gpu_time > 0 else 0,
                "cpu_efficiency": (1 / cpu_time) if cpu_time > 0 else 0,
                "memory_efficiency": ((1 / gpu_result.gpu_memory_usage) if gpu_result.gpu_memory_usage > 0 else 0),
            },
        }

        self.logger.info(f"æ€§èƒ½å¯¹æ¯”å®Œæˆ - GPU: {gpu_time:.4f}s, CPU: {cpu_time:.4f}s, åŠ é€Ÿæ¯”: {speedup:.2f}x")
        return benchmark_result

    def _calculate_speedup_factor(self, data: pd.DataFrame, processing_time: float) -> float:
        """è®¡ç®—åŠ é€Ÿå› å­"""
        # åŸºäºæ•°æ®å¤§å°å’Œæ—¶é—´çš„ç®€å•ä¼°ç®—
        data_size = len(data)
        estimated_cpu_time = data_size * 0.001  # å‡è®¾CPUæ¯æ¯«ç§’å¤„ç†1000è¡Œ

        if processing_time > 0 and estimated_cpu_time > 0:
            return estimated_cpu_time / processing_time
        return 1.0

    def _get_gpu_memory_usage(self) -> float:
        """è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if not self.gpu_enabled:
            return 0.0

        try:
            import cupy as cp

            return cp.cuda.get_default_memory_pool().used_bytes() / 1024 / 1024
        except Exception:
            return 0.0

    def _collect_performance_metrics(self) -> Dict[str, float]:
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        metrics = {}

        # è·å–ä»·æ ¼é¢„æµ‹å™¨æ€§èƒ½
        price_performance = self.price_predictor.get_performance_summary()
        metrics.update(price_performance)

        # æ·»åŠ GPUç›¸å…³æŒ‡æ ‡
        metrics["gpu_enabled"] = self.gpu_enabled
        metrics["n_jobs"] = self.config.n_jobs
        metrics["chunk_size"] = self.config.chunk_size

        return metrics

    def _update_performance_stats(self, result: GPUProcessingResult):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        self.performance_stats["total_processing_time"] += result.processing_time
        self.performance_stats["total_data_processed"] += result.data_size

        if self.gpu_enabled and result.errors:
            self.performance_stats["gpu_enabled_operations"] += 1
        else:
            self.performance_stats["cpu_fallback_operations"] += 1

        # è®¡ç®—å¹³å‡åŠ é€Ÿæ¯”
        total_operations = (
            self.performance_stats["gpu_enabled_operations"] + self.performance_stats["cpu_fallback_operations"]
        )
        if total_operations > 0:
            self.performance_stats["average_speedup"] = self.performance_stats["total_processing_time"] / max(
                1, result.processing_time * total_operations
            )

    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ€»ç»“"""
        return {
            **self.performance_stats,
            "gpu_components_status": {
                "data_processor": {
                    "enabled": self.data_processor.gpu_enabled,
                    "chunks_processed": getattr(self.data_processor, "chunks_processed", 0),
                },
                "feature_generator": {
                    "enabled": self.feature_generator.gpu_enabled,
                    "features_generated": getattr(self.feature_generator, "features_generated", 0),
                },
                "price_predictor": {
                    "enabled": self.price_predictor.gpu_enabled,
                    "is_fitted": self.price_predictor.is_fitted,
                    "total_predictions": self.price_predictor.performance_stats["total_predictions"],
                },
            },
            "configuration": {
                "gpu_enabled": self.gpu_enabled,
                "n_jobs": self.config.n_jobs,
                "chunk_size": self.config.chunk_size,
                "memory_limit_gb": self.config.memory_limit_gb,
            },
        }

    def save_gpu_models(self, filepath: str):
        """ä¿å­˜GPUæ¨¡å‹"""
        if self.price_predictor.is_fitted:
            self.price_predictor.save_model(filepath)
            self.logger.info(f"GPUæ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
        else:
            self.logger.warning("æ²¡æœ‰è®­ç»ƒè¿‡çš„æ¨¡å‹å¯ä»¥ä¿å­˜")

    def load_gpu_models(self, filepath: str):
        """åŠ è½½GPUæ¨¡å‹"""
        try:
            self.price_predictor.load_model(filepath)
            self.logger.info(f"GPUæ¨¡å‹å·²ä» {filepath} åŠ è½½")
        except Exception as e:
            self.logger.error(f"åŠ è½½GPUæ¨¡å‹å¤±è´¥: {e}")

    def generate_gpu_report(self) -> str:
        """ç”ŸæˆGPUä½¿ç”¨æŠ¥å‘Š"""
        summary = self.get_performance_summary()

        report = f"""
MyStocks GPUåŠ é€Ÿä½¿ç”¨æŠ¥å‘Š
==========================

ğŸš€ GPUç¯å¢ƒçŠ¶æ€: {"âœ… å¯ç”¨" if self.gpu_enabled else "âŒ ç¦ç”¨"}

ğŸ“Š æ€§èƒ½ç»Ÿè®¡:
  â€¢ æ€»å¤„ç†æ—¶é—´: {summary["total_processing_time"]:.2f}ç§’
  â€¢ æ€»å¤„ç†æ•°æ®é‡: {summary["total_data_processed"]}è¡Œ
  â€¢ GPUæ“ä½œæ¬¡æ•°: {summary["gpu_enabled_operations"]}
  â€¢ CPUå›é€€æ¬¡æ•°: {summary["cpu_fallback_operations"]}
  â€¢ å¹³å‡åŠ é€Ÿæ¯”: {summary["average_speedup"]:.2f}x

ğŸ”§ GPUç»„ä»¶çŠ¶æ€:
  â€¢ æ•°æ®å¤„ç†å™¨: {"âœ… å¯ç”¨" if summary["gpu_components_status"]["data_processor"]["enabled"] else "âŒ ç¦ç”¨"}
  â€¢ ç‰¹å¾ç”Ÿæˆå™¨: {"âœ… å¯ç”¨" if summary["gpu_components_status"]["feature_generator"]["enabled"] else "âŒ ç¦ç”¨"}
  â€¢ ä»·æ ¼é¢„æµ‹å™¨: {"âœ… å¯ç”¨" if summary["gpu_components_status"]["price_predictor"]["enabled"] else "âŒ ç¦ç”¨"}
  â€¢ æ¨¡å‹è®­ç»ƒçŠ¶æ€: {"âœ… å·²è®­ç»ƒ" if summary["gpu_components_status"]["price_predictor"]["is_fitted"] else "âŒ æœªè®­ç»ƒ"}
  â€¢ æ€»é¢„æµ‹æ¬¡æ•°: {summary["gpu_components_status"]["price_predictor"]["total_predictions"]}

âš™ï¸  é…ç½®ä¿¡æ¯:
  â€¢ å¹¶è¡Œä»»åŠ¡æ•°: {summary["configuration"]["n_jobs"]}
  â€¢ å—å¤§å°: {summary["configuration"]["chunk_size"]}
  â€¢ å†…å­˜é™åˆ¶: {summary["configuration"]["memory_limit_gb"]}GB

ğŸ’¡ ä½¿ç”¨å»ºè®®:
  â€¢ å¦‚æœGPUæ“ä½œå¤±è´¥é¢‘ç¹å¢åŠ ï¼Œå»ºè®®æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
  â€¢ å¯¹äºå¤§æ•°æ®é›†ï¼Œé€‚å½“å¢åŠ chunk_sizeå¯ä»¥æé«˜å¤„ç†æ•ˆç‡
  â€¢ å®šæœŸä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ä»¥é¿å…é‡å¤è®­ç»ƒ

"""
        return report


# é›†æˆåˆ°ç»Ÿä¸€ç®¡ç†å™¨çš„é€‚é…å™¨
class MyStocksGPUAdapter:
    """MyStocksé¡¹ç›®GPUé€‚é…å™¨"""

    def __init__(self, unified_manager=None):
        self.gpu_manager = unified_manager or GPUUnifiedManager()
        self.logger = logging.getLogger(__name__)

    def enable_gpu_acceleration(self) -> bool:
        """å¯ç”¨GPUåŠ é€Ÿ"""
        self.gpu_manager.gpu_enabled = True
        self.logger.info("GPUåŠ é€Ÿå·²å¯ç”¨")
        return True

    def disable_gpu_acceleration(self) -> bool:
        """ç¦ç”¨GPUåŠ é€Ÿ"""
        self.gpu_manager.gpu_enabled = False
        self.logger.info("GPUåŠ é€Ÿå·²ç¦ç”¨")
        return True

    def is_gpu_available(self) -> bool:
        """æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨"""
        return self.gpu_manager.gpu_enabled

    def get_gpu_status(self) -> Dict[str, Any]:
        """è·å–GPUçŠ¶æ€"""
        return {
            "gpu_available": self.gpu_manager.gpu_enabled,
            "gpu_models_trained": self.gpu_manager.price_predictor.is_fitted,
            "performance_summary": self.gpu_manager.get_performance_summary(),
        }


def main():
    """ä¸»å‡½æ•° - ç¤ºä¾‹ç”¨æ³•"""
    # åˆ›å»ºGPUç»Ÿä¸€ç®¡ç†å™¨
    gpu_manager = GPUUnifiedManager()

    # è·å–ç¤ºä¾‹æ•°æ®
    import yfinance as yf

    data = yf.download("AAPL", start="2023-01-01", end="2024-01-01")

    # æ•°æ®å¤„ç†ç¤ºä¾‹
    print("ğŸš€ å¼€å§‹GPUæ•°æ®å¤„ç†ç¤ºä¾‹...")
    process_result = gpu_manager.process_data_with_gpu(data)
    print(f"æ•°æ®å¤„ç†ç»“æœ: {process_result.processing_time:.4f}ç§’")

    # é¢„æµ‹ç¤ºä¾‹
    print("\nğŸ¯ å¼€å§‹GPUé¢„æµ‹ç¤ºä¾‹...")
    predict_result = gpu_manager.generate_predictions_with_gpu(data)
    if predict_result.results:
        prediction = predict_result.results
        print(f"é¢„æµ‹ä»·æ ¼: {prediction.predicted_price:.2f}")
        print(f"ç½®ä¿¡åº¦: {prediction.confidence_score:.2f}")

    # æ€§èƒ½å¯¹æ¯”
    print("\nğŸ“Š å¼€å§‹GPU vs CPUæ€§èƒ½å¯¹æ¯”...")
    benchmark = gpu_manager.benchmark_gpu_vs_cpu(data, operation="predict")
    print(f"GPUæ—¶é—´: {benchmark['gpu_time']:.4f}s")
    print(f"CPUæ—¶é—´: {benchmark['cpu_time']:.4f}s")
    print(f"åŠ é€Ÿæ¯”: {benchmark['speedup']:.2f}x")

    # ç”ŸæˆæŠ¥å‘Š
    print("\nğŸ“‹ GPUä½¿ç”¨æŠ¥å‘Š:")
    print(gpu_manager.generate_gpu_report())


if __name__ == "__main__":
    main()
