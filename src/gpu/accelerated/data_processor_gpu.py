#!/usr/bin/env python3
"""
GPUåŠ é€Ÿçš„æ•°æ®å¤„ç†å™¨
ä½¿ç”¨cuDFå’ŒcuPyå®ç°é«˜æ€§èƒ½æ•°æ®å¤„ç†
æ”¯æŒå¤§è§„æ¨¡é‡‘èæ•°æ®çš„å¹¶è¡Œå¤„ç†å’Œå®æ—¶åˆ†æ
"""

import time
import numpy as np
import pandas as pd
import cupy as cp
import cudf
from cuml.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from cuml.feature_selection import SelectKBest, mutual_info_regression
from cuml.decomposition import PCA, IncrementalPCA
from cuml.cluster import KMeans, DBSCAN
from typing import Dict, List, Tuple, Optional, Union, Any, Callable
from dataclasses import dataclass
import logging
from datetime import datetime, timedelta
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import dask.dataframe as dd
from dask.distributed import Client


@dataclass
class ProcessingResult:
    """æ•°æ®å¤„ç†ç»“æœ"""

    processed_data: pd.DataFrame
    processing_time: float
    memory_usage: Dict[str, float]
    gpu_accelerated: bool
    quality_metrics: Dict[str, float]
    data_shape: Tuple[int, int]


@dataclass
class BatchProcessingResult:
    """æ‰¹é‡æ•°æ®å¤„ç†ç»“æœ"""

    results: List[ProcessingResult]
    total_time: float
    avg_processing_time: float
    total_records_processed: int
    gpu_utilization: float


class GPUDataProcessor:
    """GPUåŠ é€Ÿçš„æ•°æ®å¤„ç†å™¨"""

    def __init__(
        self, gpu_enabled: bool = True, n_jobs: int = 1, chunk_size: int = 10000
    ):
        self.gpu_enabled = gpu_enabled
        self.n_jobs = n_jobs
        self.chunk_size = chunk_size
        self.logger = logging.getLogger(__name__)
        self.scalers = {}
        self.preprocessing_pipeline = {}

        # åˆå§‹åŒ–GPUå†…å­˜æ± 
        if self.gpu_enabled:
            cp.cuda.set_allocator(cp.cuda.MemoryPool())

    def load_and_preprocess(self, data: pd.DataFrame) -> ProcessingResult:
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        start_time = time.time()

        # æ•°æ®è´¨é‡æ£€æŸ¥
        quality_metrics = self._check_data_quality(data)

        # å¤„ç†ç¼ºå¤±å€¼
        cleaned_data = self._handle_missing_values(data)

        # æ•°æ®ç±»å‹ä¼˜åŒ–
        optimized_data = self._optimize_dtypes(cleaned_data)

        # å¼‚å¸¸å€¼å¤„ç†
        cleaned_data = self._handle_outliers(optimized_data)

        # ç‰¹å¾æ ‡å‡†åŒ–
        processed_data = self._normalize_features(cleaned_data)

        processing_time = time.time() - start_time

        # è®¡ç®—å†…å­˜ä½¿ç”¨
        memory_usage = {
            "original_memory": data.memory_usage(deep=True).sum(),
            "processed_memory": processed_data.memory_usage(deep=True).sum(),
            "compression_ratio": data.memory_usage(deep=True).sum()
            / processed_data.memory_usage(deep=True).sum(),
        }

        return ProcessingResult(
            processed_data=processed_data,
            processing_time=processing_time,
            memory_usage=memory_usage,
            gpu_accelerated=self.gpu_enabled,
            quality_metrics=quality_metrics,
            data_shape=processed_data.shape,
        )

    def _check_data_quality(self, data: pd.DataFrame) -> Dict[str, float]:
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        quality_metrics = {}

        # æ£€æŸ¥ç¼ºå¤±å€¼
        missing_values = data.isnull().sum()
        total_cells = data.shape[0] * data.shape[1]
        missing_ratio = missing_values.sum() / total_cells
        quality_metrics["missing_ratio"] = missing_ratio

        # æ£€æŸ¥é‡å¤å€¼
        duplicate_ratio = data.duplicated().sum() / len(data)
        quality_metrics["duplicate_ratio"] = duplicate_ratio

        # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        if len(numeric_columns) > 0:
            # æ£€æŸ¥æ˜¯å¦æœ‰æ— ç©·å¤§å€¼
            inf_count = np.isinf(data[numeric_columns]).sum().sum()
            inf_ratio = inf_count / total_cells
            quality_metrics["inf_ratio"] = inf_ratio

        # æ£€æŸ¥æ•°æ®ç±»å‹åˆ†å¸ƒ
        dtype_counts = data.dtypes.value_counts()
        quality_metrics["dtype_distribution"] = dict(dtype_counts)

        return quality_metrics

    def _handle_missing_values(self, data: pd.DataFrame) -> pd.DataFrame:
        """å¤„ç†ç¼ºå¤±å€¼"""
        if self.gpu_enabled:
            df_gpu = cudf.DataFrame(data)

            # æ•°å€¼åˆ—ï¼šç”¨ä¸­ä½æ•°å¡«å……
            numeric_columns = df_gpu.select_dtypes(include=["float64", "int64"]).columns
            for col in numeric_columns:
                median_val = df_gpu[col].median()
                df_gpu[col] = df_gpu[col].fillna(median_val)

            # åˆ†ç±»åˆ—ï¼šç”¨ä¼—æ•°å¡«å……
            categorical_columns = df_gpu.select_dtypes(include=["object"]).columns
            for col in categorical_columns:
                mode_val = df_gpu[col].mode()
                if len(mode_val) > 0:
                    df_gpu[col] = df_gpu[col].fillna(mode_val[0])

            return df_gpu.to_pandas()
        else:
            data = data.copy()
            # æ•°å€¼åˆ—ï¼šç”¨ä¸­ä½æ•°å¡«å……
            numeric_columns = data.select_dtypes(include=[np.number]).columns
            for col in numeric_columns:
                median_val = data[col].median()
                data[col] = data[col].fillna(median_val)

            # åˆ†ç±»åˆ—ï¼šç”¨ä¼—æ•°å¡«å……
            categorical_columns = data.select_dtypes(include=["object"]).columns
            for col in categorical_columns:
                mode_val = data[col].mode()
                if len(mode_val) > 0:
                    data[col] = data[col].fillna(mode_val[0])

            return data

    def _optimize_dtypes(self, data: pd.DataFrame) -> pd.DataFrame:
        """ä¼˜åŒ–æ•°æ®ç±»å‹"""
        if self.gpu_enabled:
            df_gpu = cudf.DataFrame(data)

            # æ•°å€¼åˆ—ç±»å‹ä¼˜åŒ–
            for col in df_gpu.select_dtypes(include=["float64"]).columns:
                df_gpu[col] = df_gpu[col].astype("float32")

            for col in df_gpu.select_dtypes(include=["int64"]).columns:
                df_gpu[col] = df_gpu[col].astype("int32")

            return df_gpu.to_pandas()
        else:
            data = data.copy()
            # æ•°å€¼åˆ—ç±»å‹ä¼˜åŒ–
            for col in data.select_dtypes(include=[np.float64]).columns:
                data[col] = pd.to_numeric(data[col], downcast="float")

            for col in data.select_dtypes(include=[np.int64]).columns:
                data[col] = pd.to_numeric(data[col], downcast="integer")

            return data

    def _handle_outliers(self, data: pd.DataFrame, method: str = "iqr") -> pd.DataFrame:
        """å¤„ç†å¼‚å¸¸å€¼"""
        if self.gpu_enabled:
            df_gpu = cudf.DataFrame(data)

            numeric_columns = df_gpu.select_dtypes(include=["float32", "int32"]).columns

            for col in numeric_columns:
                if method == "iqr":
                    Q1 = df_gpu[col].quantile(0.25)
                    Q3 = df_gpu[col].quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR

                    df_gpu[col] = df_gpu[col].clip(lower_bound, upper_bound)

                elif method == "zscore":
                    mean_val = df_gpu[col].mean()
                    std_val = df_gpu[col].std()
                    z_scores = (df_gpu[col] - mean_val) / std_val
                    df_gpu[col] = (
                        df_gpu[col].abs().clip(3, upper=None)
                    )  # é™åˆ¶z-scoreåœ¨3ä»¥å†…

            return df_gpu.to_pandas()
        else:
            data = data.copy()
            numeric_columns = data.select_dtypes(include=[np.number]).columns

            for col in numeric_columns:
                if method == "iqr":
                    Q1 = data[col].quantile(0.25)
                    Q3 = data[col].quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR

                    data[col] = data[col].clip(lower_bound, upper_bound)

                elif method == "zscore":
                    mean_val = data[col].mean()
                    std_val = data[col].std()
                    z_scores = (data[col] - mean_val) / std_val
                    data[col] = data[col].clip(
                        mean_val - 3 * std_val, mean_val + 3 * std_val
                    )

            return data

    def _normalize_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾æ ‡å‡†åŒ–"""
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        feature_data = data[numeric_columns]

        if self.gpu_enabled:
            df_gpu = cudf.DataFrame(feature_data)
            scaler = StandardScaler()
            normalized_features = scaler.fit_transform(df_gpu)
            normalized_df = cudf.DataFrame(normalized_features, columns=df_gpu.columns)

            # å­˜å‚¨scalerç”¨äºåç»­ä½¿ç”¨
            self.scalers["standard_scaler"] = scaler

            result = data.copy()
            for col in normalized_df.columns:
                result[f"{col}_normalized"] = normalized_df[col].to_numpy()
        else:
            scaler = StandardScaler()
            normalized_features = scaler.fit_transform(feature_data)
            normalized_df = pd.DataFrame(
                normalized_features, columns=feature_data.columns
            )

            # å­˜å‚¨scalerç”¨äºåç»­ä½¿ç”¨
            self.scalers["standard_scaler"] = scaler

            result = data.copy()
            for col in normalized_df.columns:
                result[f"{col}_normalized"] = normalized_df[col].values

        return result

    def parallel_processing(
        self, data_list: List[pdDataFrame]
    ) -> BatchProcessingResult:
        """å¹¶è¡Œæ•°æ®å¤„ç†"""
        start_time = time.time()
        results = []

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=self.n_jobs) as executor:
            futures = [
                executor.submit(self.load_and_preprocess, data) for data in data_list
            ]

            for future in as_completed(futures):
                try:
                    result = future.result(timeout=300)
                    results.append(result)
                except Exception as e:
                    self.logger.error(f"å¹¶è¡Œå¤„ç†å¤±è´¥: {e}")

        total_time = time.time() - start_time
        avg_processing_time = total_time / len(results) if results else 0
        total_records_processed = sum(result.data_shape[0] for result in results)

        return BatchProcessingResult(
            results=results,
            total_time=total_time,
            avg_processing_time=avg_processing_time,
            total_records_processed=total_records_processed,
            gpu_utilization=self._estimate_gpu_utilization(),
        )

    def _estimate_gpu_utilization(self) -> float:
        """ä¼°ç®—GPUåˆ©ç”¨ç‡"""
        if self.gpu_enabled:
            try:
                # è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
                memory_info = cp.cuda.mem_get_info()
                used_memory = memory_info[1] - memory_info[0]
                total_memory = memory_info[1]
                return used_memory / total_memory
            except:
                return 0.0
        else:
            return 0.0

    def real_time_data_streaming(
        self, data_stream: Callable[[], pd.DataFrame], window_size: int = 1000
    ) -> ProcessingResult:
        """å®æ—¶æ•°æ®æµå¤„ç†"""
        window_data = []
        processing_times = []

        for _ in range(window_size):
            try:
                # è·å–æ–°çš„æ•°æ®ç‚¹
                new_data = data_stream()
                window_data.append(new_data)

                # æ¯å¤„ç†ä¸€å®šæ•°é‡æ•°æ®ç‚¹åè¿›è¡Œæ‰¹é‡å¤„ç†
                if len(window_data) >= 100:
                    batch_result = self.process_real_time_batch(window_data)
                    processing_times.append(batch_result.processing_time)

                    # æ¸…ç©ºçª—å£
                    window_data = []

            except Exception as e:
                self.logger.error(f"å®æ—¶æ•°æ®å¤„ç†é”™è¯¯: {e}")
                continue

        # å¤„ç†å‰©ä½™æ•°æ®
        if window_data:
            batch_result = self.process_real_time_batch(window_data)
            processing_times.append(batch_result.processing_time)

        # è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´
        avg_processing_time = np.mean(processing_times) if processing_times else 0

        return ProcessingResult(
            processed_data=pd.DataFrame(),  # ç®€åŒ–å¤„ç†
            processing_time=avg_processing_time,
            memory_usage={},
            gpu_accelerated=self.gpu_enabled,
            quality_metrics={},
            data_shape=(0, 0),
        )

    def process_real_time_batch(
        self, batch_data: List[pd.DataFrame]
    ) -> ProcessingResult:
        """å¤„ç†å®æ—¶æ•°æ®æ‰¹æ¬¡"""
        start_time = time.time()

        # åˆå¹¶æ‰¹æ¬¡æ•°æ®
        combined_data = pd.concat(batch_data, ignore_index=True)

        # ä½¿ç”¨GPUå¤„ç†
        result = self.load_and_preprocess(combined_data)

        processing_time = time.time() - start_time
        result.processing_time = processing_time

        return result

    def feature_engineering_pipeline(self, data: pd.DataFrame) -> ProcessingResult:
        """ç‰¹å¾å·¥ç¨‹æµæ°´çº¿"""
        start_time = time.time()

        # 1. åŸºç¡€é¢„å¤„ç†
        preprocessed_data = self.load_and_preprocess(data).processed_data

        # 2. ç‰¹å¾é€‰æ‹©
        selected_features = self.feature_selection(preprocessed_data)

        # 3. é™ç»´å¤„ç†
        reduced_data = self.dimensionality_reduction(
            preprocessed_data[selected_features]
        )

        # 4. ç‰¹å¾é‡è¦æ€§åˆ†æ
        importance_scores = self.feature_importance_analysis(
            preprocessed_data[selected_features]
        )

        processing_time = time.time() - start_time

        return ProcessingResult(
            processed_data=reduced_data,
            processing_time=processing_time,
            memory_usage={},
            gpu_accelerated=self.gpu_enabled,
            quality_metrics=importance_scores,
            data_shape=reduced_data.shape,
        )

    def feature_selection(self, data: pd.DataFrame, k: int = 50) -> List[str]:
        """ç‰¹å¾é€‰æ‹©"""
        feature_columns = [
            col for col in data.columns if col != "target" and "normalized" in col
        ]

        if len(feature_columns) == 0:
            return []

        X = data[feature_columns]
        y = (
            data["target"] if "target" in data.columns else data["close"]
        )  # é»˜è®¤ä½¿ç”¨æ”¶ç›˜ä»·ä½œä¸ºç›®æ ‡

        if self.gpu_enabled:
            X_gpu = cudf.DataFrame(X)
            y_gpu = cudf.Series(y)

            # ä½¿ç”¨äº’ä¿¡æ¯è¿›è¡Œç‰¹å¾é€‰æ‹©
            selector = SelectKBest(k=k)
            X_selected = selector.fit_transform(X_gpu, y_gpu)

            # è·å–é€‰ä¸­çš„ç‰¹å¾ç´¢å¼•
            selected_indices = selector.get_support(indices=True)
            selected_features = [feature_columns[i] for i in selected_indices]
        else:
            from sklearn.feature_selection import SelectKBest, mutual_info_regression

            selector = SelectKBest(k=k)
            X_selected = selector.fit_transform(X, y)
            selected_indices = selector.get_support(indices=True)
            selected_features = [feature_columns[i] for i in selected_indices]

        return selected_features

    def dimensionality_reduction(
        self, data: pd.DataFrame, n_components: int = 20
    ) -> pd.DataFrame:
        """é™ç»´å¤„ç†"""
        if self.gpu_enabled:
            df_gpu = cudf.DataFrame(data)

            # ä½¿ç”¨PCAè¿›è¡Œé™ç»´
            pca = PCA(n_components=min(n_components, len(data.columns)))
            reduced_features = pca.fit_transform(df_gpu)

            # åˆ›å»ºé™ç»´åçš„DataFrame
            reduced_df = cudf.DataFrame(
                reduced_features,
                columns=[f"pca_{i}" for i in range(reduced_features.shape[1])],
            )

            return reduced_df.to_pandas()
        else:
            from sklearn.decomposition import PCA

            pca = PCA(n_components=min(n_components, len(data.columns)))
            reduced_features = pca.fit_transform(data)

            reduced_df = pd.DataFrame(
                reduced_features,
                columns=[f"pca_{i}" for i in range(reduced_features.shape[1])],
            )

            return reduced_df

    def feature_importance_analysis(self, data: pd.DataFrame) -> Dict[str, float]:
        """ç‰¹å¾é‡è¦æ€§åˆ†æ"""
        feature_columns = [col for col in data.columns if col != "target"]
        if len(feature_columns) == 0:
            return {}

        X = data[feature_columns]
        y = data["target"] if "target" in data.columns else data["close"]

        if self.gpu_enabled:
            from cuml.ensemble import RandomForestRegressor

            X_gpu = cudf.DataFrame(X)
            y_gpu = cudf.Series(y)

            model = RandomForestRegressor(n_estimators=100)
            model.fit(X_gpu, y_gpu)

            # è·å–ç‰¹å¾é‡è¦æ€§
            importances = model.feature_importances_
            importance_dict = dict(zip(feature_columns, importances))
        else:
            from sklearn.ensemble import RandomForestRegressor

            model = RandomForestRegressor(n_estimators=100)
            model.fit(X, y)

            importances = model.feature_importances_
            importance_dict = dict(zip(feature_columns, importances))

        return importance_dict

    def time_series_processing(
        self, data: pd.DataFrame, frequency: str = "D"
    ) -> ProcessingResult:
        """æ—¶é—´åºåˆ—æ•°æ®å¤„ç†"""
        start_time = time.time()

        # è½¬æ¢æ—¶é—´ç´¢å¼•
        if "date" not in data.columns:
            data["date"] = pd.to_datetime(data.index)

        data = data.set_index("date")

        # é‡é‡‡æ ·
        if self.gpu_enabled:
            df_gpu = cudf.DataFrame(data)
            resampled_data = df_gpu.resample(frequency).agg(
                {
                    "open": "first",
                    "high": "max",
                    "low": "min",
                    "close": "last",
                    "volume": "sum",
                }
            )
        else:
            resampled_data = data.resample(frequency).agg(
                {
                    "open": "first",
                    "high": "max",
                    "low": "min",
                    "close": "last",
                    "volume": "sum",
                }
            )

        # å¤„ç†ç¼ºå¤±å€¼
        resampled_data = resampled_data.fillna(method="ffill")

        processing_time = time.time() - start_time

        return ProcessingResult(
            processed_data=resampled_data,
            processing_time=processing_time,
            memory_usage={},
            gpu_accelerated=self.gpu_enabled,
            quality_metrics={},
            data_shape=resampled_data.shape,
        )

    def anomaly_detection(
        self, data: pd.DataFrame, method: str = "isolation_forest"
    ) -> pd.DataFrame:
        """å¼‚å¸¸æ£€æµ‹"""
        if self.gpu_enabled:
            from cuml.ensemble import IsolationForest

            feature_columns = [
                col for col in data.columns if col not in ["date", "target"]
            ]
            if len(feature_columns) == 0:
                return data

            df_gpu = cudf.DataFrame(data[feature_columns])

            # ä½¿ç”¨Isolation Forestè¿›è¡Œå¼‚å¸¸æ£€æµ‹
            iso_forest = IsolationForest(contamination=0.1)
            anomaly_labels = iso_forest.fit_predict(df_gpu)

            # æ ‡è®°å¼‚å¸¸
            result_data = data.copy()
            result_data["anomaly"] = anomaly_labels

            return result_data
        else:
            from sklearn.ensemble import IsolationForest

            feature_columns = [
                col for col in data.columns if col not in ["date", "target"]
            ]
            if len(feature_columns) == 0:
                return data

            iso_forest = IsolationForest(contamination=0.1)
            anomaly_labels = iso_forest.fit_predict(data[feature_columns])

            result_data = data.copy()
            result_data["anomaly"] = anomaly_labels

            return result_data

    def market_regime_detection(self, data: pd.DataFrame) -> pd.DataFrame:
        """å¸‚åœºçŠ¶æ€æ£€æµ‹"""
        if self.gpu_enabled:
            from cuml.cluster import KMeans

            # ä½¿ç”¨æ”¶ç›Šç‡å’Œæ³¢åŠ¨ç‡ä½œä¸ºç‰¹å¾
            returns = data["close"].pct_change()
            volatility = returns.rolling(window=20).std()

            features = cudf.DataFrame(
                {"return": returns, "volatility": volatility}
            ).dropna()

            # ä½¿ç”¨K-meansèšç±»æ£€æµ‹å¸‚åœºçŠ¶æ€
            kmeans = KMeans(n_clusters=3)
            cluster_labels = kmeans.fit_predict(features)

            result_data = data.copy()
            result_data["market_regime"] = cluster_labels.reindex(data.index)

            return result_data
        else:
            from sklearn.cluster import KMeans

            returns = data["close"].pct_change()
            volatility = returns.rolling(window=20).std()

            features = pd.DataFrame(
                {"return": returns, "volatility": volatility}
            ).dropna()

            kmeans = KMeans(n_clusters=3)
            cluster_labels = kmeans.fit_predict(features)

            result_data = data.copy()
            result_data["market_regime"] = cluster_labels.reindex(data.index)

            return result_data

    def save_processing_pipeline(self, filepath: str):
        """ä¿å­˜å¤„ç†æµæ°´çº¿"""
        import joblib

        pipeline_data = {
            "scalers": self.scalers,
            "preprocessing_pipeline": self.preprocessing_pipeline,
            "gpu_enabled": self.gpu_enabled,
            "n_jobs": self.n_jobs,
            "chunk_size": self.chunk_size,
        }
        joblib.dump(pipeline_data, filepath)

    def load_processing_pipeline(self, filepath: str):
        """åŠ è½½å¤„ç†æµæ°´çº¿"""
        import joblib

        pipeline_data = joblib.load(filepath)
        self.scalers = pipeline_data["scalers"]
        self.preprocessing_pipeline = pipeline_data["preprocessing_pipeline"]
        self.gpu_enabled = pipeline_data["gpu_enabled"]
        self.n_jobs = pipeline_data["n_jobs"]
        self.chunk_size = pipeline_data["chunk_size"]


class BatchDataProcessor:
    """æ‰¹é‡æ•°æ®å¤„ç†å™¨"""

    def __init__(self, gpu_enabled: bool = True, max_workers: int = 4):
        self.gpu_enabled = gpu_enabled
        self.max_workers = max_workers
        self.base_processor = GPUDataProcessor(gpu_enabled, max_workers)

    def process_large_dataset(
        self, data_path: str, output_path: str, chunk_size: int = 100000
    ) -> Dict:
        """å¤„ç†å¤§å‹æ•°æ®é›†"""
        start_time = time.time()

        # ä½¿ç”¨åˆ†å—å¤„ç†
        reader = pd.read_csv(data_path, chunksize=chunk_size)
        processed_chunks = []

        for i, chunk in enumerate(reader):
            print(f"å¤„ç†ç¬¬ {i+1} å—æ•°æ®...")
            result = self.base_processor.load_and_preprocess(chunk)
            processed_chunks.append(result.processed_data)

            # ä¿å­˜ä¸­é—´ç»“æœ
            chunk_path = f"{output_path}_chunk_{i}.csv"
            result.processed_data.to_csv(chunk_path, index=False)

        # åˆå¹¶æ‰€æœ‰å—
        final_result = pd.concat(processed_chunks, ignore_index=True)
        final_path = f"{output_path}_final.csv"
        final_result.to_csv(final_path, index=False)

        total_time = time.time() - start_time

        return {
            "total_time": total_time,
            "total_chunks": len(processed_chunks),
            "final_records": len(final_result),
            "output_path": final_path,
        }

    def distributed_processing(self, data_paths: List[str], output_path: str) -> Dict:
        """åˆ†å¸ƒå¼æ•°æ®å¤„ç†"""
        try:
            # åˆå§‹åŒ–Daskå®¢æˆ·ç«¯
            client = Client(n_workers=self.max_workers)

            # è¯»å–æ•°æ®
            dfs = [dd.read_csv(path) for path in data_paths]
            combined_df = dd.concat(dfs)

            # å¹¶è¡Œå¤„ç†
            processed_dfs = []
            for i in range(len(dfs)):
                processed_dfs.append(
                    combined_df.map_partitions(
                        lambda df: self.base_processor.load_and_preprocess(
                            df
                        ).processed_data,
                        meta=combined_df._meta,
                    )
                )

            # ä¿å­˜ç»“æœ
            final_df = dd.concat(processed_dfs)
            final_df.to_csv(output_path, index=False, single_file=True)

            total_time = time.time() - start_time

            client.close()

            return {
                "total_time": total_time,
                "input_files": len(data_paths),
                "output_path": output_path,
            }

        except Exception as e:
            self.logger.error(f"åˆ†å¸ƒå¼å¤„ç†å¤±è´¥: {e}")
            return {"error": str(e)}


def benchmark_data_processing(data: pd.DataFrame, gpu_enabled: bool = True):
    """æ•°æ®å¤„ç†æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("ğŸ”¬ å¼€å§‹æ•°æ®å¤„ç†æ€§èƒ½æµ‹è¯•...")

    # GPUç‰ˆæœ¬
    gpu_processor = GPUDataProcessor(gpu_enabled=True)
    gpu_start = time.time()
    gpu_result = gpu_processor.load_and_preprocess(data)
    gpu_time = time.time() - gpu_start

    # CPUç‰ˆæœ¬
    cpu_processor = GPUDataProcessor(gpu_enabled=False)
    cpu_start = time.time()
    cpu_result = cpu_processor.load_and_preprocess(data)
    cpu_time = time.time() - cpu_start

    # å¯¹æ¯”ç»“æœ
    print(f"\nğŸ“Š æ•°æ®å¤„ç†æ€§èƒ½å¯¹æ¯”:")
    print(f"GPUå¤„ç†æ—¶é—´: {gpu_time:.2f}ç§’")
    print(f"CPUå¤„ç†æ—¶é—´: {cpu_time:.2f}ç§’")
    print(f"åŠ é€Ÿæ¯”: {cpu_time/gpu_time:.2f}x")
    print(f"GPUå‹ç¼©æ¯”: {gpu_result.memory_usage['compression_ratio']:.2f}x")
    print(f"CPUå‹ç¼©æ¯”: {cpu_result.memory_usage['compression_ratio']:.2f}x")
    print(f"GPUå¤„ç†è®°å½•æ•°: {gpu_result.data_shape[0]}")
    print(f"CPUå¤„ç†è®°å½•æ•°: {cpu_result.data_shape[0]}")

    return {
        "gpu_time": gpu_time,
        "cpu_time": cpu_time,
        "speedup": cpu_time / gpu_time,
        "gpu_result": gpu_result,
        "cpu_result": cpu_result,
    }


if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    import yfinance as yf

    # è·å–ç¤ºä¾‹æ•°æ®
    data = yf.download("AAPL", start="2023-01-01", end="2024-01-01")

    # åˆ›å»ºæ•°æ®å¤„ç†å™¨
    processor = GPUDataProcessor(gpu_enabled=True)

    # æ•°æ®å¤„ç†
    result = processor.load_and_preprocess(data)

    print(f"æ•°æ®å¤„ç†å®Œæˆ:")
    print(f"å¤„ç†æ—¶é—´: {result.processing_time:.2f}ç§’")
    print(f"æ•°æ®å½¢çŠ¶: {result.data_shape}")
    print(f"å†…å­˜å‹ç¼©æ¯”: {result.memory_usage['compression_ratio']:.2f}x")

    # åŸºå‡†æµ‹è¯•
    benchmark_results = benchmark_data_processing(data)
    print(f"\nGPUåŠ é€Ÿæ€§èƒ½æå‡: {benchmark_results['speedup']:.2f}x")
