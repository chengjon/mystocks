"""
# åŠŸèƒ½ï¼šæ€§èƒ½ç›‘æ§æ¨¡å—ï¼Œè·Ÿè¸ªæŸ¥è¯¢æ—¶é—´ã€æ…¢æŸ¥è¯¢å’Œæ€§èƒ½æŒ‡æ ‡
# ä½œè€…ï¼šJohnC (ninjas@sina.com) & Claude
# åˆ›å»ºæ—¥æœŸï¼š2025-10-16
# ç‰ˆæœ¬ï¼š2.1.0
# ä¾èµ–ï¼šè¯¦è§requirements.txtæˆ–æ–‡ä»¶å¯¼å…¥éƒ¨åˆ†
# æ³¨æ„äº‹é¡¹ï¼š
#   æœ¬æ–‡ä»¶æ˜¯MyStocks v2.1æ ¸å¿ƒç»„ä»¶ï¼Œéµå¾ª5-tieræ•°æ®åˆ†ç±»æ¶æ„
# ç‰ˆæƒï¼šMyStocks Project Â© 2025
"""

import time
import logging
from typing import Dict, Any, Optional, Callable
from functools import wraps
from contextlib import contextmanager
from datetime import datetime

from src.monitoring.monitoring_database import (
    MonitoringDatabase,
    get_monitoring_database,
)

logger = logging.getLogger(__name__)


class PerformanceMonitor:
    """
    æ€§èƒ½ç›‘æ§å™¨

    è´Ÿè´£è·Ÿè¸ªå’Œè®°å½•æ‰€æœ‰æ•°æ®åº“æ“ä½œçš„æ€§èƒ½æŒ‡æ ‡,
    è‡ªåŠ¨æ£€æµ‹æ…¢æŸ¥è¯¢å¹¶ç”Ÿæˆå‘Šè­¦ã€‚
    """

    # æ…¢æŸ¥è¯¢é˜ˆå€¼ (æ¯«ç§’)
    SLOW_QUERY_THRESHOLD_MS = 5000  # 5ç§’

    # è­¦å‘Šé˜ˆå€¼ (æ¯«ç§’)
    WARNING_THRESHOLD_MS = 2000  # 2ç§’

    def __init__(self, monitoring_db: Optional[MonitoringDatabase] = None):
        """
        åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨

        Args:
            monitoring_db: ç›‘æ§æ•°æ®åº“å®ä¾‹ (å¯é€‰)
        """
        self.monitoring_db = monitoring_db or get_monitoring_database()
        self._metrics_cache = []  # æœ¬åœ°ç¼“å­˜ (æ‰¹é‡å†™å…¥ä¼˜åŒ–)
        self._cache_size_limit = 100

        logger.info("âœ… PerformanceMonitor initialized")

    @contextmanager
    def track_operation(
        self,
        operation_name: str,
        classification: Optional[str] = None,
        database_type: Optional[str] = None,
        table_name: Optional[str] = None,
        query_sql: Optional[str] = None,
        auto_alert: bool = True,
    ):
        """
        è·Ÿè¸ªæ“ä½œæ€§èƒ½çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨

        ç”¨æ³•:
        ```python
        with monitor.track_operation('query_daily_kline', 'DAILY_KLINE', 'PostgreSQL'):
            # æ‰§è¡Œæ•°æ®åº“æ“ä½œ
            result = execute_query()
        ```

        Args:
            operation_name: æ“ä½œåç§°
            classification: æ•°æ®åˆ†ç±»
            database_type: æ•°æ®åº“ç±»å‹
            table_name: è¡¨å
            query_sql: SQLè¯­å¥
            auto_alert: æ˜¯å¦è‡ªåŠ¨å‘Šè­¦æ…¢æŸ¥è¯¢
        """
        start_time = time.time()
        error_occurred = False
        error_message = None

        try:
            yield
        except Exception as e:
            error_occurred = True
            error_message = str(e)
            raise
        finally:
            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            end_time = time.time()
            execution_time_ms = int((end_time - start_time) * 1000)

            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            self._record_metric(
                metric_name=operation_name,
                metric_value=execution_time_ms,
                metric_type="QUERY_TIME",
                classification=classification,
                database_type=database_type,
                table_name=table_name,
                query_sql=query_sql,
                error_occurred=error_occurred,
            )

            # æ…¢æŸ¥è¯¢æ£€æµ‹å’Œå‘Šè­¦
            if auto_alert and execution_time_ms >= self.SLOW_QUERY_THRESHOLD_MS:
                self._alert_slow_query(
                    operation_name=operation_name,
                    execution_time_ms=execution_time_ms,
                    classification=classification,
                    database_type=database_type,
                    table_name=table_name,
                    query_sql=query_sql,
                )
            elif auto_alert and execution_time_ms >= self.WARNING_THRESHOLD_MS:
                logger.warning(
                    f"âš ï¸  è¾ƒæ…¢æŸ¥è¯¢: {operation_name} è€—æ—¶ {execution_time_ms}ms "
                    f"({database_type}.{table_name})"
                )

    def _record_metric(
        self,
        metric_name: str,
        metric_value: float,
        metric_type: str = "QUERY_TIME",
        classification: Optional[str] = None,
        database_type: Optional[str] = None,
        table_name: Optional[str] = None,
        query_sql: Optional[str] = None,
        error_occurred: bool = False,
    ):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        is_slow_query = metric_value >= self.SLOW_QUERY_THRESHOLD_MS

        # æ·»åŠ é”™è¯¯æ ‡ç­¾
        tags = {"error_occurred": error_occurred} if error_occurred else None

        # è®°å½•åˆ°ç›‘æ§æ•°æ®åº“
        self.monitoring_db.record_performance_metric(
            metric_name=metric_name,
            metric_value=metric_value,
            metric_type=metric_type,
            metric_unit="ms",
            classification=classification,
            database_type=database_type,
            table_name=table_name,
            is_slow_query=is_slow_query,
            query_sql=query_sql if is_slow_query else None,
            tags=tags,
        )

    def _alert_slow_query(
        self,
        operation_name: str,
        execution_time_ms: int,
        classification: Optional[str] = None,
        database_type: Optional[str] = None,
        table_name: Optional[str] = None,
        query_sql: Optional[str] = None,
    ):
        """å‘Šè­¦æ…¢æŸ¥è¯¢"""
        alert_title = f"æ…¢æŸ¥è¯¢æ£€æµ‹: {operation_name}"
        alert_message = (
            f"æŸ¥è¯¢æ‰§è¡Œæ—¶é—´è¿‡é•¿: {execution_time_ms}ms (é˜ˆå€¼: {self.SLOW_QUERY_THRESHOLD_MS}ms)\n"
            f"æ•°æ®åº“: {database_type}\n"
            f"è¡¨: {table_name}\n"
            f"åˆ†ç±»: {classification}"
        )

        if query_sql:
            alert_message += f"\nSQL: {query_sql[:200]}..."

        # åˆ›å»ºå‘Šè­¦
        from src.monitoring.alert_manager import get_alert_manager

        alert_manager = get_alert_manager()
        alert_manager.send_alert(
            alert_level="WARNING",
            alert_type="SLOW_QUERY",
            alert_title=alert_title,
            alert_message=alert_message,
            source="PerformanceMonitor",
            classification=classification,
            database_type=database_type,
            table_name=table_name,
            additional_data={
                "execution_time_ms": execution_time_ms,
                "query_sql": query_sql,
            },
        )

    def record_connection_time(
        self,
        database_type: str,
        connection_time_ms: float,
        connection_status: str = "SUCCESS",
    ):
        """
        è®°å½•æ•°æ®åº“è¿æ¥æ—¶é—´

        Args:
            database_type: æ•°æ®åº“ç±»å‹
            connection_time_ms: è¿æ¥æ—¶é—´(æ¯«ç§’)
            connection_status: è¿æ¥çŠ¶æ€ (SUCCESS/FAILED)
        """
        self.monitoring_db.record_performance_metric(
            metric_name=f"{database_type}_connection",
            metric_value=connection_time_ms,
            metric_type="CONNECTION_TIME",
            metric_unit="ms",
            database_type=database_type,
            tags={"status": connection_status},
        )

        # è¿æ¥æ—¶é—´è¿‡é•¿å‘Šè­¦ (>1ç§’)
        if connection_time_ms > 1000:
            logger.warning(
                f"âš ï¸  æ•°æ®åº“è¿æ¥è¾ƒæ…¢: {database_type} è€—æ—¶ {connection_time_ms}ms"
            )

    def record_batch_operation(
        self,
        operation_name: str,
        batch_size: int,
        execution_time_ms: float,
        classification: Optional[str] = None,
        database_type: Optional[str] = None,
        table_name: Optional[str] = None,
    ):
        """
        è®°å½•æ‰¹é‡æ“ä½œæ€§èƒ½

        Args:
            operation_name: æ“ä½œåç§°
            batch_size: æ‰¹é‡å¤§å°
            execution_time_ms: æ‰§è¡Œæ—¶é—´(æ¯«ç§’)
            classification: æ•°æ®åˆ†ç±»
            database_type: æ•°æ®åº“ç±»å‹
            table_name: è¡¨å
        """
        # è®°å½•æ‰¹é‡å¤§å°
        self.monitoring_db.record_performance_metric(
            metric_name=f"{operation_name}_batch_size",
            metric_value=batch_size,
            metric_type="BATCH_SIZE",
            metric_unit="count",
            classification=classification,
            database_type=database_type,
            table_name=table_name,
        )

        # è®°å½•æ‰§è¡Œæ—¶é—´
        self.monitoring_db.record_performance_metric(
            metric_name=f"{operation_name}_batch_time",
            metric_value=execution_time_ms,
            metric_type="QUERY_TIME",
            metric_unit="ms",
            classification=classification,
            database_type=database_type,
            table_name=table_name,
        )

        # è®¡ç®—ååé‡ (records/second)
        if execution_time_ms > 0:
            throughput = (batch_size / execution_time_ms) * 1000
            logger.info(
                f"ğŸ“Š æ‰¹é‡æ“ä½œ: {operation_name} - {batch_size}æ¡è®°å½• "
                f"è€—æ—¶{execution_time_ms}ms (ååé‡: {throughput:.0f} records/s)"
            )

    """
        è·å–æ€§èƒ½æ‘˜è¦

        Args:
            hours: ç»Ÿè®¡æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰

        Returns:
            dict: æ€§èƒ½æ‘˜è¦
        """
        # ä»ç›‘æ§æ•°æ®åº“æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯
        try:
            from src.monitoring.monitoring_database import get_monitoring_database
            monitoring_db = get_monitoring_database()
            
            if monitoring_db:
                # æŸ¥è¯¢æ…¢æŸ¥è¯¢æ•°é‡
                slow_query_count = monitoring_db.get_slow_query_count(hours)
                
                # æŸ¥è¯¢å¹³å‡æŸ¥è¯¢æ—¶é—´
                avg_query_time = monitoring_db.get_average_query_time(hours)
                
                # æŸ¥è¯¢æœ€å¤§æŸ¥è¯¢æ—¶é—´
                max_query_time = monitoring_db.get_max_query_time(hours)
                
                # æŸ¥è¯¢æ€»æŸ¥è¯¢æ•°
                total_queries = monitoring_db.get_total_query_count(hours)
                
                return {
                    "period_hours": hours,
                    "slow_query_count": slow_query_count,
                    "avg_query_time_ms": avg_query_time,
                    "max_query_time_ms": max_query_time,
                    "total_queries": total_queries,
                }
            else:
                # å¦‚æœæ²¡æœ‰ç›‘æ§æ•°æ®åº“è¿æ¥ï¼Œè¿”å›åŸºæœ¬æ•°æ®
                return {
                    "period_hours": hours,
                    "slow_query_count": 0,
                    "avg_query_time_ms": 0,
                    "max_query_time_ms": 0,
                    "total_queries": 0,
                }
        except Exception as e:
            logger.warning(f"æŸ¥è¯¢æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            # å‡ºé”™æ—¶è¿”å›åŸºæœ¬æ•°æ®
            return {
                "period_hours": hours,
                "slow_query_count": 0,
                "avg_query_time_ms": 0,
                "max_query_time_ms": 0,
                "total_queries": 0,
            }


def performance_tracked(
    operation_name: str,
    classification: Optional[str] = None,
    database_type: Optional[str] = None,
    table_name: Optional[str] = None,
):
    """
    æ€§èƒ½è·Ÿè¸ªè£…é¥°å™¨

    ç”¨æ³•:
    ```python
    @performance_tracked('query_daily_kline', 'DAILY_KLINE', 'PostgreSQL', 'daily_kline')
    def query_data():
        # æ‰§è¡ŒæŸ¥è¯¢
        pass
    ```
    """

    def decorator(func: Callable):
        @wraps(func)
        def wrapper(*args, **kwargs):
            monitor = get_performance_monitor()
            with monitor.track_operation(
                operation_name=operation_name,
                classification=classification,
                database_type=database_type,
                table_name=table_name,
            ):
                return func(*args, **kwargs)

        return wrapper

    return decorator


# å…¨å±€æ€§èƒ½ç›‘æ§å™¨å®ä¾‹ (å•ä¾‹æ¨¡å¼)
_performance_monitor: Optional[PerformanceMonitor] = None


def get_performance_monitor() -> PerformanceMonitor:
    """è·å–å…¨å±€æ€§èƒ½ç›‘æ§å™¨å®ä¾‹ (å•ä¾‹æ¨¡å¼)"""
    global _performance_monitor
    if _performance_monitor is None:
        _performance_monitor = PerformanceMonitor()
    return _performance_monitor


if __name__ == "__main__":
    """æµ‹è¯•æ€§èƒ½ç›‘æ§å™¨"""
    import sys

    sys.path.insert(0, ".")

    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
    )

    print("\næµ‹è¯•PerformanceMonitor...\n")

    # åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
    monitor = PerformanceMonitor()

    # æµ‹è¯•1: æ­£å¸¸æŸ¥è¯¢
    print("1. æµ‹è¯•æ­£å¸¸æŸ¥è¯¢è·Ÿè¸ª...")
    with monitor.track_operation(
        "test_query_normal", "DAILY_KLINE", "PostgreSQL", "daily_kline"
    ):
        time.sleep(0.1)  # æ¨¡æ‹Ÿ100msæŸ¥è¯¢
    print("   âœ… æ­£å¸¸æŸ¥è¯¢å·²è®°å½•\n")

    # æµ‹è¯•2: è¾ƒæ…¢æŸ¥è¯¢
    print("2. æµ‹è¯•è¾ƒæ…¢æŸ¥è¯¢ (è­¦å‘Š)...")
    with monitor.track_operation(
        "test_query_warning", "DAILY_KLINE", "PostgreSQL", "daily_kline"
    ):
        time.sleep(2.5)  # æ¨¡æ‹Ÿ2.5ç§’æŸ¥è¯¢ (è¶…è¿‡è­¦å‘Šé˜ˆå€¼)
    print("   âœ… è¾ƒæ…¢æŸ¥è¯¢å·²è®°å½•\n")

    # æµ‹è¯•3: æ…¢æŸ¥è¯¢ (è·³è¿‡å‘Šè­¦ä»¥é¿å…ä¾èµ–AlertManager)
    print("3. æµ‹è¯•æ…¢æŸ¥è¯¢æ£€æµ‹...")
    with monitor.track_operation(
        "test_query_slow",
        "TICK_DATA",
        "TDengine",
        "tick_data",
        auto_alert=False,  # ç¦ç”¨å‘Šè­¦
    ):
        time.sleep(5.5)  # æ¨¡æ‹Ÿ5.5ç§’æ…¢æŸ¥è¯¢
    print("   âœ… æ…¢æŸ¥è¯¢å·²è®°å½•\n")

    # æµ‹è¯•4: è¿æ¥æ—¶é—´è®°å½•
    print("4. æµ‹è¯•è¿æ¥æ—¶é—´è®°å½•...")
    monitor.record_connection_time("TDengine", 150.5, "SUCCESS")
    print("   âœ… è¿æ¥æ—¶é—´å·²è®°å½•\n")

    # æµ‹è¯•5: æ‰¹é‡æ“ä½œè®°å½•
    print("5. æµ‹è¯•æ‰¹é‡æ“ä½œè®°å½•...")
    monitor.record_batch_operation(
        "batch_insert_tick",
        batch_size=10000,
        execution_time_ms=850.2,
        classification="TICK_DATA",
        database_type="TDengine",
        table_name="tick_data",
    )
    print("   âœ… æ‰¹é‡æ“ä½œå·²è®°å½•\n")

    # æµ‹è¯•6: è£…é¥°å™¨ç”¨æ³•
    print("6. æµ‹è¯•æ€§èƒ½è·Ÿè¸ªè£…é¥°å™¨...")

    @performance_tracked("decorated_query", "DAILY_KLINE", "PostgreSQL", "daily_kline")
    def test_decorated_function():
        time.sleep(0.05)
        return "æŸ¥è¯¢ç»“æœ"

    result = test_decorated_function()
    print(f"   âœ… è£…é¥°å™¨æµ‹è¯•å®Œæˆ: {result}\n")

    print("âœ… PerformanceMonitor æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
