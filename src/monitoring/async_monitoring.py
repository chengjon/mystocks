"""
# åŠŸèƒ½ï¼šå¼‚æ­¥ç›‘æ§æ¨¡å— - äº‹ä»¶é©±åŠ¨çš„ç›‘æ§æ•°æ®é‡‡é›†
# ä½œè€…ï¼šClaude (åŸºäºå¤šè§’è‰²æ¶æ„è¯„ä¼°å»ºè®®)
# åˆ›å»ºæ—¥æœŸï¼š2026-01-03
# ç‰ˆæœ¬ï¼š1.0.0
# ROIï¼š9/10 - ä¸šåŠ¡å»¶è¿Ÿå‡å°‘15-30%
# æ³¨æ„äº‹é¡¹ï¼š
#   æœ¬æ–‡ä»¶å®ç°å¼‚æ­¥ç›‘æ§æ¶æ„ï¼Œè§£è€¦ç›‘æ§ä¸ä¸šåŠ¡é€»è¾‘
#   ä½¿ç”¨Redis Pub/Subæ¨¡å¼ + åå°Workeræ‰¹é‡å†™å…¥
# ç‰ˆæƒï¼šMyStocks Project Â© 2026
"""

import json
import logging
import threading
import time
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)


@dataclass
class MonitoringEvent:
    """ç›‘æ§äº‹ä»¶æ•°æ®ç±»"""

    event_type: str  # 'operation', 'performance', 'quality_check', 'alert', 'metric_update'
    data: Dict[str, Any]
    timestamp: datetime
    retry_count: int = 0
    max_retries: int = 3

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "event_type": self.event_type,
            "data": self.data,
            "timestamp": self.timestamp.isoformat(),
            "retry_count": self.retry_count,
            "max_retries": self.max_retries,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "MonitoringEvent":
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        data["timestamp"] = datetime.fromisoformat(data["timestamp"])
        return cls(**data)


class MonitoringEventPublisher:
    """
    ç›‘æ§äº‹ä»¶å‘å¸ƒå™¨

    è´Ÿè´£å°†ç›‘æ§äº‹ä»¶å‘å¸ƒåˆ°Redisé˜Ÿåˆ—ï¼Œå®ç°å¼‚æ­¥è§£è€¦ã€‚
    """

    def __init__(self, redis_channel: str = "mystocks:monitoring:events"):
        """
        åˆå§‹åŒ–äº‹ä»¶å‘å¸ƒå™¨

        Args:
            redis_channel: Redisé¢‘é“åç§°
        """
        self.redis_channel = redis_channel
        self._redis_client = None
        self._enabled = True
        self._fallback_cache: List[MonitoringEvent] = []  # Redisä¸å¯ç”¨æ—¶çš„é™çº§ç¼“å­˜
        self._fallback_cache_size = 100

        logger.info("âœ… MonitoringEventPublisher initialized (channel=%s)", redis_channel)

    def _get_redis_client(self):
        """è·å–Rediså®¢æˆ·ç«¯ï¼ˆå»¶è¿Ÿè¿æ¥ï¼‰"""
        if self._redis_client is None:
            try:
                # å°è¯•ä»ç¯å¢ƒå˜é‡è¯»å–Redisé…ç½®
                import os

                import redis

                redis_host = os.getenv("REDIS_HOST", "localhost")
                redis_port = int(os.getenv("REDIS_PORT", 6379))
                redis_db = int(os.getenv("REDIS_DB", 0))

                # ä½¿ç”¨è¿æ¥æ± 
                self._redis_pool = redis.ConnectionPool(
                    host=redis_host,
                    port=redis_port,
                    db=redis_db,
                    decode_responses=False,  # ä¿æŒäºŒè¿›åˆ¶æ¨¡å¼
                    socket_timeout=2,
                    socket_connect_timeout=2,
                    max_connections=10,
                )
                self._redis_client = redis.Redis(connection_pool=self._redis_pool)

                # æµ‹è¯•è¿æ¥
                self._redis_client.ping()
                logger.info("âœ… Redisè¿æ¥æˆåŠŸ: %s:%d", redis_host, redis_port)
            except Exception as e:
                logger.warning("âš ï¸ Redisè¿æ¥å¤±è´¥ï¼Œä½¿ç”¨é™çº§ç¼“å­˜: %s", e)
                self._redis_client = False  # æ ‡è®°è¿æ¥å¤±è´¥

        return self._redis_client if self._redis_client is not False else None

    def publish_event(self, event: MonitoringEvent) -> bool:
        """
        å‘å¸ƒç›‘æ§äº‹ä»¶

        Args:
            event: ç›‘æ§äº‹ä»¶

        Returns:
            bool: å‘å¸ƒæ˜¯å¦æˆåŠŸ
        """
        if not self._enabled:
            return True

        try:
            redis_client = self._get_redis_client()
            if redis_client:
                # å‘å¸ƒåˆ°Redis
                event_data = json.dumps(event.to_dict())
                redis_client.lpush(self.redis_channel, event_data)
                logger.debug("ğŸ“¤ ç›‘æ§äº‹ä»¶å·²å‘å¸ƒ: %s", event.event_type)
                return True
            else:
                # Redisä¸å¯ç”¨ï¼Œä½¿ç”¨é™çº§ç¼“å­˜
                self._add_to_fallback_cache(event)
                return True

        except Exception as e:
            logger.warning("å‘å¸ƒç›‘æ§äº‹ä»¶å¤±è´¥: %s", e)
            # é™çº§åˆ°ç¼“å­˜
            self._add_to_fallback_cache(event)
            return False

    def _add_to_fallback_cache(self, event: MonitoringEvent):
        """æ·»åŠ åˆ°é™çº§ç¼“å­˜"""
        self._fallback_cache.append(event)
        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(self._fallback_cache) > self._fallback_cache_size:
            self._fallback_cache.pop(0)
        logger.debug("ğŸ“¦ äº‹ä»¶å·²æ·»åŠ åˆ°é™çº§ç¼“å­˜ (ç¼“å­˜å¤§å°: %d)", len(self._fallback_cache))

    def get_fallback_events(self) -> List[MonitoringEvent]:
        """è·å–é™çº§ç¼“å­˜ä¸­çš„äº‹ä»¶"""
        events = self._fallback_cache.copy()
        self._fallback_cache.clear()
        return events

    def enable(self):
        """å¯ç”¨äº‹ä»¶å‘å¸ƒ"""
        self._enabled = True
        logger.info("âœ… MonitoringEventPublisher å·²å¯ç”¨")

    def disable(self):
        """ç¦ç”¨äº‹ä»¶å‘å¸ƒ"""
        self._enabled = False
        logger.info("âš ï¸ MonitoringEventPublisher å·²ç¦ç”¨")

    def close(self):
        """å…³é—­Redisè¿æ¥"""
        if self._redis_client is not None and self._redis_client is not False:
            try:
                self._redis_client.close()
                if hasattr(self, "_redis_pool") and self._redis_pool is not None:
                    self._redis_pool.disconnect()
                logger.info("âœ… MonitoringEventPublisher Redisè¿æ¥å·²å…³é—­")
            except Exception as e:
                logger.warning("âš ï¸ å…³é—­Redisè¿æ¥å¤±è´¥: %s", e)


class MonitoringEventWorker:
    """
    ç›‘æ§äº‹ä»¶Worker

    åå°çº¿ç¨‹ï¼ŒæŒç»­ä»Redisé˜Ÿåˆ—æ¶ˆè´¹ç›‘æ§äº‹ä»¶ï¼Œæ‰¹é‡å†™å…¥ç›‘æ§æ•°æ®åº“ã€‚
    """

    def __init__(
        self,
        redis_channel: str = "mystocks:monitoring:events",
        batch_size: int = 50,
        poll_interval: float = 0.1,
    ):
        """
        åˆå§‹åŒ–äº‹ä»¶Worker

        Args:
            redis_channel: Redisé¢‘é“åç§°
            batch_size: æ‰¹é‡å†™å…¥å¤§å°
            poll_interval: è½®è¯¢é—´éš”ï¼ˆç§’ï¼‰
        """
        self.redis_channel = redis_channel
        self.batch_size = batch_size
        self.poll_interval = poll_interval
        self._running = False
        self._worker_thread: Optional[threading.Thread] = None
        self._redis_client = None
        self._event_buffer: List[MonitoringEvent] = []  # äº‹ä»¶ç¼“å†²åŒº

        logger.info("âœ… MonitoringEventWorker initialized (batch_size=%d)", batch_size)

    def _get_redis_client(self):
        """è·å–Rediså®¢æˆ·ç«¯"""
        if self._redis_client is None:
            try:
                import os

                import redis

                redis_host = os.getenv("REDIS_HOST", "localhost")
                redis_port = int(os.getenv("REDIS_PORT", 6379))
                redis_db = int(os.getenv("REDIS_DB", 0))

                # ä½¿ç”¨è¿æ¥æ± 
                self._redis_pool = redis.ConnectionPool(
                    host=redis_host,
                    port=redis_port,
                    db=redis_db,
                    decode_responses=False,
                    socket_timeout=2,
                    socket_connect_timeout=2,
                    max_connections=10,
                )
                self._redis_client = redis.Redis(connection_pool=self._redis_pool)

                self._redis_client.ping()
            except Exception as e:
                logger.warning("âš ï¸ Worker Redisè¿æ¥å¤±è´¥: %s", e)
                self._redis_client = False

        return self._redis_client if self._redis_client is not False else None

    def start(self):
        """å¯åŠ¨Workerçº¿ç¨‹"""
        if self._running:
            logger.warning("âš ï¸ Workerå·²åœ¨è¿è¡Œä¸­")
            return

        self._running = True
        self._worker_thread = threading.Thread(target=self._worker_loop, daemon=True)
        self._worker_thread.start()
        logger.info("ğŸš€ ç›‘æ§äº‹ä»¶Workerå·²å¯åŠ¨")

    def stop(self):
        """åœæ­¢Workerçº¿ç¨‹"""
        if not self._running:
            return

        logger.info("â¹ï¸ æ­£åœ¨åœæ­¢ç›‘æ§äº‹ä»¶Worker...")
        self._running = False

        if self._worker_thread:
            self._worker_thread.join(timeout=5)
            if self._worker_thread.is_alive():
                logger.warning("âš ï¸ Workerçº¿ç¨‹æœªèƒ½åŠæ—¶åœæ­¢")

        # åˆ·æ–°å‰©ä½™äº‹ä»¶
        if self._event_buffer:
            logger.info("ğŸ“ åˆ·æ–°å‰©ä½™ %d ä¸ªäº‹ä»¶", len(self._event_buffer))
            self._flush_events()

        logger.info("âœ… ç›‘æ§äº‹ä»¶Workerå·²åœæ­¢")

        # å…³é—­Redisè¿æ¥
        if self._redis_client is not None and self._redis_client is not False:
            try:
                self._redis_client.close()
                if hasattr(self, "_redis_pool") and self._redis_pool is not None:
                    self._redis_pool.disconnect()
                logger.info("âœ… Worker Redisè¿æ¥å·²å…³é—­")
            except Exception as e:
                logger.warning("âš ï¸ å…³é—­Worker Redisè¿æ¥å¤±è´¥: %s", e)

    def _fetch_events(self):
        """ä»Redisè·å–äº‹ä»¶"""
        events = []
        try:
            # ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥ä»Redisé˜Ÿåˆ—è·å–
            pass
        except Exception as e:
            logger.error("âŒ è·å–äº‹ä»¶å¤±è´¥: %(e)s")
        return events

    def _worker_loop(self):
        """Workerä¸»å¾ªç¯"""
        logger.info("ğŸ”„ Workerå¾ªç¯å·²å¯åŠ¨")

        # åœ¨Workerçº¿ç¨‹ä¸­åˆ›å»ºä¸€ä¸ªæ–°çš„äº‹ä»¶å¾ªç¯
        import asyncio

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        # åˆå§‹åŒ–å¼‚æ­¥DBè¿æ¥
        try:
            from src.monitoring.infrastructure.postgresql_async import postgres_async

            loop.run_until_complete(postgres_async.initialize())
        except Exception as e:
            logger.error("âŒ åˆå§‹åŒ–å¼‚æ­¥DBå¤±è´¥: %(e)s")

        while self._running:
            try:
                # ä»Redisè·å–äº‹ä»¶
                events = self._fetch_events()

                if events:
                    # æ·»åŠ åˆ°ç¼“å†²åŒº
                    self._event_buffer.extend(events)
                    logger.debug("ğŸ“¦ è·å– %d ä¸ªäº‹ä»¶ (ç¼“å†²åŒºå¤§å°: %d)", len(events), len(self._event_buffer))

                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°
                if len(self._event_buffer) >= self.batch_size:
                    # ä½¿ç”¨loopè¿è¡Œå¼‚æ­¥åˆ·æ–°
                    loop.run_until_complete(self._flush_events_async())

                # çŸ­æš‚ä¼‘çœ 
                time.sleep(self.poll_interval)

            except Exception as e:
                logger.error("âŒ Workerå¾ªç¯é”™è¯¯: %s", e)
                time.sleep(1)  # å‡ºé”™åç­‰å¾…1ç§’

        # æ¸…ç†èµ„æº
        try:
            from src.monitoring.infrastructure.postgresql_async import postgres_async

            loop.run_until_complete(postgres_async.close())
            loop.close()
        except Exception as e:
            logger.error("âŒ å…³é—­å¾ªç¯å¤±è´¥: %(e)s")

    async def _flush_events_async(self):
        """å¼‚æ­¥æ‰¹é‡åˆ·æ–°äº‹ä»¶"""
        if not self._event_buffer:
            return

        try:
            # å¯¼å…¥ç›‘æ§æ•°æ®åº“ï¼ˆå»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–ï¼‰
            from src.monitoring.infrastructure.postgresql_async import postgres_async
            from src.monitoring.monitoring_database import get_monitoring_database

            monitoring_db = get_monitoring_database()

            # æŒ‰äº‹ä»¶ç±»å‹åˆ†ç»„
            grouped_events = self._group_events_by_type()

            # æ‰¹é‡å†™å…¥
            success_count = 0
            failed_count = 0

            for event_type, events in grouped_events.items():
                # ç‰¹æ®Šå¤„ç† metric_update äº‹ä»¶ (v3.0 æ–°å¢)
                if event_type == "metric_update":
                    try:
                        scores_data = [e.data for e in events]
                        # å¯¼å…¥ç›‘æ§æ•°æ®åº“è®¿é—®å±‚ï¼ˆv3.0ï¼‰
                        from src.monitoring.infrastructure.postgresql_async_v3 import get_postgres_async

                        postgres_async = get_postgres_async()
                        await postgres_async.batch_save_health_scores(scores_data)
                        success_count += len(events)
                        logger.info("âœ… æ‰¹é‡å†™å…¥å¥åº·åº¦è¯„åˆ†: {len(events)} æ¡ (å«é«˜çº§é£é™©æŒ‡æ ‡)")
                    except ImportError:
                        logger.warning("âš ï¸ postgres_async_v3 ä¸å¯ç”¨ï¼Œè·³è¿‡ metric_update å¤„ç†")
                        failed_count += len(events)
                    except Exception as e:
                        logger.warning("âš ï¸ æ‰¹é‡å†™å…¥å¥åº·åº¦è¯„åˆ†å¤±è´¥: %(e)s")
                        failed_count += len(events)
                else:
                    # å¤„ç†ä¼ ç»ŸåŒæ­¥äº‹ä»¶
                    for event in events:
                        try:
                            if event_type == "operation":
                                monitoring_db.log_operation(**event.data)
                            elif event_type == "performance":
                                monitoring_db.record_performance_metric(**event.data)
                            elif event_type == "quality_check":
                                monitoring_db.log_quality_check(**event.data)
                            elif event_type == "alert":
                                monitoring_db.create_alert(**event.data)
                            else:
                                logger.warning("âš ï¸ æœªçŸ¥äº‹ä»¶ç±»å‹: %s", event_type)
                                continue

                            success_count += 1
                        except Exception as e:
                            logger.warning("âš ï¸ å†™å…¥äº‹ä»¶å¤±è´¥: %s", e)
                            failed_count += 1

            # æ¸…ç©ºç¼“å†²åŒº
            self._event_buffer.clear()

            if success_count > 0 or failed_count > 0:
                logger.info(
                    "ğŸ“Š æ‰¹é‡å†™å…¥å®Œæˆ: æˆåŠŸ %d, å¤±è´¥ %d",
                    success_count,
                    failed_count,
                )

        except Exception as e:
            logger.error("âŒ åˆ·æ–°äº‹ä»¶å¤±è´¥: %s", e)

    def _flush_events(self):
        """ä¿ç•™åŒæ­¥æ¥å£ä»¥å…¼å®¹ï¼ˆå®é™…é€»è¾‘å·²ç§»è‡³ _flush_events_asyncï¼‰"""

    def _group_events_by_type(self) -> Dict[str, List[MonitoringEvent]]:
        """æŒ‰äº‹ä»¶ç±»å‹åˆ†ç»„"""
        grouped = defaultdict(list)
        for event in self._event_buffer:
            grouped[event.event_type].append(event)
        return dict(grouped)


# å…¨å±€å®ä¾‹
_event_publisher: Optional[MonitoringEventPublisher] = None
_event_worker: Optional[MonitoringEventWorker] = None


def get_event_publisher() -> MonitoringEventPublisher:
    """è·å–å…¨å±€äº‹ä»¶å‘å¸ƒå™¨"""
    global _event_publisher
    if _event_publisher is None:
        _event_publisher = MonitoringEventPublisher()
    return _event_publisher


def get_event_worker() -> MonitoringEventWorker:
    """è·å–å…¨å±€äº‹ä»¶Worker"""
    global _event_worker
    if _event_worker is None:
        _event_worker = MonitoringEventWorker()
    return _event_worker


def start_async_monitoring():
    """å¯åŠ¨å¼‚æ­¥ç›‘æ§ç³»ç»Ÿ"""
    logger.info("ğŸš€ å¯åŠ¨å¼‚æ­¥ç›‘æ§ç³»ç»Ÿ...")
    worker = get_event_worker()
    worker.start()
    logger.info("âœ… å¼‚æ­¥ç›‘æ§ç³»ç»Ÿå·²å¯åŠ¨")


def stop_async_monitoring():
    """åœæ­¢å¼‚æ­¥ç›‘æ§ç³»ç»Ÿ"""
    logger.info("â¹ï¸ åœæ­¢å¼‚æ­¥ç›‘æ§ç³»ç»Ÿ...")
    worker = get_event_worker()
    worker.stop()
    logger.info("âœ… å¼‚æ­¥ç›‘æ§ç³»ç»Ÿå·²åœæ­¢")


if __name__ == "__main__":
    """æµ‹è¯•å¼‚æ­¥ç›‘æ§æ¨¡å—"""
    import os
    import sys

    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")

    print("\næµ‹è¯•å¼‚æ­¥ç›‘æ§æ¨¡å—...\n")

    # æµ‹è¯•1: å‘å¸ƒäº‹ä»¶
    print("1. æµ‹è¯•å‘å¸ƒäº‹ä»¶...")
    publisher = get_event_publisher()

    event1 = MonitoringEvent(
        event_type="operation",
        data={
            "operation_type": "SAVE",
            "classification": "DAILY_KLINE",
            "target_database": "PostgreSQL",
            "table_name": "daily_kline",
            "record_count": 100,
            "operation_status": "SUCCESS",
        },
        timestamp=datetime.now(),
    )

    publisher.publish_event(event1)
    print("   âœ… äº‹ä»¶å·²å‘å¸ƒ\n")

    # æµ‹è¯•2: å¯åŠ¨Workerï¼ˆéœ€è¦Redisï¼‰
    print("2. æµ‹è¯•å¯åŠ¨Workerï¼ˆéœ€è¦Redisï¼‰...")
    try:
        start_async_monitoring()
        print("   âœ… Workerå·²å¯åŠ¨\n")

        # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©Workerå¤„ç†
        print("3. ç­‰å¾…Workerå¤„ç†äº‹ä»¶...")
        time.sleep(2)

        # åœæ­¢Worker
        print("4. åœæ­¢Worker...")
        stop_async_monitoring()
        print("   âœ… Workerå·²åœæ­¢\n")

    except Exception as e:
        print(f"   âš ï¸ Workeræµ‹è¯•éœ€è¦Redis: {e}\n")

    print("âœ… å¼‚æ­¥ç›‘æ§æ¨¡å—æµ‹è¯•å®Œæˆ!")
