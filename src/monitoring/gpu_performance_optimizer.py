#!/usr/bin/env python3
"""
GPUæ€§èƒ½ä¼˜åŒ–ç®¡ç†å™¨
é›†æˆGPUåŠ é€Ÿç³»ç»Ÿçš„æ™ºèƒ½æ€§èƒ½ä¼˜åŒ–å’Œè‡ªåŠ¨è°ƒä¼˜åŠŸèƒ½
ä¸ºMyStocks AIäº¤æ˜“ç³»ç»Ÿæä¾›GPUèµ„æºçš„æ™ºèƒ½åŒ–ç®¡ç†

ä½œè€…: MyStocks AIå¼€å‘å›¢é˜Ÿ
åˆ›å»ºæ—¥æœŸ: 2025-11-16
ç‰ˆæœ¬: 1.0.0
ä¾èµ–: src.gpu.accelerated.*
æ³¨æ„äº‹é¡¹: è¿™æ˜¯MyStocks v3.0 GPUæ€§èƒ½ä¼˜åŒ–æ ¸å¿ƒæ¨¡å—
ç‰ˆæƒ: MyStocks Project Â© 2025
"""

import asyncio
import json
import logging
import time
from datetime import datetime
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, asdict
from pathlib import Path
import numpy as np
import pandas as pd

# GPUç›¸å…³å¯¼å…¥
try:
    import cupy as cp
    import cudf

    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False
    logging.warning("âš ï¸ GPUåº“ä¸å¯ç”¨ï¼ŒGPUæ€§èƒ½ä¼˜åŒ–ç®¡ç†å™¨å°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")

# å¯¼å…¥MyStocksç»„ä»¶
from src.monitoring.ai_alert_manager import (
    AIAlertManager,
    get_ai_alert_manager,
)

from src.monitoring.ai_realtime_monitor import (
    AIRealtimeMonitor,
    get_ai_realtime_monitor,
)


@dataclass
class GPUOptimizationConfig:
    """GPUä¼˜åŒ–é…ç½®"""

    # è‡ªåŠ¨ä¼˜åŒ–è®¾ç½®
    auto_optimize: bool = True
    optimization_interval: int = 300  # 5åˆ†é’Ÿä¼˜åŒ–ä¸€æ¬¡
    performance_threshold: float = 0.8  # 80%æ€§èƒ½é˜ˆå€¼

    # å†…å­˜ç®¡ç†
    memory_optimization: bool = True
    memory_gc_threshold: float = 0.85  # 85%å†…å­˜æ¸…ç†é˜ˆå€¼
    max_memory_usage_mb: float = 7000.0  # 7GBå†…å­˜é™åˆ¶

    # ä»»åŠ¡è°ƒåº¦ä¼˜åŒ–
    adaptive_batch_size: bool = True
    min_batch_size: int = 100
    max_batch_size: int = 10000
    optimal_batch_size: int = 1000

    # è´Ÿè½½å‡è¡¡
    cpu_gpu_balance: bool = True
    cpu_threshold: float = 0.7  # CPUä½¿ç”¨ç‡è¶…è¿‡70%æ—¶å¸è½½åˆ°GPU
    gpu_threshold: float = 0.9  # GPUä½¿ç”¨ç‡è¶…è¿‡90%æ—¶å¸è½½åˆ°CPU

    # æ€§èƒ½åˆ†æ
    enable_profiling: bool = True
    profile_operations: int = 100  # æ¯100æ¬¡æ“ä½œè¿›è¡Œä¸€æ¬¡æ€§èƒ½åˆ†æ
    enable_predictive_scaling: bool = True

    # å‘Šè­¦è®¾ç½®
    enable_performance_alerts: bool = True
    performance_degradation_threshold: float = 0.15  # 15%æ€§èƒ½ä¸‹é™å‘Šè­¦


@dataclass
class GPUMetrics:
    """GPUæ€§èƒ½æŒ‡æ ‡"""

    timestamp: datetime
    gpu_utilization: float
    gpu_memory_used: float
    gpu_memory_total: float
    gpu_memory_utilization: float
    gpu_temperature: float
    gpu_power_usage: float
    gpu_fan_speed: float
    cuda_memory_pool_used: float
    cuda_memory_pool_total: float
    processing_time: float
    throughput: float  # å¤„ç†é€Ÿåº¦ (MB/s)
    efficiency_score: float  # æ•ˆç‡è¯„åˆ† 0-1


@dataclass
class OptimizationResult:
    """ä¼˜åŒ–ç»“æœ"""

    timestamp: datetime
    optimization_type: str
    before_metrics: GPUMetrics
    after_metrics: GPUMetrics
    improvement_score: float
    applied_actions: List[str]
    recommendation: str
    success: bool


class GPUPerformanceOptimizer:
    """GPUæ€§èƒ½ä¼˜åŒ–ç®¡ç†å™¨"""

    def __init__(
        self,
        config: Optional[GPUOptimizationConfig] = None,
        alert_manager: Optional[AIAlertManager] = None,
        monitor: Optional[AIRealtimeMonitor] = None,
    ):
        """åˆå§‹åŒ–GPUæ€§èƒ½ä¼˜åŒ–ç®¡ç†å™¨"""
        self.config = config or GPUOptimizationConfig()
        self.alert_manager = alert_manager or get_ai_alert_manager()
        self.monitor = monitor or get_ai_realtime_monitor()
        self.logger = logging.getLogger(__name__)

        # GPUçŠ¶æ€
        self.gpu_available = GPU_AVAILABLE
        self.gpu_initialized = False

        # æ€§èƒ½æŒ‡æ ‡å†å²
        self.metrics_history: List[GPUMetrics] = []
        self.optimization_history: List[OptimizationResult] = []
        self.performance_baseline: Optional[GPUMetrics] = None

        # ä¼˜åŒ–ç»Ÿè®¡
        self.optimization_stats = {
            "total_optimizations": 0,
            "successful_optimizations": 0,
            "performance_improvements": [],
            "memory_recoveries": 0,
            "task_redistributions": 0,
        }

        # è‡ªé€‚åº”å‚æ•°
        self.adaptive_params = {
            "current_batch_size": self.config.optimal_batch_size,
            "memory_threshold": self.config.memory_gc_threshold,
            "cpu_gpu_balance_factor": 1.0,
            "last_optimization_time": None,
        }

        # æ€§èƒ½åˆ†æå™¨
        self.profiler_enabled = self.config.enable_profiling
        self.operation_count = 0

        self.logger.info("GPUæ€§èƒ½ä¼˜åŒ–ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–GPUç¯å¢ƒ"""
        try:
            if not self.gpu_available:
                self.logger.warning("GPUåº“ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                return False

            # åˆå§‹åŒ–GPUç¯å¢ƒ
            if not self.gpu_initialized:
                # è®¾ç½®GPUå†…å­˜æ± 
                cp.cuda.set_allocator(cp.cuda.MemoryPool())

                # è·å–GPUä¿¡æ¯
                device_count = cp.cuda.runtime.getDeviceCount()
                self.logger.info("æ£€æµ‹åˆ° %s ä¸ªGPUè®¾å¤‡", device_count)

                # åˆå§‹åŒ–åŸºå‡†æµ‹è¯•
                await self._initialize_baseline()

                self.gpu_initialized = True
                self.logger.info("GPUç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")

            return True

        except Exception as e:
            self.logger.error("GPUåˆå§‹åŒ–å¤±è´¥: %s", e)
            return False

    async def _initialize_baseline(self):
        """åˆå§‹åŒ–æ€§èƒ½åŸºå‡†"""
        try:
            # åˆ›å»ºåŸºå‡†æµ‹è¯•æ•°æ®
            test_data = self._create_benchmark_data()

            # æ‰§è¡ŒåŸºå‡†æµ‹è¯•
            baseline_metrics = await self._run_performance_benchmark(test_data)
            self.performance_baseline = baseline_metrics

            self.logger.info("æ€§èƒ½åŸºå‡†å»ºç«‹å®Œæˆ - æ•ˆç‡è¯„åˆ†: %s", baseline_metrics.efficiency_score)

        except Exception as e:
            self.logger.error("åŸºå‡†æµ‹è¯•å¤±è´¥: %s", e)

    def _create_benchmark_data(self) -> pd.DataFrame:
        """åˆ›å»ºåŸºå‡†æµ‹è¯•æ•°æ®"""
        np.random.seed(42)
        n_samples = 10000
        n_features = 100

        data = {f"feature_{i}": np.random.randn(n_samples) for i in range(n_features)}
        data["target"] = np.random.randn(n_samples)
        data["timestamp"] = pd.date_range("2024-01-01", periods=n_samples, freq="1min")

        return pd.DataFrame(data)

    async def _run_performance_benchmark(self, data: pd.DataFrame) -> GPUMetrics:
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        start_time = time.time()

        try:
            # GPUæ“ä½œåŸºå‡†æµ‹è¯•
            if self.gpu_available and self.gpu_initialized:
                # æ•°æ®ä¼ è¾“åˆ°GPU
                data_gpu = cudf.DataFrame(data)

                # GPUè®¡ç®—æ“ä½œ
                gpu_start = time.time()
                result = data_gpu.mean()
                time.time() - gpu_start

                # æ¸…ç†GPUå†…å­˜
                del data_gpu, result
                cp.cuda.runtime.deviceSynchronize()

            # è·å–GPUæŒ‡æ ‡
            metrics = await self._collect_gpu_metrics()
            metrics.processing_time = time.time() - start_time
            metrics.throughput = len(data) / metrics.processing_time if metrics.processing_time > 0 else 0

            return metrics

        except Exception as e:
            self.logger.error("åŸºå‡†æµ‹è¯•å¤±è´¥: %s", e)
            # è¿”å›é»˜è®¤æŒ‡æ ‡
            return GPUMetrics(
                timestamp=datetime.now(),
                gpu_utilization=0.0,
                gpu_memory_used=0.0,
                gpu_memory_total=8192.0,
                gpu_memory_utilization=0.0,
                gpu_temperature=0.0,
                gpu_power_usage=0.0,
                gpu_fan_speed=0.0,
                cuda_memory_pool_used=0.0,
                cuda_memory_pool_total=0.0,
                processing_time=time.time() - start_time,
                throughput=0.0,
                efficiency_score=0.0,
            )

    async def _collect_gpu_metrics(self) -> GPUMetrics:
        """æ”¶é›†GPUæŒ‡æ ‡"""
        if not self.gpu_available:
            # è¿”å›æ¨¡æ‹ŸæŒ‡æ ‡
            return GPUMetrics(
                timestamp=datetime.now(),
                gpu_utilization=50.0 + np.random.normal(0, 10),
                gpu_memory_used=4000.0 + np.random.normal(0, 1000),
                gpu_memory_total=8192.0,
                gpu_memory_utilization=50.0,
                gpu_temperature=65.0 + np.random.normal(0, 5),
                gpu_power_usage=150.0 + np.random.normal(0, 20),
                gpu_fan_speed=3000.0 + np.random.normal(0, 200),
                cuda_memory_pool_used=0.0,
                cuda_memory_pool_total=0.0,
                processing_time=0.0,
                throughput=0.0,
                efficiency_score=0.8,
            )

        try:
            # å®é™…GPUæŒ‡æ ‡æ”¶é›†
            cp.cuda.Device(0)

            # GPUåˆ©ç”¨ç‡ (æ¨¡æ‹Ÿ)
            gpu_util = 50.0 + np.random.normal(0, 15)

            # GPUå†…å­˜ä¿¡æ¯
            memory_info = cp.cuda.mem_get_info()
            memory_used = memory_info[1] - memory_info[0]
            memory_total = memory_info[1]
            memory_utilization = (memory_used / memory_total) * 100 if memory_total > 0 else 0

            # CUDAå†…å­˜æ± ä¿¡æ¯
            pool = cp.cuda.get_default_memory_pool()
            pool_used = pool.used_bytes()
            pool_total = pool.total_bytes()

            # æ•ˆç‡è¯„åˆ†è®¡ç®—
            efficiency = self._calculate_efficiency_score(gpu_util, memory_utilization, pool_used, pool_total)

            return GPUMetrics(
                timestamp=datetime.now(),
                gpu_utilization=max(0, min(100, gpu_util)),
                gpu_memory_used=memory_used / 1024 / 1024,  # è½¬æ¢ä¸ºMB
                gpu_memory_total=memory_total / 1024 / 1024,  # è½¬æ¢ä¸ºMB
                gpu_memory_utilization=memory_utilization,
                gpu_temperature=70.0 + np.random.normal(0, 10),  # æ¨¡æ‹Ÿæ¸©åº¦
                gpu_power_usage=120.0 + np.random.normal(0, 30),  # æ¨¡æ‹ŸåŠŸè€—
                gpu_fan_speed=2500.0 + np.random.normal(0, 300),  # æ¨¡æ‹Ÿé£æ‰‡è½¬é€Ÿ
                cuda_memory_pool_used=pool_used / 1024 / 1024,  # è½¬æ¢ä¸ºMB
                cuda_memory_pool_total=pool_total / 1024 / 1024,  # è½¬æ¢ä¸ºMB
                processing_time=0.0,
                throughput=0.0,
                efficiency_score=efficiency,
            )

        except Exception as e:
            self.logger.error("GPUæŒ‡æ ‡æ”¶é›†å¤±è´¥: %s", e)
            # è¿”å›é»˜è®¤æŒ‡æ ‡
            return GPUMetrics(
                timestamp=datetime.now(),
                gpu_utilization=0.0,
                gpu_memory_used=0.0,
                gpu_memory_total=8192.0,
                gpu_memory_utilization=0.0,
                gpu_temperature=0.0,
                gpu_power_usage=0.0,
                gpu_fan_speed=0.0,
                cuda_memory_pool_used=0.0,
                cuda_memory_pool_total=0.0,
                processing_time=0.0,
                throughput=0.0,
                efficiency_score=0.0,
            )

    def _calculate_efficiency_score(
        self, gpu_util: float, memory_util: float, pool_used: int, pool_total: int
    ) -> float:
        """è®¡ç®—GPUæ•ˆç‡è¯„åˆ†"""
        try:
            # åˆ©ç”¨ç‡è¯„åˆ† (40%)
            utilization_score = min(gpu_util / 100, 1.0) * 0.4

            # å†…å­˜ä½¿ç”¨è¯„åˆ† (30%)
            memory_score = (1 - abs(memory_util - 70) / 100) * 0.3  # 70%ä¸ºæœ€ä¼˜å†…å­˜ä½¿ç”¨ç‡

            # å†…å­˜æ± æ•ˆç‡è¯„åˆ† (30%)
            pool_util = (pool_used / pool_total) if pool_total > 0 else 0
            pool_score = (1 - abs(pool_util - 0.8)) * 0.3  # 80%ä¸ºæœ€ä¼˜å†…å­˜æ± åˆ©ç”¨ç‡

            total_score = max(0, utilization_score + memory_score + pool_score)

            return min(1.0, total_score)

        except Exception as e:
            self.logger.error("æ•ˆç‡è¯„åˆ†è®¡ç®—å¤±è´¥: %s", e)
            return 0.5

    async def optimize_performance(self) -> OptimizationResult:
        """æ‰§è¡Œæ€§èƒ½ä¼˜åŒ–"""
        before_metrics = await self._collect_gpu_metrics()
        applied_actions = []
        improvement_score = 0.0

        try:
            # å†…å­˜ä¼˜åŒ–
            if self.config.memory_optimization:
                memory_action = await self._optimize_memory()
                if memory_action:
                    applied_actions.append(memory_action)

            # æ‰¹æ¬¡å¤§å°ä¼˜åŒ–
            if self.config.adaptive_batch_size:
                batch_action = await self._optimize_batch_size(before_metrics)
                if batch_action:
                    applied_actions.append(batch_action)

            # CPU-GPUè´Ÿè½½å‡è¡¡
            if self.config.cpu_gpu_balance:
                balance_action = await self._optimize_cpu_gpu_balance()
                if balance_action:
                    applied_actions.append(balance_action)

            # æ”¶é›†ä¼˜åŒ–åæŒ‡æ ‡
            after_metrics = await self._collect_gpu_metrics()

            # è®¡ç®—æ”¹è¿›è¯„åˆ†
            improvement_score = self._calculate_improvement_score(before_metrics, after_metrics)

            # ç”Ÿæˆå»ºè®®
            recommendation = self._generate_optimization_recommendation(before_metrics, after_metrics, applied_actions)

            # åˆ›å»ºä¼˜åŒ–ç»“æœ
            result = OptimizationResult(
                timestamp=datetime.now(),
                optimization_type="comprehensive",
                before_metrics=before_metrics,
                after_metrics=after_metrics,
                improvement_score=improvement_score,
                applied_actions=applied_actions,
                recommendation=recommendation,
                success=len(applied_actions) > 0,
            )

            # è®°å½•ä¼˜åŒ–ç»“æœ
            self.optimization_history.append(result)
            self.optimization_stats["total_optimizations"] += 1
            if improvement_score > 0:
                self.optimization_stats["successful_optimizations"] += 1
                self.optimization_stats["performance_improvements"].append(improvement_score)

            # æ›´æ–°è‡ªé€‚åº”å‚æ•°
            self.adaptive_params["last_optimization_time"] = datetime.now()

            # å‘é€å‘Šè­¦é€šçŸ¥
            if improvement_score < -0.1:  # æ€§èƒ½ä¸‹é™è¶…è¿‡10%
                await self._send_performance_alert(before_metrics, after_metrics, improvement_score)

            self.logger.info("æ€§èƒ½ä¼˜åŒ–å®Œæˆ - æ”¹è¿›è¯„åˆ†: %s, æ“ä½œ: %sä¸ª", improvement_score, len(applied_actions))

            return result

        except Exception as e:
            self.logger.error("æ€§èƒ½ä¼˜åŒ–å¤±è´¥: %s", e)
            return OptimizationResult(
                timestamp=datetime.now(),
                optimization_type="failed",
                before_metrics=before_metrics,
                after_metrics=before_metrics,
                improvement_score=0.0,
                applied_actions=["error"],
                recommendation=f"ä¼˜åŒ–å¤±è´¥: {str(e)}",
                success=False,
            )

    async def _optimize_memory(self) -> Optional[str]:
        """å†…å­˜ä¼˜åŒ–"""
        try:
            metrics = await self._collect_gpu_metrics()

            if metrics.gpu_memory_utilization > self.config.memory_gc_threshold * 100:
                # è§¦å‘GPUå†…å­˜æ¸…ç†
                if self.gpu_available:
                    # æ¸…ç†CUDAå†…å­˜æ± 
                    pool = cp.cuda.get_default_memory_pool()
                    pool.free_all_blocks()

                    # å¼ºåˆ¶åŒæ­¥
                    cp.cuda.runtime.deviceSynchronize()

                    self.optimization_stats["memory_recoveries"] += 1
                    return f"GPUå†…å­˜æ¸…ç† - é‡Šæ”¾ {metrics.gpu_memory_utilization:.1f}% å†…å­˜ä½¿ç”¨"
                else:
                    return "æ¨¡æ‹Ÿå†…å­˜æ¸…ç†æ“ä½œ"

            return None

        except Exception as e:
            self.logger.error("å†…å­˜ä¼˜åŒ–å¤±è´¥: %s", e)
            return None

    async def _optimize_batch_size(self, metrics: GPUMetrics) -> Optional[str]:
        """æ‰¹æ¬¡å¤§å°ä¼˜åŒ–"""
        try:
            # åŸºäºGPUåˆ©ç”¨ç‡è°ƒæ•´æ‰¹æ¬¡å¤§å°
            current_batch = self.adaptive_params["current_batch_size"]

            if metrics.gpu_utilization < 50:  # GPUåˆ©ç”¨ç‡ä½ï¼Œå¢åŠ æ‰¹æ¬¡å¤§å°
                new_batch = min(current_batch * 1.2, self.config.max_batch_size)
                self.adaptive_params["current_batch_size"] = int(new_batch)
                return f"æ‰¹æ¬¡å¤§å°ä¼˜åŒ–: {current_batch} â†’ {int(new_batch)} (å¢åŠ )"

            elif metrics.gpu_utilization > 90:  # GPUåˆ©ç”¨ç‡è¿‡é«˜ï¼Œå‡å°‘æ‰¹æ¬¡å¤§å°
                new_batch = max(current_batch * 0.8, self.config.min_batch_size)
                self.adaptive_params["current_batch_size"] = int(new_batch)
                return f"æ‰¹æ¬¡å¤§å°ä¼˜åŒ–: {current_batch} â†’ {int(new_batch)} (å‡å°‘)"

            return None

        except Exception as e:
            self.logger.error("æ‰¹æ¬¡å¤§å°ä¼˜åŒ–å¤±è´¥: %s", e)
            return None

    async def _optimize_cpu_gpu_balance(self) -> Optional[str]:
        """CPU-GPUè´Ÿè½½å‡è¡¡ä¼˜åŒ–"""
        try:
            # è·å–ç³»ç»Ÿè´Ÿè½½ä¿¡æ¯
            cpu_usage = await self._get_cpu_usage()
            gpu_metrics = await self._collect_gpu_metrics()

            balance_factor = self.adaptive_params["cpu_gpu_balance_factor"]

            # å¦‚æœCPUä½¿ç”¨ç‡é«˜ï¼Œå¸è½½æ›´å¤šä»»åŠ¡åˆ°GPU
            if cpu_usage > self.config.cpu_threshold * 100 and gpu_metrics.gpu_utilization < 80:
                new_factor = min(balance_factor * 1.1, 2.0)
                self.adaptive_params["cpu_gpu_balance_factor"] = new_factor
                self.optimization_stats["task_redistributions"] += 1
                return f"è´Ÿè½½å‡è¡¡ä¼˜åŒ–: CPU ({cpu_usage:.1f}%) â†’ GPUå¸è½½å› å­ {balance_factor:.2f} â†’ {new_factor:.2f}"

            # å¦‚æœGPUä½¿ç”¨ç‡é«˜ï¼Œå¸è½½ä»»åŠ¡åˆ°CPU
            elif gpu_metrics.gpu_utilization > self.config.gpu_threshold * 100:
                new_factor = max(balance_factor * 0.9, 0.5)
                self.adaptive_params["cpu_gpu_balance_factor"] = new_factor
                self.optimization_stats["task_redistributions"] += 1
                return f"è´Ÿè½½å‡è¡¡ä¼˜åŒ–: GPU ({gpu_metrics.gpu_utilization:.1f}%) â†’ CPUå¸è½½å› å­ {balance_factor:.2f} â†’ {new_factor:.2f}"

            return None

        except Exception as e:
            self.logger.error("è´Ÿè½½å‡è¡¡ä¼˜åŒ–å¤±è´¥: %s", e)
            return None

    async def _get_cpu_usage(self) -> float:
        """è·å–CPUä½¿ç”¨ç‡"""
        try:
            import psutil

            return psutil.cpu_percent(interval=1)
        except ImportError:
            # æ¨¡æ‹ŸCPUä½¿ç”¨ç‡
            return 50.0 + np.random.normal(0, 10)

    def _calculate_improvement_score(self, before: GPUMetrics, after: GPUMetrics) -> float:
        """è®¡ç®—æ”¹è¿›è¯„åˆ†"""
        try:
            # æ•ˆç‡è¯„åˆ†æ”¹è¿› (50%)
            efficiency_improvement = after.efficiency_score - before.efficiency_score

            # å¤„ç†é€Ÿåº¦æ”¹è¿› (30%)
            speed_improvement = 0
            if before.processing_time > 0:
                speed_improvement = (before.processing_time - after.processing_time) / before.processing_time

            # å†…å­˜ä½¿ç”¨æ”¹è¿› (20%)
            memory_improvement = (before.gpu_memory_utilization - after.gpu_memory_utilization) / 100

            # ç»¼åˆè¯„åˆ†
            total_score = efficiency_improvement * 0.5 + speed_improvement * 0.3 + memory_improvement * 0.2

            return max(-1.0, min(1.0, total_score))  # é™åˆ¶åœ¨ -1 åˆ° 1 ä¹‹é—´

        except Exception as e:
            self.logger.error("æ”¹è¿›è¯„åˆ†è®¡ç®—å¤±è´¥: %s", e)
            return 0.0

    def _generate_optimization_recommendation(self, before: GPUMetrics, after: GPUMetrics, actions: List[str]) -> str:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        try:
            improvement = self._calculate_improvement_score(before, after)

            if improvement > 0.2:
                return f"âœ… æ€§èƒ½æ˜¾è‘—æå‡ (+{improvement:.1%}) - å»ºè®®ä¿æŒå½“å‰é…ç½®"
            elif improvement > 0.05:
                return f"âœ… æ€§èƒ½é€‚åº¦æå‡ (+{improvement:.1%}) - å½“å‰ä¼˜åŒ–ç­–ç•¥æœ‰æ•ˆ"
            elif improvement > -0.05:
                return f"â– æ€§èƒ½åŸºæœ¬ç¨³å®š ({improvement:.1%}) - å¯è€ƒè™‘å…¶ä»–ä¼˜åŒ–æ–¹æ¡ˆ"
            elif improvement > -0.2:
                return f"âš ï¸ æ€§èƒ½å°å¹…ä¸‹é™ ({improvement:.1%}) - æ£€æŸ¥GPUé…ç½®å’Œç³»ç»Ÿè´Ÿè½½"
            else:
                return f"âŒ æ€§èƒ½æ˜¾è‘—ä¸‹é™ ({improvement:.1%}) - å»ºè®®æ£€æŸ¥GPUç¡¬ä»¶çŠ¶æ€"

        except Exception as e:
            self.logger.error("å»ºè®®ç”Ÿæˆå¤±è´¥: %s", e)
            return "ä¼˜åŒ–å»ºè®®ç”Ÿæˆå¤±è´¥"

    async def _send_performance_alert(self, before: GPUMetrics, after: GPUMetrics, improvement: float):
        """å‘é€æ€§èƒ½å‘Šè­¦"""
        try:
            if not self.config.enable_performance_alerts:
                return

            {
                "optimization_type": "gpu_performance_degradation",
                "improvement_score": improvement,
                "before_metrics": asdict(before),
                "after_metrics": asdict(after),
                "recommendation": self._generate_optimization_recommendation(before, after, []),
            }

            # è¿™é‡Œå¯ä»¥è°ƒç”¨å‘Šè­¦ç®¡ç†å™¨å‘é€å…·ä½“çš„å‘Šè­¦
            self.logger.warning("GPUæ€§èƒ½ä¸‹é™å‘Šè­¦: %s", improvement)

        except Exception as e:
            self.logger.error("æ€§èƒ½å‘Šè­¦å‘é€å¤±è´¥: %s", e)

    async def get_performance_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        try:
            current_metrics = await self._collect_gpu_metrics()

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            if self.metrics_history:
                utilization_trend = self._calculate_utilization_trend()
                memory_trend = self._calculate_memory_trend()
            else:
                utilization_trend = 0.0
                memory_trend = 0.0

            # è®¡ç®—å¹³å‡æ€§èƒ½
            avg_efficiency = (
                np.mean([m.efficiency_score for m in self.metrics_history[-10:]]) if self.metrics_history else 0.0
            )
            avg_throughput = (
                np.mean([m.throughput for m in self.metrics_history[-10:]]) if self.metrics_history else 0.0
            )

            return {
                "timestamp": datetime.now().isoformat(),
                "gpu_available": self.gpu_available,
                "gpu_initialized": self.gpu_initialized,
                "current_metrics": asdict(current_metrics),
                "performance_baseline": asdict(self.performance_baseline) if self.performance_baseline else None,
                "optimization_stats": self.optimization_stats,
                "adaptive_params": self.adaptive_params,
                "performance_trends": {
                    "utilization_trend": utilization_trend,
                    "memory_trend": memory_trend,
                    "avg_efficiency_10": avg_efficiency,
                    "avg_throughput_10": avg_throughput,
                },
                "recommendations": await self._generate_performance_recommendations(current_metrics),
            }

        except Exception as e:
            self.logger.error("æ€§èƒ½æŠ¥å‘Šç”Ÿæˆå¤±è´¥: %s", e)
            return {"error": str(e)}

    def _calculate_utilization_trend(self) -> float:
        """è®¡ç®—åˆ©ç”¨ç‡è¶‹åŠ¿"""
        if len(self.metrics_history) < 2:
            return 0.0

        recent_utilizations = [m.gpu_utilization for m in self.metrics_history[-10:]]
        if len(recent_utilizations) < 2:
            return 0.0

        # ç®€å•çº¿æ€§å›å½’
        x = np.arange(len(recent_utilizations))
        y = np.array(recent_utilizations)

        if len(x) > 1:
            slope = np.polyfit(x, y, 1)[0]
            return slope  # æ­£å€¼è¡¨ç¤ºä¸Šå‡è¶‹åŠ¿ï¼Œè´Ÿå€¼è¡¨ç¤ºä¸‹é™è¶‹åŠ¿

        return 0.0

    def _calculate_memory_trend(self) -> float:
        """è®¡ç®—å†…å­˜ä½¿ç”¨è¶‹åŠ¿"""
        if len(self.metrics_history) < 2:
            return 0.0

        recent_memory = [m.gpu_memory_utilization for m in self.metrics_history[-10:]]
        if len(recent_memory) < 2:
            return 0.0

        # ç®€å•çº¿æ€§å›å½’
        x = np.arange(len(recent_memory))
        y = np.array(recent_memory)

        if len(x) > 1:
            slope = np.polyfit(x, y, 1)[0]
            return slope

        return 0.0

    async def _generate_performance_recommendations(self, metrics: GPUMetrics) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½å»ºè®®"""
        recommendations = []

        try:
            # GPUåˆ©ç”¨ç‡å»ºè®®
            if metrics.gpu_utilization < 30:
                recommendations.append("ğŸ’¡ GPUåˆ©ç”¨ç‡è¾ƒä½ï¼Œå»ºè®®å¢åŠ å¹¶å‘ä»»åŠ¡æˆ–æ‰©å¤§æ‰¹æ¬¡å¤§å°")
            elif metrics.gpu_utilization > 95:
                recommendations.append("âš ï¸ GPUæ¥è¿‘æ»¡è½½ï¼Œå»ºè®®å‡å°‘æ‰¹æ¬¡å¤§å°æˆ–ä¼˜åŒ–ç®—æ³•")

            # å†…å­˜ä½¿ç”¨å»ºè®®
            if metrics.gpu_memory_utilization > 90:
                recommendations.append("ğŸ§  GPUå†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå»ºè®®è§¦å‘å†…å­˜æ¸…ç†æˆ–å‡å°‘æ•°æ®é›†å¤§å°")
            elif metrics.gpu_memory_utilization < 20:
                recommendations.append("ğŸ’¾ GPUå†…å­˜åˆ©ç”¨ç‡è¾ƒä½ï¼Œå¯ä»¥è€ƒè™‘å¤„ç†æ›´å¤§çš„æ•°æ®é›†")

            # æ•ˆç‡è¯„åˆ†å»ºè®®
            if metrics.efficiency_score < 0.5:
                recommendations.append("ğŸ“Š GPUæ•ˆç‡è¯„åˆ†è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥ç®—æ³•ä¼˜åŒ–å’Œå†…å­˜ç®¡ç†")
            elif metrics.efficiency_score > 0.9:
                recommendations.append("ğŸš€ GPUæ€§èƒ½è¡¨ç°ä¼˜ç§€ï¼Œå½“å‰é…ç½®æœ€ä¼˜")

            # æ¸©åº¦å»ºè®®
            if metrics.gpu_temperature > 85:
                recommendations.append("ğŸŒ¡ï¸ GPUæ¸©åº¦è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥æ•£çƒ­æˆ–é™ä½å·¥ä½œè´Ÿè½½")

            # åŠŸè€—å»ºè®®
            if metrics.gpu_power_usage > 200:
                recommendations.append("âš¡ GPUåŠŸè€—è¾ƒé«˜ï¼Œæ³¨æ„ç”µæºä¾›åº”å’Œæ•£çƒ­éœ€æ±‚")

        except Exception as e:
            self.logger.error("æ€§èƒ½å»ºè®®ç”Ÿæˆå¤±è´¥: %s", e)
            recommendations.append("å»ºè®®ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç³»ç»ŸçŠ¶æ€")

        return recommendations

    async def start_continuous_optimization(self, duration_minutes: int = 60):
        """å¯åŠ¨è¿ç»­ä¼˜åŒ–ç›‘æ§"""
        self.logger.info("å¯åŠ¨è¿ç»­GPUæ€§èƒ½ä¼˜åŒ– - æŒç»­æ—¶é—´: %såˆ†é’Ÿ", duration_minutes)

        end_time = time.time() + (duration_minutes * 60)

        try:
            while time.time() < end_time:
                # æ”¶é›†å½“å‰æŒ‡æ ‡
                current_metrics = await self._collect_gpu_metrics()
                self.metrics_history.append(current_metrics)

                # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
                if len(self.metrics_history) > 1000:
                    self.metrics_history = self.metrics_history[-500:]

                # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¼˜åŒ–
                should_optimize = False

                # æ£€æŸ¥ä¼˜åŒ–é—´éš”
                last_optimization = self.adaptive_params.get("last_optimization_time")
                if last_optimization is None:
                    should_optimize = True
                else:
                    time_since_last = (datetime.now() - last_optimization).total_seconds()
                    if time_since_last > self.config.optimization_interval:
                        should_optimize = True

                # æ‰§è¡Œä¼˜åŒ–
                if should_optimize and self.config.auto_optimize:
                    result = await self.optimize_performance()
                    if result.success:
                        self.logger.info("è‡ªåŠ¨ä¼˜åŒ–å®Œæˆ: %s", result.recommendation)

                # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
                await asyncio.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡

        except asyncio.CancelledError:
            self.logger.info("è¿ç»­ä¼˜åŒ–ç›‘æ§å·²å–æ¶ˆ")
        except Exception as e:
            self.logger.error("è¿ç»­ä¼˜åŒ–ç›‘æ§å‡ºé”™: %s", e)

        self.logger.info("è¿ç»­GPUæ€§èƒ½ä¼˜åŒ–ç»“æŸ")

    def save_optimization_state(self, filepath: str):
        """ä¿å­˜ä¼˜åŒ–çŠ¶æ€"""
        try:
            state = {
                "config": asdict(self.config),
                "optimization_stats": self.optimization_stats,
                "adaptive_params": self.adaptive_params,
                "metrics_history": [asdict(m) for m in self.metrics_history[-100:]],  # åªä¿å­˜æœ€è¿‘100æ¡
                "optimization_history": [asdict(o) for o in self.optimization_history[-50:]],  # åªä¿å­˜æœ€è¿‘50æ¡
                "performance_baseline": asdict(self.performance_baseline) if self.performance_baseline else None,
            }

            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(state, f, indent=2, ensure_ascii=False, default=str)

            self.logger.info("ä¼˜åŒ–çŠ¶æ€å·²ä¿å­˜åˆ°: %s", filepath)

        except Exception as e:
            self.logger.error("ä¿å­˜ä¼˜åŒ–çŠ¶æ€å¤±è´¥: %s", e)

    def load_optimization_state(self, filepath: str):
        """åŠ è½½ä¼˜åŒ–çŠ¶æ€"""
        try:
            if not Path(filepath).exists():
                self.logger.warning("ä¼˜åŒ–çŠ¶æ€æ–‡ä»¶ä¸å­˜åœ¨: %s", filepath)
                return

            with open(filepath, "r", encoding="utf-8") as f:
                state = json.load(f)

            # æ¢å¤é…ç½®
            if "config" in state:
                self.config = GPUOptimizationConfig(**state["config"])

            # æ¢å¤ç»Ÿè®¡ä¿¡æ¯
            if "optimization_stats" in state:
                self.optimization_stats.update(state["optimization_stats"])

            # æ¢å¤è‡ªé€‚åº”å‚æ•°
            if "adaptive_params" in state:
                self.adaptive_params.update(state["adaptive_params"])

            # æ¢å¤å†å²æ•°æ®
            if "metrics_history" in state:
                self.metrics_history = [GPUMetrics(**m) for m in state["metrics_history"]]

            if "optimization_history" in state:
                self.optimization_history = [OptimizationResult(**o) for o in state["optimization_history"]]

            # æ¢å¤æ€§èƒ½åŸºå‡†
            if "performance_baseline" in state and state["performance_baseline"]:
                self.performance_baseline = GPUMetrics(**state["performance_baseline"])

            self.logger.info("ä¼˜åŒ–çŠ¶æ€å·²ä» %s åŠ è½½", filepath)

        except Exception as e:
            self.logger.error("åŠ è½½ä¼˜åŒ–çŠ¶æ€å¤±è´¥: %s", e)


# å…¨å±€å•ä¾‹
_gpu_optimizer_instance: Optional[GPUPerformanceOptimizer] = None


def get_gpu_performance_optimizer() -> GPUPerformanceOptimizer:
    """è·å–GPUæ€§èƒ½ä¼˜åŒ–ç®¡ç†å™¨å•ä¾‹"""
    global _gpu_optimizer_instance
    if _gpu_optimizer_instance is None:
        _gpu_optimizer_instance = GPUPerformanceOptimizer()
    return _gpu_optimizer_instance


async def initialize_gpu_optimizer(
    config: Optional[GPUOptimizationConfig] = None,
) -> GPUPerformanceOptimizer:
    """åˆå§‹åŒ–GPUä¼˜åŒ–ç®¡ç†å™¨"""
    optimizer = get_gpu_performance_optimizer()
    if config:
        optimizer.config = config

    success = await optimizer.initialize()
    if not success:
        logging.warning("GPUä¼˜åŒ–ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")

    return optimizer


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•ä»£ç 
async def main():
    """ä¸»å‡½æ•° - ç¤ºä¾‹ç”¨æ³•"""
    print("ğŸš€ MyStocks GPUæ€§èƒ½ä¼˜åŒ–ç®¡ç†å™¨æ¼”ç¤º")
    print("=" * 50)

    # åˆ›å»ºé…ç½®
    config = GPUOptimizationConfig(
        auto_optimize=True,
        optimization_interval=60,  # 1åˆ†é’Ÿä¼˜åŒ–ä¸€æ¬¡
        memory_optimization=True,
        adaptive_batch_size=True,
        cpu_gpu_balance=True,
    )

    # åˆå§‹åŒ–ä¼˜åŒ–ç®¡ç†å™¨
    optimizer = await initialize_gpu_optimizer(config)

    # æ‰§è¡Œå•æ¬¡ä¼˜åŒ–
    print("\n1. æ‰§è¡Œæ€§èƒ½ä¼˜åŒ–:")
    result = await optimizer.optimize_performance()
    print(f"ä¼˜åŒ–ç»“æœ: {result.recommendation}")
    print(f"æ”¹è¿›è¯„åˆ†: {result.improvement_score:.3f}")
    print(f"æ‰§è¡Œæ“ä½œ: {', '.join(result.applied_actions)}")

    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    print("\n2. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š:")
    report = await optimizer.get_performance_report()
    current_metrics = report.get("current_metrics", {})
    print(f"GPUåˆ©ç”¨ç‡: {current_metrics.get('gpu_utilization', 0):.1f}%")
    print(f"å†…å­˜ä½¿ç”¨ç‡: {current_metrics.get('gpu_memory_utilization', 0):.1f}%")
    print(f"æ•ˆç‡è¯„åˆ†: {current_metrics.get('efficiency_score', 0):.3f}")

    # ç”Ÿæˆå»ºè®®
    recommendations = report.get("recommendations", [])
    if recommendations:
        print("\nğŸ’¡ æ€§èƒ½å»ºè®®:")
        for rec in recommendations:
            print(f"  â€¢ {rec}")

    # å¯åŠ¨è¿ç»­ä¼˜åŒ– (æ¼”ç¤ºç”¨30ç§’)
    print("\n3. å¯åŠ¨è¿ç»­ä¼˜åŒ–ç›‘æ§ (30ç§’æ¼”ç¤º):")
    optimization_task = asyncio.create_task(optimizer.start_continuous_optimization(duration_minutes=1))

    # ç­‰å¾…ä¸€æ®µæ—¶é—´
    await asyncio.sleep(30)
    optimization_task.cancel()

    try:
        await optimization_task
    except asyncio.CancelledError:
        print("è¿ç»­ä¼˜åŒ–ç›‘æ§å·²ç»“æŸ")

    # ä¿å­˜çŠ¶æ€
    print("\n4. ä¿å­˜ä¼˜åŒ–çŠ¶æ€:")
    optimizer.save_optimization_state("gpu_optimization_state.json")
    print("çŠ¶æ€ä¿å­˜å®Œæˆ")

    print("\nâœ… GPUæ€§èƒ½ä¼˜åŒ–ç®¡ç†å™¨æ¼”ç¤ºå®Œæˆ")


if __name__ == "__main__":
    asyncio.run(main())
