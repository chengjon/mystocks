#!/usr/bin/env python3
"""
æ™ºèƒ½é˜ˆå€¼ç®—æ³•å’Œè¯¯æŠ¥ä¼˜åŒ–æ¨¡å—

åŸºäºæœºå™¨å­¦ä¹ çš„æ™ºèƒ½é˜ˆå€¼ç®—æ³•ï¼Œè‡ªåŠ¨å­¦ä¹ å’Œè°ƒæ•´ç›‘æ§é˜ˆå€¼ï¼Œå‡å°‘è¯¯æŠ¥ç‡ã€‚
é›†æˆå†å²æ•°æ®åˆ†æå’Œè¶‹åŠ¿é¢„æµ‹ï¼Œæä¾›åŠ¨æ€é˜ˆå€¼ä¼˜åŒ–åŠŸèƒ½ã€‚

ä½œè€…: MyStocks AIå¼€å‘å›¢é˜Ÿ
åˆ›å»ºæ—¥æœŸ: 2025-11-16
ç‰ˆæœ¬: 1.0.0
ä¾èµ–: numpy, pandas, scikit-learn
ç‰ˆæƒ: MyStocks Project Â© 2025
"""

import asyncio
import json
import logging
import warnings
from collections import deque
from dataclasses import asdict, dataclass
from datetime import datetime
from typing import Any, Dict, List, Optional

import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.ensemble import IsolationForest

# ç›‘æ§ç»„ä»¶å¯¼å…¥
try:
    from .monitoring_database import get_monitoring_database
    from .performance_monitor import SystemMetrics
except ImportError:
    # å…¼å®¹æ¨¡å¼
    SystemMetrics = Any
    get_monitoring_database = None

warnings.filterwarnings("ignore", category=UserWarning)

logger = logging.getLogger(__name__)


@dataclass
class ThresholdRule:
    """é˜ˆå€¼è§„åˆ™å®šä¹‰"""

    name: str
    metric_name: str
    current_threshold: float
    optimal_threshold: Optional[float] = None
    threshold_type: str = "upper"  # 'upper', 'lower', 'range'
    confidence_score: float = 0.5
    learning_rate: float = 0.1
    adaptation_speed: float = 0.05
    false_positive_rate: float = 0.0
    false_negative_rate: float = 0.0
    adjustment_count: int = 0
    last_adjustment: Optional[datetime] = None
    history: List[Dict[str, Any]] = None

    def __post_init__(self):
        if self.history is None:
            self.history = []


@dataclass
class ThresholdAdjustment:
    """é˜ˆå€¼è°ƒæ•´è®°å½•"""

    timestamp: datetime
    rule_name: str
    old_threshold: float
    new_threshold: float
    reason: str
    confidence: float
    metrics_snapshot: Dict[str, Any]
    predicted_effectiveness: float = 0.0
    actual_effectiveness: Optional[float] = None


@dataclass
class OptimizationResult:
    """ä¼˜åŒ–ç»“æœ"""

    rule_name: str
    optimization_type: str  # 'anomaly_detection', 'trend_analysis', 'clustering', 'statistical'
    recommended_threshold: float
    confidence_score: float
    expected_improvement: float
    reasoning: str
    supporting_evidence: List[str]
    metadata: Dict[str, Any]


class DataAnalyzer:
    """æ•°æ®åˆ†æå™¨ - åŸºç¡€é˜ˆå€¼åˆ†æåŠŸèƒ½"""

    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        self.data_buffer = deque(maxlen=window_size)
        self.statistics_cache = {}

    def add_data_point(self, value: float, timestamp: datetime, rule_name: str):
        """æ·»åŠ æ•°æ®ç‚¹"""
        self.data_buffer.append({"value": value, "timestamp": timestamp, "rule_name": rule_name})

    def calculate_statistics(self) -> Dict[str, float]:
        """è®¡ç®—åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯"""
        if len(self.data_buffer) < 10:
            return {}

        values = [point["value"] for point in self.data_buffer]

        stats = {
            "mean": np.mean(values),
            "std": np.std(values),
            "min": np.min(values),
            "max": np.max(values),
            "median": np.median(values),
            "q25": np.percentile(values, 25),
            "q75": np.percentile(values, 75),
            "iqr": np.percentile(values, 75) - np.percentile(values, 25),
            "skewness": self._calculate_skewness(values),
            "kurtosis": self._calculate_kurtosis(values),
        }

        self.statistics_cache = stats
        return stats

    def _calculate_skewness(self, values: List[float]) -> float:
        """è®¡ç®—ååº¦"""
        if len(values) < 3:
            return 0.0

        mean_val = np.mean(values)
        std_val = np.std(values)
        if std_val == 0:
            return 0.0

        return np.mean([((x - mean_val) / std_val) ** 3 for x in values])

    def _calculate_kurtosis(self, values: List[float]) -> float:
        """è®¡ç®—å³°åº¦"""
        if len(values) < 4:
            return 0.0

        mean_val = np.mean(values)
        std_val = np.std(values)
        if std_val == 0:
            return 0.0

        return np.mean([((x - mean_val) / std_val) ** 4 for x in values]) - 3

    def detect_anomalies(self, contamination: float = 0.1) -> List[int]:
        """æ£€æµ‹å¼‚å¸¸å€¼"""
        if len(self.data_buffer) < 20:
            return []

        values = np.array([point["value"] for point in self.data_buffer]).reshape(-1, 1)

        # ä½¿ç”¨Isolation Forestæ£€æµ‹å¼‚å¸¸
        iso_forest = IsolationForest(contamination=contamination, random_state=42)
        anomaly_labels = iso_forest.fit_predict(values)

        # è¿”å›å¼‚å¸¸ç‚¹çš„ç´¢å¼•
        return [i for i, label in enumerate(anomaly_labels) if label == -1]

    def analyze_trend(self) -> Dict[str, Any]:
        """åˆ†æè¶‹åŠ¿"""
        if len(self.data_buffer) < 5:
            return {"trend": "insufficient_data"}

        values = [point["value"] for point in self.data_buffer]
        [point["timestamp"] for point in self.data_buffer]

        # ç®€å•çº¿æ€§è¶‹åŠ¿
        x = np.arange(len(values))
        coeffs = np.polyfit(x, values, 1)
        slope = coeffs[0]

        # è®¡ç®—è¶‹åŠ¿å¼ºåº¦
        correlation = np.corrcoef(x, values)[0, 1] if len(values) > 2 else 0

        # ç¡®å®šè¶‹åŠ¿æ–¹å‘
        if abs(slope) < 0.01:
            trend_direction = "stable"
        elif slope > 0:
            trend_direction = "increasing"
        else:
            trend_direction = "decreasing"

        return {
            "trend": trend_direction,
            "slope": slope,
            "strength": abs(correlation),
            "correlation": correlation,
            "current_value": values[-1],
            "previous_value": values[-2] if len(values) > 1 else values[-1],
        }


class StatisticalOptimizer:
    """ç»Ÿè®¡ä¼˜åŒ–å™¨"""

    def __init__(self, min_data_points: int = 30):
        self.min_data_points = min_data_points

    def optimize_threshold_statistical(
        self,
        values: List[float],
        current_threshold: float,
        threshold_type: str = "upper",
    ) -> OptimizationResult:
        """åŸºäºç»Ÿè®¡æ–¹æ³•ä¼˜åŒ–é˜ˆå€¼"""

        if len(values) < self.min_data_points:
            return self._create_insufficient_data_result(current_threshold)

        values_array = np.array(values)

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        mean_val = np.mean(values_array)
        std_val = np.std(values_array)
        q75 = np.percentile(values_array, 75)
        q95 = np.percentile(values_array, 95)
        q99 = np.percentile(values_array, 99)

        if threshold_type == "upper":
            # ä¸Šé˜ˆå€¼ä¼˜åŒ–
            if std_val > 0:
                # åŸºäºæ ‡å‡†å·®çš„æ–¹æ³• (å‡å€¼ + k*æ ‡å‡†å·®)
                k = 2.0  # 2-sigmaè§„åˆ™
                recommended_threshold = mean_val + k * std_val
            else:
                recommended_threshold = q95

            # ç¡®ä¿æ–°é˜ˆå€¼æ¯”å½“å‰é˜ˆå€¼æ›´åˆç†
            recommended_threshold = min(recommended_threshold, q99)

        elif threshold_type == "lower":
            # ä¸‹é˜ˆå€¼ä¼˜åŒ–
            if std_val > 0:
                k = 2.0
                recommended_threshold = mean_val - k * std_val
            else:
                recommended_threshold = np.percentile(values_array, 5)

            recommended_threshold = max(recommended_threshold, q1=np.percentile(values_array, 1))

        else:  # range
            recommended_threshold = q75

        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = self._calculate_confidence(values_array, recommended_threshold, threshold_type)

        # è®¡ç®—é¢„æœŸæ”¹è¿›
        improvement = self._estimate_improvement(values, current_threshold, recommended_threshold, threshold_type)

        return OptimizationResult(
            rule_name="statistical_optimizer",
            optimization_type="statistical",
            recommended_threshold=float(recommended_threshold),
            confidence_score=confidence,
            expected_improvement=improvement,
            reasoning=f"åŸºäº{len(values)}ä¸ªæ•°æ®ç‚¹çš„ç»Ÿè®¡åˆ†æ",
            supporting_evidence=[
                f"å‡å€¼: {mean_val:.2f}",
                f"æ ‡å‡†å·®: {std_val:.2f}",
                f"95%åˆ†ä½æ•°: {q95:.2f}",
                "å¼‚å¸¸å€¼æ£€æµ‹ä½¿ç”¨IQRæ–¹æ³•",
            ],
            metadata={
                "mean": float(mean_val),
                "std": float(std_val),
                "q75": float(q75),
                "q95": float(q95),
                "data_points": len(values),
            },
        )

    def _calculate_confidence(self, values: np.ndarray, threshold: float, threshold_type: str) -> float:
        """è®¡ç®—é˜ˆå€¼ç½®ä¿¡åº¦"""

        # åŸºäºé˜ˆå€¼ä¸æ•°æ®åˆ†å¸ƒçš„åŒ¹é…åº¦
        if threshold_type == "upper":
            # æ£€æŸ¥æœ‰å¤šå°‘æ•°æ®åœ¨é˜ˆå€¼ä»¥ä¸‹
            ratio_below = np.sum(values <= threshold) / len(values)
            # ç†æƒ³æƒ…å†µä¸‹åº”è¯¥æ˜¯95-99%çš„æ•°æ®åœ¨é˜ˆå€¼ä»¥ä¸‹
            target_ratio = 0.97
            confidence = 1.0 - abs(ratio_below - target_ratio)
        else:
            # å¯¹äºä¸‹é˜ˆå€¼ï¼Œæ£€æŸ¥æœ‰å¤šå°‘æ•°æ®åœ¨é˜ˆå€¼ä»¥ä¸Š
            ratio_above = np.sum(values >= threshold) / len(values)
            target_ratio = 0.97
            confidence = 1.0 - abs(ratio_above - target_ratio)

        return max(0.0, min(1.0, confidence))

    def _estimate_improvement(
        self,
        values: List[float],
        old_threshold: float,
        new_threshold: float,
        threshold_type: str,
    ) -> float:
        """ä¼°è®¡é˜ˆå€¼æ”¹è¿›æ•ˆæœ"""

        if not values:
            return 0.0

        # è®¡ç®—æ—§é˜ˆå€¼å’Œæ–°é˜ˆå€¼ä¸‹çš„å¼‚å¸¸ç‡
        old_anomaly_count = sum(1 for v in values if self._is_anomaly(v, old_threshold, threshold_type))
        new_anomaly_count = sum(1 for v in values if self._is_anomaly(v, new_threshold, threshold_type))

        old_rate = old_anomaly_count / len(values)
        new_rate = new_anomaly_count / len(values)

        # æ”¹è¿›ç‡ä¸ºå¼‚å¸¸ç‡å‡å°‘çš„æ¯”ä¾‹
        if old_rate > 0:
            improvement = (old_rate - new_rate) / old_rate
        else:
            improvement = 0.0 if new_rate == 0 else -1.0

        return max(-1.0, improvement)

    def _is_anomaly(self, value: float, threshold: float, threshold_type: str) -> bool:
        """åˆ¤æ–­å€¼æ˜¯å¦ä¸ºå¼‚å¸¸"""
        if threshold_type == "upper":
            return value > threshold
        elif threshold_type == "lower":
            return value < threshold
        else:
            return False

    def _create_insufficient_data_result(self, current_threshold: float) -> OptimizationResult:
        """æ•°æ®ä¸è¶³æ—¶çš„ç»“æœ"""
        return OptimizationResult(
            rule_name="statistical_optimizer",
            optimization_type="statistical",
            recommended_threshold=current_threshold,
            confidence_score=0.1,
            expected_improvement=0.0,
            reasoning="æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œç»Ÿè®¡ä¼˜åŒ–",
            supporting_evidence=["éœ€è¦è‡³å°‘30ä¸ªæ•°æ®ç‚¹"],
            metadata={"data_insufficient": True},
        )


class TrendOptimizer:
    """è¶‹åŠ¿åˆ†æä¼˜åŒ–å™¨"""

    def optimize_threshold_trend(
        self,
        values: List[float],
        timestamps: List[datetime],
        current_threshold: float,
        threshold_type: str = "upper",
    ) -> OptimizationResult:
        """åŸºäºè¶‹åŠ¿åˆ†æä¼˜åŒ–é˜ˆå€¼"""

        if len(values) < 10:
            return OptimizationResult(
                rule_name="trend_optimizer",
                optimization_type="trend_analysis",
                recommended_threshold=current_threshold,
                confidence_score=0.1,
                expected_improvement=0.0,
                reasoning="æ•°æ®ç‚¹ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œè¶‹åŠ¿åˆ†æ",
                supporting_evidence=["éœ€è¦è‡³å°‘10ä¸ªæ•°æ®ç‚¹"],
                metadata={"insufficient_data": True},
            )

        # è½¬æ¢æ—¶é—´æˆ³ä¸ºæ•°å€¼
        time_values = [(t - timestamps[0]).total_seconds() for t in timestamps]

        # è®¡ç®—çº¿æ€§è¶‹åŠ¿
        slope, intercept = np.polyfit(time_values, values, 1)

        # è®¡ç®—è¶‹åŠ¿å¼ºåº¦
        correlation = np.corrcoef(time_values, values)[0, 1] if len(values) > 2 else 0

        # é¢„æµ‹æœªæ¥å€¼
        last_time = time_values[-1]
        future_time = last_time + 3600  # é¢„æµ‹1å°æ—¶å
        predicted_value = slope * future_time + intercept

        # æ ¹æ®è¶‹åŠ¿è°ƒæ•´é˜ˆå€¼
        if threshold_type == "upper":
            if slope > 0:  # ä¸Šå‡è¶‹åŠ¿
                adjustment_factor = 1.1  # å¢åŠ 10%
            else:  # ä¸‹é™è¶‹åŠ¿
                adjustment_factor = 0.95  # å‡å°‘5%
        else:  # lower
            if slope < 0:  # ä¸‹é™è¶‹åŠ¿
                adjustment_factor = 0.9  # å‡å°‘10%
            else:  # ä¸Šå‡è¶‹åŠ¿
                adjustment_factor = 1.05  # å¢åŠ 5%

        recommended_threshold = current_threshold * adjustment_factor

        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = min(1.0, abs(correlation))

        # ä¼°è®¡æ”¹è¿›
        improvement = abs(slope) * correlation if correlation > 0.5 else 0.0

        return OptimizationResult(
            rule_name="trend_optimizer",
            optimization_type="trend_analysis",
            recommended_threshold=recommended_threshold,
            confidence_score=confidence,
            expected_improvement=improvement,
            reasoning=f"åŸºäºè¶‹åŠ¿åˆ†æ: {slope:.4f}/ç§’çš„å˜åŒ–ç‡, ç›¸å…³æ€§: {correlation:.3f}",
            supporting_evidence=[
                f"è¶‹åŠ¿æ–œç‡: {slope:.6f}",
                f"ç›¸å…³ç³»æ•°: {correlation:.3f}",
                f"é¢„æµ‹å€¼: {predicted_value:.2f}",
                f"è°ƒæ•´å› å­: {adjustment_factor:.2f}",
            ],
            metadata={
                "slope": slope,
                "correlation": correlation,
                "predicted_value": predicted_value,
                "adjustment_factor": adjustment_factor,
                "data_points": len(values),
            },
        )


class ClusteringOptimizer:
    """èšç±»åˆ†æä¼˜åŒ–å™¨"""

    def __init__(self, min_cluster_size: int = 3):
        self.min_cluster_size = min_cluster_size

    def optimize_threshold_clustering(
        self,
        values: List[float],
        current_threshold: float,
        threshold_type: str = "upper",
    ) -> OptimizationResult:
        """åŸºäºèšç±»åˆ†æä¼˜åŒ–é˜ˆå€¼"""

        if len(values) < 10:
            return OptimizationResult(
                rule_name="clustering_optimizer",
                optimization_type="clustering",
                recommended_threshold=current_threshold,
                confidence_score=0.1,
                expected_improvement=0.0,
                reasoning="æ•°æ®ç‚¹ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œèšç±»åˆ†æ",
                supporting_evidence=["éœ€è¦è‡³å°‘10ä¸ªæ•°æ®ç‚¹"],
                metadata={"insufficient_data": True},
            )

        values_array = np.array(values).reshape(-1, 1)

        # ä½¿ç”¨DBSCANè¿›è¡Œèšç±»
        clustering = DBSCAN(eps=0.5, min_samples=self.min_cluster_size)
        cluster_labels = clustering.fit_predict(values_array)

        # åˆ†æèšç±»ç»“æœ
        unique_labels = set(cluster_labels)
        cluster_sizes = {}

        for label in unique_labels:
            if label != -1:  # å¿½ç•¥å™ªå£°ç‚¹
                cluster_sizes[label] = np.sum(cluster_labels == label)

        if not cluster_sizes:
            return OptimizationResult(
                rule_name="clustering_optimizer",
                optimization_type="clustering",
                recommended_threshold=current_threshold,
                confidence_score=0.1,
                expected_improvement=0.0,
                reasoning="æ— æ³•å½¢æˆæœ‰æ•ˆèšç±»",
                supporting_evidence=["æ‰€æœ‰ç‚¹è¢«æ ‡è®°ä¸ºå™ªå£°"],
                metadata={"no_clusters": True},
            )

        # æ‰¾åˆ°æœ€å¤§èšç±»å’Œæ¬¡å¤§èšç±»
        largest_cluster_label = max(cluster_sizes, key=cluster_sizes.get)
        largest_cluster_size = cluster_sizes[largest_cluster_label]

        # è·å–æœ€å¤§èšç±»çš„è¾¹ç•Œ
        largest_cluster_values = values_array[cluster_labels == largest_cluster_label]

        if threshold_type == "upper":
            # ä¸Šé˜ˆå€¼è®¾ä¸ºæœ€å¤§èšç±»çš„ä¸Šç•Œ
            boundary = np.percentile(largest_cluster_values, 95)
        else:
            # ä¸‹é˜ˆå€¼è®¾ä¸ºæœ€å¤§èšç±»çš„ä¸‹ç•Œ
            boundary = np.percentile(largest_cluster_values, 5)

        recommended_threshold = float(boundary)

        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = min(1.0, largest_cluster_size / len(values))

        # ä¼°è®¡æ”¹è¿›æ•ˆæœ
        improvement = confidence * 0.3  # åŸºäºèšç±»è´¨é‡çš„æ”¹è¿›ä¼°è®¡

        return OptimizationResult(
            rule_name="clustering_optimizer",
            optimization_type="clustering",
            recommended_threshold=recommended_threshold,
            confidence_score=confidence,
            expected_improvement=improvement,
            reasoning=f"åŸºäºèšç±»åˆ†æï¼Œæœ€å¤§èšç±»åŒ…å«{largest_cluster_size}ä¸ªç‚¹",
            supporting_evidence=[
                f"èšç±»æ•°é‡: {len(unique_labels) - (1 if -1 in cluster_labels else 0)}",
                f"æœ€å¤§èšç±»å¤§å°: {largest_cluster_size}",
                f"å™ªå£°ç‚¹æ•°é‡: {np.sum(cluster_labels == -1)}",
                f"è¾¹ç•Œå€¼: {boundary:.2f}",
            ],
            metadata={
                "num_clusters": len(unique_labels) - (1 if -1 in cluster_labels else 0),
                "largest_cluster_size": largest_cluster_size,
                "noise_points": np.sum(cluster_labels == -1),
                "boundary_value": boundary,
            },
        )


class IntelligentThresholdManager:
    """æ™ºèƒ½é˜ˆå€¼ç®¡ç†å™¨ - ä¸»æ§åˆ¶å™¨"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or self._get_default_config()
        self.threshold_rules: Dict[str, ThresholdRule] = {}
        self.adjustment_history: List[ThresholdAdjustment] = []
        self.data_analyzers: Dict[str, DataAnalyzer] = {}

        # ä¼˜åŒ–å™¨
        self.statistical_optimizer = StatisticalOptimizer()
        self.trend_optimizer = TrendOptimizer()
        self.clustering_optimizer = ClusteringOptimizer()

        # ç›‘æ§æ•°æ®åº“
        self.monitoring_db = None
        if get_monitoring_database:
            try:
                self.monitoring_db = get_monitoring_database()
            except Exception as e:
                logger.warning("ç›‘æ§æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: %s", e)

        # åˆå§‹åŒ–é»˜è®¤é˜ˆå€¼è§„åˆ™
        self._initialize_default_rules()

        logger.info("âœ… æ™ºèƒ½é˜ˆå€¼ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "learning_rate": 0.1,
            "adaptation_speed": 0.05,
            "min_data_points": 30,
            "optimization_interval": 3600,  # 1å°æ—¶
            "max_history_size": 1000,
            "false_positive_threshold": 0.1,
            "false_negative_threshold": 0.05,
            "confidence_threshold": 0.7,
            "trend_analysis_window": 24,  # 24å°æ—¶
            "anomaly_detection_contamination": 0.1,
        }

    def _initialize_default_rules(self):
        """åˆå§‹åŒ–é»˜è®¤é˜ˆå€¼è§„åˆ™"""
        default_rules = [
            ThresholdRule(
                name="cpu_usage_high",
                metric_name="cpu_usage",
                current_threshold=80.0,
                threshold_type="upper",
            ),
            ThresholdRule(
                name="gpu_memory_high",
                metric_name="gpu_memory_usage",
                current_threshold=85.0,
                threshold_type="upper",
            ),
            ThresholdRule(
                name="memory_usage_high",
                metric_name="memory_usage",
                current_threshold=85.0,
                threshold_type="upper",
            ),
            ThresholdRule(
                name="strategy_win_rate_low",
                metric_name="strategy_win_rate",
                current_threshold=30.0,
                threshold_type="lower",
            ),
            ThresholdRule(
                name="strategy_drawdown_high",
                metric_name="strategy_drawdown",
                current_threshold=5.0,
                threshold_type="upper",
            ),
            ThresholdRule(
                name="query_time_high",
                metric_name="query_time_ms",
                current_threshold=5000.0,
                threshold_type="upper",
            ),
        ]

        for rule in default_rules:
            self.add_threshold_rule(rule)

        logger.info("âœ… å·²åˆå§‹åŒ–%sä¸ªé»˜è®¤é˜ˆå€¼è§„åˆ™", len(default_rules))

    def add_threshold_rule(self, rule: ThresholdRule):
        """æ·»åŠ é˜ˆå€¼è§„åˆ™"""
        self.threshold_rules[rule.name] = rule
        self.data_analyzers[rule.name] = DataAnalyzer()
        logger.info("âœ… å·²æ·»åŠ é˜ˆå€¼è§„åˆ™: %s", rule.name)

    def remove_threshold_rule(self, rule_name: str) -> bool:
        """ç§»é™¤é˜ˆå€¼è§„åˆ™"""
        if rule_name in self.threshold_rules:
            del self.threshold_rules[rule_name]
            if rule_name in self.data_analyzers:
                del self.data_analyzers[rule_name]
            logger.info("âœ… å·²ç§»é™¤é˜ˆå€¼è§„åˆ™: %s", rule_name)
            return True
        return False

    async def process_metric_value(
        self, rule_name: str, value: float, timestamp: Optional[datetime] = None
    ) -> Dict[str, Any]:
        """å¤„ç†æŒ‡æ ‡å€¼ï¼Œè¯„ä¼°æ˜¯å¦è§¦å‘å‘Šè­¦"""

        if rule_name not in self.threshold_rules:
            return {"triggered": False, "reason": "rule_not_found"}

        rule = self.threshold_rules[rule_name]
        if timestamp is None:
            timestamp = datetime.now()

        # æ·»åŠ æ•°æ®ç‚¹åˆ°åˆ†æå™¨
        analyzer = self.data_analyzers[rule_name]
        analyzer.add_data_point(value, timestamp, rule_name)

        # è®°å½•å†å²æ•°æ®
        self._record_metric_data(rule_name, value, timestamp)

        # è¯„ä¼°æ˜¯å¦è§¦å‘å‘Šè­¦
        triggered = self._evaluate_threshold(rule, value)

        # å¦‚æœè§¦å‘äº†å‘Šè­¦ï¼Œè®°å½•è¯¯æŠ¥æ£€æµ‹ç»“æœ
        if triggered:
            await self._handle_potential_false_positive(rule_name, value, timestamp)

        # æ›´æ–°è§„åˆ™ç»Ÿè®¡
        self._update_rule_statistics(rule_name, value, triggered, timestamp)

        return {
            "triggered": triggered,
            "rule_name": rule_name,
            "value": value,
            "threshold": rule.current_threshold,
            "threshold_type": rule.threshold_type,
            "confidence": rule.confidence_score,
        }

    def _evaluate_threshold(self, rule: ThresholdRule, value: float) -> bool:
        """è¯„ä¼°é˜ˆå€¼è§¦å‘æ¡ä»¶"""

        if rule.threshold_type == "upper":
            return value > rule.current_threshold
        elif rule.threshold_type == "lower":
            return value < rule.current_threshold
        else:
            return False  # rangeç±»å‹æš‚æœªå®ç°

    def _record_metric_data(self, rule_name: str, value: float, timestamp: datetime):
        """è®°å½•æŒ‡æ ‡æ•°æ®"""

        if not self.monitoring_db:
            return

        try:
            # è®°å½•åˆ°ç›‘æ§æ•°æ®åº“
            pass

            # è¿™é‡Œå¯ä»¥è°ƒç”¨å…·ä½“çš„æ•°æ®åº“å†™å…¥æ–¹æ³•
            # self.monitoring_db.record_intelligent_metric_data(record)

        except Exception as e:
            logger.warning("è®°å½•æŒ‡æ ‡æ•°æ®å¤±è´¥: %s", e)

    async def _handle_potential_false_positive(self, rule_name: str, value: float, timestamp: datetime):
        """å¤„ç†å¯èƒ½çš„è¯¯æŠ¥"""

        try:
            analyzer = self.data_analyzers[rule_name]

            # æ£€æµ‹å¼‚å¸¸å€¼
            anomalies = analyzer.detect_anomalies(contamination=self.config["anomaly_detection_contamination"])

            # å¦‚æœå½“å‰å€¼è¢«æ ‡è®°ä¸ºå¼‚å¸¸ï¼Œå¯èƒ½æ˜¯çœŸæ­£çš„å‘Šè­¦
            if len(anomalies) > 0:
                current_index = len(analyzer.data_buffer) - 1
                if current_index in anomalies:
                    # è®°å½•å¼‚å¸¸ç¡®è®¤
                    self._confirm_true_positive(rule_name, value, timestamp)
                else:
                    # è®°å½•å¯èƒ½çš„è¯¯æŠ¥
                    self._flag_potential_false_positive(rule_name, value, timestamp)

        except Exception as e:
            logger.warning("è¯¯æŠ¥æ£€æµ‹å¤±è´¥: %s", e)

    def _confirm_true_positive(self, rule_name: str, value: float, timestamp: datetime):
        """ç¡®è®¤çœŸæ­£çš„æ­£ä¾‹"""

        if rule_name in self.threshold_rules:
            rule = self.threshold_rules[rule_name]

            # å‡å°‘è¯¯æŠ¥ç‡
            rule.false_positive_rate *= 0.95  # é€’å‡
            rule.false_negative_rate *= 1.05  # å¾®å¢

            # è®°å½•å†å²
            self._add_to_rule_history(
                rule,
                {
                    "timestamp": timestamp.isoformat(),
                    "type": "true_positive",
                    "value": value,
                    "threshold": rule.current_threshold,
                },
            )

    def _flag_potential_false_positive(self, rule_name: str, value: float, timestamp: datetime):
        """æ ‡è®°å¯èƒ½çš„è¯¯æŠ¥"""

        if rule_name in self.threshold_rules:
            rule = self.threshold_rules[rule_name]

            # å¢åŠ è¯¯æŠ¥ç‡
            rule.false_positive_rate = min(1.0, rule.false_positive_rate * 1.1)

            # å¦‚æœè¯¯æŠ¥ç‡è¿‡é«˜ï¼Œè§¦å‘é˜ˆå€¼è°ƒæ•´
            if rule.false_positive_rate > self.config["false_positive_threshold"]:
                logger.info("è§„åˆ™%sè¯¯æŠ¥ç‡è¿‡é«˜(%s)ï¼Œå»ºè®®è°ƒæ•´é˜ˆå€¼", rule_name, rule.false_positive_rate)

            # è®°å½•å†å²
            self._add_to_rule_history(
                rule,
                {
                    "timestamp": timestamp.isoformat(),
                    "type": "potential_false_positive",
                    "value": value,
                    "threshold": rule.current_threshold,
                },
            )

    def _update_rule_statistics(self, rule_name: str, value: float, triggered: bool, timestamp: datetime):
        """æ›´æ–°è§„åˆ™ç»Ÿè®¡ä¿¡æ¯"""

        if rule_name not in self.threshold_rules:
            return

        rule = self.threshold_rules[rule_name]

        # æ·»åŠ åˆ°å†å²è®°å½•
        self._add_to_rule_history(
            rule,
            {
                "timestamp": timestamp.isoformat(),
                "value": value,
                "triggered": triggered,
                "threshold": rule.current_threshold,
            },
        )

        # è®¡ç®—ç½®ä¿¡åº¦
        rule.confidence_score = self._calculate_confidence_score(rule)

    def _add_to_rule_history(self, rule: ThresholdRule, entry: Dict[str, Any]):
        """æ·»åŠ åˆ°è§„åˆ™å†å²"""

        rule.history.append(entry)

        # é™åˆ¶å†å²å¤§å°
        if len(rule.history) > self.config["max_history_size"]:
            rule.history = rule.history[-self.config["max_history_size"] :]

    def _calculate_confidence_score(self, rule: ThresholdRule) -> float:
        """è®¡ç®—è§„åˆ™ç½®ä¿¡åº¦"""

        if not rule.history:
            return 0.5

        recent_history = rule.history[-50:]  # æœ€è¿‘50æ¡è®°å½•

        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        triggered_count = sum(1 for entry in recent_history if entry.get("triggered", False))
        trigger_rate = triggered_count / len(recent_history)

        # ç†æƒ³è§¦å‘ç‡åº”è¯¥åœ¨5-15%ä¹‹é—´
        ideal_rate = 0.1
        rate_score = 1.0 - abs(trigger_rate - ideal_rate) / ideal_rate

        # è€ƒè™‘è¯¯æŠ¥ç‡
        fp_penalty = rule.false_positive_rate * 0.5
        fn_penalty = rule.false_negative_rate * 0.3

        # ç»¼åˆç½®ä¿¡åº¦
        confidence = (rate_score + fp_penalty + fn_penalty) / 2.0
        confidence = max(0.0, min(1.0, confidence))

        return confidence

    async def optimize_thresholds(self, rule_name: Optional[str] = None) -> Dict[str, OptimizationResult]:
        """ä¼˜åŒ–é˜ˆå€¼"""

        if rule_name:
            # ä¼˜åŒ–æŒ‡å®šè§„åˆ™
            if rule_name not in self.threshold_rules:
                return {}

            results = await self._optimize_single_rule(rule_name)
            return {rule_name: results}
        else:
            # ä¼˜åŒ–æ‰€æœ‰è§„åˆ™
            results = {}

            for name in self.threshold_rules.keys():
                try:
                    result = await self._optimize_single_rule(name)
                    results[name] = result
                except Exception as e:
                    logger.error("ä¼˜åŒ–è§„åˆ™%så¤±è´¥: %s", name, e)
                    continue

            return results

    async def _optimize_single_rule(self, rule_name: str) -> OptimizationResult:
        """ä¼˜åŒ–å•ä¸ªè§„åˆ™"""

        if rule_name not in self.threshold_rules:
            raise ValueError(f"è§„åˆ™{rule_name}ä¸å­˜åœ¨")

        rule = self.threshold_rules[rule_name]
        self.data_analyzers[rule_name]

        # è·å–å†å²æ•°æ®
        values = [entry["value"] for entry in rule.history if "value" in entry]
        timestamps = [datetime.fromisoformat(entry["timestamp"]) for entry in rule.history if "timestamp" in entry]

        if len(values) < self.config["min_data_points"]:
            return OptimizationResult(
                rule_name=rule_name,
                optimization_type="insufficient_data",
                recommended_threshold=rule.current_threshold,
                confidence_score=0.1,
                expected_improvement=0.0,
                reasoning="æ•°æ®ä¸è¶³ï¼Œæ— æ³•ä¼˜åŒ–",
                supporting_evidence=[f"éœ€è¦è‡³å°‘{self.config['min_data_points']}ä¸ªæ•°æ®ç‚¹"],
                metadata={"data_insufficient": True},
            )

        # æ‰§è¡Œå¤šç§ä¼˜åŒ–ç­–ç•¥
        optimization_results = []

        # 1. ç»Ÿè®¡ä¼˜åŒ–
        try:
            stat_result = self.statistical_optimizer.optimize_threshold_statistical(
                values, rule.current_threshold, rule.threshold_type
            )
            optimization_results.append(stat_result)
        except Exception as e:
            logger.warning("ç»Ÿè®¡ä¼˜åŒ–å¤±è´¥: %s", e)

        # 2. è¶‹åŠ¿ä¼˜åŒ–
        try:
            trend_result = self.trend_optimizer.optimize_threshold_trend(
                values, timestamps, rule.current_threshold, rule.threshold_type
            )
            optimization_results.append(trend_result)
        except Exception as e:
            logger.warning("è¶‹åŠ¿ä¼˜åŒ–å¤±è´¥: %s", e)

        # 3. èšç±»ä¼˜åŒ–
        try:
            cluster_result = self.clustering_optimizer.optimize_threshold_clustering(
                values, rule.current_threshold, rule.threshold_type
            )
            optimization_results.append(cluster_result)
        except Exception as e:
            logger.warning("èšç±»ä¼˜åŒ–å¤±è´¥: %s", e)

        if not optimization_results:
            return OptimizationResult(
                rule_name=rule_name,
                optimization_type="failed",
                recommended_threshold=rule.current_threshold,
                confidence_score=0.0,
                expected_improvement=0.0,
                reasoning="æ‰€æœ‰ä¼˜åŒ–ç­–ç•¥éƒ½å¤±è´¥äº†",
                supporting_evidence=["æ£€æŸ¥æ•°æ®è´¨é‡å’Œç®—æ³•å‚æ•°"],
                metadata={"all_optimizations_failed": True},
            )

        # é€‰æ‹©æœ€ä½³ä¼˜åŒ–ç»“æœ
        best_result = max(
            optimization_results,
            key=lambda x: x.confidence_score * x.expected_improvement,
        )

        logger.info("è§„åˆ™%sä¼˜åŒ–å®Œæˆ: %s -> %s", rule_name, rule.current_threshold, best_result.recommended_threshold)

        return best_result

    async def apply_optimization(self, rule_name: str, optimization_result: OptimizationResult) -> bool:
        """åº”ç”¨ä¼˜åŒ–ç»“æœ"""

        if rule_name not in self.threshold_rules:
            logger.error("è§„åˆ™%sä¸å­˜åœ¨", rule_name)
            return False

        rule = self.threshold_rules[rule_name]

        # éªŒè¯ç½®ä¿¡åº¦
        if optimization_result.confidence_score < self.config["confidence_threshold"]:
            logger.info("ä¼˜åŒ–ç»“æœç½®ä¿¡åº¦è¿‡ä½(%s)ï¼Œè·³è¿‡åº”ç”¨", optimization_result.confidence_score)
            return False

        old_threshold = rule.current_threshold
        new_threshold = optimization_result.recommended_threshold

        # åº”ç”¨æ–°é˜ˆå€¼
        rule.current_threshold = new_threshold
        rule.optimal_threshold = new_threshold
        rule.adjustment_count += 1
        rule.last_adjustment = datetime.now()

        # è®°å½•è°ƒæ•´å†å²
        adjustment = ThresholdAdjustment(
            timestamp=datetime.now(),
            rule_name=rule_name,
            old_threshold=old_threshold,
            new_threshold=new_threshold,
            reason=optimization_result.reasoning,
            confidence=optimization_result.confidence_score,
            metrics_snapshot=optimization_result.metadata,
            predicted_effectiveness=optimization_result.expected_improvement,
        )

        self.adjustment_history.append(adjustment)

        # é™åˆ¶è°ƒæ•´å†å²å¤§å°
        if len(self.adjustment_history) > self.config["max_history_size"]:
            self.adjustment_history = self.adjustment_history[-self.config["max_history_size"] :]

        logger.info("âœ… å·²åº”ç”¨é˜ˆå€¼ä¼˜åŒ–: %s %s -> %s", rule_name, old_threshold, new_threshold)
        return True

    def get_threshold_status(self) -> Dict[str, Any]:
        """è·å–é˜ˆå€¼çŠ¶æ€"""

        status = {
            "total_rules": len(self.threshold_rules),
            "optimization_enabled": True,
            "last_optimization": None,
            "rules_status": {},
            "adjustment_statistics": self._get_adjustment_statistics(),
        }

        # è·å–å„è§„åˆ™çŠ¶æ€
        for rule_name, rule in self.threshold_rules.items():
            rule_status = {
                "current_threshold": rule.current_threshold,
                "optimal_threshold": rule.optimal_threshold,
                "confidence_score": rule.confidence_score,
                "adjustment_count": rule.adjustment_count,
                "false_positive_rate": rule.false_positive_rate,
                "false_negative_rate": rule.false_negative_rate,
                "last_adjustment": rule.last_adjustment.isoformat() if rule.last_adjustment else None,
                "data_points": len(rule.history),
            }

            # è®¡ç®—é˜ˆå€¼åˆç†æ€§
            rule_status["thresholdåˆç†æ€§"] = self._evaluate_threshold_reasonableness(rule)

            status["rules_status"][rule_name] = rule_status

        # è·å–æœ€åä¼˜åŒ–æ—¶é—´
        if self.adjustment_history:
            status["last_optimization"] = self.adjustment_history[-1].timestamp.isoformat()

        return status

    def _evaluate_threshold_reasonableness(self, rule: ThresholdRule) -> Dict[str, Any]:
        """è¯„ä¼°é˜ˆå€¼åˆç†æ€§"""

        if not rule.history:
            return {"status": "insufficient_data", "score": 0.0}

        recent_values = [entry["value"] for entry in rule.history[-20:] if "value" in entry]
        if not recent_values:
            return {"status": "insufficient_data", "score": 0.0}

        # è®¡ç®—æŒ‡æ ‡
        recent_mean = np.mean(recent_values)
        recent_std = np.std(recent_values)

        # è®¡ç®—è§¦å‘ç‡
        triggered_count = sum(1 for entry in rule.history[-20:] if entry.get("triggered", False))
        trigger_rate = triggered_count / min(20, len(rule.history))

        # è¯„ä¼°åˆç†æ€§
        reasonableness_score = 0.5  # åŸºç¡€åˆ†

        # è§¦å‘ç‡åˆç†æ€§ (ç†æƒ³5-15%)
        if 0.05 <= trigger_rate <= 0.15:
            reasonableness_score += 0.3
        elif 0.02 <= trigger_rate <= 0.25:
            reasonableness_score += 0.1

        # ç½®ä¿¡åº¦
        reasonableness_score += rule.confidence_score * 0.2

        reasonableness_score = min(1.0, reasonableness_score)

        if reasonableness_score >= 0.8:
            status = "excellent"
        elif reasonableness_score >= 0.6:
            status = "good"
        elif reasonableness_score >= 0.4:
            status = "acceptable"
        else:
            status = "needs_optimization"

        return {
            "status": status,
            "score": reasonableness_score,
            "trigger_rate": trigger_rate,
            "recent_mean": recent_mean,
            "recent_std": recent_std,
        }

    def _get_adjustment_statistics(self) -> Dict[str, Any]:
        """è·å–è°ƒæ•´ç»Ÿè®¡ä¿¡æ¯"""

        if not self.adjustment_history:
            return {"total_adjustments": 0}

        recent_adjustments = self.adjustment_history[-30:]  # æœ€è¿‘30æ¬¡è°ƒæ•´

        return {
            "total_adjustments": len(self.adjustment_history),
            "recent_adjustments": len(recent_adjustments),
            "avg_confidence": np.mean([adj.confidence for adj in recent_adjustments]),
            "avg_effectiveness": np.mean(
                [adj.predicted_effectiveness for adj in recent_adjustments if adj.predicted_effectiveness is not None]
            ),
            "most_adjusted_rule": self._get_most_adjusted_rule(),
            "optimization_types": list(set([adj.reason.split(":")[0] for adj in recent_adjustments])),
        }

    def _get_most_adjusted_rule(self) -> Optional[str]:
        """è·å–è°ƒæ•´æœ€å¤šçš„è§„åˆ™"""

        if not self.adjustment_history:
            return None

        rule_counts = {}
        for adj in self.adjustment_history:
            rule_counts[adj.rule_name] = rule_counts.get(adj.rule_name, 0) + 1

        if rule_counts:
            return max(rule_counts, key=rule_counts.get)

        return None

    def export_configuration(self) -> str:
        """å¯¼å‡ºé…ç½®"""

        config_data = {
            "timestamp": datetime.now().isoformat(),
            "threshold_rules": {name: asdict(rule) for name, rule in self.threshold_rules.items()},
            "adjustment_history": [asdict(adj) for adj in self.adjustment_history[-100:]],  # æœ€è¿‘100æ¡
            "config": self.config,
        }

        return json.dumps(config_data, indent=2, default=str)

    async def import_configuration(self, config_json: str) -> bool:
        """å¯¼å…¥é…ç½®"""

        try:
            config_data = json.loads(config_json)

            # æ¢å¤é˜ˆå€¼è§„åˆ™
            for name, rule_data in config_data.get("threshold_rules", {}).items():
                rule = ThresholdRule(**rule_data)
                self.threshold_rules[name] = rule
                self.data_analyzers[name] = DataAnalyzer()

            # æ¢å¤è°ƒæ•´å†å²
            for adj_data in config_data.get("adjustment_history", []):
                adj_data["timestamp"] = datetime.fromisoformat(adj_data["timestamp"])
                adjustment = ThresholdAdjustment(**adj_data)
                self.adjustment_history.append(adjustment)

            # æ›´æ–°é…ç½®
            self.config.update(config_data.get("config", {}))

            logger.info("âœ… é…ç½®å¯¼å…¥æˆåŠŸ: %sä¸ªè§„åˆ™", len(self.threshold_rules))
            return True

        except Exception as e:
            logger.error("é…ç½®å¯¼å…¥å¤±è´¥: %s", e)
            return False


# å…¨å±€å•ä¾‹ç®¡ç†å™¨
_intelligent_threshold_manager = None


def get_intelligent_threshold_manager() -> IntelligentThresholdManager:
    """è·å–æ™ºèƒ½é˜ˆå€¼ç®¡ç†å™¨å•ä¾‹"""
    global _intelligent_threshold_manager

    if _intelligent_threshold_manager is None:
        _intelligent_threshold_manager = IntelligentThresholdManager()

    return _intelligent_threshold_manager


# ä¾¿æ·å‡½æ•°
async def create_intelligent_threshold(
    config: Optional[Dict[str, Any]] = None,
) -> IntelligentThresholdManager:
    """åˆ›å»ºæ™ºèƒ½é˜ˆå€¼ç®¡ç†å™¨"""
    return IntelligentThresholdManager(config)


async def optimize_all_thresholds() -> Dict[str, OptimizationResult]:
    """ä¼˜åŒ–æ‰€æœ‰é˜ˆå€¼"""
    manager = get_intelligent_threshold_manager()
    return await manager.optimize_thresholds()


async def process_metric(rule_name: str, value: float, timestamp: Optional[datetime] = None) -> Dict[str, Any]:
    """å¤„ç†æŒ‡æ ‡å€¼"""
    manager = get_intelligent_threshold_manager()
    return await manager.process_metric_value(rule_name, value, timestamp)


if __name__ == "__main__":
    """ç¤ºä¾‹ç”¨æ³•"""

    async def main():
        # åˆ›å»ºæ™ºèƒ½é˜ˆå€¼ç®¡ç†å™¨
        manager = IntelligentThresholdManager()

        print("ğŸ¤– æ™ºèƒ½é˜ˆå€¼ç®—æ³•å’Œè¯¯æŠ¥ä¼˜åŒ–æ¨¡å—æ¼”ç¤º")
        print("=" * 50)

        # æ¨¡æ‹Ÿæ•°æ®
        import random

        rule_name = "cpu_usage_high"

        # æ¨¡æ‹ŸCPUä½¿ç”¨ç‡æ•°æ®
        for i in range(50):
            value = random.gauss(60, 15)  # æ­£æ€åˆ†å¸ƒï¼Œå‡å€¼60ï¼Œæ ‡å‡†å·®15
            await manager.process_metric_value(rule_name, value)

        # è·å–è§„åˆ™çŠ¶æ€
        status = manager.get_threshold_status()
        print(f"\nğŸ“Š é˜ˆå€¼çŠ¶æ€: {status['total_rules']}ä¸ªè§„åˆ™")

        # ä¼˜åŒ–é˜ˆå€¼
        print("\nğŸ”§ å¼€å§‹ä¼˜åŒ–é˜ˆå€¼...")
        results = await manager.optimize_thresholds(rule_name)

        for rule_name, result in results.items():
            print(f"è§„åˆ™: {rule_name}")
            print(f"  å½“å‰é˜ˆå€¼: {status['rules_status'][rule_name]['current_threshold']:.2f}")
            print(f"  æ¨èé˜ˆå€¼: {result.recommended_threshold:.2f}")
            print(f"  ç½®ä¿¡åº¦: {result.confidence_score:.3f}")
            print(f"  é¢„æœŸæ”¹è¿›: {result.expected_improvement:.3f}")
            print(f"  æ¨ç†: {result.reasoning}")

        # åº”ç”¨ä¼˜åŒ–
        if results:
            rule_name = list(results.keys())[0]
            optimization_result = results[rule_name]
            success = await manager.apply_optimization(rule_name, optimization_result)
            print(f"\nâœ… ä¼˜åŒ–åº”ç”¨{'æˆåŠŸ' if success else 'å¤±è´¥'}")

        # å¯¼å‡ºé…ç½®
        config = manager.export_configuration()
        print(f"\nğŸ’¾ é…ç½®å·²å¯¼å‡º ({len(config)}å­—ç¬¦)")

        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆ!")

    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main())
