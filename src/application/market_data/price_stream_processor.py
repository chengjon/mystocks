"""
Price Stream Processor
å®æ—¶è¡Œæƒ…æµå¤„ç†å™¨

è´Ÿè´£ï¼š
1. æ¥æ”¶ PriceUpdate æ•°æ®
2. è½¬æ¢ä¸º PriceChangedEvent é¢†åŸŸäº‹ä»¶
3. å‘å¸ƒåˆ° Redis äº‹ä»¶æ€»çº¿
4. è§¦å‘æŠ•èµ„ç»„åˆé‡æ–°è®¡ç®—
"""

import asyncio
import logging
from collections import defaultdict
from datetime import datetime
from typing import Dict, List, Optional, Set

from redis import Redis

from src.domain.market_data.streaming import IPriceStreamAdapter, PriceChangedEvent, PriceUpdate
from src.domain.portfolio.service import PortfolioValuationService
from src.domain.shared.event_bus import IEventBus
from src.infrastructure.cache.redis_lock import RedisDistributedLock

logger = logging.getLogger(__name__)


class PriceStreamProcessor:
    """
    å®æ—¶è¡Œæƒ…æµå¤„ç†å™¨

    èŒè´£ï¼š
    - æ¥æ”¶å®æ—¶è¡Œæƒ…æ•°æ®
    - å‘å¸ƒä»·æ ¼å˜æ›´äº‹ä»¶åˆ°äº‹ä»¶æ€»çº¿
    - è§¦å‘æŠ•èµ„ç»„åˆé‡æ–°è®¡ç®—
    - æä¾›æ‰¹å¤„ç†å’ŒèŠ‚æµä¼˜åŒ–
    - é›†æˆåˆ†å¸ƒå¼é”ä¿è¯å¹¶å‘å®‰å…¨

    Args:
        event_bus: äº‹ä»¶æ€»çº¿ï¼ˆRedisEventBusï¼‰
        redis_client: Redis å®¢æˆ·ç«¯ï¼ˆç”¨äºåˆ†å¸ƒå¼é”ï¼‰
        batch_size: æ‰¹å¤„ç†å¤§å°
        batch_timeout: æ‰¹å¤„ç†è¶…æ—¶ï¼ˆç§’ï¼‰
        enable_throttling: æ˜¯å¦å¯ç”¨èŠ‚æµ
    """

    def __init__(
        self,
        event_bus: IEventBus,
        valuation_service: PortfolioValuationService,
        redis_client: Optional[Redis] = None,
        batch_size: int = 100,
        batch_timeout: float = 0.1,
        enable_throttling: bool = True,
    ):
        self.event_bus = event_bus
        self.valuation_service = valuation_service
        self.redis_client = redis_client
        self.batch_size = batch_size
        self.batch_timeout = batch_timeout
        self.enable_throttling = enable_throttling

        self._stream_adapters: List[IPriceStreamAdapter] = []
        self._price_cache: Dict[str, float] = {}
        self._update_queue: List[PriceUpdate] = []
        self._last_flush: datetime = datetime.now()

        self._portfolio_symbols: Dict[str, Set[str]] = {}  # portfolio_id -> set of symbols
        self._lock_manager: Optional[RedisDistributedLock] = None

        self._running = False
        self._processing_task: Optional[asyncio.Task] = None

        if redis_client:
            self._lock_manager = RedisDistributedLock(redis_client)

        self.metrics = {
            "total_updates": 0,
            "events_published": 0,
            "batches_processed": 0,
            "portfolio_revaluations": 0,
            "last_update_time": None,
        }

        logger.info(
            f"âœ… Price Stream Processor initialized (batch_size={batch_size}, batch_timeout={batch_timeout}, throttling={enable_throttling})"
        )

    async def start(self) -> None:
        """å¯åŠ¨å¤„ç†å™¨"""
        if self._running:
            logger.warning("Price Stream Processor already running")
            return

        logger.info("ğŸš€ Starting Price Stream Processor...")
        self._running = True

        # ä¸ºæ‰€æœ‰é€‚é…å™¨æ³¨å†Œå›è°ƒ
        for adapter in self._stream_adapters:
            adapter.on_message(self._on_price_update)

        # å¯åŠ¨æ‰¹å¤„ç†ä»»åŠ¡
        self._processing_task = asyncio.create_task(self._batch_processing_loop())

        logger.info("âœ… Price Stream Processor started")

    async def stop(self) -> None:
        """åœæ­¢å¤„ç†å™¨"""
        if not self._running:
            return

        logger.info("ğŸ›‘ Stopping Price Stream Processor...")
        self._running = False

        # åˆ·æ–°å‰©ä½™æ›´æ–°
        if self._update_queue:
            await self._flush_updates()

        # åœæ­¢å¤„ç†ä»»åŠ¡
        if self._processing_task:
            self._processing_task.cancel()
            try:
                await self._processing_task
            except asyncio.CancelledError:
                pass

        # åœæ­¢æ‰€æœ‰é€‚é…å™¨
        for adapter in self._stream_adapters:
            await adapter.stop()

        logger.info("âœ… Price Stream Processor stopped")

    def add_stream_adapter(self, adapter: IPriceStreamAdapter) -> None:
        """æ·»åŠ è¡Œæƒ…æµé€‚é…å™¨"""
        self._stream_adapters.append(adapter)
        logger.info("âœ… Added stream adapter: {adapter.__class__.__name__")

    def register_portfolio_symbols(self, portfolio_id: str, symbols: Set[str]) -> None:
        """
        æ³¨å†ŒæŠ•èµ„ç»„åˆçš„è‚¡ç¥¨ä»£ç 

        Args:
            portfolio_id: æŠ•èµ„ç»„åˆID
            symbols: è‚¡ç¥¨ä»£ç é›†åˆ
        """
        self._portfolio_symbols[portfolio_id] = symbols
        logger.info("âœ… Registered portfolio %(portfolio_id)s with {len(symbols)} symbols")

    def unregister_portfolio_symbols(self, portfolio_id: str) -> None:
        """
        æ³¨é”€æŠ•èµ„ç»„åˆ

        Args:
            portfolio_id: æŠ•èµ„ç»„åˆID
        """
        if portfolio_id in self._portfolio_symbols:
            symbols = self._portfolio_symbols.pop(portfolio_id)
            logger.info("âœ… Unregistered portfolio %(portfolio_id)s with {len(symbols)} symbols")

    def _on_price_update(self, update: PriceUpdate) -> None:
        """
        å¤„ç†ä»·æ ¼æ›´æ–°ï¼ˆåŒæ­¥å›è°ƒï¼‰

        Args:
            update: ä»·æ ¼æ›´æ–°å¯¹è±¡
        """
        self.metrics["total_updates"] += 1
        self.metrics["last_update_time"] = datetime.now()

        # æ·»åŠ åˆ°æ‰¹å¤„ç†é˜Ÿåˆ—
        self._update_queue.append(update)

        # å¦‚æœä¸å¯ç”¨æ‰¹å¤„ç†ï¼Œç«‹å³åˆ·æ–°
        if not self.enable_throttling:
            asyncio.create_task(self._flush_updates())

    async def _batch_processing_loop(self) -> None:
        """æ‰¹å¤„ç†å¾ªç¯"""
        logger.info("ğŸ”„ Starting batch processing loop...")

        while self._running:
            try:
                await asyncio.sleep(self.batch_timeout)

                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°
                if self._update_queue:
                    should_flush = len(self._update_queue) >= self.batch_size
                    time_since_flush = (datetime.now() - self._last_flush).total_seconds()
                    should_flush = should_flush or time_since_flush >= self.batch_timeout

                    if should_flush:
                        await self._flush_updates()

            except asyncio.CancelledError:
                logger.info("ğŸ›‘ Batch processing loop cancelled")
                break
            except Exception as e:
                logger.error("Error in batch processing loop: %s", e)

        logger.info("â¹ï¸ Batch processing loop stopped")

    async def _flush_updates(self) -> None:
        """åˆ·æ–°æ›´æ–°é˜Ÿåˆ—"""
        if not self._update_queue:
            return

        # è·å–æ‰€æœ‰æ›´æ–°
        updates = list(self._update_queue)
        self._update_queue.clear()
        self._last_flush = datetime.now()

        logger.debug("ğŸ”„ Flushing %s price updates...", len(updates))

        # æŒ‰æŠ•èµ„ç»„åˆåˆ†ç»„
        portfolio_updates: Dict[str, Dict[str, float]] = defaultdict(dict)
        all_prices: Dict[str, float] = {}

        for update in updates:
            # è®°å½•æ‰€æœ‰ä»·æ ¼
            all_prices[update.symbol] = update.price

            # æŒ‰æŠ•èµ„ç»„åˆåˆ†ç»„
            for portfolio_id, symbols in self._portfolio_symbols.items():
                if update.symbol in symbols:
                    portfolio_updates[portfolio_id][update.symbol] = update.price

        # å‘å¸ƒä»·æ ¼å˜æ›´äº‹ä»¶
        if all_prices:
            events = PriceChangedEvent.create_batch(all_prices, self._price_cache)
            for event in events:
                try:
                    self.event_bus.publish(event)
                    self.metrics["events_published"] += 1
                except Exception as e:
                    logger.error("Failed to publish PriceChangedEvent: %(e)s")

            # æ›´æ–°ä»·æ ¼ç¼“å­˜
            self._price_cache.update(all_prices)

        # è§¦å‘æŠ•èµ„ç»„åˆé‡æ–°è®¡ç®—
        if portfolio_updates:
            for portfolio_id, prices in portfolio_updates.items():
                try:
                    # å¦‚æœæœ‰åˆ†å¸ƒå¼é”ï¼Œå…ˆè·å–é”
                    identifier = None
                    if self._lock_manager:
                        lock_name = f"portfolio:revaluate:{portfolio_id}"
                        identifier = self._lock_manager.acquire(lock_name, expire_seconds=5, wait_timeout=1)

                    if identifier or not self._lock_manager:
                        try:
                            # è°ƒç”¨ PortfolioValuationService é‡æ–°è®¡ç®—
                            performance = self.valuation_service.revaluate_portfolio(
                                portfolio_id=portfolio_id, prices=prices, force_save=True
                            )

                            if performance:
                                logger.info(
                                    f"ğŸ“Š Revaluated portfolio {portfolio_id}: {len(prices)} symbols, "
                                    f"holdings_value={performance.holdings_value:.2f}, return={performance.total_return:.2f}%"
                                )
                                self.metrics["portfolio_revaluations"] += 1
                            else:
                                logger.warning("âš ï¸ Failed to revaluate portfolio %(portfolio_id)s")

                        finally:
                            if identifier and self._lock_manager:
                                self._lock_manager.release(lock_name, identifier)
                    else:
                        logger.warning("âš ï¸ Could not acquire lock for portfolio %(portfolio_id)s")

                except Exception as e:
                    logger.error("Failed to revaluate portfolio %(portfolio_id)s: %(e)s")

        self.metrics["batches_processed"] += 1
        logger.debug("âœ… Flushed {len(updates)} price updates")

    def get_metrics(self) -> dict:
        """è·å–å¤„ç†å™¨æŒ‡æ ‡"""
        return {
            **self.metrics,
            "queue_size": len(self._update_queue),
            "cached_symbols": len(self._price_cache),
            "registered_portfolios": len(self._portfolio_symbols),
            "stream_adapters": len(self._stream_adapters),
        }
