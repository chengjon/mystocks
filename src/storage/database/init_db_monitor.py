# æ­£å¸¸æ‰§è¡Œï¼ˆä¸åˆ é™¤å·²æœ‰è¡¨ï¼‰ï¼špython execute_sql_with_env.py
# å¼ºåˆ¶åˆ é™¤å¹¶é‡å»ºè¡¨ï¼špython execute_sql_with_env.py --drop-existing

import argparse
import os
import time

import sqlalchemy
from loguru import logger
from sqlalchemy import text
from sqlalchemy.exc import SQLAlchemyError

# é…ç½® loguru æ—¥å¿—
logger.remove()  # ç§»é™¤é»˜è®¤å¤„ç†å™¨
logger.add(
    "logs/db_monitor_init_{time:YYYY-MM-DD}.log",
    rotation="1 day",
    retention="30 days",
    level="INFO",
    format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{function}:{line} | {message}",
    encoding="utf-8",
)
logger.add(
    lambda msg: print(msg, end=""),
    level="INFO",
    format="<green>{time:HH:mm:ss.SSS}</green> | <level>{level: <8}</level> | <cyan>{function}</cyan> | {message}",
)


def find_env_file(default_path="mystocks/.env"):
    """
    æ™ºèƒ½æŸ¥æ‰¾ç¯å¢ƒå˜é‡æ–‡ä»¶ï¼Œæ”¯æŒå¤šç§å·¥ä½œç›®å½•

    Args:
        default_path (str): é»˜è®¤ç›¸å¯¹è·¯å¾„

    Returns:
        str: æ‰¾åˆ°çš„ç¯å¢ƒæ–‡ä»¶ç»å¯¹è·¯å¾„

    Raises:
        FileNotFoundError: å¦‚æœæ‰€æœ‰è·¯å¾„éƒ½æ‰¾ä¸åˆ°æ–‡ä»¶
    """
    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))

    # å®šä¹‰å¤šä¸ªå¯èƒ½çš„è·¯å¾„ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
    possible_paths = [
        # 1. å½“å‰å·¥ä½œç›®å½•çš„ç›¸å¯¹è·¯å¾„
        default_path,
        # 2. å½“å‰ç›®å½•çš„ .env æ–‡ä»¶
        ".env",
        # 3. ä¸Šçº§ç›®å½•çš„ mystocks/.env
        "../mystocks/.env",
        # 4. ä»è„šæœ¬ç›®å½•å‘ä¸Šæ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
        os.path.join(script_dir, "../../../mystocks/.env"),
        os.path.join(script_dir, "../../.env"),
        os.path.join(script_dir, "../.env"),
        # 5. å›ºå®šçš„å·²çŸ¥è·¯å¾„
        r"D:\MyData\GITHUB\mystocks\.env",
        # 6. è„šæœ¬ç›®å½•çš„å…„å¼Ÿç›®å½•
        os.path.join(os.path.dirname(script_dir), ".env"),
    ]

    logger.debug("ğŸ” å¼€å§‹æ™ºèƒ½æœç´¢ç¯å¢ƒæ–‡ä»¶ï¼Œé»˜è®¤è·¯å¾„: %s", default_path)
    logger.debug("ğŸ“ è„šæœ¬æ‰€åœ¨ç›®å½•: %s", script_dir)
    logger.debug("ğŸ“‚ å½“å‰å·¥ä½œç›®å½•: %s", os.getcwd())

    for i, path in enumerate(possible_paths, 1):
        try:
            # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            abs_path = os.path.abspath(path)
            logger.debug("ğŸ“‹ [%s/%s] æ£€æŸ¥è·¯å¾„: %s", i, len(possible_paths), abs_path)

            if os.path.exists(abs_path):
                logger.success(f"âœ… æ‰¾åˆ°ç¯å¢ƒæ–‡ä»¶: {abs_path}")
                return abs_path
            else:
                logger.debug("âŒ è·¯å¾„ä¸å­˜åœ¨: %s", abs_path)

        except Exception as e:
            logger.debug("âš ï¸ æ£€æŸ¥è·¯å¾„æ—¶å‡ºé”™: %s - %s", path, str(e))
            continue

    # å¦‚æœæ‰€æœ‰è·¯å¾„éƒ½æ‰¾ä¸åˆ°ï¼ŒæŠ›å‡ºè¯¦ç»†é”™è¯¯
    error_msg = f"""
ç¯å¢ƒå˜é‡æ–‡ä»¶æœªæ‰¾åˆ°ï¼å·²å°è¯•ä»¥ä¸‹è·¯å¾„ï¼š
{"".join([f"  {i}. {os.path.abspath(path)}\n" for i, path in enumerate(possible_paths, 1)])}
è¯·ç¡®ä¿ï¼š
1. .env æ–‡ä»¶å­˜åœ¨äºæ­£ç¡®ä½ç½®
2. å½“å‰å·¥ä½œç›®å½•æ­£ç¡® (å½“å‰: {os.getcwd()})
3. æ–‡ä»¶è·¯å¾„æƒé™æ­£ç¡®
"""

    logger.error(error_msg)
    raise FileNotFoundError(error_msg)


def load_env_config(env_file=None):
    """ä»ç¯å¢ƒå˜é‡æ–‡ä»¶åŠ è½½é…ç½®"""
    # å¦‚æœæ²¡æœ‰æŒ‡å®šè·¯å¾„ï¼Œä½¿ç”¨æ™ºèƒ½æœç´¢
    if env_file is None:
        env_file = find_env_file()
    else:
        # å¦‚æœæŒ‡å®šäº†è·¯å¾„ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™ä½¿ç”¨æ™ºèƒ½æœç´¢
        if not os.path.exists(env_file):
            logger.warning("âš ï¸ æŒ‡å®šçš„ç¯å¢ƒæ–‡ä»¶ä¸å­˜åœ¨: %sï¼Œå°è¯•æ™ºèƒ½æœç´¢...", env_file)
            env_file = find_env_file()
        else:
            env_file = os.path.abspath(env_file)

    logger.info("ğŸ” å¼€å§‹åŠ è½½ç¯å¢ƒé…ç½®æ–‡ä»¶: %s", env_file)
    config = {}
    start_time = time.time()

    try:
        logger.success(f"âœ“ ç¯å¢ƒæ–‡ä»¶å­˜åœ¨: {env_file}")

        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(env_file, "r", encoding="utf-8") as f:
            lines = f.readlines()
            logger.info("ğŸ“„ è¯»å–åˆ° %s è¡Œé…ç½®", len(lines))

            for line_num, line in enumerate(lines, 1):
                line = line.strip()
                # è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
                if not line or line.startswith("#"):
                    continue

                # è§£æé”®å€¼å¯¹
                if "=" in line:
                    key, value = line.split("=", 1)
                    config[key.strip()] = value.strip()
                    logger.debug("ç¬¬%sè¡Œ: åŠ è½½é…ç½® %s", line_num, key.strip())

        host = config.get("MONITOR_DB_HOST") or config.get("POSTGRESQL_HOST")
        user = config.get("MONITOR_DB_USER") or config.get("POSTGRESQL_USER")
        password = config.get("MONITOR_DB_PASSWORD") or config.get("POSTGRESQL_PASSWORD")
        port = config.get("MONITOR_DB_PORT") or config.get("POSTGRESQL_PORT")

        missing_keys = []
        if not host:
            missing_keys.append("MONITOR_DB_HOST/POSTGRESQL_HOST")
        if not user:
            missing_keys.append("MONITOR_DB_USER/POSTGRESQL_USER")
        if not password:
            missing_keys.append("MONITOR_DB_PASSWORD/POSTGRESQL_PASSWORD")
        if not port:
            missing_keys.append("MONITOR_DB_PORT/POSTGRESQL_PORT")

        if missing_keys:
            raise ValueError(f"ç¯å¢ƒå˜é‡æ–‡ä»¶ç¼ºå°‘å¿…è¦é…ç½®: {', '.join(missing_keys)}")

        # æ„å»ºæ•°æ®åº“é…ç½®
        db_config = {
            "user": user,
            "password": password,
            "host": host,
            "port": int(port),
            "database": config.get("MONITOR_DB_DATABASE") or "mystocks_monitoring",
            "admin_database": config.get("MONITOR_DB_ADMIN_DB") or config.get("POSTGRESQL_ADMIN_DB") or "postgres",
        }

        load_time = time.time() - start_time
        logger.success(f"âœ“ ç¯å¢ƒé…ç½®åŠ è½½æˆåŠŸ! è€—æ—¶: {load_time:.3f}s")
        logger.info("ğŸ”— æ•°æ®åº“è¿æ¥ä¿¡æ¯: %s@%s:%s", db_config["user"], db_config["host"], db_config["port"])

        return db_config

    except Exception as e:
        load_time = time.time() - start_time
        logger.error("âŒ åŠ è½½é…ç½®å¤±è´¥ (è€—æ—¶: %ss): %s", load_time, str(e))
        raise


def get_sql_commands(drop_existing=False):
    """ç”ŸæˆPostgreSQL SQLå‘½ä»¤ï¼Œæ”¯æŒåˆ é™¤å·²æœ‰è¡¨é€‰é¡¹"""
    drop_commands = ""
    if drop_existing:
        drop_commands = """
        DROP TABLE IF EXISTS table_validation_log;
        DROP TABLE IF EXISTS table_operation_log;
        DROP TABLE IF EXISTS column_definition_log;
        DROP TABLE IF EXISTS table_creation_log;
        """

    create_table_prefix = "CREATE TABLE IF NOT EXISTS" if not drop_existing else "CREATE TABLE"

    return f"""
{drop_commands}

{create_table_prefix} table_creation_log (
    id SERIAL PRIMARY KEY,
    table_name VARCHAR(255) NOT NULL,
    database_type VARCHAR(20) NOT NULL,
    database_name VARCHAR(255) NOT NULL,
    creation_time TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
    modification_time TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
    status VARCHAR(10) NOT NULL,
    table_parameters JSONB NOT NULL,
    ddl_command TEXT NOT NULL,
    error_message TEXT
);

CREATE INDEX IF NOT EXISTS idx_table_creation_db_type ON table_creation_log (database_type);
CREATE INDEX IF NOT EXISTS idx_table_creation_time ON table_creation_log (creation_time);

{create_table_prefix} column_definition_log (
    id SERIAL PRIMARY KEY,
    table_log_id INT NOT NULL,
    column_name VARCHAR(255) NOT NULL,
    data_type VARCHAR(100) NOT NULL,
    col_length INT,
    col_precision INT,
    col_scale INT,
    is_nullable BOOLEAN DEFAULT TRUE,
    is_primary_key BOOLEAN DEFAULT FALSE,
    default_value VARCHAR(255),
    comment TEXT,
    CONSTRAINT fk_table_log FOREIGN KEY (table_log_id) REFERENCES table_creation_log(id) ON DELETE CASCADE
);

CREATE INDEX IF NOT EXISTS idx_column_table_log_id ON column_definition_log (table_log_id);

{create_table_prefix} table_operation_log (
    id SERIAL PRIMARY KEY,
    operation_id VARCHAR(100) NOT NULL,
    table_name VARCHAR(255) NOT NULL,
    database_type VARCHAR(20) NOT NULL,
    database_name VARCHAR(255) NOT NULL,
    operation_type VARCHAR(50) NOT NULL,
    operation_time TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
    operation_status VARCHAR(20) NOT NULL,
    operation_details JSONB NOT NULL,
    ddl_command TEXT,
    error_message TEXT,
    data_count INT DEFAULT 0,
    duration_seconds NUMERIC(10,3) DEFAULT 0,
    end_time TIMESTAMPTZ NULL,
    UNIQUE (operation_id)
);

CREATE INDEX IF NOT EXISTS idx_operation_time ON table_operation_log (operation_time);
CREATE INDEX IF NOT EXISTS idx_operation_type ON table_operation_log (operation_type);

{create_table_prefix} table_validation_log (
    id SERIAL PRIMARY KEY,
    table_name VARCHAR(255) NOT NULL,
    database_type VARCHAR(20) NOT NULL,
    database_name VARCHAR(255) NOT NULL,
    validation_time TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP,
    validation_status VARCHAR(10) NOT NULL,
    validation_details JSONB NOT NULL,
    issues_found TEXT
);

CREATE INDEX IF NOT EXISTS idx_validation_time ON table_validation_log (validation_time);
"""


def create_database_and_tables(drop_existing=False):
    """åˆ›å»ºæ•°æ®åº“å’Œè¡¨ç»“æ„"""
    logger.info("ğŸš€ å¼€å§‹åˆ›å»ºæ•°æ®åº“å’Œè¡¨ç»“æ„ (drop_existing=%s)", drop_existing)
    start_time = time.time()

    try:
        # ä» env æ–‡ä»¶åŠ è½½é…ç½®
        db_config = load_env_config()

        def is_safe_identifier(name: str) -> bool:
            import re

            return bool(re.fullmatch(r"[A-Za-z_][A-Za-z0-9_]*", name))

        if not is_safe_identifier(db_config["database"]):
            raise ValueError(f"éæ³•æ•°æ®åº“åç§°: {db_config['database']}")

        # è¿æ¥åˆ°ç®¡ç†åº“ï¼Œåˆ›å»ºç›‘æ§æ•°æ®åº“ï¼ˆå¦‚ä¸å­˜åœ¨ï¼‰
        admin_connection_str = (
            f"postgresql+psycopg2://{db_config['user']}:{db_config['password']}@"
            f"{db_config['host']}:{db_config['port']}/{db_config['admin_database']}"
        )

        logger.info("ğŸ”— è¿æ¥ç®¡ç†åº“: %s@%s:%s", db_config["user"], db_config["host"], db_config["port"])

        admin_engine = sqlalchemy.create_engine(admin_connection_str)
        with admin_engine.connect() as admin_connection:
            admin_connection = admin_connection.execution_options(autocommit=True)
            logger.success("âœ“ ç®¡ç†åº“è¿æ¥æˆåŠŸ")

            exists = admin_connection.execute(
                text("SELECT 1 FROM pg_database WHERE datname = :db_name"),
                {"db_name": db_config["database"]},
            ).fetchone()
            if not exists:
                admin_connection.execute(text(f'CREATE DATABASE "{db_config["database"]}"'))
                logger.info("ğŸ“ åˆ›å»ºæ•°æ®åº“: %s", db_config["database"])
            else:
                logger.info("ğŸ“ æ•°æ®åº“å·²å­˜åœ¨: %s", db_config["database"])

        # è¿æ¥ç›‘æ§æ•°æ®åº“
        connection_str = (
            f"postgresql+psycopg2://{db_config['user']}:{db_config['password']}@"
            f"{db_config['host']}:{db_config['port']}/{db_config['database']}"
        )

        logger.info("ğŸ”— è¿æ¥ç›‘æ§æ•°æ®åº“: %s@%s:%s", db_config["user"], db_config["host"], db_config["port"])

        engine = sqlalchemy.create_engine(connection_str)
        with engine.connect() as connection:
            logger.success("âœ“ æ•°æ®åº“è¿æ¥æˆåŠŸ")

            # è·å– SQL å‘½ä»¤
            sql_commands = get_sql_commands(drop_existing=drop_existing).split(";")

            # ç»Ÿè®¡ä¿¡æ¯
            total_commands = len([cmd for cmd in sql_commands if cmd.strip()])
            executed_commands = 0
            failed_commands = 0

            logger.info("ğŸ“„ å°†æ‰§è¡Œ %s æ¡ SQL å‘½ä»¤", total_commands)

            # æ‰§è¡Œ SQL å‘½ä»¤
            for i, cmd in enumerate(sql_commands, 1):
                cmd = cmd.strip()
                if cmd:  # è·³è¿‡ç©ºå‘½ä»¤
                    cmd_start_time = time.time()
                    try:
                        # åˆ¤æ–­å‘½ä»¤ç±»å‹
                        if "CREATE TABLE" in cmd:
                            table_name = extract_table_name(cmd)
                            logger.info("ğŸ“Š [%s/%s] åˆ›å»ºè¡¨: %s", i, total_commands, table_name)
                        elif "DROP TABLE" in cmd:
                            logger.warning("ğŸ—‘ï¸ [%s/%s] åˆ é™¤è¡¨", i, total_commands)
                        else:
                            logger.debug("ğŸ“‹ [%s/%s] æ‰§è¡Œ SQL: %s...", i, total_commands, cmd[:100])

                        connection.execute(text(cmd))
                        cmd_time = time.time() - cmd_start_time
                        executed_commands += 1

                        if cmd_time > 0.1:  # åªè®°å½•è¾ƒæ…¢çš„å‘½ä»¤
                            logger.debug("â±ï¸ å‘½ä»¤æ‰§è¡Œæ—¶é—´: %ss", cmd_time)

                    except Exception as cmd_error:
                        cmd_time = time.time() - cmd_start_time
                        failed_commands += 1
                        logger.error(
                            "âŒ [%s/%s] SQLæ‰§è¡Œå¤±è´¥ (è€—æ—¶: %ss): %s", i, total_commands, cmd_time, str(cmd_error)
                        )
                        logger.debug("å¤±è´¥çš„SQL: %s...", cmd[:200])

        total_time = time.time() - start_time

        # è¾“å‡ºæˆåŠŸç»Ÿè®¡
        logger.success("âœ“ æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ!")
        logger.info("ğŸ“Š æ‰§è¡Œç»Ÿè®¡: æˆåŠŸ %s / å¤±è´¥ %s / æ€»è®¡ %s", executed_commands, failed_commands, total_commands)
        logger.info("â±ï¸ æ€»æ‰§è¡Œæ—¶é—´: %ss", total_time)

        # è¾“å‡ºåˆ›å»ºçš„èµ„æºæ±‡æ€»
        logger.info("ğŸ“¦ åˆ›å»ºçš„èµ„æºæ±‡æ€»:")
        logger.info("  â€¢ æ•°æ®åº“: %s", db_config["database"])
        logger.info("  â€¢ è¡¨ç»“æ„:")
        tables = [
            "table_creation_log - è¡¨åˆ›å»ºæ—¥å¿—è¡¨",
            "column_definition_log - åˆ—å®šä¹‰æ—¥å¿—è¡¨",
            "table_operation_log - è¡¨æ“ä½œæ—¥å¿—è¡¨",
            "table_validation_log - è¡¨ç»“æ„éªŒè¯æ—¥å¿—è¡¨",
        ]
        for table in tables:
            logger.info("    â–« %s", table)

        return True

    except SQLAlchemyError as e:
        total_time = time.time() - start_time
        logger.error("âŒ æ‰§è¡Œ SQL æ—¶å‘ç”Ÿé”™è¯¯ (è€—æ—¶: %ss): %s", total_time, str(e))
        return False
    except Exception as e:
        total_time = time.time() - start_time
        logger.error("âŒ å‘ç”Ÿæ„å¤–é”™è¯¯ (è€—æ—¶: %ss): %s", total_time, str(e))
        return False


def init_monitoring_database(drop_existing=False):
    """
    åˆå§‹åŒ–ç›‘æ§æ•°æ®åº“ï¼ˆä¸“ç”¨äº Jupyter ç¯å¢ƒè°ƒç”¨ï¼‰

    Args:
        drop_existing (bool): æ˜¯å¦åˆ é™¤å·²å­˜åœ¨çš„è¡¨

    Returns:
        bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ

    Examples:
        # åœ¨ Jupyter ä¸­ä½¿ç”¨
        success = init_monitoring_database()

        # åˆ é™¤å·²å­˜åœ¨çš„è¡¨å¹¶é‡å»º
        success = init_monitoring_database(drop_existing=True)
    """
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    os.makedirs("logs", exist_ok=True)

    logger.info("=" * 60)
    logger.info("ğŸ¯ æ•°æ®åº“ç›‘æ§åˆå§‹åŒ–ç¨‹åºå¯åŠ¨ (Jupyter API)")
    logger.info("âš™ï¸ å‚æ•°è®¾ç½®: drop_existing=%s", drop_existing)
    logger.info("=" * 60)

    # æ‰§è¡Œæ•°æ®åº“åˆå§‹åŒ–
    success = create_database_and_tables(drop_existing=drop_existing)

    # ç¨‹åºç»“æŸè®°å½•
    if success:
        logger.success("ğŸ‰ æ•°æ®åº“ç›‘æ§åˆå§‹åŒ–ç¨‹åºæ‰§è¡ŒæˆåŠŸ!")
    else:
        logger.error("ğŸ’¥ æ•°æ®åº“ç›‘æ§åˆå§‹åŒ–ç¨‹åºæ‰§è¡Œå¤±è´¥!")

    logger.info("=" * 60)
    return success


def extract_table_name(sql_cmd):
    """ä» CREATE TABLE å‘½ä»¤ä¸­æå–è¡¨å"""
    try:
        # åŒ¹é… CREATE TABLE [IF NOT EXISTS] table_name
        import re

        pattern = r"CREATE\s+TABLE\s+(?:IF\s+NOT\s+EXISTS\s+)?(\w+)"
        match = re.search(pattern, sql_cmd, re.IGNORECASE)
        return match.group(1) if match else "æœªçŸ¥è¡¨"
    except Exception:
        return "æœªçŸ¥è¡¨"


if __name__ == "__main__":
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    os.makedirs("logs", exist_ok=True)

    # æ£€æµ‹æ˜¯å¦åœ¨ Jupyter ç¯å¢ƒä¸­è¿è¡Œ
    in_jupyter = False
    try:
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ ipykernel
        from IPython import get_ipython

        if get_ipython() is not None:
            in_jupyter = True
    except ImportError:
        pass

    if in_jupyter:
        # åœ¨ Jupyter ç¯å¢ƒä¸­ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°
        logger.info("ğŸ”¬ æ£€æµ‹åˆ° Jupyter ç¯å¢ƒï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
        drop_existing = False
    else:
        # åœ¨å‘½ä»¤è¡Œç¯å¢ƒä¸­ï¼Œè§£æå‘½ä»¤è¡Œå‚æ•°
        parser = argparse.ArgumentParser(description="åˆ›å»ºç›‘æ§æ•°æ®åº“å’Œè¡¨ç»“æ„")
        parser.add_argument("--drop-existing", action="store_true", help="åˆ é™¤å·²å­˜åœ¨çš„è¡¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰")
        args = parser.parse_args()
        drop_existing = args.drop_existing

    # è®°å½•ç¨‹åºå¯åŠ¨
    logger.info("=" * 60)
    logger.info("ğŸ¯ æ•°æ®åº“ç›‘æ§åˆå§‹åŒ–ç¨‹åºå¯åŠ¨")
    logger.info("âš™ï¸ å‚æ•°è®¾ç½®: drop_existing=%s", drop_existing)
    logger.info("ğŸŒ è¿è¡Œç¯å¢ƒ: %s", "Jupyter" if in_jupyter else "Command Line")
    logger.info("=" * 60)

    # æ‰§è¡Œæ•°æ®åº“åˆå§‹åŒ–
    success = create_database_and_tables(drop_existing=drop_existing)

    # ç¨‹åºç»“æŸè®°å½•
    if success:
        logger.success("ğŸ‰ æ•°æ®åº“ç›‘æ§åˆå§‹åŒ–ç¨‹åºæ‰§è¡ŒæˆåŠŸ!")
    else:
        logger.error("ğŸ’¥ æ•°æ®åº“ç›‘æ§åˆå§‹åŒ–ç¨‹åºæ‰§è¡Œå¤±è´¥!")

    logger.info("=" * 60)
