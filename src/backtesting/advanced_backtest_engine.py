"""
é«˜çº§å›æµ‹å¼•æ“ (Advanced Backtest Engine)

Phase 5: é«˜çº§åˆ†æåŠŸèƒ½
- Walk-forwardåˆ†æï¼šæ»šåŠ¨çª—å£éªŒè¯
- Monte Carloæ¨¡æ‹Ÿï¼šé²æ£’æ€§æµ‹è¯•
- ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
- è¿‡æ‹Ÿåˆæ£€æµ‹

ä½œè€…: MyStocksé‡åŒ–äº¤æ˜“å›¢é˜Ÿ
åˆ›å»ºæ—¶é—´: 2025-01-12
ç‰ˆæœ¬: 1.0.0
"""

import logging
import multiprocessing
from concurrent.futures import ProcessPoolExecutor, as_completed
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

from src.ml_strategy.backtest.backtest_engine import BacktestConfig, BacktestEngine
from src.ml_strategy.backtest.performance_metrics import PerformanceMetrics
from src.ml_strategy.backtest.risk_metrics import RiskMetrics


@dataclass
class WalkForwardConfig:
    """Walk-forwardåˆ†æé…ç½®"""

    initial_train_window: int = 252  # åˆå§‹è®­ç»ƒçª—å£ï¼ˆäº¤æ˜“æ—¥ï¼‰
    test_window: int = 63  # æµ‹è¯•çª—å£ï¼ˆäº¤æ˜“æ—¥ï¼‰
    step_size: int = 21  # æ­¥é•¿ï¼ˆäº¤æ˜“æ—¥ï¼‰
    expanding_window: bool = True  # æ˜¯å¦ä½¿ç”¨æ‰©å±•çª—å£
    min_train_window: int = 126  # æœ€å°è®­ç»ƒçª—å£
    max_train_window: int = 504  # æœ€å¤§è®­ç»ƒçª—å£


@dataclass
class MonteCarloConfig:
    """Monte Carloæ¨¡æ‹Ÿé…ç½®"""

    num_simulations: int = 1000  # æ¨¡æ‹Ÿæ¬¡æ•°
    bootstrap_sample_size: Optional[int] = None  # è‡ªä¸¾æ ·æœ¬å¤§å°
    random_seed: int = 42  # éšæœºç§å­
    parallel_processes: Optional[int] = None  # å¹¶è¡Œè¿›ç¨‹æ•°


@dataclass
class AdvancedBacktestConfig:
    """é«˜çº§å›æµ‹é…ç½®"""

    walk_forward: WalkForwardConfig = field(default_factory=WalkForwardConfig)
    monte_carlo: MonteCarloConfig = field(default_factory=MonteCarloConfig)
    base_config: BacktestConfig = field(default_factory=BacktestConfig)
    enable_walk_forward: bool = True
    enable_monte_carlo: bool = True
    confidence_level: float = 0.95  # ç½®ä¿¡æ°´å¹³


class WalkForwardAnalysis:
    """
    Walk-forwardåˆ†æå¼•æ“

    åŠŸèƒ½ï¼š
    - æ»šåŠ¨çª—å£éªŒè¯
    - æ‰©å±•çª—å£éªŒè¯
    - å¤šå‘¨æœŸå›æµ‹
    - è¿‡æ‹Ÿåˆæ£€æµ‹
    """


def __init__(self, config: WalkForwardConfig):
    self.config = config
    self.logger = logging.getLogger(f"{__name__}.WalkForwardAnalysis")


def run_analysis(self, price_data: pd.DataFrame, signals_func: callable, **kwargs) -> Dict[str, Any]:
    """
    æ‰§è¡ŒWalk-forwardåˆ†æ

    å‚æ•°ï¼š
        price_data: ä»·æ ¼æ•°æ®
        signals_func: ä¿¡å·ç”Ÿæˆå‡½æ•°
        **kwargs: ä¼ é€’ç»™ä¿¡å·å‡½æ•°çš„å‚æ•°

    è¿”å›ï¼š
        dict: åˆ†æç»“æœ
    """
    self.logger.info("å¼€å§‹Walk-forwardåˆ†æ")

    # æ•°æ®éªŒè¯
    if not self._validate_data(price_data):
        raise ValueError("ä»·æ ¼æ•°æ®éªŒè¯å¤±è´¥")

    # ç”Ÿæˆåˆ†æçª—å£
    windows = self._generate_analysis_windows(price_data)

    # æ‰§è¡Œé€çª—å£å›æµ‹
    window_results = []
    for i, (train_data, test_data) in enumerate(windows):
        self.logger.info("æ‰§è¡Œçª—å£ {i + 1}/%s")

        # ç”Ÿæˆæµ‹è¯•ä¿¡å·
        test_signals = signals_func(test_data, **kwargs)

        # æ‰§è¡Œå›æµ‹
        backtest_engine = BacktestEngine()
        result = backtest_engine.run(test_data, test_signals)

        # è®°å½•çª—å£ä¿¡æ¯
        window_result = {
            "window_id": i + 1,
            "train_period": (train_data.index[0], train_data.index[-1]),
            "test_period": (test_data.index[0], test_data.index[-1]),
            "train_size": len(train_data),
            "test_size": len(test_data),
            "result": result,
        }
        window_results.append(window_result)

    # æ±‡æ€»åˆ†æç»“æœ
    summary = self._summarize_results(window_results)

    result = {
        "config": self.config,
        "windows": window_results,
        "summary": summary,
        "analysis_timestamp": datetime.now(),
    }

    self.logger.info("Walk-forwardåˆ†æå®Œæˆ")
    return result


def _validate_data(self, price_data: pd.DataFrame) -> bool:
    """éªŒè¯ä»·æ ¼æ•°æ®"""
    required_columns = ["open", "high", "low", "close", "volume"]
    if not all(col in price_data.columns for col in required_columns):
        self.logger.error("ç¼ºå°‘å¿…éœ€åˆ—: %(required_columns)s")
        return False

    min_required_periods = self.config.initial_train_window + self.config.test_window
    if len(price_data) < min_required_periods:
        self.logger.error("æ•°æ®é•¿åº¦ä¸è¶³: éœ€è¦è‡³å°‘%(min_required_periods)sä¸ªå‘¨æœŸ")
        return False

    return True


def _generate_analysis_windows(self, price_data: pd.DataFrame) -> List[Tuple[pd.DataFrame, pd.DataFrame]]:
    """ç”Ÿæˆåˆ†æçª—å£"""
    windows = []
    total_periods = len(price_data)

    start_idx = 0

    while True:
        if self.config.expanding_window:
            # æ‰©å±•çª—å£ï¼šè®­ç»ƒçª—å£é€æ¸æ‰©å¤§
            train_end_idx = min(start_idx + self.config.initial_train_window, total_periods)
        else:
            # æ»šåŠ¨çª—å£ï¼šè®­ç»ƒçª—å£å›ºå®šå¤§å°
            train_end_idx = start_idx + self.config.initial_train_window

        test_end_idx = train_end_idx + self.config.test_window

        if test_end_idx > total_periods:
            break

        train_data = price_data.iloc[start_idx:train_end_idx]
        test_data = price_data.iloc[train_end_idx:test_end_idx]

        windows.append((train_data, test_data))

        start_idx += self.config.step_size

    self.logger.info("ç”Ÿæˆäº† {len(windows)} ä¸ªåˆ†æçª—å£")
    return windows


def _summarize_results(self, window_results: List[Dict]) -> Dict[str, Any]:
    """æ±‡æ€»åˆ†æç»“æœ"""
    if not window_results:
        return {}

    # æå–å„é¡¹æŒ‡æ ‡
    total_returns = []
    sharpe_ratios = []
    max_drawdowns = []
    win_rates = []

    for w in window_results:
        metrics = w["result"]["metrics"]
        total_returns.append(metrics.get("total_return", 0))
        sharpe_ratios.append(metrics.get("sharpe_ratio", 0))
        max_drawdowns.append(metrics.get("max_drawdown", 0))
        win_rates.append(metrics.get("win_rate", 0))

    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    summary = {
        "total_windows": len(window_results),
        "total_return": {
            "mean": np.mean(total_returns),
            "std": np.std(total_returns),
            "min": np.min(total_returns),
            "max": np.max(total_returns),
            "median": np.median(total_returns),
        },
        "sharpe_ratio": {
            "mean": np.mean(sharpe_ratios),
            "std": np.std(sharpe_ratios),
            "min": np.min(sharpe_ratios),
            "max": np.max(sharpe_ratios),
            "median": np.median(sharpe_ratios),
        },
        "max_drawdown": {
            "mean": np.mean(max_drawdowns),
            "std": np.std(max_drawdowns),
            "min": np.min(max_drawdowns),
            "max": np.max(max_drawdowns),
            "median": np.median(max_drawdowns),
        },
        "win_rate": {
            "mean": np.mean(win_rates),
            "std": np.std(win_rates),
            "min": np.min(win_rates),
            "max": np.max(win_rates),
            "median": np.median(win_rates),
        },
        "robustness_score": self._calculate_robustness_score(total_returns),
        "consistency_score": self._calculate_consistency_score(total_returns),
    }

    return summary


def _calculate_robustness_score(self, returns: List[float]) -> float:
    """è®¡ç®—é²æ£’æ€§å¾—åˆ†ï¼ˆæ­£æ”¶ç›Šçª—å£æ¯”ä¾‹ï¼‰"""
    positive_returns = [r for r in returns if r > 0]
    return len(positive_returns) / len(returns) if returns else 0


def _calculate_consistency_score(self, returns: List[float]) -> float:
    """è®¡ç®—ä¸€è‡´æ€§å¾—åˆ†ï¼ˆæ”¶ç›Šæ ‡å‡†å·®çš„å€’æ•°ï¼‰"""
    if not returns or np.std(returns) == 0:
        return 0
    return 1 / np.std(returns)


class MonteCarloSimulation:
    """
    Monte Carloæ¨¡æ‹Ÿå¼•æ“

    åŠŸèƒ½ï¼š
    - è‡ªä¸¾é‡é‡‡æ ·
    - æ”¶ç›Šåˆ†å¸ƒåˆ†æ
    - é£é™©æ¦‚ç‡ä¼°è®¡
    - å¹¶è¡Œå¤„ç†æ”¯æŒ
    """


def __init__(self, config: MonteCarloConfig):
    self.config = config
    self.logger = logging.getLogger(f"{__name__}.MonteCarloSimulation")

    # è®¾ç½®éšæœºç§å­
    np.random.seed(self.config.random_seed)

    # è®¾ç½®å¹¶è¡Œè¿›ç¨‹æ•°
    if self.config.parallel_processes is None:
        self.config.parallel_processes = max(1, multiprocessing.cpu_count() - 1)


def run_simulation(self, returns: pd.Series, simulation_func: callable, **kwargs) -> Dict[str, Any]:
    """
    æ‰§è¡ŒMonte Carloæ¨¡æ‹Ÿ

    å‚æ•°ï¼š
        returns: å†å²æ”¶ç›Šç‡åºåˆ—
        simulation_func: æ¨¡æ‹Ÿå‡½æ•°
        **kwargs: ä¼ é€’ç»™æ¨¡æ‹Ÿå‡½æ•°çš„å‚æ•°

    è¿”å›ï¼š
        dict: æ¨¡æ‹Ÿç»“æœ
    """
    self.logger.info("å¼€å§‹Monte Carloæ¨¡æ‹Ÿ: {self.config.num_simulations} æ¬¡")

    # æ•°æ®éªŒè¯
    if len(returns) < 30:
        raise ValueError("æ”¶ç›Šç‡æ•°æ®é•¿åº¦ä¸è¶³ï¼Œè‡³å°‘éœ€è¦30ä¸ªè§‚æµ‹å€¼")

    # æ‰§è¡Œæ¨¡æ‹Ÿï¼ˆæš‚æ—¶ä½¿ç”¨é¡ºåºæ‰§è¡Œä»¥é¿å…multiprocessingé—®é¢˜ï¼‰
    results = self._run_sequential_simulations(returns, simulation_func, **kwargs)

    # åˆ†æç»“æœ
    analysis = self._analyze_simulation_results(results)

    result = {
        "config": self.config,
        "simulation_results": results,
        "analysis": analysis,
        "simulation_timestamp": datetime.now(),
    }

    self.logger.info("Monte Carloæ¨¡æ‹Ÿå®Œæˆ")
    return result


def _run_parallel_simulations(self, returns: pd.Series, simulation_func: callable, **kwargs) -> List[Dict]:
    """å¹¶è¡Œæ‰§è¡Œæ¨¡æ‹Ÿ"""
    results = []

    with ProcessPoolExecutor(max_workers=self.config.parallel_processes) as executor:
        # æäº¤æ‰€æœ‰æ¨¡æ‹Ÿä»»åŠ¡
        future_to_sim = {
            executor.submit(self._single_simulation, returns, simulation_func, sim_id, **kwargs): sim_id
            for sim_id in range(self.config.num_simulations)
        }

        # æ”¶é›†ç»“æœ
        for future in as_completed(future_to_sim):
            sim_id = future_to_sim[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as exc:
                self.logger.error("æ¨¡æ‹Ÿ %(sim_id)s å¤±è´¥: %(exc)s")
                results.append({"simulation_id": sim_id, "error": str(exc)})

    return sorted(results, key=lambda x: x.get("simulation_id", 0))


def _run_sequential_simulations(self, returns: pd.Series, simulation_func: callable, **kwargs) -> List[Dict]:
    """é¡ºåºæ‰§è¡Œæ¨¡æ‹Ÿ"""
    results = []

    for sim_id in range(self.config.num_simulations):
        try:
            result = self._single_simulation(returns, simulation_func, sim_id, **kwargs)
            results.append(result)
        except Exception as exc:
            self.logger.error("æ¨¡æ‹Ÿ %(sim_id)s å¤±è´¥: %(exc)s")
            results.append({"simulation_id": sim_id, "error": str(exc)})

    return results


def _single_simulation(self, returns: pd.Series, simulation_func: callable, sim_id: int, **kwargs) -> Dict:
    """å•ä¸ªæ¨¡æ‹Ÿæ‰§è¡Œ"""
    # è‡ªä¸¾é‡é‡‡æ ·
    bootstrap_returns = self._bootstrap_sample(returns)

    # æ‰§è¡Œæ¨¡æ‹Ÿå‡½æ•°
    result = simulation_func(bootstrap_returns, **kwargs)

    return {
        "simulation_id": sim_id,
        "bootstrap_returns": bootstrap_returns,
        "result": result,
    }


def _bootstrap_sample(self, returns: pd.Series) -> pd.Series:
    """è‡ªä¸¾é‡é‡‡æ ·"""
    sample_size = self.config.bootstrap_sample_size or len(returns)
    indices = np.random.choice(len(returns), size=sample_size, replace=True)
    return returns.iloc[indices].reset_index(drop=True)


def _analyze_simulation_results(self, results: List[Dict]) -> Dict[str, Any]:
    """åˆ†ææ¨¡æ‹Ÿç»“æœ"""
    # è¿‡æ»¤å‡ºæˆåŠŸçš„æ¨¡æ‹Ÿ
    successful_results = [r for r in results if "error" not in r]

    if not successful_results:
        return {"error": "æ‰€æœ‰æ¨¡æ‹Ÿéƒ½å¤±è´¥äº†"}

    # æå–å„é¡¹æŒ‡æ ‡
    total_returns = []
    sharpe_ratios = []
    max_drawdowns = []
    win_rates = []

    for result in successful_results:
        metrics = result["result"].get("metrics", {})
        total_returns.append(metrics.get("total_return", 0))
        sharpe_ratios.append(metrics.get("sharpe_ratio", 0))
        max_drawdowns.append(metrics.get("max_drawdown", 0))
        win_rates.append(metrics.get("win_rate", 0))

    # è®¡ç®—ç»Ÿè®¡åˆ†å¸ƒ
    analysis = {
        "total_simulations": len(results),
        "successful_simulations": len(successful_results),
        "success_rate": len(successful_results) / len(results),
        "total_return_distribution": {
            "mean": np.mean(total_returns),
            "std": np.std(total_returns),
            "percentiles": {
                "5th": np.percentile(total_returns, 5),
                "25th": np.percentile(total_returns, 25),
                "50th": np.percentile(total_returns, 50),
                "75th": np.percentile(total_returns, 75),
                "95th": np.percentile(total_returns, 95),
            },
            "var_95": np.percentile(total_returns, 5),  # Value at Risk
            "cvar_95": np.mean([r for r in total_returns if r <= np.percentile(total_returns, 5)]),  # Conditional VaR
        },
        "sharpe_ratio_distribution": {
            "mean": np.mean(sharpe_ratios),
            "std": np.std(sharpe_ratios),
            "percentiles": {
                "5th": np.percentile(sharpe_ratios, 5),
                "25th": np.percentile(sharpe_ratios, 25),
                "50th": np.percentile(sharpe_ratios, 50),
                "75th": np.percentile(sharpe_ratios, 75),
                "95th": np.percentile(sharpe_ratios, 95),
            },
        },
        "max_drawdown_distribution": {
            "mean": np.mean(max_drawdowns),
            "std": np.std(max_drawdowns),
            "worst_case": np.max(max_drawdowns),
            "percentiles": {
                "5th": np.percentile(max_drawdowns, 5),
                "25th": np.percentile(max_drawdowns, 25),
                "50th": np.percentile(max_drawdowns, 50),
                "75th": np.percentile(max_drawdowns, 75),
                "95th": np.percentile(max_drawdowns, 95),
            },
        },
        "win_rate_distribution": {
            "mean": np.mean(win_rates),
            "std": np.std(win_rates),
            "percentiles": {
                "5th": np.percentile(win_rates, 5),
                "25th": np.percentile(win_rates, 25),
                "50th": np.percentile(win_rates, 50),
                "75th": np.percentile(win_rates, 75),
                "95th": np.percentile(win_rates, 95),
            },
        },
        "probability_analysis": {
            "prob_positive_return": np.mean([1 if r > 0 else 0 for r in total_returns]),
            "prob_sharpe_gt_1": np.mean([1 if r > 1 else 0 for r in sharpe_ratios]),
            "prob_max_dd_lt_20pct": np.mean([1 if r < 0.20 else 0 for r in max_drawdowns]),
        },
    }

    return analysis


def _create_simulation_function(self, price_data: pd.DataFrame, signals_func: callable, **kwargs):
    """åˆ›å»ºå¯åºåˆ—åŒ–çš„æ¨¡æ‹Ÿå‡½æ•°ï¼ˆç”¨äºMonte Carloå¹¶è¡Œå¤„ç†ï¼‰"""

    # ä¸ºäº†é¿å…multiprocessingçš„pickleé—®é¢˜ï¼Œæˆ‘ä»¬è¿”å›ä¸€ä¸ªé™æ€æ–¹æ³•
    def simulation_wrapper(bootstrap_returns, **sim_kwargs):
        # ä½¿ç”¨ç›¸åŒçš„ä¿¡å·å‡½æ•°è¿›è¡Œæ¨¡æ‹Ÿ
        simulated_signals = signals_func(price_data, **sim_kwargs)
        # æ‰§è¡Œå›æµ‹
        backtest_engine = BacktestEngine()
        sim_result = backtest_engine.run(price_data, simulated_signals)
        return sim_result

    return simulation_wrapper


class AdvancedBacktestEngine:
    """
    é«˜çº§å›æµ‹å¼•æ“ä¸»æ§åˆ¶å™¨

    åŠŸèƒ½ï¼š
    - æ•´åˆWalk-forwardåˆ†æå’ŒMonte Carloæ¨¡æ‹Ÿ
    - ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
    - è¿‡æ‹Ÿåˆæ£€æµ‹
    - ç»¼åˆæŠ¥å‘Šç”Ÿæˆ
    """


def __init__(self, config: AdvancedBacktestConfig):
    self.config = config
    self.walk_forward = WalkForwardAnalysis(config.walk_forward) if config.enable_walk_forward else None
    self.monte_carlo = MonteCarloSimulation(config.monte_carlo) if config.enable_monte_carlo else None

    self.logger = logging.getLogger(f"{__name__}.AdvancedBacktestEngine")

    # åŸºç¡€ç»„ä»¶
    self.base_engine = BacktestEngine(config.base_config)
    self.perf_metrics = PerformanceMetrics()
    self.risk_metrics = RiskMetrics()


def run_advanced_backtest(
    self,
    price_data: pd.DataFrame,
    signals_func: callable,
    benchmark_returns: Optional[pd.Series] = None,
    **kwargs,
) -> Dict[str, Any]:
    """
    æ‰§è¡Œé«˜çº§å›æµ‹åˆ†æ

    å‚æ•°ï¼š
        price_data: ä»·æ ¼æ•°æ®
        signals_func: ä¿¡å·ç”Ÿæˆå‡½æ•°
        benchmark_returns: åŸºå‡†æ”¶ç›Šç‡
        **kwargs: ä¼ é€’ç»™ä¿¡å·å‡½æ•°çš„å‚æ•°

    è¿”å›ï¼š
        dict: å®Œæ•´é«˜çº§å›æµ‹ç»“æœ
    """
    self.logger.info("å¼€å§‹é«˜çº§å›æµ‹åˆ†æ")

    results = {
        "config": self.config,
        "timestamp": datetime.now(),
        "base_backtest": None,
        "walk_forward_analysis": None,
        "monte_carlo_analysis": None,
        "statistical_tests": None,
        "overfitting_analysis": None,
        "comprehensive_report": None,
    }

    try:
        self.logger.info("æ‰§è¡ŒåŸºç¡€å›æµ‹")
        base_signals = signals_func(price_data, **kwargs)
        base_result = self.base_engine.run(price_data, base_signals, benchmark_returns)
        results["base_backtest"] = base_result

        if self.walk_forward:
            self.logger.info("æ‰§è¡ŒWalk-forwardåˆ†æ")
            wf_result = self.walk_forward.run_analysis(price_data, signals_func, **kwargs)
            results["walk_forward_analysis"] = wf_result

        if self.monte_carlo:
            self.logger.info("æ‰§è¡ŒMonte Carloæ¨¡æ‹Ÿ")
            daily_returns = base_result["backtest"]["daily_returns"]

            # åˆ›å»ºå¯åºåˆ—åŒ–çš„æ¨¡æ‹Ÿå‡½æ•°
            simulation_func = self._create_simulation_function(price_data, signals_func, **kwargs)
            mc_result = self.monte_carlo.run_simulation(daily_returns, simulation_func, **kwargs)
            results["monte_carlo_analysis"] = mc_result

        self.logger.info("æ‰§è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ")
        stat_tests = self._perform_statistical_tests(results)
        results["statistical_tests"] = stat_tests

        self.logger.info("æ‰§è¡Œè¿‡æ‹Ÿåˆæ£€æµ‹")
        overfitting_analysis = self._detect_overfitting(results)
        results["overfitting_analysis"] = overfitting_analysis

        self.logger.info("ç”Ÿæˆç»¼åˆæŠ¥å‘Š")
        comprehensive_report = self._generate_comprehensive_report(results)
        results["comprehensive_report"] = comprehensive_report

    except Exception as e:
        self.logger.error("é«˜çº§å›æµ‹åˆ†æå¤±è´¥: %(e)s")
        results["error"] = str(e)

    self.logger.info("é«˜çº§å›æµ‹åˆ†æå®Œæˆ")
    return results


def _perform_statistical_tests(self, results: Dict[str, Any]) -> Dict[str, Any]:
    """æ‰§è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""
    tests = {}

    if results.get("walk_forward_analysis"):
        wf_summary = results["walk_forward_analysis"]["summary"]

        # tæ£€éªŒï¼šæ£€éªŒå¹³å‡æ”¶ç›Šç‡æ˜¯å¦æ˜¾è‘—å¤§äº0
        mean_return = wf_summary["total_return"]["mean"]
        std_return = wf_summary["total_return"]["std"]
        n_windows = wf_summary["total_windows"]

        if std_return > 0:
            t_stat = mean_return / (std_return / np.sqrt(n_windows))
            p_value = 2 * (1 - self._t_cdf(abs(t_stat), n_windows - 1))

            tests["return_significance"] = {
                "t_statistic": t_stat,
                "p_value": p_value,
                "significant_at_95pct": p_value < 0.05,
                "significant_at_99pct": p_value < 0.01,
            }

    if results.get("monte_carlo_analysis"):
        mc_analysis = results["monte_carlo_analysis"]["analysis"]
        base_return = results["base_backtest"]["metrics"]["total_return"]

        # è®¡ç®—åœ¨Monte Carloåˆ†å¸ƒä¸­çš„åˆ†ä½æ•°
        return_distribution = mc_analysis["total_return_distribution"]
        percentile_5th = return_distribution["percentiles"]["5th"]
        percentile_95th = return_distribution["percentiles"]["95th"]

        tests["monte_carlo_percentile"] = {
            "base_return": base_return,
            "percentile_5th": percentile_5th,
            "percentile_95th": percentile_95th,
            "within_90pct_confidence": percentile_5th <= base_return <= percentile_95th,
            "probability_better_than_random": np.mean(
                [
                    1 if r > 0 else 0
                    for r in [
                        result["result"]["metrics"]["total_return"]
                        for result in results["monte_carlo_analysis"]["simulation_results"]
                        if "result" in result
                    ]
                ]
            ),
        }

    return tests


def _detect_overfitting(self, results: Dict[str, Any]) -> Dict[str, Any]:
    """æ£€æµ‹è¿‡æ‹Ÿåˆ"""
    analysis = {}

    if results.get("walk_forward_analysis") and results.get("base_backtest"):
        wf_summary = results["walk_forward_analysis"]["summary"]
        base_metrics = results["base_backtest"]["metrics"]

        # æ¯”è¾ƒè®­ç»ƒæœŸå’Œæµ‹è¯•æœŸè¡¨ç°
        wf_avg_return = wf_summary["total_return"]["mean"]
        base_return = base_metrics["total_return"]

        # è¿‡æ‹ŸåˆæŒ‡æ ‡
        overfitting_ratio = base_return / wf_avg_return if wf_avg_return != 0 else float("inf")

        analysis.update(
            {
                "overfitting_ratio": overfitting_ratio,
                "is_overfitted": overfitting_ratio > 2.0,  # ç»éªŒé˜ˆå€¼
                "base_return": base_return,
                "walk_forward_avg_return": wf_avg_return,
                "performance_decay": wf_avg_return - base_return,
            }
        )

    # ç¨³å®šæ€§åˆ†æ
    if results.get("monte_carlo_analysis"):
        mc_analysis = results["monte_carlo_analysis"]["analysis"]
        return_std = mc_analysis["total_return_distribution"]["std"]
        return_mean = mc_analysis["total_return_distribution"]["mean"]

        # å˜å¼‚ç³»æ•° (CV)
        coefficient_of_variation = return_std / abs(return_mean) if return_mean != 0 else float("inf")

        analysis.update(
            {
                "coefficient_of_variation": coefficient_of_variation,
                "high_variability": coefficient_of_variation > 1.0,  # ç»éªŒé˜ˆå€¼
                "return_stability_score": 1 / (1 + coefficient_of_variation),  # 0-1ä¹‹é—´çš„ç¨³å®šæ€§å¾—åˆ†
            }
        )

    return analysis


def _generate_comprehensive_report(self, results: Dict[str, Any]) -> str:
    """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
    report_lines = [
        "=" * 80,
        "é«˜çº§å›æµ‹åˆ†æç»¼åˆæŠ¥å‘Š",
        "=" * 80,
        f"åˆ†ææ—¶é—´: {results['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}",
        "",
    ]

    # åŸºç¡€å›æµ‹ç»“æœ
    if results.get("base_backtest"):
        base = results["base_backtest"]["metrics"]
        report_lines.extend(
            [
                "ğŸ“Š åŸºç¡€å›æµ‹ç»“æœ:",
                f"  æ€»æ”¶ç›Šç‡: {base['total_return']:.2%}",
                f"  å¹´åŒ–æ”¶ç›Šç‡: {base['annualized_return']:.2%}",
                f"  å¤æ™®æ¯”ç‡: {base['sharpe_ratio']:.3f}",
                f"  æœ€å¤§å›æ’¤: {base['max_drawdown']:.2%}",
                f"  èƒœç‡: {base['win_rate']:.2%}",
                "",
            ]
        )

    # Walk-forwardåˆ†æç»“æœ
    if results.get("walk_forward_analysis"):
        wf = results["walk_forward_analysis"]["summary"]
        report_lines.extend(
            [
                "ğŸ”„ Walk-forwardåˆ†æç»“æœ:",
                f"  åˆ†æçª—å£æ•°: {wf['total_windows']}",
                f"  å¹³å‡æ”¶ç›Šç‡: {wf['total_return']['mean']:.2%}",
                f"  é²æ£’æ€§å¾—åˆ†: {wf['robustness_score']:.2%}",
                f"  ä¸€è‡´æ€§å¾—åˆ†: {wf['consistency_score']:.3f}",
                "",
            ]
        )

    # Monte Carloåˆ†æç»“æœ
    if results.get("monte_carlo_analysis"):
        mc = results["monte_carlo_analysis"]["analysis"]
        report_lines.extend(
            [
                "ğŸ² Monte Carloæ¨¡æ‹Ÿç»“æœ:",
                f"  æ¨¡æ‹Ÿæ¬¡æ•°: {mc['successful_simulations']}/{mc['total_simulations']}",
                f"  å¹³å‡æ”¶ç›Šç‡: {mc['total_return_distribution']['mean']:.2%}",
                f"  95% VaR: {mc['total_return_distribution']['var_95']:.2%}",
                f"  æ­£æ”¶ç›Šæ¦‚ç‡: {mc['probability_analysis']['prob_positive_return']:.2%}",
                "",
            ]
        )

    # ç»Ÿè®¡æ£€éªŒç»“æœ
    if results.get("statistical_tests"):
        stat = results["statistical_tests"]
        report_lines.extend(
            [
                "ğŸ“ˆ ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ:",
            ]
        )

        if "return_significance" in stat:
            sig = stat["return_significance"]
            report_lines.extend(
                [
                    "  æ”¶ç›Šç‡æ˜¾è‘—æ€§ (tæ£€éªŒ):",
                    f"    tç»Ÿè®¡é‡: {sig['t_statistic']:.3f}",
                    f"    på€¼: {sig['p_value']:.4f}",
                    f"    95%æ˜¾è‘—: {'æ˜¯' if sig['significant_at_95pct'] else 'å¦'}",
                    f"    99%æ˜¾è‘—: {'æ˜¯' if sig['significant_at_99pct'] else 'å¦'}",
                ]
            )

        if "monte_carlo_percentile" in stat:
            mc_pct = stat["monte_carlo_percentile"]
            report_lines.extend(
                [
                    "  Monte Carloåˆ†ä½æ•°åˆ†æ:",
                    f"    åŸºå‡†æ”¶ç›Šç‡: {
                        mc_pct['base_return']:.2%}",
                    f"    90%ç½®ä¿¡åŒºé—´: [{
                        mc_pct['percentile_5th']:.2%}, {
                        mc_pct['percentile_95th']:.2%}]",
                    f"    åœ¨ç½®ä¿¡åŒºé—´å†…: {
                        'æ˜¯' if mc_pct['within_90pct_confidence'] else 'å¦'}",
                ]
            )

        report_lines.append("")

    # è¿‡æ‹Ÿåˆæ£€æµ‹ç»“æœ
    if results.get("overfitting_analysis"):
        of = results["overfitting_analysis"]
        report_lines.extend(
            [
                "ğŸ¯ è¿‡æ‹Ÿåˆæ£€æµ‹:",
                f"  è¿‡æ‹Ÿåˆæ¯”ç‡: {of.get('overfitting_ratio', 'N/A')}",
                f"  æ˜¯å¦è¿‡æ‹Ÿåˆ: {'æ˜¯' if of.get('is_overfitted', False) else 'å¦'}",
                f"  å˜å¼‚ç³»æ•°: {of.get('coefficient_of_variation', 'N/A')}",
                f"  ç¨³å®šæ€§å¾—åˆ†: {of.get('return_stability_score', 'N/A')}",
                "",
            ]
        )

    # ç»“è®ºå’Œå»ºè®®
    report_lines.extend(
        [
            "ğŸ“‹ ç»“è®ºå’Œå»ºè®®:",
        ]
    )

    # åŸºäºåˆ†æç»“æœç»™å‡ºå»ºè®®
    if results.get("statistical_tests"):
        stat = results["statistical_tests"]
        if stat.get("return_significance", {}).get("significant_at_95pct", False):
            report_lines.append("  âœ… ç­–ç•¥æ”¶ç›Šç‡åœ¨ç»Ÿè®¡ä¸Šæ˜¾è‘—ï¼Œè¡¨ç°è‰¯å¥½")
        else:
            report_lines.append("  âš ï¸ ç­–ç•¥æ”¶ç›Šç‡ç»Ÿè®¡æ˜¾è‘—æ€§ä¸è¶³ï¼Œéœ€è°¨æ…")

    if results.get("overfitting_analysis"):
        of = results["overfitting_analysis"]
        if of.get("is_overfitted", False):
            report_lines.append("  âš ï¸ æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆè¿¹è±¡ï¼Œå»ºè®®è°ƒæ•´ç­–ç•¥å¤æ‚åº¦")
        else:
            report_lines.append("  âœ… æœªæ£€æµ‹åˆ°æ˜æ˜¾è¿‡æ‹Ÿåˆï¼Œç­–ç•¥ç¨³å®šæ€§è‰¯å¥½")

    report_lines.extend(["", "=" * 80])

    return "\n".join(report_lines)


def _t_cdf(self, t: float, df: int) -> float:
    """è®¡ç®—tåˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆç®€åŒ–å®ç°ï¼‰"""
    # ä½¿ç”¨æ­£æ€åˆ†å¸ƒè¿‘ä¼¼ï¼ˆå¯¹äºå¤§æ ·æœ¬ï¼‰
    from scipy.stats import t as t_dist

    return t_dist.cdf(t, df)


# ä¾¿æ·å‡½æ•°
def create_advanced_backtest_engine(
    enable_walk_forward: bool = True,
    enable_monte_carlo: bool = True,
    num_simulations: int = 1000,
    initial_train_window: int = 252,
    test_window: int = 63,
) -> AdvancedBacktestEngine:
    """
    åˆ›å»ºé«˜çº§å›æµ‹å¼•æ“çš„ä¾¿æ·å‡½æ•°

    å‚æ•°ï¼š
        enable_walk_forward: æ˜¯å¦å¯ç”¨Walk-forwardåˆ†æ
        enable_monte_carlo: æ˜¯å¦å¯ç”¨Monte Carloæ¨¡æ‹Ÿ
        num_simulations: Monte Carloæ¨¡æ‹Ÿæ¬¡æ•°
        initial_train_window: åˆå§‹è®­ç»ƒçª—å£
        test_window: æµ‹è¯•çª—å£

    è¿”å›ï¼š
        AdvancedBacktestEngine: é…ç½®å¥½çš„é«˜çº§å›æµ‹å¼•æ“
    """
    config = AdvancedBacktestConfig(
        walk_forward=WalkForwardConfig(initial_train_window=initial_train_window, test_window=test_window),
        monte_carlo=MonteCarloConfig(num_simulations=num_simulations),
        enable_walk_forward=enable_walk_forward,
        enable_monte_carlo=enable_monte_carlo,
    )

    return AdvancedBacktestEngine(config)


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("é«˜çº§å›æµ‹å¼•æ“æµ‹è¯•")
    print("=" * 80)

    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    np.random.seed(42)
    test_n = 500
    test_dates = pd.date_range("2023-01-01", periods=test_n, freq="D")

    # ä»·æ ¼æ•°æ®
    test_close_prices = 100 + np.cumsum(np.random.randn(test_n) * 0.5 + 0.01)
    test_price_data = pd.DataFrame(
        {
            "open": test_close_prices + np.random.randn(test_n) * 0.3,
            "high": test_close_prices + np.abs(np.random.randn(test_n)) * 0.5,
            "low": test_close_prices - np.abs(np.random.randn(test_n)) * 0.5,
            "close": test_close_prices,
            "volume": np.random.uniform(1000000, 10000000, test_n),
        },
        index=test_dates,
    )

    # ç®€å•ä¿¡å·å‡½æ•°ï¼ˆç¤ºä¾‹ï¼‰
    def simple_signal_func(price_data, **kwargs):
        signals = pd.DataFrame(index=price_data.index)
        signals["signal"] = None
        signals["strength"] = 0.0

        close_prices = price_data["close"]
        sma_20 = close_prices.rolling(20).mean()
        sma_50 = close_prices.rolling(50).mean()

        # ç®€å•çš„å‡çº¿äº¤å‰ç­–ç•¥
        for i in range(len(close_prices)):
            if i >= 50:
                if sma_20.iloc[i] > sma_50.iloc[i] and sma_20.iloc[i - 1] <= sma_50.iloc[i - 1]:
                    signals.iloc[i] = ["buy", 0.8]
                elif sma_20.iloc[i] < sma_50.iloc[i] and sma_20.iloc[i - 1] >= sma_50.iloc[i - 1]:
                    signals.iloc[i] = ["sell", 0.8]

        return signals

    # Create advanced backtest engine
    engine = create_advanced_backtest_engine(
        enable_walk_forward=True,
        enable_monte_carlo=True,
        num_simulations=100,
        initial_train_window=200,
        test_window=50,
    )

    # Run advanced backtest
    print("Running advanced backtest analysis...")
    results = engine.run_advanced_backtest(test_price_data, simple_signal_func)

    # Print comprehensive report
    if "comprehensive_report" in results and results["comprehensive_report"]:
        print(results["comprehensive_report"])
    else:
        print("Analysis failed or no report generated")

    print("\nTest completed!")
