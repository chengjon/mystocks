"""
# åŠŸèƒ½ï¼šç›‘æ§æ•°æ®åº“æ¨¡å—ï¼Œç‹¬ç«‹è®°å½•æ‰€æœ‰æ“ä½œæ—¥å¿—å’ŒæŒ‡æ ‡
# ä½œè€…ï¼šJohnC (ninjas@sina.com) & Claude
# åˆ›å»ºæ—¥æœŸï¼š2025-10-16
# ç‰ˆæœ¬ï¼š2.1.0
# ä¾èµ–ï¼šè¯¦è§requirements.txtæˆ–æ–‡ä»¶å¯¼å…¥éƒ¨åˆ†
# æ³¨æ„äº‹é¡¹ï¼š
#   æœ¬æ–‡ä»¶æ˜¯MyStocks v2.1æ ¸å¿ƒç»„ä»¶ï¼Œéµå¾ª5-tieræ•°æ®åˆ†ç±»æ¶æ„
# ç‰ˆæƒï¼šMyStocks Project Â© 2025
"""

import uuid
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from contextlib import contextmanager

from db_manager.connection_manager import DatabaseConnectionManager

logger = logging.getLogger(__name__)


class MonitoringDatabase:
    """
    ç›‘æ§æ•°æ®åº“è®¿é—®ç±»

    è´Ÿè´£å°†æ‰€æœ‰ç›‘æ§æ•°æ®å†™å…¥ç‹¬ç«‹çš„ç›‘æ§æ•°æ®åº“ã€‚
    """

    def __init__(self, enable_monitoring: bool = True):
        """
        åˆå§‹åŒ–ç›‘æ§æ•°æ®åº“

        Args:
            enable_monitoring: æ˜¯å¦å¯ç”¨ç›‘æ§ (é»˜è®¤True)
        """
        self.enable_monitoring = enable_monitoring
        self.conn_manager = DatabaseConnectionManager()
        self._write_failures = 0
        self._total_writes = 0

        logger.info(f"âœ… MonitoringDatabase initialized (enabled={enable_monitoring})")

    @contextmanager
    def _get_connection(self):
        """è·å–ç›‘æ§æ•°æ®åº“è¿æ¥çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        pool = None
        conn = None
        try:
            pool = self.conn_manager.get_postgresql_connection()
            conn = pool.getconn()
            yield conn
            conn.commit()
        except Exception as e:
            if conn:
                conn.rollback()
            logger.error(f"ç›‘æ§æ•°æ®åº“è¿æ¥é”™è¯¯: {e}")
            raise
        finally:
            if conn and pool:
                pool.putconn(conn)

    def log_operation(
        self,
        operation_type: str,
        classification: str,
        target_database: str,
        table_name: Optional[str] = None,
        record_count: int = 0,
        operation_status: str = "SUCCESS",
        error_message: Optional[str] = None,
        execution_time_ms: Optional[int] = None,
        user_agent: Optional[str] = None,
        client_ip: Optional[str] = None,
        additional_info: Optional[Dict] = None,
    ) -> bool:
        """
        è®°å½•æ“ä½œæ—¥å¿—

        Args:
            operation_type: æ“ä½œç±»å‹ (SAVE/LOAD/DELETE/UPDATE)
            classification: æ•°æ®åˆ†ç±»
            target_database: ç›®æ ‡æ•°æ®åº“ (TDengine/PostgreSQL/MySQL/Redis)
            table_name: ç›®æ ‡è¡¨å
            record_count: å½±å“è®°å½•æ•°
            operation_status: çŠ¶æ€ (SUCCESS/FAILED/PARTIAL)
            error_message: é”™è¯¯ä¿¡æ¯ (å¤±è´¥æ—¶)
            execution_time_ms: æ‰§è¡Œæ—¶é—´(æ¯«ç§’)
            user_agent: è°ƒç”¨æ¥æº
            client_ip: å®¢æˆ·ç«¯IP
            additional_info: é¢å¤–ä¿¡æ¯ (å­—å…¸,ä¼šè½¬ä¸ºJSONB)

        Returns:
            bool: è®°å½•æ˜¯å¦æˆåŠŸ
        """
        if not self.enable_monitoring:
            return True

        self._total_writes += 1

        try:
            operation_id = str(uuid.uuid4())

            with self._get_connection() as conn:
                cursor = conn.cursor()

                cursor.execute(
                    """
                    INSERT INTO operation_logs (
                        operation_id, operation_type, classification,
                        target_database, table_name, record_count,
                        operation_status, error_message, execution_time_ms,
                        user_agent, client_ip, additional_info
                    ) VALUES (
                        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
                    )
                """,
                    (
                        operation_id,
                        operation_type,
                        classification,
                        target_database,
                        table_name,
                        record_count,
                        operation_status,
                        error_message,
                        execution_time_ms,
                        user_agent,
                        client_ip,
                        additional_info,
                    ),
                )

                cursor.close()

            return True

        except Exception as e:
            self._write_failures += 1
            logger.warning(f"è®°å½•æ“ä½œæ—¥å¿—å¤±è´¥ (é™çº§åˆ°æœ¬åœ°æ—¥å¿—): {e}")
            logger.info(
                f"æ“ä½œæ—¥å¿—: {operation_type} {classification} -> {target_database}.{table_name} "
                f"({record_count} records, {operation_status}, {execution_time_ms}ms)"
            )
            return False

    def record_performance_metric(
        self,
        metric_name: str,
        metric_value: float,
        metric_type: str = "QUERY_TIME",
        metric_unit: str = "ms",
        classification: Optional[str] = None,
        database_type: Optional[str] = None,
        table_name: Optional[str] = None,
        is_slow_query: bool = False,
        query_sql: Optional[str] = None,
        execution_plan: Optional[str] = None,
        tags: Optional[Dict] = None,
    ) -> bool:
        """
        è®°å½•æ€§èƒ½æŒ‡æ ‡

        Args:
            metric_name: æŒ‡æ ‡åç§°
            metric_value: æŒ‡æ ‡å€¼
            metric_type: æŒ‡æ ‡ç±»å‹ (QUERY_TIME/CONNECTION_TIME/BATCH_SIZE)
            metric_unit: å•ä½ (ms/seconds/count)
            classification: å…³è”æ•°æ®åˆ†ç±»
            database_type: å…³è”æ•°æ®åº“ç±»å‹
            table_name: å…³è”è¡¨å
            is_slow_query: æ˜¯å¦æ…¢æŸ¥è¯¢ (>5ç§’)
            query_sql: SQLè¯­å¥ (æ…¢æŸ¥è¯¢æ—¶è®°å½•)
            execution_plan: æ‰§è¡Œè®¡åˆ’
            tags: æ ‡ç­¾ (å­—å…¸)

        Returns:
            bool: è®°å½•æ˜¯å¦æˆåŠŸ
        """
        if not self.enable_monitoring:
            return True

        try:
            with self._get_connection() as conn:
                cursor = conn.cursor()

                cursor.execute(
                    """
                    INSERT INTO performance_metrics (
                        metric_name, metric_type, metric_value, metric_unit,
                        classification, database_type, table_name,
                        is_slow_query, query_sql, execution_plan, tags
                    ) VALUES (
                        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
                    )
                """,
                    (
                        metric_name,
                        metric_type,
                        metric_value,
                        metric_unit,
                        classification,
                        database_type,
                        table_name,
                        is_slow_query,
                        query_sql,
                        execution_plan,
                        tags,
                    ),
                )

                cursor.close()

            return True

        except Exception as e:
            logger.warning(f"è®°å½•æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
            return False

    def log_quality_check(
        self,
        check_type: str,
        classification: str,
        database_type: str,
        table_name: str,
        check_status: str,
        total_records: Optional[int] = None,
        null_records: Optional[int] = None,
        missing_rate: Optional[float] = None,
        latest_timestamp: Optional[datetime] = None,
        data_delay_seconds: Optional[int] = None,
        invalid_records: Optional[int] = None,
        validation_rules: Optional[str] = None,
        check_message: Optional[str] = None,
        threshold_config: Optional[Dict] = None,
        check_duration_ms: Optional[int] = None,
    ) -> bool:
        """
        è®°å½•æ•°æ®è´¨é‡æ£€æŸ¥

        Args:
            check_type: æ£€æŸ¥ç±»å‹ (COMPLETENESS/FRESHNESS/ACCURACY)
            classification: æ•°æ®åˆ†ç±»
            database_type: æ•°æ®åº“ç±»å‹
            table_name: è¡¨å
            check_status: æ£€æŸ¥çŠ¶æ€ (PASS/FAIL/WARNING)
            total_records: æ€»è®°å½•æ•°
            null_records: ç©ºå€¼è®°å½•æ•°
            missing_rate: ç¼ºå¤±ç‡ (%)
            latest_timestamp: æœ€æ–°æ—¶é—´æˆ³
            data_delay_seconds: æ•°æ®å»¶è¿Ÿ(ç§’)
            invalid_records: æ— æ•ˆè®°å½•æ•°
            validation_rules: éªŒè¯è§„åˆ™
            check_message: æ£€æŸ¥ä¿¡æ¯
            threshold_config: é˜ˆå€¼é…ç½®
            check_duration_ms: æ£€æŸ¥è€—æ—¶(æ¯«ç§’)

        Returns:
            bool: è®°å½•æ˜¯å¦æˆåŠŸ
        """
        if not self.enable_monitoring:
            return True

        try:
            check_id = str(uuid.uuid4())

            with self._get_connection() as conn:
                cursor = conn.cursor()

                cursor.execute(
                    """
                    INSERT INTO data_quality_checks (
                        check_id, check_type, classification, database_type,
                        table_name, check_status, total_records, null_records,
                        missing_rate, latest_timestamp, data_delay_seconds,
                        invalid_records, validation_rules, check_message,
                        threshold_config, check_duration_ms
                    ) VALUES (
                        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
                    )
                """,
                    (
                        check_id,
                        check_type,
                        classification,
                        database_type,
                        table_name,
                        check_status,
                        total_records,
                        null_records,
                        missing_rate,
                        latest_timestamp,
                        data_delay_seconds,
                        invalid_records,
                        validation_rules,
                        check_message,
                        threshold_config,
                        check_duration_ms,
                    ),
                )

                cursor.close()

            return True

        except Exception as e:
            logger.warning(f"è®°å½•è´¨é‡æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def create_alert(
        self,
        alert_level: str,
        alert_type: str,
        alert_title: str,
        alert_message: str,
        source: Optional[str] = None,
        classification: Optional[str] = None,
        database_type: Optional[str] = None,
        table_name: Optional[str] = None,
        additional_data: Optional[Dict] = None,
        notification_channels: Optional[List[str]] = None,
    ) -> Optional[str]:
        """
        åˆ›å»ºå‘Šè­¦

        Args:
            alert_level: å‘Šè­¦çº§åˆ« (CRITICAL/WARNING/INFO)
            alert_type: å‘Šè­¦ç±»å‹ (SLOW_QUERY/DATA_QUALITY/SYSTEM_ERROR)
            alert_title: å‘Šè­¦æ ‡é¢˜
            alert_message: å‘Šè­¦è¯¦ç»†ä¿¡æ¯
            source: å‘Šè­¦æ¥æº (æ¨¡å—åç§°)
            classification: å…³è”æ•°æ®åˆ†ç±»
            database_type: å…³è”æ•°æ®åº“ç±»å‹
            table_name: å…³è”è¡¨å
            additional_data: é¢å¤–æ•°æ®
            notification_channels: é€šçŸ¥æ¸ é“ ['email', 'webhook', 'log']

        Returns:
            str: å‘Šè­¦ID (å¤±è´¥è¿”å›None)
        """
        if not self.enable_monitoring:
            return None

        try:
            alert_id = str(uuid.uuid4())
            now = datetime.now()

            with self._get_connection() as conn:
                cursor = conn.cursor()

                cursor.execute(
                    """
                    INSERT INTO alert_records (
                        alert_id, alert_level, alert_type, alert_title,
                        alert_message, source, classification, database_type,
                        table_name, first_occurred_at, last_occurred_at,
                        notification_channels, additional_data
                    ) VALUES (
                        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
                    )
                """,
                    (
                        alert_id,
                        alert_level,
                        alert_type,
                        alert_title,
                        alert_message,
                        source,
                        classification,
                        database_type,
                        table_name,
                        now,
                        now,
                        notification_channels,
                        additional_data,
                    ),
                )

                cursor.close()

            logger.warning(f"ğŸš¨ å‘Šè­¦åˆ›å»º: [{alert_level}] {alert_title}")
            return alert_id

        except Exception as e:
            logger.error(f"åˆ›å»ºå‘Šè­¦å¤±è´¥: {e}")
            logger.warning(f"å‘Šè­¦å†…å®¹: [{alert_level}] {alert_title} - {alert_message}")
            return None

    def update_alert_status(
        self,
        alert_id: str,
        alert_status: str,
        operator: str,
        resolution_notes: Optional[str] = None,
    ) -> bool:
        """
        æ›´æ–°å‘Šè­¦çŠ¶æ€

        Args:
            alert_id: å‘Šè­¦ID
            alert_status: æ–°çŠ¶æ€ (ACKNOWLEDGED/RESOLVED)
            operator: æ“ä½œäºº
            resolution_notes: è§£å†³è¯´æ˜

        Returns:
            bool: æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        if not self.enable_monitoring:
            return True

        try:
            with self._get_connection() as conn:
                cursor = conn.cursor()

                if alert_status == "ACKNOWLEDGED":
                    cursor.execute(
                        """
                        UPDATE alert_records
                        SET alert_status = %s,
                            acknowledged_by = %s,
                            acknowledged_at = CURRENT_TIMESTAMP,
                            updated_at = CURRENT_TIMESTAMP
                        WHERE alert_id = %s
                    """,
                        (alert_status, operator, alert_id),
                    )

                elif alert_status == "RESOLVED":
                    cursor.execute(
                        """
                        UPDATE alert_records
                        SET alert_status = %s,
                            resolved_by = %s,
                            resolved_at = CURRENT_TIMESTAMP,
                            resolution_notes = %s,
                            updated_at = CURRENT_TIMESTAMP
                        WHERE alert_id = %s
                    """,
                        (alert_status, operator, resolution_notes, alert_id),
                    )

                cursor.close()

            return True

        except Exception as e:
            logger.error(f"æ›´æ–°å‘Šè­¦çŠ¶æ€å¤±è´¥: {e}")
            return False

    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–ç›‘æ§ç»Ÿè®¡ä¿¡æ¯

        Returns:
            dict: ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            "total_writes": self._total_writes,
            "write_failures": self._write_failures,
            "write_success_rate": 0.0,
        }

        if self._total_writes > 0:
            stats["write_success_rate"] = (
                (self._total_writes - self._write_failures) / self._total_writes * 100
            )

        return stats

    def cleanup_old_records(
        self, days_to_keep: Optional[Dict[str, int]] = None
    ) -> Dict[str, int]:
        """
        æ¸…ç†è¿‡æœŸè®°å½•

        Args:
            days_to_keep: å„è¡¨ä¿ç•™å¤©æ•°é…ç½® {
                'operation_logs': 30,
                'performance_metrics': 90,
                'data_quality_checks': 7,
                'alert_records': 90
            }

        Returns:
            dict: å„è¡¨åˆ é™¤çš„è®°å½•æ•°
        """
        if not self.enable_monitoring:
            return {}

        if days_to_keep is None:
            days_to_keep = {
                "operation_logs": 30,
                "performance_metrics": 90,
                "data_quality_checks": 7,
                "alert_records": 90,
            }

        deleted_counts = {}

        try:
            with self._get_connection() as conn:
                cursor = conn.cursor()

                for table_name, days in days_to_keep.items():
                    cutoff_date = datetime.now() - timedelta(days=days)

                    cursor.execute(
                        f"""
                        DELETE FROM {table_name}
                        WHERE created_at < %s
                    """,
                        (cutoff_date,),
                    )

                    deleted_counts[table_name] = cursor.rowcount
                    logger.info(
                        f"æ¸…ç† {table_name}: åˆ é™¤ {cursor.rowcount} æ¡è®°å½• (>{days}å¤©)"
                    )

                cursor.close()

            return deleted_counts

        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡æœŸè®°å½•å¤±è´¥: {e}")
            return deleted_counts


# å…¨å±€ç›‘æ§æ•°æ®åº“å®ä¾‹ (å•ä¾‹æ¨¡å¼)
_monitoring_db: Optional[MonitoringDatabase] = None


def get_monitoring_database(enable_monitoring: bool = True) -> MonitoringDatabase:
    """è·å–å…¨å±€ç›‘æ§æ•°æ®åº“å®ä¾‹ (å•ä¾‹æ¨¡å¼)"""
    global _monitoring_db
    if _monitoring_db is None:
        _monitoring_db = MonitoringDatabase(enable_monitoring=enable_monitoring)
    return _monitoring_db


if __name__ == "__main__":
    """æµ‹è¯•ç›‘æ§æ•°æ®åº“"""
    logging.basicConfig(
        level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
    )

    print("\næµ‹è¯•MonitoringDatabase...")

    # åˆ›å»ºç›‘æ§æ•°æ®åº“å®ä¾‹
    monitor_db = MonitoringDatabase(enable_monitoring=True)

    # æµ‹è¯•1: è®°å½•æ“ä½œæ—¥å¿—
    print("\n1. æµ‹è¯•è®°å½•æ“ä½œæ—¥å¿—...")
    success = monitor_db.log_operation(
        operation_type="SAVE",
        classification="DAILY_KLINE",
        target_database="PostgreSQL",
        table_name="daily_kline",
        record_count=100,
        operation_status="SUCCESS",
        execution_time_ms=45,
    )
    print(f"   è®°å½•æ“ä½œæ—¥å¿—: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")

    # æµ‹è¯•2: è®°å½•æ€§èƒ½æŒ‡æ ‡
    print("\n2. æµ‹è¯•è®°å½•æ€§èƒ½æŒ‡æ ‡...")
    success = monitor_db.record_performance_metric(
        metric_name="query_daily_kline",
        metric_value=150.5,
        metric_type="QUERY_TIME",
        metric_unit="ms",
        classification="DAILY_KLINE",
        database_type="PostgreSQL",
        is_slow_query=False,
    )
    print(f"   è®°å½•æ€§èƒ½æŒ‡æ ‡: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")

    # æµ‹è¯•3: è®°å½•è´¨é‡æ£€æŸ¥
    print("\n3. æµ‹è¯•è®°å½•è´¨é‡æ£€æŸ¥...")
    success = monitor_db.log_quality_check(
        check_type="COMPLETENESS",
        classification="DAILY_KLINE",
        database_type="PostgreSQL",
        table_name="daily_kline",
        check_status="PASS",
        total_records=10000,
        null_records=5,
        missing_rate=0.05,
    )
    print(f"   è®°å½•è´¨é‡æ£€æŸ¥: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}")

    # æµ‹è¯•4: åˆ›å»ºå‘Šè­¦
    print("\n4. æµ‹è¯•åˆ›å»ºå‘Šè­¦...")
    alert_id = monitor_db.create_alert(
        alert_level="WARNING",
        alert_type="DATA_QUALITY",
        alert_title="æ•°æ®ç¼ºå¤±ç‡åé«˜",
        alert_message="daily_klineè¡¨æ•°æ®ç¼ºå¤±ç‡è¾¾åˆ°5%,è¶…è¿‡é˜ˆå€¼3%",
        source="DataQualityMonitor",
        classification="DAILY_KLINE",
        notification_channels=["log", "webhook"],
    )
    print(f"   åˆ›å»ºå‘Šè­¦: {'âœ… æˆåŠŸ' if alert_id else 'âŒ å¤±è´¥'} (ID={alert_id})")

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\n5. ç›‘æ§ç»Ÿè®¡ä¿¡æ¯:")
    stats = monitor_db.get_statistics()
    print(f"   æ€»å†™å…¥æ¬¡æ•°: {stats['total_writes']}")
    print(f"   å†™å…¥å¤±è´¥æ¬¡æ•°: {stats['write_failures']}")
    print(f"   å†™å…¥æˆåŠŸç‡: {stats['write_success_rate']:.2f}%")

    print("\nâœ… MonitoringDatabaseæµ‹è¯•å®Œæˆ!")
