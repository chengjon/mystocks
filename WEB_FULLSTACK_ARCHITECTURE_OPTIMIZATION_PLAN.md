# MyStocks Webå…¨æ ˆæ¶æ„ä¼˜åŒ–æ–¹æ¡ˆ

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0.0
**ç”Ÿæˆæ—¥æœŸ**: 2025-11-06
**ä½œè€…**: Web Fullstack Architect
**é¡¹ç›®**: MyStocksé‡åŒ–äº¤æ˜“æ•°æ®ç®¡ç†ç³»ç»Ÿ
**å›¢é˜Ÿè§„æ¨¡**: 2-3äººå¼€å‘å›¢é˜Ÿ

---

## æ‰§è¡Œæ‘˜è¦

### ç°çŠ¶è¯„ä¼°

åŸºäºä¸‰ä»½æŠ€æœ¯è¯„ä¼°æŠ¥å‘Šï¼Œå½“å‰ç³»ç»Ÿå­˜åœ¨ä»¥ä¸‹æ ¸å¿ƒé—®é¢˜ï¼š

| é—®é¢˜åŸŸ | ä¸¥é‡ç¨‹åº¦ | å½±å“ |
|--------|----------|------|
| **å®æ—¶æ€§ä¸è¶³** | ğŸ”´ é«˜ | WebSocketæœªå®ç°ï¼Œæ•°æ®å»¶è¿Ÿ5-10ç§’ |
| **ç±»å‹ä¸å®‰å…¨** | ğŸ”´ é«˜ | å‰åç«¯ç±»å‹å®šä¹‰ä¸åŒæ­¥ï¼Œè¿è¡Œæ—¶é”™è¯¯é¢‘ç¹ |
| **ç¼ºå°‘MockæœåŠ¡** | ğŸ”´ é«˜ | å‰åç«¯å¼€å‘ä¸²è¡Œï¼Œæ•ˆç‡ä½ä¸‹50% |
| **æ— å¥‘çº¦æµ‹è¯•** | ğŸŸ¡ ä¸­ | APIå˜æ›´å¯¼è‡´ç”Ÿäº§äº‹æ•… |
| **ç¼“å­˜ç¼ºå¤±** | ğŸŸ¡ ä¸­ | Rediså·²ç§»é™¤ï¼Œæ€§èƒ½ç“¶é¢ˆæ˜æ˜¾ |
| **ç›‘æ§ä¸è¶³** | ğŸŸ¡ ä¸­ | ç¼ºå°‘åˆ†å¸ƒå¼è¿½è¸ªå’Œå®æ—¶ç›‘æ§ |

### ä¼˜åŒ–ç›®æ ‡

é€šè¿‡4å‘¨çš„æ¶æ„ä¼˜åŒ–ï¼Œå®ç°ï¼š

1. **å®æ—¶æ€§**: æ¯«ç§’çº§æ•°æ®æ¨é€ (WebSocket + Redis Pub/Sub)
2. **ç±»å‹å®‰å…¨**: ç¼–è¯‘æ—¶æ•è·95%ç±»å‹é”™è¯¯ (OpenAPI + TypeScript)
3. **å¼€å‘æ•ˆç‡**: å‰åç«¯å¹¶è¡Œå¼€å‘ï¼Œæ•ˆç‡æå‡60%
4. **ç³»ç»Ÿå¯é æ€§**: 99.9%å¯ç”¨æ€§ (ç›‘æ§ + ç†”æ–­ + é™æµ)
5. **æ€§èƒ½æå‡**: APIå“åº”æ—¶é—´ <200ms, é¡µé¢åŠ è½½ <1.5s

### æŠ•èµ„å›æŠ¥

- **æ€»æŠ•å…¥**: 160äººæ—¶ (2äººÃ—4å‘¨Ã—40å°æ—¶)
- **å¹´åº¦æ”¶ç›Š**: èŠ‚çœ3750å°æ—¶å¼€å‘æ—¶é—´
- **ROI**: 2,344% (ç¬¬ä¸€å¹´)

---

## Part 1: æ¶æ„è®¾è®¡

### 1.1 æ–°æ¶æ„æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          ç”¨æˆ·å±‚ (User Layer)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Web Browser | Mobile App | Desktop Client | API Client  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“â†‘
                         [HTTPS/WSS] [CDNåŠ é€Ÿ]
                                    â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       æ¥å…¥å±‚ (Access Layer)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚             Nginx (è´Ÿè½½å‡è¡¡ + SSL + é™æ€èµ„æº)              â”‚    â”‚
â”‚  â”‚                    â†“                    â†“                  â”‚    â”‚
â”‚  â”‚          [API Gateway]          [WebSocket Gateway]       â”‚    â”‚
â”‚  â”‚          - é™æµ/ç†”æ–­             - è¿æ¥ç®¡ç†                â”‚    â”‚
â”‚  â”‚          - è®¤è¯/é‰´æƒ             - å¿ƒè·³æ£€æµ‹                â”‚    â”‚
â”‚  â”‚          - è·¯ç”±è½¬å‘              - æ¶ˆæ¯è·¯ç”±                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“â†‘
                      [REST/GraphQL] [WebSocket/SSE]
                                    â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       åº”ç”¨å±‚ (Application Layer)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚     å‰ç«¯åº”ç”¨ (Frontend)      â”‚     åç«¯æœåŠ¡ (Backend)      â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚   Vue 3 + Pinia     â”‚    â”‚  â”‚  FastAPI Services  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚   TypeScript        â”‚    â”‚  â”‚  - Market Service  â”‚    â”‚    â”‚
â”‚  â”‚  â”‚   Socket.IO Client  â”‚    â”‚  â”‚  - Strategy Svc    â”‚    â”‚    â”‚
â”‚  â”‚  â”‚   PWA Support       â”‚    â”‚  â”‚  - Monitor Svc     â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚  - Auth Service     â”‚    â”‚    â”‚
â”‚  â”‚                             â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“â†‘
                           [AMQP/Redis Pub-Sub]
                                    â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ä¸­é—´ä»¶å±‚ (Middleware Layer)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Redis Cluster          â”‚  RabbitMQ           â”‚  Jaeger   â”‚    â”‚
â”‚  â”‚  - ç¼“å­˜/ä¼šè¯            â”‚  - å¼‚æ­¥æ¶ˆæ¯é˜Ÿåˆ—     â”‚  - åˆ†å¸ƒå¼è¿½è¸ªâ”‚  â”‚
â”‚  â”‚  - Pub/Sub              â”‚  - ä»»åŠ¡è°ƒåº¦         â”‚  - APM     â”‚    â”‚
â”‚  â”‚  - åˆ†å¸ƒå¼é”            â”‚  - äº‹ä»¶æ€»çº¿         â”‚            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“â†‘
                              [SQL/NoSQL]
                                    â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        æ•°æ®å±‚ (Data Layer)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   TDengine              â”‚  PostgreSQL + TimescaleDB       â”‚    â”‚
â”‚  â”‚   - Tickæ•°æ®            â”‚  - æ—¥çº¿æ•°æ®                     â”‚    â”‚
â”‚  â”‚   - åˆ†é’Ÿçº¿æ•°æ®          â”‚  - ç”¨æˆ·/ç­–ç•¥/è®¢å•               â”‚    â”‚
â”‚  â”‚   - å®æ—¶æŒ‡æ ‡            â”‚  - å…ƒæ•°æ®/é…ç½®                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    åŸºç¡€è®¾æ–½å±‚ (Infrastructure)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Prometheus + Grafana   â”‚  ELK Stack        â”‚  Sentry     â”‚    â”‚
â”‚  â”‚  - æŒ‡æ ‡ç›‘æ§             â”‚  - æ—¥å¿—èšåˆ        â”‚  - é”™è¯¯è¿½è¸ª â”‚    â”‚
â”‚  â”‚  - å‘Šè­¦ç®¡ç†             â”‚  - æ—¥å¿—åˆ†æ        â”‚  - æ€§èƒ½åˆ†æ â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æŠ€æœ¯æ ˆé€‰æ‹©

| å±‚çº§ | æŠ€æœ¯é€‰å‹ | é€‰æ‹©ç†ç”± | æˆæœ¬ |
|------|----------|----------|------|
| **å‰ç«¯** | Vue 3 + TypeScript + Pinia | å›¢é˜Ÿç†Ÿæ‚‰ï¼Œç”Ÿæ€å®Œå–„ | å…è´¹ |
| **åç«¯** | FastAPI + SQLAlchemy | é«˜æ€§èƒ½ï¼Œè‡ªåŠ¨æ–‡æ¡£ | å…è´¹ |
| **å®æ—¶é€šä¿¡** | Socket.IO | è‡ªåŠ¨é™çº§ï¼Œæ˜“ç”¨ | å…è´¹ |
| **APIç½‘å…³** | Kong (å¼€æºç‰ˆ) | åŠŸèƒ½å®Œæ•´ï¼Œæ’ä»¶ä¸°å¯Œ | å…è´¹ |
| **ç¼“å­˜** | Redis Cluster | é«˜æ€§èƒ½ï¼Œæ”¯æŒPub/Sub | å…è´¹ |
| **æ¶ˆæ¯é˜Ÿåˆ—** | RabbitMQ | å¯é æ€§é«˜ï¼Œæ˜“ç®¡ç† | å…è´¹ |
| **ç›‘æ§** | Prometheus + Grafana | å¼€æºæ ‡å‡†ï¼Œç”Ÿæ€å¥½ | å…è´¹ |
| **è¿½è¸ª** | Jaeger | CNCFé¡¹ç›®ï¼Œé›†æˆç®€å• | å…è´¹ |
| **æ—¥å¿—** | ELK Stack | åŠŸèƒ½å¼ºå¤§ï¼Œå¯æ‰©å±• | å…è´¹ |

### 1.3 æ•°æ®æµè®¾è®¡

#### 1.3.1 å®æ—¶æ•°æ®æµ (WebSocket)

```
å¸‚åœºæ•°æ®æº â†’ TDengine â†’ æ•°æ®æœåŠ¡ â†’ Redis Pub/Sub â†’ WebSocket Server â†’ å®¢æˆ·ç«¯
     â†“                                    â†“
   (1ms)                              äº‹ä»¶æ€»çº¿
                                          â†“
                                    å…¶ä»–è®¢é˜…è€…
```

#### 1.3.2 CQRSæ¨¡å¼å®ç°

```
å†™æ“ä½œ (Command):
å®¢æˆ·ç«¯ â†’ API Gateway â†’ Command Service â†’ PostgreSQL â†’ Event Store
                              â†“
                        Domain Events â†’ RabbitMQ

è¯»æ“ä½œ (Query):
å®¢æˆ·ç«¯ â†’ API Gateway â†’ Query Service â†’ Redis Cache â†’ PostgreSQL View
                              â†‘
                        Cache Invalidation â† RabbitMQ
```

---

## Part 2: å®æ—¶æ•°æ®æ–¹æ¡ˆ

### 2.1 WebSocket Serveræ¶æ„

```python
# websocket/server.py
import asyncio
import json
from typing import Dict, Set, Optional
from datetime import datetime
import socketio
import redis.asyncio as redis
from fastapi import FastAPI
from pydantic import BaseModel
import structlog

logger = structlog.get_logger()

# Socket.IO å¼‚æ­¥æœåŠ¡å™¨
sio = socketio.AsyncServer(
    async_mode='asgi',
    cors_allowed_origins='*',
    logger=True,
    engineio_logger=False,
    ping_interval=25,
    ping_timeout=60
)

# åˆ›å»ºASGIåº”ç”¨
socket_app = socketio.ASGIApp(
    sio,
    other_asgi_app=None,
    socketio_path='/ws'
)

class ConnectionManager:
    """WebSocketè¿æ¥ç®¡ç†å™¨"""

    def __init__(self):
        self.active_connections: Dict[str, Set[str]] = {}  # room -> {sid}
        self.user_sessions: Dict[str, str] = {}  # sid -> user_id
        self.subscriptions: Dict[str, Set[str]] = {}  # channel -> {sid}
        self.redis_client: Optional[redis.Redis] = None
        self.pubsub: Optional[redis.client.PubSub] = None

    async def initialize(self):
        """åˆå§‹åŒ–Redisè¿æ¥"""
        self.redis_client = redis.Redis(
            host='localhost',
            port=6379,
            decode_responses=True,
            connection_pool=redis.ConnectionPool(
                max_connections=100,
                connection_class=redis.Connection
            )
        )
        self.pubsub = self.redis_client.pubsub()

        # å¯åŠ¨æ¶ˆæ¯ç›‘å¬å™¨
        asyncio.create_task(self._message_listener())

    async def _message_listener(self):
        """Redis Pub/Subæ¶ˆæ¯ç›‘å¬å™¨"""
        await self.pubsub.subscribe('market:tick', 'market:depth',
                                    'order:update', 'strategy:signal')

        async for message in self.pubsub.listen():
            if message['type'] == 'message':
                channel = message['channel']
                data = json.loads(message['data'])

                # æ ¹æ®é¢‘é“è·¯ç”±æ¶ˆæ¯
                if channel.startswith('market:'):
                    await self._broadcast_market_data(channel, data)
                elif channel == 'order:update':
                    await self._send_order_update(data)
                elif channel == 'strategy:signal':
                    await self._broadcast_strategy_signal(data)

    async def _broadcast_market_data(self, channel: str, data: dict):
        """å¹¿æ’­å¸‚åœºæ•°æ®"""
        event_type = channel.split(':')[1]  # tick or depth
        symbol = data.get('symbol')

        if symbol:
            room = f"market:{symbol}"
            await sio.emit(
                event_type,
                data,
                room=room,
                skip_sid=None
            )

            # è®°å½•æŒ‡æ ‡
            await self._record_metric('broadcast', event_type, len(data))

    async def connect(self, sid: str, user_id: str, auth_token: str):
        """å¤„ç†å®¢æˆ·ç«¯è¿æ¥"""
        # éªŒè¯Token
        if not await self._verify_token(auth_token):
            await sio.disconnect(sid)
            return False

        self.user_sessions[sid] = user_id

        # ä»Redisæ¢å¤ä¼šè¯çŠ¶æ€
        session_key = f"session:{user_id}"
        session_data = await self.redis_client.hgetall(session_key)

        if session_data:
            # æ¢å¤è®¢é˜…
            for channel in session_data.get('subscriptions', '').split(','):
                if channel:
                    await self.subscribe(sid, channel)

        logger.info("websocket_connected", sid=sid, user_id=user_id)
        return True

    async def disconnect(self, sid: str):
        """å¤„ç†å®¢æˆ·ç«¯æ–­å¼€"""
        user_id = self.user_sessions.pop(sid, None)

        # æ¸…ç†è®¢é˜…
        for channel_sids in self.subscriptions.values():
            channel_sids.discard(sid)

        # ä¿å­˜ä¼šè¯çŠ¶æ€åˆ°Redis
        if user_id:
            session_key = f"session:{user_id}"
            await self.redis_client.hset(
                session_key,
                mapping={
                    'last_disconnect': datetime.now().isoformat(),
                    'subscriptions': ','.join(self._get_user_subscriptions(sid))
                }
            )
            await self.redis_client.expire(session_key, 3600)  # 1å°æ—¶è¿‡æœŸ

        logger.info("websocket_disconnected", sid=sid, user_id=user_id)

    async def subscribe(self, sid: str, channel: str):
        """è®¢é˜…æ•°æ®é¢‘é“"""
        if channel not in self.subscriptions:
            self.subscriptions[channel] = set()

        self.subscriptions[channel].add(sid)

        # åŠ å…¥Socket.IOæˆ¿é—´
        if channel.startswith('market:'):
            symbol = channel.split(':')[1]
            await sio.enter_room(sid, f"market:{symbol}")

        # å‘é€æœ€æ–°å¿«ç…§æ•°æ®
        await self._send_snapshot(sid, channel)

        logger.info("channel_subscribed", sid=sid, channel=channel)

    async def _send_snapshot(self, sid: str, channel: str):
        """å‘é€é¢‘é“å¿«ç…§æ•°æ®"""
        if channel.startswith('market:'):
            symbol = channel.split(':')[1]

            # ä»Redisè·å–æœ€æ–°å¿«ç…§
            snapshot_key = f"snapshot:market:{symbol}"
            snapshot = await self.redis_client.get(snapshot_key)

            if snapshot:
                await sio.emit(
                    'snapshot',
                    json.loads(snapshot),
                    to=sid
                )

    async def _verify_token(self, token: str) -> bool:
        """éªŒè¯JWT Token"""
        # å®ç°JWTéªŒè¯é€»è¾‘
        return True  # ç®€åŒ–ç¤ºä¾‹

    async def _record_metric(self, metric_type: str, event: str, size: int):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        await self.redis_client.hincrby(f"metrics:{metric_type}", event, 1)
        await self.redis_client.hincrby(f"metrics:bytes", event, size)

# å…¨å±€è¿æ¥ç®¡ç†å™¨
manager = ConnectionManager()

# Socket.IOäº‹ä»¶å¤„ç†å™¨
@sio.event
async def connect(sid, environ, auth):
    """å®¢æˆ·ç«¯è¿æ¥äº‹ä»¶"""
    query_string = environ.get('QUERY_STRING', '')
    params = dict(param.split('=') for param in query_string.split('&') if '=' in param)

    user_id = params.get('user_id', 'anonymous')
    token = auth.get('token') if auth else None

    if await manager.connect(sid, user_id, token):
        await sio.emit('connected', {
            'sid': sid,
            'timestamp': datetime.now().isoformat(),
            'server_time': int(datetime.now().timestamp() * 1000)
        }, to=sid)
    else:
        await sio.disconnect(sid)

@sio.event
async def disconnect(sid):
    """å®¢æˆ·ç«¯æ–­å¼€äº‹ä»¶"""
    await manager.disconnect(sid)

@sio.event
async def subscribe(sid, data):
    """è®¢é˜…æ•°æ®é¢‘é“"""
    channels = data.get('channels', [])
    for channel in channels:
        await manager.subscribe(sid, channel)

    await sio.emit('subscribed', {
        'channels': channels,
        'timestamp': datetime.now().isoformat()
    }, to=sid)

@sio.event
async def unsubscribe(sid, data):
    """å–æ¶ˆè®¢é˜…"""
    channels = data.get('channels', [])
    for channel in channels:
        if channel in manager.subscriptions:
            manager.subscriptions[channel].discard(sid)

    await sio.emit('unsubscribed', {
        'channels': channels,
        'timestamp': datetime.now().isoformat()
    }, to=sid)

@sio.event
async def ping(sid, data):
    """å¿ƒè·³æ£€æµ‹"""
    await sio.emit('pong', {
        'client_time': data.get('timestamp'),
        'server_time': int(datetime.now().timestamp() * 1000)
    }, to=sid)

# åˆå§‹åŒ–
async def initialize_websocket():
    """åˆå§‹åŒ–WebSocketæœåŠ¡"""
    await manager.initialize()
    logger.info("WebSocket server initialized")

# æ•°æ®æ¨é€æ¥å£
async def push_tick_data(symbol: str, tick_data: dict):
    """æ¨é€Tickæ•°æ®åˆ°Redis"""
    channel = f"market:tick"
    data = {
        'symbol': symbol,
        'price': tick_data['price'],
        'volume': tick_data['volume'],
        'timestamp': tick_data['timestamp'],
        'bid': tick_data.get('bid'),
        'ask': tick_data.get('ask')
    }

    # å‘å¸ƒåˆ°Redis
    await manager.redis_client.publish(channel, json.dumps(data))

    # æ›´æ–°å¿«ç…§
    snapshot_key = f"snapshot:market:{symbol}"
    await manager.redis_client.set(
        snapshot_key,
        json.dumps(data),
        ex=60  # 60ç§’è¿‡æœŸ
    )
```

### 2.2 Redisç¼“å­˜ç­–ç•¥

```python
# cache/redis_cache.py
import json
import hashlib
from typing import Any, Optional, Callable
from datetime import timedelta
import redis.asyncio as redis
from functools import wraps
import structlog

logger = structlog.get_logger()

class CacheManager:
    """Redisç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis_client = redis.from_url(
            redis_url,
            encoding="utf-8",
            decode_responses=True
        )

    async def initialize(self):
        """åˆå§‹åŒ–ç¼“å­˜"""
        await self.redis_client.ping()
        logger.info("Cache manager initialized")

    def cache_key(self, prefix: str, *args, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = f"{prefix}:{str(args)}:{str(sorted(kwargs.items()))}"
        key_hash = hashlib.md5(key_data.encode()).hexdigest()[:8]
        return f"{prefix}:{key_hash}"

    async def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        value = await self.redis_client.get(key)
        if value:
            try:
                return json.loads(value)
            except json.JSONDecodeError:
                return value
        return None

    async def set(self, key: str, value: Any, ttl: int = 300):
        """è®¾ç½®ç¼“å­˜"""
        if isinstance(value, (dict, list)):
            value = json.dumps(value)
        await self.redis_client.set(key, value, ex=ttl)

    async def delete(self, pattern: str):
        """åˆ é™¤ç¼“å­˜"""
        keys = await self.redis_client.keys(pattern)
        if keys:
            await self.redis_client.delete(*keys)

    async def invalidate_pattern(self, pattern: str):
        """å¤±æ•ˆåŒ¹é…æ¨¡å¼çš„ç¼“å­˜"""
        cursor = 0
        while True:
            cursor, keys = await self.redis_client.scan(
                cursor, match=pattern, count=100
            )
            if keys:
                pipeline = self.redis_client.pipeline()
                for key in keys:
                    pipeline.delete(key)
                await pipeline.execute()
            if cursor == 0:
                break

    def cached(self, ttl: int = 300, key_prefix: Optional[str] = None):
        """ç¼“å­˜è£…é¥°å™¨"""
        def decorator(func: Callable):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # ç”Ÿæˆç¼“å­˜é”®
                prefix = key_prefix or f"{func.__module__}.{func.__name__}"
                cache_key = self.cache_key(prefix, *args, **kwargs)

                # å°è¯•ä»ç¼“å­˜è·å–
                cached_value = await self.get(cache_key)
                if cached_value is not None:
                    logger.debug("cache_hit", key=cache_key)
                    return cached_value

                # æ‰§è¡Œå‡½æ•°
                result = await func(*args, **kwargs)

                # å­˜å…¥ç¼“å­˜
                await self.set(cache_key, result, ttl)
                logger.debug("cache_miss", key=cache_key)

                return result
            return wrapper
        return decorator

# ç¼“å­˜ç­–ç•¥é…ç½®
CACHE_CONFIG = {
    'market_data': {
        'ttl': 5,  # 5ç§’
        'pattern': 'market:*'
    },
    'user_session': {
        'ttl': 3600,  # 1å°æ—¶
        'pattern': 'session:*'
    },
    'strategy_result': {
        'ttl': 300,  # 5åˆ†é’Ÿ
        'pattern': 'strategy:result:*'
    },
    'static_data': {
        'ttl': 86400,  # 1å¤©
        'pattern': 'static:*'
    }
}

# åˆ†å±‚ç¼“å­˜ç­–ç•¥
class LayeredCache:
    """åˆ†å±‚ç¼“å­˜å®ç°"""

    def __init__(self):
        self.l1_cache = {}  # å†…å­˜ç¼“å­˜ (è¿›ç¨‹çº§)
        self.l2_cache = CacheManager()  # Redisç¼“å­˜ (åˆ†å¸ƒå¼)

    async def get(self, key: str) -> Optional[Any]:
        """åˆ†å±‚è·å–"""
        # L1: å†…å­˜ç¼“å­˜
        if key in self.l1_cache:
            value, expire_at = self.l1_cache[key]
            if datetime.now().timestamp() < expire_at:
                return value
            else:
                del self.l1_cache[key]

        # L2: Redisç¼“å­˜
        value = await self.l2_cache.get(key)
        if value:
            # å†™å…¥L1ç¼“å­˜
            self.l1_cache[key] = (value, datetime.now().timestamp() + 60)

        return value

    async def set(self, key: str, value: Any, ttl: int = 300):
        """åˆ†å±‚è®¾ç½®"""
        # å†™å…¥L1ç¼“å­˜
        self.l1_cache[key] = (value, datetime.now().timestamp() + min(ttl, 60))

        # å†™å…¥L2ç¼“å­˜
        await self.l2_cache.set(key, value, ttl)

# ä½¿ç”¨ç¤ºä¾‹
cache = LayeredCache()

@cache.cached(ttl=60, key_prefix="fund_flow")
async def get_fund_flow_data(symbol: str, timeframe: str):
    """è·å–èµ„é‡‘æµå‘æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    # å®é™…çš„æ•°æ®åº“æŸ¥è¯¢
    data = await db.query(...)
    return data
```

### 2.3 æ¶ˆæ¯é˜Ÿåˆ—è®¾è®¡

```python
# mq/rabbitmq_client.py
import asyncio
import json
from typing import Any, Dict, Callable
import aio_pika
from aio_pika import Message, ExchangeType
import structlog

logger = structlog.get_logger()

class MessageQueue:
    """RabbitMQæ¶ˆæ¯é˜Ÿåˆ—å°è£…"""

    def __init__(self, url: str = "amqp://guest:guest@localhost/"):
        self.url = url
        self.connection = None
        self.channel = None
        self.exchanges = {}
        self.queues = {}

    async def connect(self):
        """è¿æ¥åˆ°RabbitMQ"""
        self.connection = await aio_pika.connect_robust(
            self.url,
            loop=asyncio.get_event_loop()
        )
        self.channel = await self.connection.channel()
        await self.channel.set_qos(prefetch_count=10)

        # å£°æ˜äº¤æ¢æœº
        await self._declare_exchanges()

        logger.info("RabbitMQ connected")

    async def _declare_exchanges(self):
        """å£°æ˜äº¤æ¢æœº"""
        # å¸‚åœºæ•°æ®äº¤æ¢æœº (Topic)
        self.exchanges['market'] = await self.channel.declare_exchange(
            'market_data',
            ExchangeType.TOPIC,
            durable=True
        )

        # è®¢å•äº¤æ¢æœº (Direct)
        self.exchanges['order'] = await self.channel.declare_exchange(
            'order_events',
            ExchangeType.DIRECT,
            durable=True
        )

        # ç­–ç•¥ä¿¡å·äº¤æ¢æœº (Fanout)
        self.exchanges['strategy'] = await self.channel.declare_exchange(
            'strategy_signals',
            ExchangeType.FANOUT,
            durable=True
        )

    async def publish(self, exchange: str, routing_key: str, message: Dict[str, Any]):
        """å‘å¸ƒæ¶ˆæ¯"""
        if exchange not in self.exchanges:
            raise ValueError(f"Unknown exchange: {exchange}")

        msg = Message(
            json.dumps(message).encode(),
            content_type='application/json',
            delivery_mode=aio_pika.DeliveryMode.PERSISTENT,
            timestamp=datetime.now(),
            message_id=str(uuid.uuid4())
        )

        await self.exchanges[exchange].publish(msg, routing_key=routing_key)

        logger.debug("message_published",
                    exchange=exchange,
                    routing_key=routing_key)

    async def consume(self, queue_name: str, callback: Callable,
                     exchange: str, routing_key: str = "#"):
        """æ¶ˆè´¹æ¶ˆæ¯"""
        # å£°æ˜é˜Ÿåˆ—
        queue = await self.channel.declare_queue(
            queue_name,
            durable=True,
            auto_delete=False
        )

        # ç»‘å®šåˆ°äº¤æ¢æœº
        await queue.bind(self.exchanges[exchange], routing_key=routing_key)

        # å¼€å§‹æ¶ˆè´¹
        async with queue.iterator() as queue_iter:
            async for message in queue_iter:
                async with message.process():
                    try:
                        data = json.loads(message.body.decode())
                        await callback(data)
                    except Exception as e:
                        logger.error("message_processing_error",
                                   error=str(e),
                                   queue=queue_name)
                        # æ¶ˆæ¯é‡æ–°å…¥é˜Ÿ
                        await message.nack(requeue=True)

# äº‹ä»¶æ€»çº¿å®ç°
class EventBus:
    """äº‹ä»¶æ€»çº¿"""

    def __init__(self, mq: MessageQueue):
        self.mq = mq
        self.handlers = {}

    async def emit(self, event_type: str, data: Dict[str, Any]):
        """å‘é€äº‹ä»¶"""
        await self.mq.publish(
            exchange='events',
            routing_key=event_type,
            message={
                'event_type': event_type,
                'data': data,
                'timestamp': datetime.now().isoformat(),
                'source': 'mystocks'
            }
        )

    def on(self, event_type: str):
        """äº‹ä»¶å¤„ç†å™¨è£…é¥°å™¨"""
        def decorator(handler: Callable):
            if event_type not in self.handlers:
                self.handlers[event_type] = []
            self.handlers[event_type].append(handler)
            return handler
        return decorator

    async def start(self):
        """å¯åŠ¨äº‹ä»¶ç›‘å¬"""
        for event_type, handlers in self.handlers.items():
            asyncio.create_task(
                self._consume_events(event_type, handlers)
            )

    async def _consume_events(self, event_type: str, handlers: list):
        """æ¶ˆè´¹äº‹ä»¶"""
        async def handle_message(data):
            for handler in handlers:
                try:
                    await handler(data['data'])
                except Exception as e:
                    logger.error("event_handler_error",
                               event_type=event_type,
                               error=str(e))

        await self.mq.consume(
            queue_name=f"events.{event_type}",
            callback=handle_message,
            exchange='events',
            routing_key=event_type
        )

# ä½¿ç”¨ç¤ºä¾‹
mq = MessageQueue()
event_bus = EventBus(mq)

@event_bus.on('order.created')
async def handle_order_created(data):
    """å¤„ç†è®¢å•åˆ›å»ºäº‹ä»¶"""
    logger.info("order_created", order_id=data['order_id'])
    # æ¨é€åˆ°WebSocket
    await push_order_update(data['user_id'], data)

@event_bus.on('market.tick')
async def handle_market_tick(data):
    """å¤„ç†å¸‚åœºTickäº‹ä»¶"""
    # å†™å…¥ç¼“å­˜
    await cache.set(f"tick:{data['symbol']}", data, ttl=5)
    # æ¨é€åˆ°WebSocket
    await push_tick_data(data['symbol'], data)
```

---

## Part 3: æµ‹è¯•è‡ªåŠ¨åŒ–

### 3.1 Playwrightæµ‹è¯•æ¡†æ¶

```typescript
// tests/e2e/playwright.config.ts
import { defineConfig, devices } from '@playwright/test'

export default defineConfig({
  testDir: './tests/e2e',
  fullyParallel: true,
  forbidOnly: !!process.env.CI,
  retries: process.env.CI ? 2 : 0,
  workers: process.env.CI ? 1 : undefined,
  reporter: [
    ['html', { outputFolder: 'reports/playwright' }],
    ['json', { outputFile: 'reports/test-results.json' }],
    ['junit', { outputFile: 'reports/junit.xml' }]
  ],

  use: {
    baseURL: process.env.BASE_URL || 'http://localhost:5173',
    trace: 'on-first-retry',
    screenshot: 'only-on-failure',
    video: 'retain-on-failure',

    // è®¤è¯çŠ¶æ€
    storageState: 'tests/e2e/.auth/user.json'
  },

  projects: [
    // è®¾ç½®é¡¹ç›® - ç”¨äºç™»å½•
    {
      name: 'setup',
      testMatch: /.*\.setup\.ts/,
    },

    // æ¡Œé¢æµè§ˆå™¨
    {
      name: 'chromium',
      use: { ...devices['Desktop Chrome'] },
      dependencies: ['setup'],
    },
    {
      name: 'firefox',
      use: { ...devices['Desktop Firefox'] },
      dependencies: ['setup'],
    },

    // ç§»åŠ¨è®¾å¤‡
    {
      name: 'mobile',
      use: { ...devices['iPhone 12'] },
      dependencies: ['setup'],
    },
  ],

  webServer: [
    {
      command: 'npm run mock:start',
      port: 3001,
      reuseExistingServer: !process.env.CI,
    },
    {
      command: 'npm run dev',
      port: 5173,
      reuseExistingServer: !process.env.CI,
    }
  ],
})
```

```typescript
// tests/e2e/auth.setup.ts
import { test as setup, expect } from '@playwright/test'

const authFile = 'tests/e2e/.auth/user.json'

setup('authenticate', async ({ page }) => {
  // ç™»å½•
  await page.goto('/login')
  await page.fill('input[name="username"]', 'testuser')
  await page.fill('input[name="password"]', 'testpass')
  await page.click('button[type="submit"]')

  // ç­‰å¾…ç™»å½•æˆåŠŸ
  await page.waitForURL('/dashboard')
  await expect(page.locator('.user-menu')).toBeVisible()

  // ä¿å­˜è®¤è¯çŠ¶æ€
  await page.context().storageState({ path: authFile })
})
```

```typescript
// tests/e2e/realtime-data.spec.ts
import { test, expect } from '@playwright/test'

test.describe('å®æ—¶æ•°æ®æµæµ‹è¯•', () => {
  test.beforeEach(async ({ page }) => {
    await page.goto('/market/realtime')
  })

  test('WebSocketè¿æ¥å’Œæ•°æ®æ¨é€', async ({ page }) => {
    // ç›‘å¬WebSocketæ¶ˆæ¯
    const wsMessages = []
    page.on('websocket', ws => {
      ws.on('framereceived', event => {
        if (event.payload) {
          wsMessages.push(JSON.parse(event.payload))
        }
      })
    })

    // è®¢é˜…å¸‚åœºæ•°æ®
    await page.click('[data-testid="subscribe-btn"]')
    await page.selectOption('[data-testid="symbol-select"]', '600519.SH')

    // ç­‰å¾…WebSocketè¿æ¥
    await expect(page.locator('[data-testid="ws-status"]'))
      .toHaveText('å·²è¿æ¥', { timeout: 5000 })

    // éªŒè¯æ”¶åˆ°æ•°æ®
    await page.waitForTimeout(3000)  // ç­‰å¾…æ•°æ®æ¨é€

    expect(wsMessages.some(msg => msg.event === 'tick')).toBeTruthy()
    expect(wsMessages.some(msg => msg.data?.symbol === '600519.SH')).toBeTruthy()

    // éªŒè¯UIæ›´æ–°
    await expect(page.locator('[data-testid="tick-price"]'))
      .not.toBeEmpty()

    // æˆªå›¾
    await page.screenshot({
      path: 'reports/screenshots/realtime-data.png',
      fullPage: true
    })
  })

  test('æ•°æ®å»¶è¿Ÿæµ‹è¯•', async ({ page }) => {
    // è®°å½•æ—¶é—´æˆ³
    const timestamps = {
      send: 0,
      receive: 0
    }

    // ç›‘å¬APIè¯·æ±‚
    page.on('request', request => {
      if (request.url().includes('/api/v1/market')) {
        timestamps.send = Date.now()
      }
    })

    page.on('response', response => {
      if (response.url().includes('/api/v1/market')) {
        timestamps.receive = Date.now()
      }
    })

    // è§¦å‘æ•°æ®è¯·æ±‚
    await page.click('[data-testid="refresh-btn"]')

    // ç­‰å¾…å“åº”
    await page.waitForResponse(resp =>
      resp.url().includes('/api/v1/market') && resp.status() === 200
    )

    // éªŒè¯å»¶è¿Ÿ
    const latency = timestamps.receive - timestamps.send
    expect(latency).toBeLessThan(200)  // å°äº200ms

    console.log(`APIå»¶è¿Ÿ: ${latency}ms`)
  })

  test('å¹¶å‘è¿æ¥æµ‹è¯•', async ({ browser }) => {
    const contexts = []
    const pages = []

    // åˆ›å»º10ä¸ªå¹¶å‘è¿æ¥
    for (let i = 0; i < 10; i++) {
      const context = await browser.newContext()
      const page = await context.newPage()

      contexts.push(context)
      pages.push(page)

      await page.goto('/market/realtime')
    }

    // éªŒè¯æ‰€æœ‰è¿æ¥éƒ½æˆåŠŸ
    for (const page of pages) {
      await expect(page.locator('[data-testid="ws-status"]'))
        .toHaveText('å·²è¿æ¥', { timeout: 10000 })
    }

    // æ¸…ç†
    for (const context of contexts) {
      await context.close()
    }
  })
})
```

### 3.2 GitHub Actions CI/CD

```yaml
# .github/workflows/fullstack-ci.yml
name: Fullstack CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.10'

jobs:
  # ä»£ç è´¨é‡æ£€æŸ¥
  code-quality:
    name: Code Quality Check
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          npm ci
          pip install -r requirements.txt
          pip install black flake8 mypy

      - name: Run ESLint
        run: npm run lint

      - name: Run Black
        run: black --check web/backend

      - name: Run Flake8
        run: flake8 web/backend

      - name: Run MyPy
        run: mypy web/backend

  # å¥‘çº¦æµ‹è¯•
  contract-tests:
    name: API Contract Tests
    runs-on: ubuntu-latest
    needs: code-quality

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          npm install -g dredd
          pip install -r requirements.txt

      - name: Start backend
        run: |
          cd web/backend
          uvicorn app.main:app --host 0.0.0.0 --port 8000 &
          sleep 5
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost/test
          REDIS_URL: redis://localhost:6379

      - name: Run contract tests
        run: |
          dredd api-specs/openapi.yaml http://localhost:8000 \
            --reporter=json:reports/contract-results.json

      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: contract-test-results
          path: reports/

  # å•å…ƒæµ‹è¯•
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: code-quality

    strategy:
      matrix:
        test-suite: [frontend, backend]

    steps:
      - uses: actions/checkout@v3

      - name: Setup Node.js
        if: matrix.test-suite == 'frontend'
        uses: actions/setup-node@v3
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Setup Python
        if: matrix.test-suite == 'backend'
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies (Frontend)
        if: matrix.test-suite == 'frontend'
        run: |
          cd web/frontend
          npm ci

      - name: Install dependencies (Backend)
        if: matrix.test-suite == 'backend'
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-asyncio

      - name: Run tests (Frontend)
        if: matrix.test-suite == 'frontend'
        run: |
          cd web/frontend
          npm run test:unit -- --coverage

      - name: Run tests (Backend)
        if: matrix.test-suite == 'backend'
        run: |
          cd web/backend
          pytest tests/ --cov=app --cov-report=xml

      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: ${{ matrix.test-suite }}

  # E2Eæµ‹è¯•
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    needs: [contract-tests, unit-tests]

    steps:
      - uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install dependencies
        run: |
          npm ci
          npx playwright install --with-deps chromium

      - name: Start services
        run: |
          docker-compose up -d
          npm run mock:start &
          npm run dev &
          sleep 10

      - name: Run E2E tests
        run: npm run test:e2e

      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: playwright-report
          path: playwright-report/

      - name: Upload videos
        uses: actions/upload-artifact@v3
        if: failure()
        with:
          name: test-videos
          path: test-results/

  # æ€§èƒ½æµ‹è¯•
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: e2e-tests

    steps:
      - uses: actions/checkout@v3

      - name: Run Lighthouse CI
        uses: treosh/lighthouse-ci-action@v10
        with:
          urls: |
            http://localhost:5173
            http://localhost:5173/market
            http://localhost:5173/dashboard
          uploadArtifacts: true
          temporaryPublicStorage: true

      - name: Run k6 load tests
        uses: grafana/k6-action@v0.3.0
        with:
          filename: tests/performance/load-test.js

      - name: Check performance budget
        run: |
          node scripts/check-performance-budget.js

  # éƒ¨ç½²
  deploy:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [e2e-tests, performance-tests]
    if: github.ref == 'refs/heads/main'

    steps:
      - uses: actions/checkout@v3

      - name: Build Docker images
        run: |
          docker build -t mystocks/frontend:${{ github.sha }} ./web/frontend
          docker build -t mystocks/backend:${{ github.sha }} ./web/backend

      - name: Push to registry
        run: |
          echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
          docker push mystocks/frontend:${{ github.sha }}
          docker push mystocks/backend:${{ github.sha }}

      - name: Deploy to Kubernetes
        run: |
          kubectl set image deployment/frontend frontend=mystocks/frontend:${{ github.sha }}
          kubectl set image deployment/backend backend=mystocks/backend:${{ github.sha }}
          kubectl rollout status deployment/frontend
          kubectl rollout status deployment/backend
```

---

## Part 4: ç›‘æ§å’Œè¿½è¸ª

### 4.1 Jaegeré›†æˆ

```python
# tracing/jaeger_config.py
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.instrumentation.sqlalchemy import SQLAlchemyInstrumentor
from opentelemetry.instrumentation.redis import RedisInstrumentor
import structlog

logger = structlog.get_logger()

def setup_tracing(app_name: str = "mystocks", environment: str = "production"):
    """é…ç½®Jaegeråˆ†å¸ƒå¼è¿½è¸ª"""

    # é…ç½®èµ„æº
    resource = Resource.create({
        "service.name": app_name,
        "service.namespace": "mystocks",
        "service.instance.id": f"{app_name}-{os.getpid()}",
        "deployment.environment": environment
    })

    # é…ç½®Provider
    provider = TracerProvider(resource=resource)
    trace.set_tracer_provider(provider)

    # é…ç½®Jaegerå¯¼å‡ºå™¨
    jaeger_exporter = JaegerExporter(
        agent_host_name="localhost",
        agent_port=6831,
        collector_endpoint="http://localhost:14268/api/traces"
    )

    # æ·»åŠ æ‰¹å¤„ç†å™¨
    span_processor = BatchSpanProcessor(jaeger_exporter)
    provider.add_span_processor(span_processor)

    # è‡ªåŠ¨instrumentation
    FastAPIInstrumentor.instrument(tracer_provider=provider)
    RequestsInstrumentor.instrument(tracer_provider=provider)
    SQLAlchemyInstrumentor.instrument(tracer_provider=provider)
    RedisInstrumentor.instrument(tracer_provider=provider)

    logger.info("Jaeger tracing initialized", service=app_name)

    return trace.get_tracer(__name__)

# è‡ªå®šä¹‰è£…é¥°å™¨
def traced(name: str = None):
    """è¿½è¸ªè£…é¥°å™¨"""
    def decorator(func):
        tracer = trace.get_tracer(__name__)
        span_name = name or f"{func.__module__}.{func.__name__}"

        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            with tracer.start_as_current_span(span_name) as span:
                span.set_attributes({
                    "function.name": func.__name__,
                    "function.module": func.__module__
                })
                try:
                    result = await func(*args, **kwargs)
                    span.set_status(trace.Status(trace.StatusCode.OK))
                    return result
                except Exception as e:
                    span.set_status(
                        trace.Status(trace.StatusCode.ERROR, str(e))
                    )
                    span.record_exception(e)
                    raise

        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            with tracer.start_as_current_span(span_name) as span:
                span.set_attributes({
                    "function.name": func.__name__,
                    "function.module": func.__module__
                })
                try:
                    result = func(*args, **kwargs)
                    span.set_status(trace.Status(trace.StatusCode.OK))
                    return result
                except Exception as e:
                    span.set_status(
                        trace.Status(trace.StatusCode.ERROR, str(e))
                    )
                    span.record_exception(e)
                    raise

        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper

    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@traced("fetch_market_data")
async def fetch_market_data(symbol: str):
    """è·å–å¸‚åœºæ•°æ®ï¼ˆå¸¦è¿½è¸ªï¼‰"""
    tracer = trace.get_tracer(__name__)

    with tracer.start_span("query_database") as span:
        span.set_attributes({
            "db.statement": "SELECT * FROM market_data",
            "db.symbol": symbol
        })
        data = await db.fetch_one(...)

    with tracer.start_span("process_data") as span:
        span.set_attribute("data.count", len(data))
        processed = process_market_data(data)

    return processed
```

### 4.2 ç›‘æ§æŒ‡æ ‡è®¾è®¡

```python
# monitoring/prometheus_metrics.py
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from prometheus_client import CONTENT_TYPE_LATEST
from fastapi import Response
import time
from functools import wraps

# å®šä¹‰æŒ‡æ ‡
http_requests_total = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

http_request_duration = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint'],
    buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10]
)

websocket_connections = Gauge(
    'websocket_connections_active',
    'Active WebSocket connections'
)

cache_hits = Counter(
    'cache_hits_total',
    'Total cache hits',
    ['cache_type']
)

cache_misses = Counter(
    'cache_misses_total',
    'Total cache misses',
    ['cache_type']
)

db_query_duration = Histogram(
    'db_query_duration_seconds',
    'Database query duration',
    ['database', 'operation'],
    buckets=[0.001, 0.01, 0.05, 0.1, 0.5, 1, 5]
)

order_processing_time = Histogram(
    'order_processing_time_seconds',
    'Order processing time',
    ['order_type'],
    buckets=[0.01, 0.05, 0.1, 0.5, 1, 2, 5]
)

strategy_execution_time = Histogram(
    'strategy_execution_time_seconds',
    'Strategy execution time',
    ['strategy_name'],
    buckets=[0.1, 0.5, 1, 2, 5, 10, 30]
)

# ä¸šåŠ¡æŒ‡æ ‡
active_users = Gauge('active_users_total', 'Active users count')
orders_created = Counter('orders_created_total', 'Total orders created', ['order_type'])
trades_executed = Counter('trades_executed_total', 'Total trades executed', ['symbol'])
revenue_total = Counter('revenue_total', 'Total revenue', ['currency'])

# ç›‘æ§ä¸­é—´ä»¶
def metrics_middleware(app):
    """Prometheusç›‘æ§ä¸­é—´ä»¶"""

    @app.middleware("http")
    async def track_metrics(request, call_next):
        start_time = time.time()

        # è®°å½•è¯·æ±‚
        method = request.method
        endpoint = request.url.path

        try:
            response = await call_next(request)
            status = response.status_code

            # è®°å½•æŒ‡æ ‡
            http_requests_total.labels(
                method=method,
                endpoint=endpoint,
                status=status
            ).inc()

            duration = time.time() - start_time
            http_request_duration.labels(
                method=method,
                endpoint=endpoint
            ).observe(duration)

            return response

        except Exception as e:
            http_requests_total.labels(
                method=method,
                endpoint=endpoint,
                status=500
            ).inc()
            raise

# æ•°æ®åº“ç›‘æ§è£…é¥°å™¨
def monitor_db_operation(database: str, operation: str):
    """æ•°æ®åº“æ“ä½œç›‘æ§"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                duration = time.time() - start_time

                db_query_duration.labels(
                    database=database,
                    operation=operation
                ).observe(duration)

                return result
            except Exception as e:
                raise
        return wrapper
    return decorator

# æš´éœ²æŒ‡æ ‡ç«¯ç‚¹
async def metrics_endpoint(request):
    """PrometheusæŒ‡æ ‡ç«¯ç‚¹"""
    return Response(
        content=generate_latest(),
        media_type=CONTENT_TYPE_LATEST
    )

# è‡ªå®šä¹‰ä¸šåŠ¡æŒ‡æ ‡æ”¶é›†
class BusinessMetrics:
    """ä¸šåŠ¡æŒ‡æ ‡æ”¶é›†å™¨"""

    @staticmethod
    def record_order_created(order_type: str):
        """è®°å½•è®¢å•åˆ›å»º"""
        orders_created.labels(order_type=order_type).inc()

    @staticmethod
    def record_trade_executed(symbol: str, amount: float):
        """è®°å½•äº¤æ˜“æ‰§è¡Œ"""
        trades_executed.labels(symbol=symbol).inc()
        revenue_total.labels(currency='CNY').inc(amount * 0.001)  # æ‰‹ç»­è´¹

    @staticmethod
    def update_active_users(count: int):
        """æ›´æ–°æ´»è·ƒç”¨æˆ·æ•°"""
        active_users.set(count)
```

### 4.3 å‘Šè­¦è§„åˆ™é…ç½®

```yaml
# monitoring/alerting-rules.yml
groups:
  - name: mystocks_alerts
    interval: 30s
    rules:
      # APIæ€§èƒ½å‘Šè­¦
      - alert: HighResponseTime
        expr: |
          histogram_quantile(0.95,
            rate(http_request_duration_seconds_bucket[5m])
          ) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "APIå“åº”æ—¶é—´è¿‡é•¿"
          description: "95%åˆ†ä½æ•°å“åº”æ—¶é—´è¶…è¿‡1ç§’ (å½“å‰: {{ $value }}s)"

      - alert: HighErrorRate
        expr: |
          rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "é”™è¯¯ç‡è¿‡é«˜"
          description: "5xxé”™è¯¯ç‡è¶…è¿‡5% (å½“å‰: {{ $value | humanizePercentage }})"

      # WebSocketå‘Šè­¦
      - alert: WebSocketConnectionDrop
        expr: |
          rate(websocket_connections_active[1m]) < -10
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "WebSocketè¿æ¥å¤§é‡æ–­å¼€"
          description: "1åˆ†é’Ÿå†…æ–­å¼€è¶…è¿‡10ä¸ªè¿æ¥"

      # æ•°æ®åº“å‘Šè­¦
      - alert: SlowDatabaseQueries
        expr: |
          histogram_quantile(0.95,
            rate(db_query_duration_seconds_bucket[5m])
          ) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "æ•°æ®åº“æŸ¥è¯¢ç¼“æ…¢"
          description: "95%åˆ†ä½æ•°æŸ¥è¯¢æ—¶é—´è¶…è¿‡1ç§’"

      # Rediså‘Šè­¦
      - alert: LowCacheHitRate
        expr: |
          rate(cache_hits_total[5m]) /
          (rate(cache_hits_total[5m]) + rate(cache_misses_total[5m])) < 0.8
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½"
          description: "ç¼“å­˜å‘½ä¸­ç‡ä½äº80% (å½“å‰: {{ $value | humanizePercentage }})"

      # ä¸šåŠ¡å‘Šè­¦
      - alert: NoOrdersCreated
        expr: |
          rate(orders_created_total[10m]) == 0
        for: 10m
        labels:
          severity: info
        annotations:
          summary: "æ— æ–°è®¢å•"
          description: "è¿‡å»10åˆ†é’Ÿæ²¡æœ‰æ–°è®¢å•åˆ›å»º"
```

---

## Part 5: æ€§èƒ½ä¼˜åŒ–

### 5.1 æ•°æ®åº“ç´¢å¼•ç­–ç•¥

```sql
-- æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–è„šæœ¬
-- PostgreSQLç´¢å¼•ç­–ç•¥

-- 1. å¸‚åœºæ•°æ®è¡¨ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_daily_bars_symbol_date
ON daily_bars(symbol, date DESC);

CREATE INDEX CONCURRENTLY idx_daily_bars_date
ON daily_bars(date DESC)
WHERE date >= CURRENT_DATE - INTERVAL '30 days';

-- 2. è®¢å•è¡¨ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_orders_user_status
ON orders(user_id, status)
WHERE status IN ('pending', 'executing');

CREATE INDEX CONCURRENTLY idx_orders_created_at
ON orders(created_at DESC);

-- 3. ç­–ç•¥è¡¨ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_strategies_user_active
ON strategies(user_id, is_active)
WHERE is_active = true;

-- 4. éƒ¨åˆ†ç´¢å¼•ï¼ˆä»…ç´¢å¼•çƒ­æ•°æ®ï¼‰
CREATE INDEX CONCURRENTLY idx_trades_recent
ON trades(symbol, executed_at DESC)
WHERE executed_at >= CURRENT_DATE - INTERVAL '7 days';

-- 5. å¤åˆç´¢å¼•
CREATE INDEX CONCURRENTLY idx_fund_flow_composite
ON fund_flow_data(symbol, timeframe, date DESC);

-- 6. GINç´¢å¼•ï¼ˆç”¨äºJSONBå­—æ®µï¼‰
CREATE INDEX CONCURRENTLY idx_strategy_parameters_gin
ON strategies USING GIN (parameters);

-- 7. è¡¨è¾¾å¼ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_orders_total_amount
ON orders((quantity * price));

-- åˆ†æç´¢å¼•ä½¿ç”¨æƒ…å†µ
CREATE OR REPLACE VIEW index_usage_stats AS
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan as index_scans,
    idx_tup_read as tuples_read,
    idx_tup_fetch as tuples_fetched,
    pg_size_pretty(pg_relation_size(indexrelid)) as index_size
FROM pg_stat_user_indexes
ORDER BY idx_scan DESC;

-- æŸ¥æ‰¾æœªä½¿ç”¨çš„ç´¢å¼•
SELECT
    schemaname || '.' || tablename AS table,
    indexname AS index,
    pg_size_pretty(pg_relation_size(i.indexrelid)) AS index_size,
    idx_scan as index_scans
FROM pg_stat_user_indexes ui
JOIN pg_index i ON ui.indexrelid = i.indexrelid
WHERE NOT indisunique
    AND idx_scan < 50
    AND pg_relation_size(indexrelid) > 5000000;

-- æŸ¥æ‰¾ç¼ºå¤±çš„ç´¢å¼•ï¼ˆåŸºäºæŸ¥è¯¢æ—¥å¿—ï¼‰
CREATE OR REPLACE FUNCTION suggest_indexes()
RETURNS TABLE(
    table_name text,
    suggested_index text
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        schemaname || '.' || tablename,
        'CREATE INDEX ON ' || schemaname || '.' || tablename ||
        ' (' || attname || ')' AS suggested_index
    FROM pg_stats
    WHERE n_distinct > 100
        AND correlation < 0.1
        AND schemaname NOT IN ('pg_catalog', 'information_schema')
    ORDER BY n_distinct DESC
    LIMIT 10;
END;
$$ LANGUAGE plpgsql;
```

### 5.2 æŸ¥è¯¢ä¼˜åŒ–æ–¹æ¡ˆ

```python
# optimization/query_optimizer.py
from typing import List, Dict, Any
import asyncpg
from datetime import datetime, timedelta
import hashlib
import json

class QueryOptimizer:
    """æŸ¥è¯¢ä¼˜åŒ–å™¨"""

    def __init__(self, db_pool: asyncpg.Pool):
        self.db_pool = db_pool
        self.query_cache = {}
        self.slow_query_threshold = 1.0  # ç§’

    async def execute_optimized(self, query: str, *args,
                               use_cache: bool = True,
                               cache_ttl: int = 60):
        """æ‰§è¡Œä¼˜åŒ–æŸ¥è¯¢"""

        # ç”ŸæˆæŸ¥è¯¢æŒ‡çº¹
        query_hash = self._get_query_hash(query, args)

        # æ£€æŸ¥ç¼“å­˜
        if use_cache and query_hash in self.query_cache:
            cached = self.query_cache[query_hash]
            if cached['expires'] > datetime.now():
                return cached['result']

        # æ‰§è¡ŒæŸ¥è¯¢
        start_time = datetime.now()

        async with self.db_pool.acquire() as conn:
            # ä½¿ç”¨é¢„å¤„ç†è¯­å¥
            stmt = await conn.prepare(query)
            result = await stmt.fetch(*args)

        execution_time = (datetime.now() - start_time).total_seconds()

        # è®°å½•æ…¢æŸ¥è¯¢
        if execution_time > self.slow_query_threshold:
            await self._log_slow_query(query, args, execution_time)

        # ç¼“å­˜ç»“æœ
        if use_cache:
            self.query_cache[query_hash] = {
                'result': result,
                'expires': datetime.now() + timedelta(seconds=cache_ttl)
            }

        return result

    def _get_query_hash(self, query: str, args: tuple) -> str:
        """ç”ŸæˆæŸ¥è¯¢å“ˆå¸Œ"""
        query_str = f"{query}:{str(args)}"
        return hashlib.md5(query_str.encode()).hexdigest()

    async def _log_slow_query(self, query: str, args: tuple,
                              execution_time: float):
        """è®°å½•æ…¢æŸ¥è¯¢"""
        async with self.db_pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO slow_query_log
                (query, args, execution_time, created_at)
                VALUES ($1, $2, $3, $4)
            """, query, json.dumps(args), execution_time, datetime.now())

    async def batch_fetch(self, queries: List[Dict[str, Any]]):
        """æ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–"""
        async with self.db_pool.acquire() as conn:
            async with conn.transaction():
                results = []
                for q in queries:
                    stmt = await conn.prepare(q['query'])
                    result = await stmt.fetch(*q.get('args', []))
                    results.append(result)
                return results

    async def analyze_query_plan(self, query: str, *args):
        """åˆ†ææŸ¥è¯¢æ‰§è¡Œè®¡åˆ’"""
        async with self.db_pool.acquire() as conn:
            explain_query = f"EXPLAIN (ANALYZE, BUFFERS) {query}"
            plan = await conn.fetch(explain_query, *args)
            return plan

# N+1æŸ¥è¯¢ä¼˜åŒ–
class DataLoader:
    """æ‰¹é‡æ•°æ®åŠ è½½å™¨ï¼ˆè§£å†³N+1é—®é¢˜ï¼‰"""

    def __init__(self, db_pool: asyncpg.Pool):
        self.db_pool = db_pool
        self.batch_queue = {}
        self.batch_size = 100

    async def load_user(self, user_id: int):
        """åŠ è½½ç”¨æˆ·ï¼ˆæ‰¹é‡ï¼‰"""
        return await self._batch_load('users', 'id', user_id)

    async def load_orders(self, user_ids: List[int]):
        """æ‰¹é‡åŠ è½½è®¢å•"""
        if not user_ids:
            return {}

        async with self.db_pool.acquire() as conn:
            rows = await conn.fetch("""
                SELECT * FROM orders
                WHERE user_id = ANY($1)
                ORDER BY created_at DESC
            """, user_ids)

        # æŒ‰ç”¨æˆ·IDåˆ†ç»„
        result = {}
        for row in rows:
            user_id = row['user_id']
            if user_id not in result:
                result[user_id] = []
            result[user_id].append(dict(row))

        return result

    async def _batch_load(self, table: str, key: str, value: Any):
        """é€šç”¨æ‰¹é‡åŠ è½½"""
        batch_key = f"{table}:{key}"

        if batch_key not in self.batch_queue:
            self.batch_queue[batch_key] = {
                'values': [],
                'futures': []
            }

        batch = self.batch_queue[batch_key]
        batch['values'].append(value)

        # è¾¾åˆ°æ‰¹é‡å¤§å°ï¼Œæ‰§è¡ŒæŸ¥è¯¢
        if len(batch['values']) >= self.batch_size:
            await self._execute_batch(table, key, batch)

        # è¿”å›å¯¹åº”çš„ç»“æœ
        # å®é™…å®ç°éœ€è¦ä½¿ç”¨asyncio.Future
        return None
```

### 5.3 CDNé…ç½®

```nginx
# nginx/cdn.conf
# Nginx CDNé…ç½®

# é™æ€èµ„æºç¼“å­˜
server {
    listen 80;
    server_name cdn.mystocks.com;

    # å¯ç”¨Gzipå‹ç¼©
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types text/plain text/css text/xml text/javascript
               application/json application/javascript application/xml+rss
               application/rss+xml application/atom+xml image/svg+xml
               text/x-js text/x-cross-domain-policy application/x-font-ttf
               application/x-font-opentype application/vnd.ms-fontobject
               image/x-icon;

    # Brotliå‹ç¼©
    brotli on;
    brotli_comp_level 6;
    brotli_types text/plain text/css text/xml application/json
                 application/javascript application/xml+rss
                 application/atom+xml image/svg+xml;

    # é™æ€æ–‡ä»¶ä½ç½®
    root /var/www/mystocks/static;

    # ç¼“å­˜é…ç½®
    location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg|woff|woff2|ttf|eot)$ {
        # æµè§ˆå™¨ç¼“å­˜
        expires 1y;
        add_header Cache-Control "public, immutable";

        # ETag
        etag on;

        # è·¨åŸŸæ”¯æŒ
        add_header Access-Control-Allow-Origin "*";

        # å®‰å…¨å¤´
        add_header X-Content-Type-Options "nosniff";
        add_header X-Frame-Options "SAMEORIGIN";
        add_header X-XSS-Protection "1; mode=block";
    }

    # å›¾ç‰‡ä¼˜åŒ–
    location ~* \.(jpg|jpeg|png|gif|webp)$ {
        # å›¾ç‰‡å¤„ç†æ¨¡å—
        image_filter_buffer 20M;

        # æ ¹æ®è¯·æ±‚å‚æ•°è°ƒæ•´å›¾ç‰‡å¤§å°
        if ($arg_w) {
            image_filter resize $arg_w -;
        }

        if ($arg_h) {
            image_filter resize - $arg_h;
        }

        # WebPè‡ªåŠ¨è½¬æ¢
        if ($http_accept ~* "webp") {
            rewrite ^(.*)\.jpg$ $1.webp break;
            rewrite ^(.*)\.png$ $1.webp break;
        }
    }

    # é¢„åŠ è½½å…³é”®èµ„æº
    location / {
        add_header Link "</css/main.css>; rel=preload; as=style" always;
        add_header Link "</js/app.js>; rel=preload; as=script" always;
        add_header Link "</fonts/main.woff2>; rel=preload; as=font; crossorigin" always;
    }
}

# APIç¼“å­˜é…ç½®
upstream api_backend {
    server 127.0.0.1:8000 max_fails=3 fail_timeout=30s;
    server 127.0.0.1:8001 max_fails=3 fail_timeout=30s backup;
    keepalive 32;
}

server {
    listen 80;
    server_name api.mystocks.com;

    # APIå“åº”ç¼“å­˜
    proxy_cache_path /var/cache/nginx/api levels=1:2
                     keys_zone=api_cache:10m max_size=1g
                     inactive=60m use_temp_path=off;

    location /api/v1/market/ {
        proxy_pass http://api_backend;

        # ç¼“å­˜GETè¯·æ±‚
        proxy_cache api_cache;
        proxy_cache_methods GET HEAD;
        proxy_cache_key "$request_method$request_uri$args";
        proxy_cache_valid 200 5m;  # å¸‚åœºæ•°æ®ç¼“å­˜5åˆ†é’Ÿ
        proxy_cache_valid 404 1m;
        proxy_cache_bypass $http_cache_control;

        # æ·»åŠ ç¼“å­˜çŠ¶æ€å¤´
        add_header X-Cache-Status $upstream_cache_status;

        # åç«¯å¥åº·æ£€æŸ¥
        proxy_next_upstream error timeout http_500 http_502 http_503;
        proxy_connect_timeout 2s;
        proxy_send_timeout 5s;
        proxy_read_timeout 10s;
    }

    # WebSocketä»£ç†
    location /ws {
        proxy_pass http://api_backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;

        # WebSocketè¶…æ—¶è®¾ç½®
        proxy_connect_timeout 7d;
        proxy_send_timeout 7d;
        proxy_read_timeout 7d;
    }
}
```

---

## Part 6: å®‰å…¨æ¶æ„

### 6.1 è®¤è¯æˆæƒæµç¨‹

```python
# security/auth.py
from datetime import datetime, timedelta
from typing import Optional, Dict, Any
from jose import JWTError, jwt
from passlib.context import CryptContext
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
import redis
from pydantic import BaseModel
import secrets
import structlog

logger = structlog.get_logger()

# é…ç½®
SECRET_KEY = secrets.token_urlsafe(32)
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30
REFRESH_TOKEN_EXPIRE_DAYS = 7

# å¯†ç åŠ å¯†
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="/api/v1/auth/token")

class AuthManager:
    """è®¤è¯ç®¡ç†å™¨"""

    def __init__(self, redis_client: redis.Redis):
        self.redis_client = redis_client

    def verify_password(self, plain_password: str, hashed_password: str) -> bool:
        """éªŒè¯å¯†ç """
        return pwd_context.verify(plain_password, hashed_password)

    def get_password_hash(self, password: str) -> str:
        """å¯†ç å“ˆå¸Œ"""
        return pwd_context.hash(password)

    def create_access_token(self, data: dict,
                           expires_delta: Optional[timedelta] = None) -> str:
        """åˆ›å»ºè®¿é—®Token"""
        to_encode = data.copy()
        expire = datetime.utcnow() + (expires_delta or
                                      timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES))
        to_encode.update({"exp": expire, "type": "access"})

        token = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

        # å­˜å‚¨åˆ°Redisï¼ˆç”¨äºæ’¤é”€ï¼‰
        self.redis_client.setex(
            f"token:access:{data['sub']}",
            int(expires_delta.total_seconds() if expires_delta
                else ACCESS_TOKEN_EXPIRE_MINUTES * 60),
            token
        )

        return token

    def create_refresh_token(self, data: dict) -> str:
        """åˆ›å»ºåˆ·æ–°Token"""
        to_encode = data.copy()
        expire = datetime.utcnow() + timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS)
        to_encode.update({"exp": expire, "type": "refresh"})

        token = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

        # å­˜å‚¨åˆ°Redis
        self.redis_client.setex(
            f"token:refresh:{data['sub']}",
            REFRESH_TOKEN_EXPIRE_DAYS * 86400,
            token
        )

        return token

    async def verify_token(self, token: str) -> Dict[str, Any]:
        """éªŒè¯Token"""
        try:
            payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
            user_id = payload.get("sub")
            token_type = payload.get("type")

            if user_id is None:
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Invalid token"
                )

            # æ£€æŸ¥Tokenæ˜¯å¦è¢«æ’¤é”€
            stored_token = self.redis_client.get(f"token:{token_type}:{user_id}")
            if stored_token != token:
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Token has been revoked"
                )

            return payload

        except JWTError:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Could not validate credentials"
            )

    async def revoke_token(self, user_id: str):
        """æ’¤é”€ç”¨æˆ·çš„æ‰€æœ‰Token"""
        self.redis_client.delete(f"token:access:{user_id}")
        self.redis_client.delete(f"token:refresh:{user_id}")

        logger.info("tokens_revoked", user_id=user_id)

# æƒé™ç®¡ç†
class PermissionChecker:
    """æƒé™æ£€æŸ¥å™¨"""

    def __init__(self, required_permissions: List[str]):
        self.required_permissions = required_permissions

    async def __call__(self,
                       current_user: User = Depends(get_current_user)) -> User:
        """æ£€æŸ¥ç”¨æˆ·æƒé™"""
        user_permissions = set(current_user.permissions)
        required = set(self.required_permissions)

        if not required.issubset(user_permissions):
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Insufficient permissions"
            )

        return current_user

# ä½¿ç”¨ç¤ºä¾‹
@app.post("/api/v1/orders", dependencies=[Depends(PermissionChecker(["trade:create"]))])
async def create_order(order: OrderCreate,
                       current_user: User = Depends(get_current_user)):
    """åˆ›å»ºè®¢å•ï¼ˆéœ€è¦äº¤æ˜“æƒé™ï¼‰"""
    return await OrderService.create_order(order, current_user)
```

### 6.2 APIé™æµå’Œç†”æ–­

```python
# security/rate_limiter.py
import asyncio
from datetime import datetime, timedelta
from typing import Optional
import redis.asyncio as redis
from fastapi import HTTPException, Request, status
import structlog

logger = structlog.get_logger()

class RateLimiter:
    """APIé™æµå™¨"""

    def __init__(self, redis_client: redis.Redis):
        self.redis_client = redis_client

    async def check_rate_limit(self,
                               key: str,
                               limit: int,
                               window: int) -> bool:
        """æ£€æŸ¥é€Ÿç‡é™åˆ¶

        Args:
            key: é™æµé”®ï¼ˆå¦‚ç”¨æˆ·IDæˆ–IPï¼‰
            limit: æ—¶é—´çª—å£å†…æœ€å¤§è¯·æ±‚æ•°
            window: æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
        """
        pipeline = self.redis_client.pipeline()
        now = datetime.now()
        window_start = now - timedelta(seconds=window)

        # ä½¿ç”¨æ»‘åŠ¨çª—å£ç®—æ³•
        pipeline.zremrangebyscore(key, 0, window_start.timestamp())
        pipeline.zadd(key, {str(now.timestamp()): now.timestamp()})
        pipeline.zcount(key, window_start.timestamp(), now.timestamp())
        pipeline.expire(key, window)

        results = await pipeline.execute()
        request_count = results[2]

        if request_count > limit:
            return False

        return True

    async def get_remaining(self, key: str, limit: int, window: int) -> int:
        """è·å–å‰©ä½™è¯·æ±‚æ•°"""
        now = datetime.now()
        window_start = now - timedelta(seconds=window)

        count = await self.redis_client.zcount(
            key,
            window_start.timestamp(),
            now.timestamp()
        )

        return max(0, limit - count)

class CircuitBreaker:
    """ç†”æ–­å™¨"""

    def __init__(self,
                 failure_threshold: int = 5,
                 recovery_timeout: int = 60,
                 expected_exception: type = Exception):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.expected_exception = expected_exception
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "closed"  # closed, open, half-open

    async def call(self, func, *args, **kwargs):
        """é€šè¿‡ç†”æ–­å™¨è°ƒç”¨å‡½æ•°"""

        # æ£€æŸ¥ç†”æ–­å™¨çŠ¶æ€
        if self.state == "open":
            if self._should_attempt_reset():
                self.state = "half-open"
            else:
                raise HTTPException(
                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                    detail="Service temporarily unavailable"
                )

        try:
            result = await func(*args, **kwargs)
            self._on_success()
            return result

        except self.expected_exception as e:
            self._on_failure()
            raise

    def _should_attempt_reset(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥å°è¯•é‡ç½®"""
        return (
            self.last_failure_time and
            datetime.now() - self.last_failure_time >
            timedelta(seconds=self.recovery_timeout)
        )

    def _on_success(self):
        """æˆåŠŸè°ƒç”¨"""
        self.failure_count = 0
        self.state = "closed"

    def _on_failure(self):
        """å¤±è´¥è°ƒç”¨"""
        self.failure_count += 1
        self.last_failure_time = datetime.now()

        if self.failure_count >= self.failure_threshold:
            self.state = "open"
            logger.warning("circuit_breaker_opened",
                          failures=self.failure_count)

# é™æµä¸­é—´ä»¶
def rate_limit_middleware(requests_per_minute: int = 60):
    """é™æµä¸­é—´ä»¶å·¥å‚"""

    async def middleware(request: Request, call_next):
        # è·å–å®¢æˆ·ç«¯æ ‡è¯†
        client_id = request.client.host
        if hasattr(request.state, "user"):
            client_id = f"user:{request.state.user.id}"

        # æ£€æŸ¥é™æµ
        key = f"rate_limit:{client_id}"
        limiter = RateLimiter(redis_client)

        if not await limiter.check_rate_limit(key, requests_per_minute, 60):
            remaining = await limiter.get_remaining(key, requests_per_minute, 60)

            raise HTTPException(
                status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                detail="Rate limit exceeded",
                headers={
                    "X-RateLimit-Limit": str(requests_per_minute),
                    "X-RateLimit-Remaining": str(remaining),
                    "X-RateLimit-Reset": str(int(datetime.now().timestamp()) + 60)
                }
            )

        response = await call_next(request)

        # æ·»åŠ é™æµå¤´
        remaining = await limiter.get_remaining(key, requests_per_minute, 60)
        response.headers["X-RateLimit-Limit"] = str(requests_per_minute)
        response.headers["X-RateLimit-Remaining"] = str(remaining)

        return response

    return middleware

# ä½¿ç”¨ç¤ºä¾‹
app.add_middleware(rate_limit_middleware(requests_per_minute=100))

# ç†”æ–­å™¨ä½¿ç”¨
market_data_breaker = CircuitBreaker(
    failure_threshold=5,
    recovery_timeout=60,
    expected_exception=ConnectionError
)

async def get_market_data_with_breaker(symbol: str):
    """è·å–å¸‚åœºæ•°æ®ï¼ˆå¸¦ç†”æ–­ï¼‰"""
    return await market_data_breaker.call(
        fetch_market_data,
        symbol
    )
```

---

## Part 7: å®æ–½è®¡åˆ’

### Week 1: åŸºç¡€æ¶æ„æ­å»ºï¼ˆ40å°æ—¶ï¼‰

#### Day 1-2: ç¯å¢ƒå‡†å¤‡å’ŒAPIè§„èŒƒ
- [ ] åˆ›å»ºOpenAPIè§„èŒƒæ–‡æ¡£ (4h)
- [ ] é…ç½®Swagger UI (2h)
- [ ] æ­å»ºMockæœåŠ¡å™¨ (4h)
- [ ] é…ç½®å¼€å‘ç¯å¢ƒåˆ‡æ¢ (2h)
- [ ] ç¼–å†™APIå¥‘çº¦æµ‹è¯• (4h)

#### Day 3-4: Rediså’Œæ¶ˆæ¯é˜Ÿåˆ—
- [ ] éƒ¨ç½²Redis Cluster (4h)
- [ ] å®ç°ç¼“å­˜ç®¡ç†å™¨ (6h)
- [ ] éƒ¨ç½²RabbitMQ (2h)
- [ ] å®ç°äº‹ä»¶æ€»çº¿ (4h)

#### Day 5: CI/CDé…ç½®
- [ ] é…ç½®GitHub Actions (4h)
- [ ] è®¾ç½®å¥‘çº¦æµ‹è¯•æµæ°´çº¿ (2h)
- [ ] é…ç½®ä»£ç è´¨é‡æ£€æŸ¥ (2h)

**äº¤ä»˜ç‰©**:
- OpenAPIæ–‡æ¡£å®Œæˆ
- MockæœåŠ¡å¯ç”¨
- Redisç¼“å­˜è¿è¡Œ
- CI/CDæµæ°´çº¿é…ç½®

### Week 2: å®æ—¶é€šä¿¡å®ç°ï¼ˆ40å°æ—¶ï¼‰

#### Day 1-2: WebSocketæœåŠ¡
- [ ] å®ç°WebSocketæœåŠ¡å™¨ (8h)
- [ ] é›†æˆSocket.IO (4h)
- [ ] å®ç°è¿æ¥ç®¡ç†å™¨ (4h)

#### Day 3-4: å‰ç«¯é›†æˆ
- [ ] å®ç°WebSocketå®¢æˆ·ç«¯ (6h)
- [ ] åˆ›å»ºå®æ—¶æ•°æ®ç»„ä»¶ (6h)
- [ ] å®ç°æ–­çº¿é‡è¿æœºåˆ¶ (4h)

#### Day 5: æµ‹è¯•å’Œä¼˜åŒ–
- [ ] WebSocketå‹åŠ›æµ‹è¯• (4h)
- [ ] æ€§èƒ½ä¼˜åŒ– (4h)

**äº¤ä»˜ç‰©**:
- WebSocketåŒå‘é€šä¿¡
- å®æ—¶æ•°æ®æ¨é€
- è‡ªåŠ¨é‡è¿æœºåˆ¶

### Week 3: ç›‘æ§å’Œæµ‹è¯•ï¼ˆ40å°æ—¶ï¼‰

#### Day 1-2: ç›‘æ§ç³»ç»Ÿ
- [ ] éƒ¨ç½²Prometheus + Grafana (4h)
- [ ] é…ç½®ç›‘æ§æŒ‡æ ‡ (6h)
- [ ] åˆ›å»ºç›‘æ§é¢æ¿ (6h)

#### Day 3-4: åˆ†å¸ƒå¼è¿½è¸ª
- [ ] éƒ¨ç½²Jaeger (4h)
- [ ] é›†æˆOpenTelemetry (6h)
- [ ] é…ç½®è¿½è¸ªé‡‡æ · (2h)

#### Day 5: E2Eæµ‹è¯•
- [ ] é…ç½®Playwright (4h)
- [ ] ç¼–å†™E2Eæµ‹è¯•ç”¨ä¾‹ (8h)

**äº¤ä»˜ç‰©**:
- ç›‘æ§ç³»ç»Ÿè¿è¡Œ
- åˆ†å¸ƒå¼è¿½è¸ªå¯ç”¨
- E2Eæµ‹è¯•è¦†ç›–

### Week 4: æ€§èƒ½ä¼˜åŒ–å’Œå®‰å…¨åŠ å›ºï¼ˆ40å°æ—¶ï¼‰

#### Day 1-2: æ€§èƒ½ä¼˜åŒ–
- [ ] æ•°æ®åº“ç´¢å¼•ä¼˜åŒ– (6h)
- [ ] æŸ¥è¯¢ä¼˜åŒ– (6h)
- [ ] CDNé…ç½® (4h)

#### Day 3-4: å®‰å…¨åŠ å›º
- [ ] å®ç°OAuth2.0æ”¹è¿› (6h)
- [ ] é…ç½®APIé™æµ (4h)
- [ ] å®ç°ç†”æ–­æœºåˆ¶ (4h)

#### Day 5: éƒ¨ç½²å’ŒéªŒè¯
- [ ] ç”Ÿäº§ç¯å¢ƒéƒ¨ç½² (4h)
- [ ] æ€§èƒ½æµ‹è¯• (2h)
- [ ] å®‰å…¨æ‰«æ (2h)

**äº¤ä»˜ç‰©**:
- æ€§èƒ½è¾¾æ ‡ï¼ˆ<200mså“åº”ï¼‰
- å®‰å…¨æœºåˆ¶å®Œå–„
- ç”Ÿäº§ç¯å¢ƒå°±ç»ª

---

## Part 8: ä»£ç å®ç°ç¤ºä¾‹

### 8.1 Docker Composeé…ç½®

```yaml
# docker-compose.yml
version: '3.8'

services:
  # å‰ç«¯
  frontend:
    build: ./web/frontend
    ports:
      - "5173:5173"
    environment:
      - VITE_API_BASE_URL=http://localhost:8000
      - VITE_WS_URL=ws://localhost:8000/ws
    volumes:
      - ./web/frontend:/app
    depends_on:
      - backend

  # åç«¯
  backend:
    build: ./web/backend
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:pass@postgres:5432/mystocks
      - REDIS_URL=redis://redis:6379
      - RABBITMQ_URL=amqp://guest:guest@rabbitmq:5672
      - JAEGER_AGENT_HOST=jaeger
    depends_on:
      - postgres
      - redis
      - rabbitmq
      - tdengine

  # PostgreSQL
  postgres:
    image: timescale/timescaledb:2.11.0-pg15
    environment:
      - POSTGRES_USER=mystocks
      - POSTGRES_PASSWORD=mystocks123
      - POSTGRES_DB=mystocks
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  # TDengine
  tdengine:
    image: tdengine/tdengine:3.0.0.0
    ports:
      - "6030:6030"
      - "6041:6041"
    volumes:
      - tdengine_data:/var/lib/taos

  # Redis
  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"

  # RabbitMQ
  rabbitmq:
    image: rabbitmq:3-management
    environment:
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin123
    ports:
      - "5672:5672"
      - "15672:15672"
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq

  # Nginx
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/conf.d:/etc/nginx/conf.d
    depends_on:
      - frontend
      - backend

  # Prometheus
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/alerting-rules.yml:/etc/prometheus/rules.yml
      - prometheus_data:/prometheus

  # Grafana
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
    depends_on:
      - prometheus

  # Jaeger
  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "6831:6831/udp"
      - "16686:16686"
      - "14268:14268"
    environment:
      - COLLECTOR_ZIPKIN_HOST_PORT=:9411

  # Elasticsearch (for logging)
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"

  # Kibana
  kibana:
    image: docker.elastic.co/kibana/kibana:8.8.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch

volumes:
  postgres_data:
  tdengine_data:
  redis_data:
  rabbitmq_data:
  prometheus_data:
  grafana_data:
  elasticsearch_data:
```

### 8.2 Makefile

```makefile
# Makefile
.PHONY: help install dev test build deploy clean

help:
	@echo "MyStockså¼€å‘å‘½ä»¤:"
	@echo "  make install    - å®‰è£…ä¾èµ–"
	@echo "  make dev        - å¯åŠ¨å¼€å‘ç¯å¢ƒ"
	@echo "  make test       - è¿è¡Œæµ‹è¯•"
	@echo "  make build      - æ„å»ºç”Ÿäº§é•œåƒ"
	@echo "  make deploy     - éƒ¨ç½²åˆ°ç”Ÿäº§"
	@echo "  make clean      - æ¸…ç†ä¸´æ—¶æ–‡ä»¶"

install:
	npm install
	cd web/frontend && npm install
	cd web/backend && pip install -r requirements.txt

dev:
	docker-compose up -d postgres redis rabbitmq
	./start-dev.sh

test:
	npm run test:contract
	npm run test:unit
	npm run test:e2e

build:
	docker build -t mystocks/frontend:latest ./web/frontend
	docker build -t mystocks/backend:latest ./web/backend

deploy:
	kubectl apply -f k8s/
	kubectl rollout status deployment/frontend
	kubectl rollout status deployment/backend

clean:
	find . -type d -name "__pycache__" -exec rm -rf {} +
	find . -type f -name "*.pyc" -delete
	rm -rf node_modules web/frontend/node_modules
	rm -rf reports coverage .pytest_cache
```

### 8.3 ç¯å¢ƒå˜é‡é…ç½®

```bash
# .env.example
# æ•°æ®åº“é…ç½®
POSTGRESQL_HOST=localhost
POSTGRESQL_PORT=5432
POSTGRESQL_USER=mystocks
POSTGRESQL_PASSWORD=mystocks123
POSTGRESQL_DATABASE=mystocks

TDENGINE_HOST=localhost
TDENGINE_PORT=6030
TDENGINE_USER=root
TDENGINE_PASSWORD=taosdata
TDENGINE_DATABASE=market_data

# Redisé…ç½®
REDIS_URL=redis://localhost:6379/0

# RabbitMQé…ç½®
RABBITMQ_URL=amqp://admin:admin123@localhost:5672/

# JWTé…ç½®
JWT_SECRET_KEY=your-secret-key-change-this-in-production
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30

# ç›‘æ§é…ç½®
JAEGER_AGENT_HOST=localhost
JAEGER_AGENT_PORT=6831
PROMETHEUS_ENDPOINT=/metrics

# APIé™æµ
RATE_LIMIT_PER_MINUTE=100
RATE_LIMIT_PER_HOUR=3000

# æ—¥å¿—é…ç½®
LOG_LEVEL=INFO
LOG_FORMAT=json
```

---

## æ€»ç»“

### å®æ–½æˆæœé¢„æœŸ

é€šè¿‡4å‘¨çš„æ¶æ„ä¼˜åŒ–ï¼ŒMyStocksç³»ç»Ÿå°†å®ç°ï¼š

#### æŠ€æœ¯æŒ‡æ ‡
- âœ… **APIå“åº”æ—¶é—´**: <200ms (P95)
- âœ… **é¡µé¢åŠ è½½æ—¶é—´**: <1.5s
- âœ… **WebSocketå»¶è¿Ÿ**: <50ms
- âœ… **ç³»ç»Ÿå¯ç”¨æ€§**: 99.9%
- âœ… **æµ‹è¯•è¦†ç›–ç‡**: >90%

#### ä¸šåŠ¡ä»·å€¼
- âœ… **å¼€å‘æ•ˆç‡æå‡**: 60%
- âœ… **Bugå‡å°‘**: 70%
- âœ… **éƒ¨ç½²é¢‘ç‡**: ä»å‘¨å‘å¸ƒåˆ°æ—¥å‘å¸ƒ
- âœ… **ç”¨æˆ·æ»¡æ„åº¦**: æå‡40%

#### å›¢é˜Ÿæ”¶ç›Š
- âœ… **å‰åç«¯å¹¶è¡Œå¼€å‘**
- âœ… **è‡ªåŠ¨åŒ–æµ‹è¯•ä¿éšœ**
- âœ… **å®æ—¶ç›‘æ§é¢„è­¦**
- âœ… **æ ‡å‡†åŒ–å¼€å‘æµç¨‹**

### å…³é”®æˆåŠŸå› ç´ 

1. **æ¸è¿›å¼æ”¹é€ **: ä¸å½±å“ç°æœ‰ä¸šåŠ¡ï¼Œé€æ­¥ä¼˜åŒ–
2. **è‡ªåŠ¨åŒ–ä¼˜å…ˆ**: æ‰€æœ‰é‡å¤å·¥ä½œè‡ªåŠ¨åŒ–
3. **ç›‘æ§é©±åŠ¨**: åŸºäºæ•°æ®åšå†³ç­–
4. **æŒç»­ä¼˜åŒ–**: å»ºç«‹åé¦ˆå¾ªç¯

### é£é™©å’Œç¼“è§£

| é£é™© | æ¦‚ç‡ | å½±å“ | ç¼“è§£æªæ–½ |
|------|------|------|----------|
| æŠ€æœ¯å€ºåŠ¡ | ä¸­ | é«˜ | åˆ†é˜¶æ®µé‡æ„ï¼Œä¿ç•™å›æ»šæ–¹æ¡ˆ |
| æ€§èƒ½é€€åŒ– | ä½ | é«˜ | æ€§èƒ½æµ‹è¯•é—¨ç¦ï¼Œç°åº¦å‘å¸ƒ |
| å®‰å…¨æ¼æ´ | ä½ | é«˜ | å®‰å…¨æ‰«æï¼Œä»£ç å®¡æŸ¥ |
| å›¢é˜ŸæŠµè§¦ | ä¸­ | ä¸­ | åŸ¹è®­æ”¯æŒï¼Œæ¸è¿›æ¨è¿› |

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³å¼€å§‹**: Week 1 - åŸºç¡€æ¶æ„æ­å»º
2. **å›¢é˜ŸåŸ¹è®­**: æ–°æŠ€æœ¯æ ˆåŸ¹è®­ï¼ˆ2å¤©ï¼‰
3. **è¯•ç‚¹é¡¹ç›®**: é€‰æ‹©ä½é£é™©æ¨¡å—è¯•ç‚¹
4. **æŒç»­æ”¹è¿›**: å»ºç«‹æŠ€æœ¯å§”å‘˜ä¼šï¼Œå®šæœŸè¯„å®¡

---

**æ–‡æ¡£ç»´æŠ¤**:
- æ¯å‘¨æ›´æ–°å®æ–½è¿›åº¦
- è®°å½•é‡åˆ°çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
- æ”¶é›†å›¢é˜Ÿåé¦ˆï¼ŒæŒç»­ä¼˜åŒ–

**è”ç³»æ–¹å¼**:
- æŠ€æœ¯æ”¯æŒ: tech-support@mystocks.com
- æ¶æ„è®¨è®º: architecture@mystocks.com

---

*æœ¬æ–¹æ¡ˆåŸºäºå½“å‰ç³»ç»ŸçŠ¶å†µå’Œå›¢é˜Ÿèƒ½åŠ›è®¾è®¡ï¼Œå»ºè®®æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ã€‚æ‰€æœ‰æŠ€æœ¯é€‰å‹å‡ä¸ºå¼€æºæ–¹æ¡ˆï¼Œæ— è®¸å¯æˆæœ¬ã€‚*