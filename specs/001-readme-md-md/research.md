# Technical Research: MyStocksæ•°æ®ç®¡ç†ç³»ç»Ÿ

**åˆ›å»ºäºº**: Claude (è‡ªåŠ¨ç”Ÿæˆ)
**ç‰ˆæœ¬**: 1.0.0
**åˆ›å»ºæ—¥æœŸ**: 2025-10-11
**å…³è”è®¡åˆ’**: [plan.md](plan.md)

## ç ”ç©¶æ¦‚è¿°

æœ¬æ–‡æ¡£æ•´åˆäº† Phase 0 (Outline & Research) é˜¶æ®µçš„8ä¸ªç ”ç©¶ä»»åŠ¡æˆæœ,ä¸ºåç»­è®¾è®¡å’Œå®æ–½æä¾›æŠ€æœ¯å†³ç­–ä¾æ®ã€‚æ‰€æœ‰å†³ç­–å‡å·²éªŒè¯å®ªæ³•åˆè§„æ€§,ç¡®ä¿ç¬¦åˆ5å±‚æ•°æ®åˆ†ç±»ã€é…ç½®é©±åŠ¨ã€æ™ºèƒ½è·¯ç”±ç­‰æ ¸å¿ƒåŸåˆ™ã€‚

## 1. TDengineé›†æˆå†³ç­– (R1)

### Decision (å†³ç­–)
- **è¿æ¥æ–¹å¼**: WebSocket + å‚æ•°ç»‘å®š (taosws connector)
- **è¡¨ç»“æ„**: Super Table (è¶…çº§è¡¨) æ¶æ„,1ä¸ªsymbolå¯¹åº”1ä¸ªå­è¡¨
- **æ•°æ®å‹ç¼©**: ZSTDå‹ç¼© + é«˜å‹ç¼©çº§åˆ«,ç›®æ ‡å‹ç¼©æ¯” 20:1
- **ç¼–ç ç­–ç•¥**: delta-d (DOUBLE), delta-i (BIGINT), simple8b (æ•´æ•°)

### Rationale (æŠ€æœ¯åŸå› )
1. **æ€§èƒ½ä¼˜åŠ¿**: WebSocketè¿æ¥æ¯”REST APIå¿«5-10å€,æ”¯æŒ10,000+ tick/ç§’å†™å…¥
2. **å‚æ•°ç»‘å®š**: é¿å…SQLæ³¨å…¥,å‡å°‘SQLè§£æå¼€é”€,æå‡æ‰¹é‡æ’å…¥æ•ˆç‡
3. **Super Tableè®¾è®¡**: æŒ‰symbolè‡ªåŠ¨åˆ†åŒº,æ”¯æŒé«˜æ•ˆçš„tagè¿‡æ»¤æŸ¥è¯¢
4. **å‹ç¼©æ•ˆç‡**: ZSTDç®—æ³•åœ¨æ—¶åºæ•°æ®ä¸Šå¯è¾¾20:1å‹ç¼©æ¯”,æ˜¾è‘—é™ä½å­˜å‚¨æˆæœ¬

### Alternatives Considered (è€ƒè™‘çš„å…¶ä»–æ–¹æ¡ˆ)
- âŒ **REST API**: å»¶è¿Ÿé«˜,ä¸é€‚åˆé«˜é¢‘æ•°æ®
- âŒ **åŸç”ŸTCPè¿æ¥**: å¤æ‚åº¦é«˜,ç»´æŠ¤æˆæœ¬å¤§
- âŒ **æ™®é€šè¡¨**: æ— æ³•åˆ©ç”¨TDengineçš„tagç´¢å¼•ä¼˜åŠ¿

### Implementation Notes (å®ç°è¦ç‚¹)

#### è¿æ¥é…ç½®
```python
import taosws

# WebSocketè¿æ¥å­—ç¬¦ä¸²
conn = taosws.connect(
    "taosws://root:taosdata@localhost:6041/market_data",
    timeout=30
)
```

#### Super Tableåˆ›å»ºæ¨¡æ¿
```sql
-- Tickæ•°æ®è¶…çº§è¡¨
CREATE STABLE IF NOT EXISTS tick_data (
    ts TIMESTAMP,                    -- æ—¶é—´æˆ³(ä¸»é”®)
    price DOUBLE,                    -- æˆäº¤ä»·æ ¼
    volume BIGINT,                   -- æˆäº¤é‡
    amount DOUBLE,                   -- æˆäº¤é¢
    buy_count INT,                   -- ä¹°ç›˜ç¬”æ•°
    sell_count INT                   -- å–ç›˜ç¬”æ•°
) TAGS (
    symbol BINARY(16),               -- è‚¡ç¥¨ä»£ç 
    exchange BINARY(16),             -- äº¤æ˜“æ‰€
    security_type BINARY(16)         -- è¯åˆ¸ç±»å‹
)
ENCODE 'delta-d,delta-i,simple8b'    -- ç¼–ç ç­–ç•¥
COMPRESS 'zstd'                      -- å‹ç¼©ç®—æ³•
LEVEL 'high';                        -- å‹ç¼©çº§åˆ«

-- åˆ†é’ŸKçº¿è¶…çº§è¡¨
CREATE STABLE IF NOT EXISTS minute_kline (
    ts TIMESTAMP,
    open DOUBLE,
    high DOUBLE,
    low DOUBLE,
    close DOUBLE,
    volume BIGINT,
    amount DOUBLE,
    num_trades INT
) TAGS (
    symbol BINARY(16),
    exchange BINARY(16),
    interval_type BINARY(8)          -- 1min/5min/15min
)
ENCODE 'delta-d,delta-i,simple8b'
COMPRESS 'zstd'
LEVEL 'high';

-- ç›˜å£å¿«ç…§è¶…çº§è¡¨ (Level-2)
CREATE STABLE IF NOT EXISTS market_snapshot (
    ts TIMESTAMP,
    bid_prices BINARY(256),          -- JSONæ ¼å¼çš„10æ¡£ä¹°ä»·
    bid_volumes BINARY(256),         -- JSONæ ¼å¼çš„10æ¡£ä¹°é‡
    ask_prices BINARY(256),          -- JSONæ ¼å¼çš„10æ¡£å–ä»·
    ask_volumes BINARY(256),         -- JSONæ ¼å¼çš„10æ¡£å–é‡
    total_bid_volume BIGINT,
    total_ask_volume BIGINT
) TAGS (
    symbol BINARY(16),
    exchange BINARY(16)
)
COMPRESS 'zstd'
LEVEL 'medium';                      -- JSONå­—æ®µå‹ç¼©ç‡è¾ƒä½
```

#### å‚æ•°åŒ–æ‰¹é‡æ’å…¥
```python
# ä½¿ç”¨å‚æ•°ç»‘å®šæ‰¹é‡æ’å…¥
stmt = conn.statement("INSERT INTO ? USING tick_data TAGS(?, ?, ?) VALUES(?, ?, ?, ?, ?, ?)")

for symbol_batch in symbol_batches:
    subtable_name = f"tick_{symbol_batch['symbol']}"

    # ç»‘å®šè¡¨åå’Œtags
    stmt.set_tbname(subtable_name)
    stmt.set_tags([
        symbol_batch['symbol'],
        symbol_batch['exchange'],
        symbol_batch['security_type']
    ])

    # æ‰¹é‡ç»‘å®šæ•°æ®
    stmt.bind_param([
        symbol_batch['timestamps'],     # TIMESTAMPæ•°ç»„
        symbol_batch['prices'],         # DOUBLEæ•°ç»„
        symbol_batch['volumes'],        # BIGINTæ•°ç»„
        symbol_batch['amounts'],        # DOUBLEæ•°ç»„
        symbol_batch['buy_counts'],     # INTæ•°ç»„
        symbol_batch['sell_counts']     # INTæ•°ç»„
    ])

    stmt.add_batch()

stmt.execute()
stmt.close()
```

#### æ€§èƒ½ä¼˜åŒ–å»ºè®®
1. **æ‰¹é‡å¤§å°**: æ¯æ‰¹1000-5000æ¡è®°å½•,æ ¹æ®ç½‘ç»œå»¶è¿Ÿè°ƒæ•´
2. **å¹¶å‘å†™å…¥**: å¤šçº¿ç¨‹æŒ‰symbolåˆ†åŒºå¹¶å‘å†™å…¥,é¿å…é”ç«äº‰
3. **ç¼“å­˜ç­–ç•¥**: å¯ç”¨TDengineçš„ç¼“å­˜ (cachemodel=both)
4. **WALé…ç½®**: ç”Ÿäº§ç¯å¢ƒè®¾ç½® wal_level=1 (æ•°æ®æŒä¹…åŒ–)

---

## 2. TimescaleDBé…ç½®å†³ç­– (R2)

### Decision (å†³ç­–)
- **Chunké—´éš”**: 1å¤© (é€‚ç”¨äºè¡ç”Ÿæ•°æ®å’Œäº¤æ˜“æ•°æ®)
- **å‹ç¼©ç­–ç•¥**: 30å¤©åè‡ªåŠ¨å‹ç¼©
- **åˆ†æ®µé”®**: symbol + æŒ‡æ ‡åç§°/æ•°æ®ç±»å‹
- **ç´¢å¼•ç­–ç•¥**: å¤åˆç´¢å¼• (symbol, calc_date) + GiSTç´¢å¼•æ”¯æŒæ—¶é—´èŒƒå›´æŸ¥è¯¢

### Rationale (æŠ€æœ¯åŸå› )
1. **1å¤©Chunk**: åŸºäº"Chunkå†…å­˜å ç”¨ä¸è¶…è¿‡æ€»å†…å­˜25%"è§„åˆ™,1å¤©æ•°æ®é‡é€‚ä¸­
2. **30å¤©å‹ç¼©**: å¹³è¡¡æŸ¥è¯¢æ€§èƒ½å’Œå­˜å‚¨æˆæœ¬,è¿‘æœŸæ•°æ®ä¿æŒæœªå‹ç¼©ä¾¿äºå¿«é€Ÿè®¿é—®
3. **åˆ†æ®µé”®**: æŒ‰symbolåˆ†æ®µä½¿å‹ç¼©æ•ˆç‡æœ€å¤§åŒ–,åŒä¸€è‚¡ç¥¨æ•°æ®ç‰¹å¾ç›¸ä¼¼
4. **è‡ªåŠ¨åˆ†åŒº**: TimescaleDBè‡ªåŠ¨ç®¡ç†åˆ†åŒº,æ— éœ€æ‰‹åŠ¨ç»´æŠ¤

### Alternatives Considered (è€ƒè™‘çš„å…¶ä»–æ–¹æ¡ˆ)
- âŒ **7å¤©Chunk**: åˆ†åŒºè¿‡å¤š,å…ƒæ•°æ®å¼€é”€å¤§
- âŒ **å³æ—¶å‹ç¼©**: å½±å“å†™å…¥æ€§èƒ½,ä¸é€‚åˆå®æ—¶æ•°æ®
- âŒ **ä¸å‹ç¼©**: å­˜å‚¨æˆæœ¬è¿‡é«˜,å†å²æ•°æ®è®¿é—®é¢‘ç‡ä½

### Implementation Notes (å®ç°è¦ç‚¹)

#### Hypertableåˆ›å»º
```sql
-- æŠ€æœ¯æŒ‡æ ‡è¡¨ (è¡ç”Ÿæ•°æ®)
CREATE TABLE IF NOT EXISTS technical_indicators (
    symbol VARCHAR(16) NOT NULL,
    calc_date TIMESTAMPTZ NOT NULL,
    indicator_name VARCHAR(32) NOT NULL,   -- MACD/RSI/BOLLç­‰
    indicator_value DOUBLE PRECISION,
    params JSONB,                          -- æŒ‡æ ‡å‚æ•°é…ç½®
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- è½¬æ¢ä¸ºHypertable
SELECT create_hypertable(
    'technical_indicators',
    'calc_date',
    chunk_time_interval => INTERVAL '1 day',
    if_not_exists => TRUE
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_tech_symbol_date ON technical_indicators (symbol, calc_date DESC);
CREATE INDEX idx_tech_indicator ON technical_indicators (indicator_name);

-- é…ç½®å‹ç¼©
ALTER TABLE technical_indicators SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'symbol, indicator_name',
    timescaledb.compress_orderby = 'calc_date DESC'
);

-- æ·»åŠ å‹ç¼©ç­–ç•¥ (30å¤©åå‹ç¼©)
SELECT add_compression_policy(
    'technical_indicators',
    INTERVAL '30 days'
);
```

#### æ—¥çº¿/å‘¨çº¿/æœˆçº¿è¡¨
```sql
-- æ—¥çº¿æ•°æ®è¡¨
CREATE TABLE IF NOT EXISTS daily_kline (
    symbol VARCHAR(16) NOT NULL,
    trade_date TIMESTAMPTZ NOT NULL,
    open DOUBLE PRECISION NOT NULL,
    high DOUBLE PRECISION NOT NULL,
    low DOUBLE PRECISION NOT NULL,
    close DOUBLE PRECISION NOT NULL,
    volume BIGINT NOT NULL,
    amount DOUBLE PRECISION,
    turnover_rate DOUBLE PRECISION,
    pe_ratio DOUBLE PRECISION,
    pb_ratio DOUBLE PRECISION,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

SELECT create_hypertable(
    'daily_kline',
    'trade_date',
    chunk_time_interval => INTERVAL '1 day',
    if_not_exists => TRUE
);

CREATE INDEX idx_daily_symbol_date ON daily_kline (symbol, trade_date DESC);

ALTER TABLE daily_kline SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'symbol',
    timescaledb.compress_orderby = 'trade_date DESC'
);

SELECT add_compression_policy('daily_kline', INTERVAL '30 days');
```

#### è®¢å•å’Œæˆäº¤è®°å½•è¡¨ (äº¤æ˜“æ•°æ®)
```sql
-- å†å²è®¢å•è¡¨
CREATE TABLE IF NOT EXISTS order_history (
    order_id VARCHAR(64) PRIMARY KEY,
    symbol VARCHAR(16) NOT NULL,
    order_time TIMESTAMPTZ NOT NULL,
    order_type VARCHAR(16) NOT NULL,      -- LIMIT/MARKET/STOP
    direction VARCHAR(8) NOT NULL,        -- BUY/SELL
    price DOUBLE PRECISION,
    quantity BIGINT NOT NULL,
    filled_quantity BIGINT DEFAULT 0,
    status VARCHAR(16) NOT NULL,          -- PENDING/FILLED/CANCELLED
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

SELECT create_hypertable(
    'order_history',
    'order_time',
    chunk_time_interval => INTERVAL '1 day',
    if_not_exists => TRUE
);

CREATE INDEX idx_order_symbol_time ON order_history (symbol, order_time DESC);
CREATE INDEX idx_order_status ON order_history (status, order_time DESC);

ALTER TABLE order_history SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'symbol, status',
    timescaledb.compress_orderby = 'order_time DESC'
);

SELECT add_compression_policy('order_history', INTERVAL '30 days');
```

#### æŒç»­èšåˆ (Continuous Aggregates)
```sql
-- æ¯æ—¥äº¤æ˜“ç»Ÿè®¡
CREATE MATERIALIZED VIEW daily_trade_summary
WITH (timescaledb.continuous) AS
SELECT
    time_bucket('1 day', order_time) AS day,
    symbol,
    COUNT(*) AS total_orders,
    SUM(CASE WHEN direction = 'BUY' THEN quantity ELSE 0 END) AS buy_volume,
    SUM(CASE WHEN direction = 'SELL' THEN quantity ELSE 0 END) AS sell_volume,
    AVG(price) AS avg_price
FROM order_history
WHERE status = 'FILLED'
GROUP BY day, symbol;

-- è‡ªåŠ¨åˆ·æ–°ç­–ç•¥
SELECT add_continuous_aggregate_policy('daily_trade_summary',
    start_offset => INTERVAL '3 days',
    end_offset => INTERVAL '1 hour',
    schedule_interval => INTERVAL '1 hour'
);
```

#### æ•°æ®ä¿ç•™ç­–ç•¥
```sql
-- æŠ€æœ¯æŒ‡æ ‡ä¿ç•™5å¹´
SELECT add_retention_policy('technical_indicators', INTERVAL '5 years');

-- æ—¥çº¿æ•°æ®ä¿ç•™10å¹´
SELECT add_retention_policy('daily_kline', INTERVAL '10 years');

-- è®¢å•è®°å½•ä¿ç•™3å¹´ (åˆè§„è¦æ±‚)
SELECT add_retention_policy('order_history', INTERVAL '3 years');
```

---

## 3. å¤šæ•°æ®åº“äº‹åŠ¡åè°ƒå†³ç­– (R3)

### Decision (å†³ç­–)
- **åè°ƒç­–ç•¥**: åŸºäºé˜Ÿåˆ—çš„æœ€ç»ˆä¸€è‡´æ€§ (Queue-based Eventual Consistency)
- **å®ç°æ¨¡å¼**: Outboxæ¨¡å¼ + SQLiteæŒä¹…åŒ–é˜Ÿåˆ—
- **é‡è¯•æœºåˆ¶**: æŒ‡æ•°é€€é¿é‡è¯•,æœ€å¤š5æ¬¡
- **ç›‘æ§é›†æˆ**: æ‰€æœ‰è·¨åº“æ“ä½œè®°å½•åˆ°ç›‘æ§æ•°æ®åº“

### Rationale (æŠ€æœ¯åŸå› )
1. **é‡åŒ–æ•°æ®ç‰¹æ€§**: æ—¶åºæ•°æ®ä¸ºè¿½åŠ å‹,æ— éœ€å¼ºä¸€è‡´æ€§,æœ€ç»ˆä¸€è‡´æ€§å³å¯æ»¡è¶³éœ€æ±‚
2. **ç®€åŒ–å¤æ‚åº¦**: é¿å…2PC/Sagaçš„å¤æ‚æ€§å’Œæ€§èƒ½å¼€é”€
3. **æ•…éšœå®¹é”™**: SQLiteé˜Ÿåˆ—æ”¯æŒæŒä¹…åŒ–,ç³»ç»Ÿé‡å¯åå¯ç»§ç»­å¤„ç†
4. **å®¡è®¡è·Ÿè¸ª**: æ‰€æœ‰æ“ä½œå¯è¿½æº¯,ä¾¿äºé—®é¢˜æ’æŸ¥

### Alternatives Considered (è€ƒè™‘çš„å…¶ä»–æ–¹æ¡ˆ)
- âŒ **2PC (ä¸¤é˜¶æ®µæäº¤)**: è¿‡åº¦å¤æ‚,æ€§èƒ½å·®,ä¸é€‚åˆå¼‚æ„æ•°æ®åº“
- âŒ **Sagaæ¨¡å¼**: éœ€è¦å®ç°è¡¥å¿äº‹åŠ¡,å¯¹åªè¯»/è¿½åŠ å‹æ•°æ®è¿‡äºå¤æ‚
- âŒ **ç›´æ¥å†™å…¥æ— åè°ƒ**: æ— æ³•ä¿è¯æ•°æ®ä¸€è‡´æ€§,æ•…éšœåæ•°æ®ä¸¢å¤±

### Implementation Notes (å®ç°è¦ç‚¹)

#### SQLite Outboxè¡¨ç»“æ„
```python
import sqlite3
import json
import uuid
from datetime import datetime

class OutboxQueue:
    def __init__(self, db_path='data/outbox_queue.db'):
        self.conn = sqlite3.connect(db_path, check_same_thread=False)
        self._create_table()

    def _create_table(self):
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS outbox_operations (
                operation_id TEXT PRIMARY KEY,
                created_at TEXT NOT NULL,
                target_databases TEXT NOT NULL,    -- JSONæ•°ç»„
                data_classification TEXT NOT NULL,
                data_payload TEXT NOT NULL,        -- JSONæ ¼å¼
                status TEXT NOT NULL,              -- pending/processing/completed/failed
                retry_count INTEGER DEFAULT 0,
                last_error TEXT,
                completed_at TEXT,
                INDEX idx_status_created (status, created_at)
            )
        """)
        self.conn.commit()

    def enqueue(self, target_dbs, classification, data_payload):
        """å…¥é˜Ÿä¸€ä¸ªè·¨åº“æ“ä½œ"""
        operation_id = str(uuid.uuid4())
        self.conn.execute("""
            INSERT INTO outbox_operations
            (operation_id, created_at, target_databases, data_classification,
             data_payload, status, retry_count)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            operation_id,
            datetime.now().isoformat(),
            json.dumps(target_dbs),
            classification.value,
            json.dumps(data_payload),
            'pending',
            0
        ))
        self.conn.commit()
        return operation_id

    def dequeue_batch(self, batch_size=10):
        """æ‰¹é‡å–å‡ºå¾…å¤„ç†æ“ä½œ"""
        cursor = self.conn.execute("""
            SELECT operation_id, target_databases, data_classification,
                   data_payload, retry_count
            FROM outbox_operations
            WHERE status = 'pending' AND retry_count < 5
            ORDER BY created_at ASC
            LIMIT ?
        """, (batch_size,))

        operations = []
        for row in cursor:
            operations.append({
                'operation_id': row[0],
                'target_databases': json.loads(row[1]),
                'classification': row[2],
                'data_payload': json.loads(row[3]),
                'retry_count': row[4]
            })
        return operations
```

#### è·¨åº“å†™å…¥åè°ƒå™¨
```python
import time
from typing import List
from enum import Enum

class MultiDatabaseCoordinator:
    def __init__(self, unified_manager, outbox_queue):
        self.manager = unified_manager
        self.outbox = outbox_queue
        self.max_retries = 5
        self.retry_delays = [1, 2, 5, 10, 30]  # æŒ‡æ•°é€€é¿(ç§’)

    def save_with_coordination(self, data, classification, target_dbs=None):
        """
        åè°ƒå¤šæ•°æ®åº“å†™å…¥

        Args:
            data: pandas DataFrame
            classification: DataClassificationæšä¸¾
            target_dbs: ç›®æ ‡æ•°æ®åº“åˆ—è¡¨,Noneè¡¨ç¤ºè‡ªåŠ¨è·¯ç”±
        """
        if target_dbs is None:
            # è‡ªåŠ¨ç¡®å®šç›®æ ‡æ•°æ®åº“
            target_dbs = self._determine_targets(classification)

        if len(target_dbs) == 1:
            # å•åº“æ“ä½œ,ç›´æ¥å†™å…¥
            return self.manager.save_data_by_classification(data, classification)

        # å¤šåº“æ“ä½œ,å…¥é˜Ÿå¤„ç†
        operation_id = self.outbox.enqueue(
            target_dbs=target_dbs,
            classification=classification,
            data_payload=data.to_dict(orient='records')
        )

        return operation_id

    def _determine_targets(self, classification):
        """æ ¹æ®æ•°æ®åˆ†ç±»ç¡®å®šç›®æ ‡æ•°æ®åº“"""
        # ç¤ºä¾‹: REALTIME_POSITIONéœ€è¦åŒæ—¶å†™å…¥Rediså’ŒPostgreSQL
        if classification == DataClassification.REALTIME_POSITION:
            return ['redis', 'postgresql']
        elif classification == DataClassification.TICK_DATA:
            return ['tdengine']
        # ... å…¶ä»–åˆ†ç±»é€»è¾‘

        return [self.manager.strategy.get_target_database(classification)]

    def process_outbox_queue(self):
        """åå°ä»»åŠ¡: å¤„ç†Outboxé˜Ÿåˆ—"""
        while True:
            operations = self.outbox.dequeue_batch(batch_size=10)

            for op in operations:
                success = self._execute_operation(op)

                if success:
                    self.outbox.mark_completed(op['operation_id'])
                else:
                    # æ›´æ–°é‡è¯•è®¡æ•°
                    retry_count = op['retry_count'] + 1
                    if retry_count >= self.max_retries:
                        self.outbox.mark_failed(op['operation_id'])
                        # å‘é€å‘Šè­¦
                        self._alert_operation_failed(op)
                    else:
                        self.outbox.increment_retry(op['operation_id'])
                        # æŒ‡æ•°é€€é¿
                        time.sleep(self.retry_delays[retry_count])

            time.sleep(5)  # è½®è¯¢é—´éš”

    def _execute_operation(self, operation):
        """æ‰§è¡Œå•ä¸ªè·¨åº“æ“ä½œ"""
        try:
            data_df = pd.DataFrame(operation['data_payload'])
            classification = DataClassification(operation['classification'])

            # ä¾æ¬¡å†™å…¥å„ç›®æ ‡åº“
            for db_name in operation['target_databases']:
                if db_name == 'tdengine':
                    self.manager.tdengine_access.save_data(
                        data_df,
                        self._get_table_name(classification)
                    )
                elif db_name == 'postgresql':
                    self.manager.postgresql_access.save_data(
                        data_df,
                        self._get_table_name(classification)
                    )
                elif db_name == 'redis':
                    self.manager.redis_access.save_data(
                        data_df,
                        self._get_redis_key_prefix(classification)
                    )
                # ... å…¶ä»–æ•°æ®åº“

            return True
        except Exception as e:
            self.outbox.update_error(operation['operation_id'], str(e))
            return False
```

#### ç›‘æ§é›†æˆ
```python
# åœ¨MonitoringDatabaseä¸­è®°å½•è·¨åº“æ“ä½œ
def log_cross_database_operation(self, operation_id, target_dbs, status, duration_ms):
    self.conn.execute("""
        INSERT INTO cross_db_operations
        (operation_id, target_databases, status, duration_ms, logged_at)
        VALUES (?, ?, ?, ?, ?)
    """, (
        operation_id,
        json.dumps(target_dbs),
        status,
        duration_ms,
        datetime.now()
    ))
    self.conn.commit()
```

#### ä½¿ç”¨ç¤ºä¾‹
```python
# å®æ—¶æŒä»“æ•°æ®: éœ€è¦åŒæ—¶å†™å…¥Redis(çƒ­æ•°æ®)å’ŒPostgreSQL(æŒä¹…åŒ–)
coordinator = MultiDatabaseCoordinator(unified_manager, outbox_queue)

position_data = pd.DataFrame({
    'account_id': ['A001', 'A001'],
    'symbol': ['000001.SZ', '600000.SH'],
    'quantity': [1000, 2000],
    'cost_price': [10.5, 8.3],
    'current_price': [11.2, 8.8],
    'update_time': [datetime.now(), datetime.now()]
})

# è‡ªåŠ¨åè°ƒå†™å…¥Rediså’ŒPostgreSQL
operation_id = coordinator.save_with_coordination(
    data=position_data,
    classification=DataClassification.REALTIME_POSITION
)

# å¯åŠ¨åå°é˜Ÿåˆ—å¤„ç†
import threading
queue_thread = threading.Thread(
    target=coordinator.process_outbox_queue,
    daemon=True
)
queue_thread.start()
```

---

## 4. YAMLé…ç½®æ¶æ„å†³ç­– (R4)

### Decision (å†³ç­–)
- **è§£æåº“**: PyYAML 6.0+
- **éªŒè¯æ¡†æ¶**: Pydantic V2 (BaseModel)
- **é…ç½®ç»“æ„**: åˆ†å±‚æ¶æ„ (å…¨å±€é…ç½® + è¡¨å®šä¹‰ + æ•°æ®åº“ç‰¹å®šé…ç½®)
- **ç‰ˆæœ¬æ§åˆ¶**: é…ç½®æ–‡ä»¶å†…åµŒç‰ˆæœ¬å·,æ”¯æŒå‘åå…¼å®¹æ€§æ£€æŸ¥

### Rationale (æŠ€æœ¯åŸå› )
1. **ç±»å‹å®‰å…¨**: Pydanticæä¾›è¿è¡Œæ—¶ç±»å‹éªŒè¯,å‡å°‘é…ç½®é”™è¯¯
2. **IDEæ”¯æŒ**: å¼ºç±»å‹å®šä¹‰æä¾›è‡ªåŠ¨è¡¥å…¨å’Œé”™è¯¯æç¤º
3. **å¯æ‰©å±•æ€§**: åˆ†å±‚ç»“æ„æ”¯æŒæ•°æ®åº“ç‰¹å®šé…ç½®è€Œä¸æ±¡æŸ“æ ¸å¿ƒå®šä¹‰
4. **å‘åå…¼å®¹**: ç‰ˆæœ¬å·æœºåˆ¶æ”¯æŒé…ç½®è¿ç§»å’Œå…¼å®¹æ€§æ£€æŸ¥

### Alternatives Considered (è€ƒè™‘çš„å…¶ä»–æ–¹æ¡ˆ)
- âŒ **JSONé…ç½®**: ä¸æ”¯æŒæ³¨é‡Š,å¯è¯»æ€§å·®
- âŒ **TOMLé…ç½®**: åµŒå¥—ç»“æ„è¡¨è¾¾ä¸å¤Ÿæ¸…æ™°
- âŒ **çº¯Pythoné…ç½®**: ç¼ºä¹å£°æ˜å¼ç‰¹æ€§,éš¾ä»¥è‡ªåŠ¨åŒ–å¤„ç†

### Implementation Notes (å®ç°è¦ç‚¹)

#### Pydanticé…ç½®æ¨¡å‹
```python
from pydantic import BaseModel, Field, validator
from typing import List, Dict, Optional, Literal
from enum import Enum

class DatabaseType(str, Enum):
    TDENGINE = "tdengine"
    POSTGRESQL = "postgresql"
    MYSQL = "mysql"
    REDIS = "redis"

class ColumnDefinition(BaseModel):
    name: str = Field(..., description="åˆ—å")
    type: str = Field(..., description="æ•°æ®ç±»å‹")
    nullable: bool = Field(default=True, description="æ˜¯å¦å…è®¸NULL")
    primary_key: bool = Field(default=False, description="æ˜¯å¦ä¸ºä¸»é”®")
    default: Optional[str] = Field(default=None, description="é»˜è®¤å€¼")
    comment: Optional[str] = Field(default=None, description="åˆ—æ³¨é‡Š")

    # TDengineç‰¹å®š
    encoding: Optional[str] = Field(default=None, description="ç¼–ç æ–¹å¼(TDengine)")

    # PostgreSQLç‰¹å®š
    index_type: Optional[str] = Field(default=None, description="ç´¢å¼•ç±»å‹(BTREE/HASH/GIST)")

class TDengineConfig(BaseModel):
    """TDengineç‰¹å®šé…ç½®"""
    is_super_table: bool = Field(default=False, description="æ˜¯å¦ä¸ºè¶…çº§è¡¨")
    tags: Optional[List[ColumnDefinition]] = Field(default=None, description="æ ‡ç­¾åˆ—")
    compression: str = Field(default="zstd", description="å‹ç¼©ç®—æ³•")
    compression_level: Literal["low", "medium", "high"] = Field(default="high")
    keep_days: int = Field(default=3650, description="æ•°æ®ä¿ç•™å¤©æ•°")

    @validator('tags')
    def validate_tags(cls, v, values):
        if values.get('is_super_table') and not v:
            raise ValueError("è¶…çº§è¡¨å¿…é¡»å®šä¹‰tags")
        return v

class TimescaleConfig(BaseModel):
    """TimescaleDBç‰¹å®šé…ç½®"""
    is_hypertable: bool = Field(default=False, description="æ˜¯å¦ä¸ºHypertable")
    time_column: Optional[str] = Field(default=None, description="æ—¶é—´åˆ—å")
    chunk_interval: str = Field(default="1 day", description="Chunké—´éš”")
    compress_after: str = Field(default="30 days", description="å‹ç¼©å»¶è¿Ÿ")
    compress_segmentby: Optional[List[str]] = Field(default=None, description="å‹ç¼©åˆ†æ®µé”®")
    retention_period: Optional[str] = Field(default=None, description="æ•°æ®ä¿ç•™æœŸ")

    @validator('time_column')
    def validate_time_column(cls, v, values):
        if values.get('is_hypertable') and not v:
            raise ValueError("Hypertableå¿…é¡»æŒ‡å®šæ—¶é—´åˆ—")
        return v

class TableDefinition(BaseModel):
    """è¡¨å®šä¹‰"""
    table_name: str = Field(..., description="è¡¨å")
    database_type: DatabaseType = Field(..., description="æ•°æ®åº“ç±»å‹")
    data_classification: str = Field(..., description="æ•°æ®åˆ†ç±»")
    description: Optional[str] = Field(default=None, description="è¡¨æè¿°")

    columns: List[ColumnDefinition] = Field(..., description="åˆ—å®šä¹‰")
    indexes: Optional[List[Dict[str, str]]] = Field(default=None, description="ç´¢å¼•å®šä¹‰")

    # æ•°æ®åº“ç‰¹å®šé…ç½®
    tdengine_config: Optional[TDengineConfig] = Field(default=None)
    timescale_config: Optional[TimescaleConfig] = Field(default=None)

    @validator('tdengine_config')
    def validate_tdengine(cls, v, values):
        if values.get('database_type') == DatabaseType.TDENGINE and not v:
            raise ValueError("TDengineè¡¨å¿…é¡»æä¾›tdengine_config")
        return v

    @validator('timescale_config')
    def validate_timescale(cls, v, values):
        if values.get('database_type') == DatabaseType.POSTGRESQL and not v:
            # PostgreSQLè¡¨å¯é€‰TimescaleDBæ‰©å±•
            pass
        return v

class DatabaseConfig(BaseModel):
    """æ•°æ®åº“è¿æ¥é…ç½®"""
    host: str
    port: int
    user: str
    password: str
    database: str

    # è¿æ¥æ± é…ç½®
    pool_size: int = Field(default=10, ge=1, le=100)
    max_overflow: int = Field(default=20, ge=0, le=100)
    pool_timeout: int = Field(default=30, ge=1)

class SystemConfig(BaseModel):
    """ç³»ç»Ÿå…¨å±€é…ç½®"""
    version: str = Field(..., description="é…ç½®æ–‡ä»¶ç‰ˆæœ¬")
    project_name: str = Field(default="MyStocks", description="é¡¹ç›®åç§°")

    # æ•°æ®åº“è¿æ¥
    databases: Dict[DatabaseType, DatabaseConfig] = Field(..., description="æ•°æ®åº“é…ç½®")

    # è¡¨å®šä¹‰
    tables: List[TableDefinition] = Field(..., description="è¡¨å®šä¹‰åˆ—è¡¨")

    # ç›‘æ§é…ç½®
    monitoring: Dict[str, any] = Field(default_factory=dict, description="ç›‘æ§é…ç½®")

    @validator('version')
    def validate_version(cls, v):
        # æ”¯æŒçš„ç‰ˆæœ¬å·
        supported = ['1.0.0', '1.1.0']
        if v not in supported:
            raise ValueError(f"ä¸æ”¯æŒçš„é…ç½®ç‰ˆæœ¬: {v}, æ”¯æŒç‰ˆæœ¬: {supported}")
        return v
```

#### YAMLé…ç½®æ–‡ä»¶ç¤ºä¾‹
```yaml
# table_config.yaml
version: "1.1.0"
project_name: "MyStocks"

# æ•°æ®åº“è¿æ¥é…ç½®(é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–)
databases:
  tdengine:
    host: "${TDENGINE_HOST:localhost}"
    port: 6041
    user: "${TDENGINE_USER:root}"
    password: "${TDENGINE_PASSWORD:taosdata}"
    database: "${TDENGINE_DATABASE:market_data}"
    pool_size: 20
    max_overflow: 30

  postgresql:
    host: "${POSTGRESQL_HOST:localhost}"
    port: 5432
    user: "${POSTGRESQL_USER:postgres}"
    password: "${POSTGRESQL_PASSWORD:postgres}"
    database: "${POSTGRESQL_DATABASE:mystocks}"
    pool_size: 15

  mysql:
    host: "${MYSQL_HOST:localhost}"
    port: 3306
    user: "${MYSQL_USER:root}"
    password: "${MYSQL_PASSWORD:root}"
    database: "${MYSQL_DATABASE:mystocks_reference}"
    pool_size: 10

  redis:
    host: "${REDIS_HOST:localhost}"
    port: 6379
    password: "${REDIS_PASSWORD:}"
    database: "${REDIS_DB:1}"  # ä½¿ç”¨1å·åº“,é¿å¼€0å·
    pool_size: 50

# è¡¨å®šä¹‰
tables:
  # TDengine - Tickæ•°æ®
  - table_name: "tick_data"
    database_type: "tdengine"
    data_classification: "TICK_DATA"
    description: "é€ç¬”æˆäº¤æ•°æ®(è¶…é«˜é¢‘)"

    columns:
      - name: "ts"
        type: "TIMESTAMP"
        nullable: false
        primary_key: true
        comment: "æ—¶é—´æˆ³"

      - name: "price"
        type: "DOUBLE"
        nullable: false
        encoding: "delta-d"
        comment: "æˆäº¤ä»·æ ¼"

      - name: "volume"
        type: "BIGINT"
        nullable: false
        encoding: "delta-i"
        comment: "æˆäº¤é‡"

      - name: "amount"
        type: "DOUBLE"
        nullable: false
        encoding: "delta-d"
        comment: "æˆäº¤é¢"

      - name: "buy_count"
        type: "INT"
        nullable: true
        encoding: "simple8b"
        comment: "ä¹°ç›˜ç¬”æ•°"

      - name: "sell_count"
        type: "INT"
        nullable: true
        encoding: "simple8b"
        comment: "å–ç›˜ç¬”æ•°"

    tdengine_config:
      is_super_table: true
      tags:
        - name: "symbol"
          type: "BINARY(16)"
          nullable: false
          comment: "è‚¡ç¥¨ä»£ç "

        - name: "exchange"
          type: "BINARY(16)"
          nullable: false
          comment: "äº¤æ˜“æ‰€"

        - name: "security_type"
          type: "BINARY(16)"
          nullable: false
          comment: "è¯åˆ¸ç±»å‹"

      compression: "zstd"
      compression_level: "high"
      keep_days: 730  # ä¿ç•™2å¹´

  # PostgreSQL - æŠ€æœ¯æŒ‡æ ‡
  - table_name: "technical_indicators"
    database_type: "postgresql"
    data_classification: "TECHNICAL_INDICATORS"
    description: "æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ç»“æœ"

    columns:
      - name: "id"
        type: "BIGSERIAL"
        nullable: false
        primary_key: true

      - name: "symbol"
        type: "VARCHAR(16)"
        nullable: false
        comment: "è‚¡ç¥¨ä»£ç "

      - name: "calc_date"
        type: "TIMESTAMPTZ"
        nullable: false
        comment: "è®¡ç®—æ—¥æœŸ"

      - name: "indicator_name"
        type: "VARCHAR(32)"
        nullable: false
        comment: "æŒ‡æ ‡åç§°"

      - name: "indicator_value"
        type: "DOUBLE PRECISION"
        nullable: true
        comment: "æŒ‡æ ‡å€¼"

      - name: "params"
        type: "JSONB"
        nullable: true
        comment: "æŒ‡æ ‡å‚æ•°"

      - name: "created_at"
        type: "TIMESTAMPTZ"
        nullable: false
        default: "NOW()"

    indexes:
      - name: "idx_tech_symbol_date"
        columns: "symbol, calc_date DESC"
        type: "BTREE"

      - name: "idx_tech_indicator"
        columns: "indicator_name"
        type: "BTREE"

    timescale_config:
      is_hypertable: true
      time_column: "calc_date"
      chunk_interval: "1 day"
      compress_after: "30 days"
      compress_segmentby: ["symbol", "indicator_name"]
      retention_period: "5 years"

  # MySQL - è‚¡ç¥¨åŸºç¡€ä¿¡æ¯
  - table_name: "stock_info"
    database_type: "mysql"
    data_classification: "STOCK_INFO"
    description: "è‚¡ç¥¨åŸºç¡€ä¿¡æ¯(å‚è€ƒæ•°æ®)"

    columns:
      - name: "id"
        type: "BIGINT UNSIGNED"
        nullable: false
        primary_key: true
        comment: "è‡ªå¢ä¸»é”®"

      - name: "symbol"
        type: "VARCHAR(16)"
        nullable: false
        comment: "è‚¡ç¥¨ä»£ç "

      - name: "name"
        type: "VARCHAR(64)"
        nullable: false
        comment: "è‚¡ç¥¨åç§°"

      - name: "exchange"
        type: "VARCHAR(16)"
        nullable: false
        comment: "äº¤æ˜“æ‰€"

      - name: "list_date"
        type: "DATE"
        nullable: true
        comment: "ä¸Šå¸‚æ—¥æœŸ"

      - name: "delist_date"
        type: "DATE"
        nullable: true
        comment: "é€€å¸‚æ—¥æœŸ"

      - name: "status"
        type: "VARCHAR(16)"
        nullable: false
        default: "'ACTIVE'"
        comment: "çŠ¶æ€"

      - name: "updated_at"
        type: "TIMESTAMP"
        nullable: false
        default: "CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP"

    indexes:
      - name: "uk_symbol"
        columns: "symbol"
        type: "UNIQUE"

      - name: "idx_exchange_status"
        columns: "exchange, status"

# ç›‘æ§é…ç½®
monitoring:
  database_url: "${MONITOR_DB_URL:postgresql://localhost/mystocks_monitor}"
  retention_days:
    operation_logs: 30
    performance_metrics: 90
    data_quality_checks: 7
  alert_channels:
    - type: "log"
      level: "ERROR"
    - type: "webhook"
      url: "${ALERT_WEBHOOK_URL:}"
```

#### é…ç½®åŠ è½½å™¨
```python
import yaml
import os
import re
from typing import Any

class ConfigLoader:
    """é…ç½®åŠ è½½å™¨,æ”¯æŒç¯å¢ƒå˜é‡æ›¿æ¢"""

    @staticmethod
    def load_config(config_path: str) -> SystemConfig:
        """åŠ è½½å¹¶éªŒè¯é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            raw_config = yaml.safe_load(f)

        # ç¯å¢ƒå˜é‡æ›¿æ¢
        processed_config = ConfigLoader._replace_env_vars(raw_config)

        # PydanticéªŒè¯
        try:
            config = SystemConfig(**processed_config)
            return config
        except Exception as e:
            raise ValueError(f"é…ç½®æ–‡ä»¶éªŒè¯å¤±è´¥: {e}")

    @staticmethod
    def _replace_env_vars(obj: Any) -> Any:
        """é€’å½’æ›¿æ¢é…ç½®ä¸­çš„ç¯å¢ƒå˜é‡"""
        if isinstance(obj, dict):
            return {k: ConfigLoader._replace_env_vars(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [ConfigLoader._replace_env_vars(item) for item in obj]
        elif isinstance(obj, str):
            # åŒ¹é… ${VAR_NAME:default_value} æ ¼å¼
            pattern = r'\$\{([^:}]+)(?::([^}]*))?\}'

            def replacer(match):
                var_name = match.group(1)
                default_value = match.group(2) or ''
                return os.getenv(var_name, default_value)

            return re.sub(pattern, replacer, obj)
        else:
            return obj

# ä½¿ç”¨ç¤ºä¾‹
config = ConfigLoader.load_config('table_config.yaml')

# è®¿é—®é…ç½®
print(f"é¡¹ç›®åç§°: {config.project_name}")
print(f"é…ç½®ç‰ˆæœ¬: {config.version}")
print(f"è¡¨æ•°é‡: {len(config.tables)}")

# éå†è¡¨å®šä¹‰
for table in config.tables:
    print(f"\nè¡¨: {table.table_name}")
    print(f"  æ•°æ®åº“: {table.database_type}")
    print(f"  åˆ†ç±»: {table.data_classification}")
    print(f"  åˆ—æ•°: {len(table.columns)}")

    if table.tdengine_config and table.tdengine_config.is_super_table:
        print(f"  TDengineè¶…çº§è¡¨,æ ‡ç­¾æ•°: {len(table.tdengine_config.tags)}")

    if table.timescale_config and table.timescale_config.is_hypertable:
        print(f"  TimescaleDB Hypertable,Chunké—´éš”: {table.timescale_config.chunk_interval}")
```

#### é…ç½®é©±åŠ¨è¡¨åˆ›å»º
```python
class ConfigDrivenTableManager:
    def __init__(self, config: SystemConfig):
        self.config = config
        self.db_connections = self._init_connections()

    def create_all_tables(self):
        """æ ¹æ®é…ç½®åˆ›å»ºæ‰€æœ‰è¡¨"""
        for table_def in self.config.tables:
            try:
                self._create_single_table(table_def)
                print(f"âœ… è¡¨åˆ›å»ºæˆåŠŸ: {table_def.table_name}")
            except Exception as e:
                print(f"âŒ è¡¨åˆ›å»ºå¤±è´¥: {table_def.table_name}, é”™è¯¯: {e}")

    def _create_single_table(self, table_def: TableDefinition):
        """åˆ›å»ºå•ä¸ªè¡¨"""
        if table_def.database_type == DatabaseType.TDENGINE:
            self._create_tdengine_table(table_def)
        elif table_def.database_type == DatabaseType.POSTGRESQL:
            self._create_postgresql_table(table_def)
        elif table_def.database_type == DatabaseType.MYSQL:
            self._create_mysql_table(table_def)
        # Redisæ— éœ€åˆ›å»ºè¡¨ç»“æ„

    def _create_tdengine_table(self, table_def: TableDefinition):
        """åˆ›å»ºTDengineè¡¨"""
        conn = self.db_connections[DatabaseType.TDENGINE]
        td_config = table_def.tdengine_config

        if td_config.is_super_table:
            # æ„å»ºè¶…çº§è¡¨SQL
            columns_sql = ", ".join([
                f"{col.name} {col.type}"
                for col in table_def.columns
            ])

            tags_sql = ", ".join([
                f"{tag.name} {tag.type}"
                for tag in td_config.tags
            ])

            # æå–ç¼–ç é…ç½®
            encodings = [col.encoding for col in table_def.columns if col.encoding]
            encode_clause = f"ENCODE '{','.join(encodings)}'" if encodings else ""

            sql = f"""
                CREATE STABLE IF NOT EXISTS {table_def.table_name} (
                    {columns_sql}
                ) TAGS (
                    {tags_sql}
                )
                {encode_clause}
                COMPRESS '{td_config.compression}'
                LEVEL '{td_config.compression_level}'
                KEEP {td_config.keep_days};
            """

            conn.execute(sql)

    def _create_postgresql_table(self, table_def: TableDefinition):
        """åˆ›å»ºPostgreSQLè¡¨"""
        conn = self.db_connections[DatabaseType.POSTGRESQL]

        # æ„å»ºæ™®é€šè¡¨SQL
        columns_sql = ", ".join([
            f"{col.name} {col.type} "
            f"{'NOT NULL' if not col.nullable else ''} "
            f"{'PRIMARY KEY' if col.primary_key else ''} "
            f"{'DEFAULT ' + col.default if col.default else ''}"
            for col in table_def.columns
        ])

        create_sql = f"""
            CREATE TABLE IF NOT EXISTS {table_def.table_name} (
                {columns_sql}
            );
        """
        conn.execute(create_sql)

        # å¦‚æœæ˜¯Hypertable,è½¬æ¢
        ts_config = table_def.timescale_config
        if ts_config and ts_config.is_hypertable:
            conn.execute(f"""
                SELECT create_hypertable(
                    '{table_def.table_name}',
                    '{ts_config.time_column}',
                    chunk_time_interval => INTERVAL '{ts_config.chunk_interval}',
                    if_not_exists => TRUE
                );
            """)

            # é…ç½®å‹ç¼©
            segmentby = ', '.join(ts_config.compress_segmentby) if ts_config.compress_segmentby else ''
            conn.execute(f"""
                ALTER TABLE {table_def.table_name} SET (
                    timescaledb.compress,
                    timescaledb.compress_segmentby = '{segmentby}',
                    timescaledb.compress_orderby = '{ts_config.time_column} DESC'
                );
            """)

            # æ·»åŠ å‹ç¼©ç­–ç•¥
            conn.execute(f"""
                SELECT add_compression_policy(
                    '{table_def.table_name}',
                    INTERVAL '{ts_config.compress_after}'
                );
            """)

        # åˆ›å»ºç´¢å¼•
        if table_def.indexes:
            for idx in table_def.indexes:
                conn.execute(f"""
                    CREATE INDEX IF NOT EXISTS {idx['name']}
                    ON {table_def.table_name} ({idx['columns']});
                """)
```

---

## 5. æ•°æ®æºAPIç­–ç•¥å†³ç­– (R5)

### Decision (å†³ç­–)
**ä¼˜å…ˆçº§é¡ºåº**:
1. **Akshare** (ä¸»è¦æ•°æ®æº) - ç»¼åˆå…è´¹æ•°æ®,APIä¸°å¯Œ
2. **Baostock** (å¿«é€Ÿè·å–å†å²) - æ— éœ€è®¤è¯,35+å¹´å†å²æ•°æ®
3. **Tushare** (æ·±åº¦è´¢åŠ¡/å†å²) - éœ€è¦token,ç§¯åˆ†ç³»ç»Ÿ
4. **Efinance** (å®æ—¶è¡Œæƒ…) - ä¸œæ–¹è´¢å¯Œæ•°æ®,é€šè¿‡CustomerAdapteré›†æˆ
5. **RiceQuant/Byapi** - ä¸æ¨è (å•†ä¸šé™åˆ¶)

### Rationale (æŠ€æœ¯åŸå› )
1. **Akshareå…¨é¢æ€§**: è¦†ç›–Aè‚¡/æ¸¯è‚¡/ç¾è‚¡/æœŸè´§/åŸºé‡‘/å®è§‚æ•°æ®,å…è´¹æ— é™åˆ¶
2. **Baostockå¯é æ€§**: è¯åˆ¸å®å®˜æ–¹,æ•°æ®è´¨é‡é«˜,å›æº¯æµ‹è¯•å¿…å¤‡
3. **Tushareä¸“ä¸šæ€§**: è´¢åŠ¡æ•°æ®ç»“æ„åŒ–å¥½,ä½†éœ€ç§¯åˆ†ç®¡ç†
4. **Efinanceå®æ—¶æ€§**: ä¸œæ–¹è´¢å¯Œå®˜æ–¹æ¥å£,Level-2æ•°æ®æ”¯æŒ
5. **é¿å…å•†ä¸šé£é™©**: RiceQuantéœ€ä»˜è´¹,APIä½¿ç”¨å—é™

### Alternatives Considered (è€ƒè™‘çš„å…¶ä»–æ–¹æ¡ˆ)
- âŒ **Wind/Choice**: æˆæœ¬è¿‡é«˜(ä¸‡å…ƒçº§å¹´è´¹)
- âŒ **çº¯çˆ¬è™«æ–¹æ¡ˆ**: ä¸ç¨³å®š,æ³•å¾‹é£é™©
- âŒ **å•ä¸€æ•°æ®æº**: æ— å†—ä½™,æ•…éšœæ—¶æ— æ³•å›é€€

### Implementation Notes (å®ç°è¦ç‚¹)

#### ç»Ÿä¸€æ•°æ®æºæ¥å£
```python
from abc import ABC, abstractmethod
import pandas as pd
from typing import List, Optional
from datetime import datetime

class IDataSource(ABC):
    """æ•°æ®æºç»Ÿä¸€æ¥å£"""

    @abstractmethod
    def get_stock_list(self) -> pd.DataFrame:
        """
        è·å–è‚¡ç¥¨åˆ—è¡¨

        Returns:
            DataFrame with columns: symbol, name, exchange, list_date
        """
        pass

    @abstractmethod
    def get_daily_kline(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """
        è·å–æ—¥çº¿æ•°æ®

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)

        Returns:
            DataFrame with columns: symbol, trade_date, open, high, low, close, volume, amount
        """
        pass

    @abstractmethod
    def get_realtime_quotes(self, symbols: List[str]) -> pd.DataFrame:
        """
        è·å–å®æ—¶è¡Œæƒ…

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨

        Returns:
            DataFrame with columns: symbol, current_price, volume, amount, bid_price, ask_price, ...
        """
        pass

    @abstractmethod
    def get_financial_report(self, symbol: str, report_type: str) -> pd.DataFrame:
        """
        è·å–è´¢åŠ¡æŠ¥è¡¨

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            report_type: æŠ¥è¡¨ç±»å‹ (balance_sheet/income_statement/cash_flow)

        Returns:
            DataFrame with financial metrics
        """
        pass

    @property
    @abstractmethod
    def source_name(self) -> str:
        """æ•°æ®æºåç§°"""
        pass

    @property
    @abstractmethod
    def supported_markets(self) -> List[str]:
        """æ”¯æŒçš„å¸‚åœºåˆ—è¡¨"""
        pass
```

#### Akshareé€‚é…å™¨ (ä¸»è¦æ•°æ®æº)
```python
import akshare as ak

class AkshareDataSource(IDataSource):
    """Akshareæ•°æ®æºé€‚é…å™¨"""

    @property
    def source_name(self) -> str:
        return "Akshare"

    @property
    def supported_markets(self) -> List[str]:
        return ["CN_A", "HK", "US", "FUTURES", "FUND"]

    def get_stock_list(self) -> pd.DataFrame:
        """è·å–Aè‚¡åˆ—è¡¨"""
        try:
            # æ²ªæ·±Aè‚¡åˆ—è¡¨
            df = ak.stock_info_a_code_name()
            df = df.rename(columns={
                'code': 'symbol',
                'name': 'name'
            })
            # æ·»åŠ äº¤æ˜“æ‰€ä¿¡æ¯
            df['exchange'] = df['symbol'].apply(
                lambda x: 'SH' if x.startswith('6') else 'SZ'
            )
            return df
        except Exception as e:
            raise DataSourceError(f"Akshareè·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")

    def get_daily_kline(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """è·å–æ—¥çº¿æ•°æ®"""
        try:
            # Akshareæ—¥çº¿æ¥å£
            df = ak.stock_zh_a_hist(
                symbol=symbol,
                period="daily",
                start_date=start_date.replace('-', ''),
                end_date=end_date.replace('-', ''),
                adjust="qfq"  # å‰å¤æƒ
            )

            # ç»Ÿä¸€åˆ—å
            df = df.rename(columns={
                'æ—¥æœŸ': 'trade_date',
                'å¼€ç›˜': 'open',
                'æœ€é«˜': 'high',
                'æœ€ä½': 'low',
                'æ”¶ç›˜': 'close',
                'æˆäº¤é‡': 'volume',
                'æˆäº¤é¢': 'amount'
            })

            df['symbol'] = symbol
            return df[['symbol', 'trade_date', 'open', 'high', 'low', 'close', 'volume', 'amount']]
        except Exception as e:
            raise DataSourceError(f"Akshareè·å–æ—¥çº¿æ•°æ®å¤±è´¥: {e}")

    def get_realtime_quotes(self, symbols: List[str]) -> pd.DataFrame:
        """è·å–å®æ—¶è¡Œæƒ…"""
        try:
            # Akshareå®æ—¶è¡Œæƒ…æ¥å£
            df = ak.stock_zh_a_spot_em()

            # è¿‡æ»¤ç›®æ ‡è‚¡ç¥¨
            df = df[df['ä»£ç '].isin(symbols)]

            # ç»Ÿä¸€åˆ—å
            df = df.rename(columns={
                'ä»£ç ': 'symbol',
                'åç§°': 'name',
                'æœ€æ–°ä»·': 'current_price',
                'æ¶¨è·Œå¹…': 'change_pct',
                'æˆäº¤é‡': 'volume',
                'æˆäº¤é¢': 'amount',
                'ä¹°ä¸€ä»·': 'bid_price_1',
                'å–ä¸€ä»·': 'ask_price_1'
            })

            return df
        except Exception as e:
            raise DataSourceError(f"Akshareè·å–å®æ—¶è¡Œæƒ…å¤±è´¥: {e}")

    def get_financial_report(self, symbol: str, report_type: str) -> pd.DataFrame:
        """è·å–è´¢åŠ¡æŠ¥è¡¨"""
        try:
            if report_type == "balance_sheet":
                df = ak.stock_financial_report_sina(stock=symbol, symbol="èµ„äº§è´Ÿå€ºè¡¨")
            elif report_type == "income_statement":
                df = ak.stock_financial_report_sina(stock=symbol, symbol="åˆ©æ¶¦è¡¨")
            elif report_type == "cash_flow":
                df = ak.stock_financial_report_sina(stock=symbol, symbol="ç°é‡‘æµé‡è¡¨")
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æŠ¥è¡¨ç±»å‹: {report_type}")

            return df
        except Exception as e:
            raise DataSourceError(f"Akshareè·å–è´¢åŠ¡æŠ¥è¡¨å¤±è´¥: {e}")

    def get_industry_classification(self, standard: str = "ç”³ä¸‡ä¸€çº§") -> pd.DataFrame:
        """è·å–è¡Œä¸šåˆ†ç±»"""
        try:
            if standard == "ç”³ä¸‡ä¸€çº§":
                df = ak.stock_board_industry_name_em()
            elif standard == "æ¦‚å¿µ":
                df = ak.stock_board_concept_name_em()
            return df
        except Exception as e:
            raise DataSourceError(f"Akshareè·å–è¡Œä¸šåˆ†ç±»å¤±è´¥: {e}")
```

#### Baostocké€‚é…å™¨ (å†å²æ•°æ®å¤‡ä»½)
```python
import baostock as bs

class BaostockDataSource(IDataSource):
    """Baostockæ•°æ®æºé€‚é…å™¨"""

    def __init__(self):
        self.logged_in = False

    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨: ç™»å½•"""
        lg = bs.login()
        if lg.error_code != '0':
            raise DataSourceError(f"Baostockç™»å½•å¤±è´¥: {lg.error_msg}")
        self.logged_in = True
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨: ç™»å‡º"""
        if self.logged_in:
            bs.logout()

    @property
    def source_name(self) -> str:
        return "Baostock"

    @property
    def supported_markets(self) -> List[str]:
        return ["CN_A"]

    def get_stock_list(self) -> pd.DataFrame:
        """è·å–è‚¡ç¥¨åˆ—è¡¨"""
        try:
            rs = bs.query_stock_basic()
            data_list = []
            while rs.next():
                data_list.append(rs.get_row_data())

            df = pd.DataFrame(data_list, columns=rs.fields)
            df = df.rename(columns={
                'code': 'symbol',
                'code_name': 'name',
                'ipoDate': 'list_date'
            })
            return df
        except Exception as e:
            raise DataSourceError(f"Baostockè·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")

    def get_daily_kline(self, symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
        """è·å–æ—¥çº¿æ•°æ®"""
        try:
            # Baostockéœ€è¦æ ¼å¼: sh.600000 æˆ– sz.000001
            bs_symbol = self._convert_symbol(symbol)

            rs = bs.query_history_k_data_plus(
                bs_symbol,
                "date,code,open,high,low,close,volume,amount,turn",
                start_date=start_date,
                end_date=end_date,
                frequency="d",
                adjustflag="2"  # å‰å¤æƒ
            )

            data_list = []
            while rs.next():
                data_list.append(rs.get_row_data())

            df = pd.DataFrame(data_list, columns=rs.fields)
            df = df.rename(columns={
                'date': 'trade_date',
                'code': 'symbol'
            })

            # æ•°æ®ç±»å‹è½¬æ¢
            for col in ['open', 'high', 'low', 'close', 'volume', 'amount']:
                df[col] = pd.to_numeric(df[col], errors='coerce')

            return df[['symbol', 'trade_date', 'open', 'high', 'low', 'close', 'volume', 'amount']]
        except Exception as e:
            raise DataSourceError(f"Baostockè·å–æ—¥çº¿æ•°æ®å¤±è´¥: {e}")

    def get_realtime_quotes(self, symbols: List[str]) -> pd.DataFrame:
        """Baostockä¸æ”¯æŒå®æ—¶è¡Œæƒ…"""
        raise NotImplementedError("Baostockä¸æä¾›å®æ—¶è¡Œæƒ…æ¥å£")

    def get_financial_report(self, symbol: str, report_type: str) -> pd.DataFrame:
        """è·å–è´¢åŠ¡æŠ¥è¡¨"""
        try:
            bs_symbol = self._convert_symbol(symbol)

            if report_type == "balance_sheet":
                rs = bs.query_balance_data(code=bs_symbol, year=2023, quarter=4)
            elif report_type == "income_statement":
                rs = bs.query_profit_data(code=bs_symbol, year=2023, quarter=4)
            elif report_type == "cash_flow":
                rs = bs.query_cash_flow_data(code=bs_symbol, year=2023, quarter=4)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æŠ¥è¡¨ç±»å‹: {report_type}")

            data_list = []
            while rs.next():
                data_list.append(rs.get_row_data())

            return pd.DataFrame(data_list, columns=rs.fields)
        except Exception as e:
            raise DataSourceError(f"Baostockè·å–è´¢åŠ¡æŠ¥è¡¨å¤±è´¥: {e}")

    @staticmethod
    def _convert_symbol(symbol: str) -> str:
        """è½¬æ¢è‚¡ç¥¨ä»£ç æ ¼å¼: 600000 -> sh.600000"""
        if symbol.startswith('6'):
            return f"sh.{symbol}"
        else:
            return f"sz.{symbol}"
```

#### æ•°æ®æºå·¥å‚ (å¤šæºè·¯ç”±)
```python
from typing import Optional

class DataSourceFactory:
    """æ•°æ®æºå·¥å‚: è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ•°æ®æº"""

    def __init__(self):
        self.sources = {
            'akshare': AkshareDataSource(),
            'baostock': BaostockDataSource(),
            'tushare': TushareDataSource(),  # éœ€å®ç°
            'efinance': EfinanceDataSource()  # éœ€å®ç°
        }

        # æ•°æ®ç±»å‹ä¼˜å…ˆçº§æ˜ å°„
        self.priority_map = {
            'stock_list': ['akshare', 'baostock'],
            'daily_kline': ['akshare', 'baostock', 'tushare'],
            'realtime_quotes': ['efinance', 'akshare'],
            'financial_report': ['tushare', 'akshare', 'baostock'],
            'industry_classification': ['akshare']
        }

    def get_data(self, data_type: str, fallback=True, **kwargs) -> pd.DataFrame:
        """
        æ™ºèƒ½è·å–æ•°æ®,æ”¯æŒè‡ªåŠ¨é™çº§

        Args:
            data_type: æ•°æ®ç±»å‹
            fallback: æ˜¯å¦å¯ç”¨é™çº§ç­–ç•¥
            **kwargs: ä¼ é€’ç»™æ•°æ®æºæ–¹æ³•çš„å‚æ•°

        Returns:
            DataFrame
        """
        priorities = self.priority_map.get(data_type, ['akshare'])

        for source_name in priorities:
            try:
                source = self.sources[source_name]

                # åŠ¨æ€è°ƒç”¨æ–¹æ³•
                method = getattr(source, f"get_{data_type}", None)
                if method:
                    print(f"ğŸ” å°è¯•æ•°æ®æº: {source.source_name}")
                    result = method(**kwargs)
                    print(f"âœ… {source.source_name} æˆåŠŸ")
                    return result

            except Exception as e:
                print(f"âŒ {source_name} å¤±è´¥: {e}")
                if not fallback:
                    raise
                # ç»§ç»­ä¸‹ä¸€ä¸ªæ•°æ®æº

        raise DataSourceError(f"æ‰€æœ‰æ•°æ®æºå‡å¤±è´¥,æ•°æ®ç±»å‹: {data_type}")

# ä½¿ç”¨ç¤ºä¾‹
factory = DataSourceFactory()

# è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ•°æ®æºè·å–æ—¥çº¿æ•°æ®
df_daily = factory.get_data(
    'daily_kline',
    symbol='000001',
    start_date='2024-01-01',
    end_date='2024-10-11'
)

# å®æ—¶è¡Œæƒ…ä¼˜å…ˆä½¿ç”¨Efinance
df_realtime = factory.get_data(
    'realtime_quotes',
    symbols=['000001', '600000']
)
```

#### å…³é”®APIå¯¹æ¯”è¡¨
| æ•°æ®ç±»å‹ | Akshare | Baostock | Tushare | Efinance |
|---------|---------|----------|---------|----------|
| è‚¡ç¥¨åˆ—è¡¨ | âœ… å…è´¹ | âœ… å…è´¹ | âš ï¸ éœ€ç§¯åˆ† | âŒ |
| æ—¥çº¿æ•°æ® | âœ… å…è´¹ | âœ… 35+å¹´ | âš ï¸ éœ€ç§¯åˆ† | âœ… å…è´¹ |
| å®æ—¶è¡Œæƒ… | âœ… å…è´¹ | âŒ | âš ï¸ éœ€ç§¯åˆ† | âœ… æœ€å¿« |
| åˆ†é’Ÿæ•°æ® | âœ… å…è´¹ | âŒ | âš ï¸ éœ€ç§¯åˆ† | âœ… å…è´¹ |
| è´¢åŠ¡æŠ¥è¡¨ | âœ… åŸºç¡€ | âœ… å®Œæ•´ | âœ… æœ€å…¨ | âŒ |
| è¡Œä¸šåˆ†ç±» | âœ… å¤šæ ‡å‡† | âœ… ç”³ä¸‡ | âœ… å¤šæ ‡å‡† | âœ… ä¸œè´¢æ¦‚å¿µ |
| Level-2 | âŒ | âŒ | âš ï¸ éœ€ç§¯åˆ† | âœ… å…è´¹ |

---

## 6. RedisæŒä¹…åŒ–ä¸å›ºåŒ–å†³ç­– (R6)

### Decision (å†³ç­–)
- **æŒä¹…åŒ–ç­–ç•¥**: AOF + RDB æ··åˆæŒä¹…åŒ–
- **AOFé…ç½®**: appendfsync everysec (æ¯ç§’åŒæ­¥)
- **å›ºåŒ–æœºåˆ¶**: åŸºäºå®šæ—¶ä»»åŠ¡çš„ä¸»åŠ¨å›ºåŒ– (240ç§’å‘¨æœŸ)
- **TTLç­–ç•¥**: çƒ­æ•°æ®TTLè®¾ç½®ä¸º300ç§’
- **æ‰¹é‡ä¼˜åŒ–**: Pipelineæ‰¹é‡è¯»å–,å‡å°‘ç½‘ç»œå¾€è¿”

**é‡è¦çº¦æŸ**: ä½¿ç”¨ Redis æ•°æ®åº“ç¼–å· 1-15,é¿å¼€ 0 å·æ•°æ®åº“ (å·²åˆ†é…ç»™å…¶ä»–ç¨‹åº)

### Rationale (æŠ€æœ¯åŸå› )
1. **æ•°æ®å®‰å…¨**: AOFç¡®ä¿æœ€å¤šä¸¢å¤±1ç§’æ•°æ®,RDBæä¾›å®šæœŸå®Œæ•´å¿«ç…§
2. **æ€§èƒ½å¹³è¡¡**: everysecåœ¨æ€§èƒ½å’Œå®‰å…¨æ€§ä¹‹é—´å–å¾—æœ€ä½³å¹³è¡¡
3. **å®šæ—¶å›ºåŒ–**: é¿å…ä¾èµ–Keyspace Notification (æ€§èƒ½å¼€é”€å¤§,ä¸å¯é )
4. **TTLç®¡ç†**: è‡ªåŠ¨æ¸…ç†è¿‡æœŸæ•°æ®,é˜²æ­¢å†…å­˜æº¢å‡º

### Alternatives Considered (è€ƒè™‘çš„å…¶ä»–æ–¹æ¡ˆ)
- âŒ **ä»…RDB**: æ•°æ®ä¸¢å¤±é£é™©é«˜ (æœ€å¤šä¸¢å¤±æ•°åˆ†é’Ÿ)
- âŒ **appendfsync always**: æ€§èƒ½æŸå¤±ä¸¥é‡ (æ¯æ¬¡å†™å…¥åŒæ­¥ç£ç›˜)
- âŒ **åŸºäºKeyspace Notification**: CPUå¼€é”€å¤§,ç”Ÿäº§ç¯å¢ƒä¸æ¨è
- âŒ **æ— TTLç­–ç•¥**: å†…å­˜æ— é™å¢é•¿,OOMé£é™©

### Implementation Notes (å®ç°è¦ç‚¹)

#### RedisæœåŠ¡å™¨é…ç½® (redis.conf)
```conf
# æŒä¹…åŒ–é…ç½®
# 1. RDBå¿«ç…§ (å®šæœŸå¤‡ä»½)
save 900 1        # 900ç§’å†…è‡³å°‘1ä¸ªkeyå˜åŒ–åˆ™ä¿å­˜
save 300 10       # 300ç§’å†…è‡³å°‘10ä¸ªkeyå˜åŒ–åˆ™ä¿å­˜
save 60 10000     # 60ç§’å†…è‡³å°‘10000ä¸ªkeyå˜åŒ–åˆ™ä¿å­˜

dbfilename dump.rdb
dir /var/lib/redis

# 2. AOFæŒä¹…åŒ– (å®æ—¶æ—¥å¿—)
appendonly yes
appendfilename "appendonly.aof"
appendfsync everysec              # æ¯ç§’åŒæ­¥,å¹³è¡¡æ€§èƒ½å’Œå®‰å…¨æ€§

# AOFé‡å†™é…ç½®
auto-aof-rewrite-percentage 100   # AOFæ–‡ä»¶å¢é•¿100%æ—¶è§¦å‘é‡å†™
auto-aof-rewrite-min-size 64mb    # AOFæœ€å°64MBæ—¶æ‰é‡å†™

# 3. æ··åˆæŒä¹…åŒ– (Redis 4.0+)
aof-use-rdb-preamble yes          # AOFé‡å†™æ—¶ä½¿ç”¨RDBæ ¼å¼

# å†…å­˜ç®¡ç†
maxmemory 4gb
maxmemory-policy allkeys-lru      # å†…å­˜æ»¡æ—¶ä½¿ç”¨LRUæ·˜æ±°

# Keyspace Notification (é»˜è®¤å…³é—­,ä»…è°ƒè¯•æ—¶å¼€å¯)
notify-keyspace-events ""         # ç”Ÿäº§ç¯å¢ƒä¸å¼€å¯,é¿å…æ€§èƒ½æŸå¤±

# æ•°æ®åº“ç¼–å·é…ç½®
databases 16                      # é»˜è®¤16ä¸ªæ•°æ®åº“(0-15)
# æ³¨æ„: æœ¬é¡¹ç›®ä½¿ç”¨æ•°æ®åº“1-15,é¿å¼€0å·
```

#### Redisæ•°æ®è®¿é—®å±‚ (å¸¦å›ºåŒ–æ”¯æŒ)
```python
import redis
import json
import pickle
from typing import Any, List, Dict
from datetime import datetime, timedelta

class RedisDataAccess:
    """Redisæ•°æ®è®¿é—®å±‚,æ”¯æŒçƒ­æ•°æ®å›ºåŒ–"""

    def __init__(self, host='localhost', port=6379, password='', db=1):  # é»˜è®¤ä½¿ç”¨1å·åº“
        """
        åˆå§‹åŒ–Redisè¿æ¥

        Args:
            db: Redisæ•°æ®åº“ç¼–å· (1-15),é¿å¼€0å·
        """
        if db == 0:
            raise ValueError("ä¸å…è®¸ä½¿ç”¨0å·æ•°æ®åº“,è¯·ä½¿ç”¨1-15å·åº“")

        self.pool = redis.ConnectionPool(
            host=host,
            port=port,
            password=password,
            db=db,
            max_connections=50,
            decode_responses=False  # æ”¯æŒäºŒè¿›åˆ¶æ•°æ®
        )
        self.redis = redis.Redis(connection_pool=self.pool)

        # å›ºåŒ–é…ç½®
        self.ttl_seconds = 300      # çƒ­æ•°æ®TTL: 5åˆ†é’Ÿ
        self.fixation_interval = 240  # å›ºåŒ–å‘¨æœŸ: 4åˆ†é’Ÿ

    def save_realtime_position(self, account_id: str, positions: pd.DataFrame):
        """
        ä¿å­˜å®æ—¶æŒä»“æ•°æ®

        Args:
            account_id: è´¦æˆ·ID
            positions: DataFrame with columns [symbol, quantity, cost_price, current_price]
        """
        key = f"position:{account_id}"

        # åºåˆ—åŒ–ä¸ºJSON
        positions_json = positions.to_json(orient='records')

        # å†™å…¥Rediså¹¶è®¾ç½®TTL
        self.redis.setex(
            name=key,
            time=self.ttl_seconds,
            value=positions_json
        )

        # è®°å½•åˆ°å›ºåŒ–å€™é€‰é›†åˆ
        self.redis.sadd("fixation:candidates", key)

    def get_realtime_position(self, account_id: str) -> Optional[pd.DataFrame]:
        """è·å–å®æ—¶æŒä»“æ•°æ®"""
        key = f"position:{account_id}"
        value = self.redis.get(key)

        if value:
            return pd.read_json(value, orient='records')
        return None

    def save_market_snapshot(self, symbols: List[str], snapshots: pd.DataFrame):
        """
        ä¿å­˜å¸‚åœºå¿«ç…§ (Level-2ç›˜å£æ•°æ®)

        Args:
            symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            snapshots: DataFrame with Level-2 data
        """
        pipe = self.redis.pipeline(transaction=False)

        for symbol in symbols:
            snapshot_data = snapshots[snapshots['symbol'] == symbol].iloc[0]
            key = f"snapshot:{symbol}"

            # ä½¿ç”¨Hashå­˜å‚¨ç»“æ„åŒ–æ•°æ®
            pipe.hset(key, mapping={
                'current_price': snapshot_data['current_price'],
                'bid_prices': json.dumps(snapshot_data['bid_prices']),
                'ask_prices': json.dumps(snapshot_data['ask_prices']),
                'bid_volumes': json.dumps(snapshot_data['bid_volumes']),
                'ask_volumes': json.dumps(snapshot_data['ask_volumes']),
                'timestamp': datetime.now().isoformat()
            })
            pipe.expire(key, self.ttl_seconds)

            # åŠ å…¥å›ºåŒ–å€™é€‰
            pipe.sadd("fixation:candidates", key)

        pipe.execute()

    def batch_get_snapshots(self, symbols: List[str]) -> Dict[str, dict]:
        """æ‰¹é‡è·å–å¸‚åœºå¿«ç…§ (Pipelineä¼˜åŒ–)"""
        pipe = self.redis.pipeline(transaction=False)

        for symbol in symbols:
            pipe.hgetall(f"snapshot:{symbol}")

        results = pipe.execute()

        # è§£æç»“æœ
        snapshots = {}
        for symbol, data in zip(symbols, results):
            if data:
                # è§£ç äºŒè¿›åˆ¶æ•°æ®
                decoded = {k.decode(): v.decode() for k, v in data.items()}
                decoded['bid_prices'] = json.loads(decoded['bid_prices'])
                decoded['ask_prices'] = json.loads(decoded['ask_prices'])
                snapshots[symbol] = decoded

        return snapshots

    def fixate_hot_data(self, postgresql_access):
        """
        å›ºåŒ–çƒ­æ•°æ®åˆ°PostgreSQL

        Args:
            postgresql_access: PostgreSQLæ•°æ®è®¿é—®å¯¹è±¡
        """
        # è·å–æ‰€æœ‰å€™é€‰key
        candidates = self.redis.smembers("fixation:candidates")

        if not candidates:
            print("æ— å¾…å›ºåŒ–æ•°æ®")
            return

        print(f"ğŸ”„ å¼€å§‹å›ºåŒ– {len(candidates)} æ¡çƒ­æ•°æ®...")

        for key in candidates:
            key_str = key.decode()

            try:
                # æ ¹æ®keyç±»å‹å†³å®šå›ºåŒ–ç­–ç•¥
                if key_str.startswith("position:"):
                    self._fixate_position(key_str, postgresql_access)
                elif key_str.startswith("snapshot:"):
                    self._fixate_snapshot(key_str, postgresql_access)
                elif key_str.startswith("account:"):
                    self._fixate_account(key_str, postgresql_access)

                # å›ºåŒ–æˆåŠŸåä»å€™é€‰é›†åˆç§»é™¤
                self.redis.srem("fixation:candidates", key)

            except Exception as e:
                print(f"âŒ å›ºåŒ–å¤±è´¥ {key_str}: {e}")
                # ä¿ç•™åœ¨å€™é€‰é›†åˆ,ä¸‹æ¬¡é‡è¯•

        print(f"âœ… å›ºåŒ–å®Œæˆ")

    def _fixate_position(self, key: str, pg_access):
        """å›ºåŒ–æŒä»“æ•°æ®"""
        value = self.redis.get(key)
        if not value:
            return

        account_id = key.split(':')[1]
        positions_df = pd.read_json(value, orient='records')

        # æ·»åŠ å…ƒæ•°æ®
        positions_df['account_id'] = account_id
        positions_df['snapshot_time'] = datetime.now()

        # å†™å…¥PostgreSQL
        pg_access.save_data(positions_df, 'position_history')

    def _fixate_snapshot(self, key: str, pg_access):
        """å›ºåŒ–å¸‚åœºå¿«ç…§"""
        snapshot = self.redis.hgetall(key)
        if not snapshot:
            return

        symbol = key.split(':')[1]

        # è§£ç å¹¶è½¬æ¢ä¸ºDataFrame
        decoded = {k.decode(): v.decode() for k, v in snapshot.items()}
        df = pd.DataFrame([{
            'symbol': symbol,
            'current_price': float(decoded['current_price']),
            'bid_prices': decoded['bid_prices'],
            'ask_prices': decoded['ask_prices'],
            'timestamp': decoded['timestamp']
        }])

        pg_access.save_data(df, 'market_snapshot_history')

    def _fixate_account(self, key: str, pg_access):
        """å›ºåŒ–è´¦æˆ·æ•°æ®"""
        account = self.redis.hgetall(key)
        if not account:
            return

        account_id = key.split(':')[1]

        # è½¬æ¢ä¸ºDataFrame
        decoded = {k.decode(): v.decode() for k, v in account.items()}
        df = pd.DataFrame([{
            'account_id': account_id,
            'total_asset': float(decoded['total_asset']),
            'available_cash': float(decoded['available_cash']),
            'market_value': float(decoded['market_value']),
            'snapshot_time': datetime.now()
        }])

        pg_access.save_data(df, 'account_history')
```

#### å®šæ—¶å›ºåŒ–è°ƒåº¦å™¨
```python
import schedule
import time
import threading

class RedisFixationScheduler:
    """Redisæ•°æ®å›ºåŒ–è°ƒåº¦å™¨"""

    def __init__(self, redis_access, postgresql_access):
        self.redis_access = redis_access
        self.postgresql_access = postgresql_access
        self.running = False

    def start(self):
        """å¯åŠ¨å®šæ—¶å›ºåŒ–ä»»åŠ¡"""
        self.running = True

        # æ¯240ç§’å›ºåŒ–ä¸€æ¬¡ (TTL=300ç§’,ç•™60ç§’ç¼“å†²)
        schedule.every(240).seconds.do(self._run_fixation)

        # åå°çº¿ç¨‹è¿è¡Œ
        def run_scheduler():
            while self.running:
                schedule.run_pending()
                time.sleep(10)

        scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
        scheduler_thread.start()
        print("âœ… Rediså›ºåŒ–è°ƒåº¦å™¨å·²å¯åŠ¨ (å‘¨æœŸ: 240ç§’)")

    def stop(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        self.running = False
        schedule.clear()
        print("â¹ï¸ Rediså›ºåŒ–è°ƒåº¦å™¨å·²åœæ­¢")

    def _run_fixation(self):
        """æ‰§è¡Œå›ºåŒ–ä»»åŠ¡"""
        try:
            self.redis_access.fixate_hot_data(self.postgresql_access)
        except Exception as e:
            print(f"âŒ å›ºåŒ–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")

# ä½¿ç”¨ç¤ºä¾‹
redis_access = RedisDataAccess(db=1)  # ä½¿ç”¨1å·åº“
postgresql_access = PostgreSQLDataAccess()

scheduler = RedisFixationScheduler(redis_access, postgresql_access)
scheduler.start()

# åº”ç”¨è¿è¡Œä¸­...
# å›ºåŒ–ä»»åŠ¡ä¼šæ¯240ç§’è‡ªåŠ¨æ‰§è¡Œ

# ä¼˜é›…å…³é—­
# scheduler.stop()
```

#### æ€§èƒ½ä¼˜åŒ–: Pipelineæ‰¹é‡æ“ä½œ
```python
def batch_save_positions(self, positions_dict: Dict[str, pd.DataFrame]):
    """
    æ‰¹é‡ä¿å­˜å¤šè´¦æˆ·æŒä»“æ•°æ® (Pipelineä¼˜åŒ–)

    Args:
        positions_dict: {account_id: positions_df}
    """
    pipe = self.redis.pipeline(transaction=False)

    for account_id, positions in positions_dict.items():
        key = f"position:{account_id}"
        value = positions.to_json(orient='records')

        pipe.setex(key, self.ttl_seconds, value)
        pipe.sadd("fixation:candidates", key)

    # ä¸€æ¬¡æ€§æäº¤æ‰€æœ‰å‘½ä»¤
    pipe.execute()
    print(f"âœ… æ‰¹é‡ä¿å­˜ {len(positions_dict)} ä¸ªè´¦æˆ·æŒä»“")
```

---

## 7. ç›‘æ§æ•°æ®åº“é€‰å‹å†³ç­– (R7)

### Decision (å†³ç­–)
- **æ•°æ®åº“é€‰æ‹©**: PostgreSQL (ç‹¬ç«‹å®ä¾‹)
- **æ‰©å±•æ’ä»¶**: pg_partman (è‡ªåŠ¨åˆ†åŒº) + pg_cron (å®šæ—¶æ¸…ç†)
- **åˆ†åŒºç­–ç•¥**: æŒ‰æ—¶é—´èŒƒå›´åˆ†åŒº (æŒ‰æœˆ)
- **ä¿ç•™ç­–ç•¥**: æ“ä½œæ—¥å¿—30å¤©,æ€§èƒ½æŒ‡æ ‡90å¤©,è´¨é‡æ£€æŸ¥7å¤©
- **ç´¢å¼•ç­–ç•¥**: JSONB GINç´¢å¼• + æ—¶é—´æˆ³BRINç´¢å¼•

### Rationale (æŠ€æœ¯åŸå› )
1. **JSONBä¼˜åŠ¿**: PostgreSQLå¯¹JSONæ•°æ®çš„æŸ¥è¯¢å’Œç´¢å¼•æ”¯æŒè¿œè¶…MySQL
2. **æ—¶åºä¼˜åŒ–**: BRINç´¢å¼•é€‚åˆæ—¶é—´æˆ³å­—æ®µ,ç©ºé—´å ç”¨å°ã€æ€§èƒ½é«˜
3. **è‡ªåŠ¨åŒ–**: pg_partmanè‡ªåŠ¨åˆ›å»ºå’Œç®¡ç†åˆ†åŒº,pg_cronå®šæ—¶æ¸…ç†è¿‡æœŸæ•°æ®
4. **ç‹¬ç«‹æ€§**: ç›‘æ§æ•°æ®åº“ä¸ä¸šåŠ¡æ•°æ®åº“éš”ç¦»,äº’ä¸å½±å“

### Alternatives Considered (è€ƒè™‘çš„å…¶ä»–æ–¹æ¡ˆ)
- âŒ **MySQL**: JSONBæ”¯æŒå¼±,æ—¶åºåˆ†åŒºç®¡ç†å¤æ‚
- âŒ **TDengine**: ç›‘æ§æ•°æ®é‡è¾ƒå°,ä¸éœ€è¦æ—¶åºæ•°æ®åº“çš„æè‡´å‹ç¼©
- âŒ **InfluxDB**: å¢åŠ æŠ€æœ¯æ ˆå¤æ‚åº¦,PostgreSQLè¶³å¤Ÿèƒœä»»

### Implementation Notes (å®ç°è¦ç‚¹)

#### ç›‘æ§æ•°æ®åº“è¡¨ç»“æ„
```sql
-- 1. æ“ä½œæ—¥å¿—è¡¨ (ä¿ç•™30å¤©)
CREATE TABLE IF NOT EXISTS operation_logs (
    id BIGSERIAL NOT NULL,
    operation_time TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    operation_type VARCHAR(64) NOT NULL,         -- save/load/delete/query
    database_target VARCHAR(32) NOT NULL,        -- tdengine/postgresql/mysql/redis
    table_name VARCHAR(128),
    data_classification VARCHAR(64),
    record_count BIGINT,
    operation_details JSONB,                     -- æ“ä½œè¯¦ç»†ä¿¡æ¯
    success BOOLEAN NOT NULL,
    error_message TEXT,
    duration_ms BIGINT,                          -- æ‰§è¡Œè€—æ—¶(æ¯«ç§’)
    PRIMARY KEY (id, operation_time)
) PARTITION BY RANGE (operation_time);

-- åˆ›å»ºBRINç´¢å¼• (æ—¶é—´æˆ³)
CREATE INDEX idx_oplog_time ON operation_logs USING BRIN (operation_time);

-- åˆ›å»ºGINç´¢å¼• (JSONB)
CREATE INDEX idx_oplog_details ON operation_logs USING GIN (operation_details);

-- åˆ›å»ºå¤åˆç´¢å¼•
CREATE INDEX idx_oplog_type_time ON operation_logs (operation_type, operation_time DESC);
CREATE INDEX idx_oplog_target_time ON operation_logs (database_target, operation_time DESC);

-- 2. æ€§èƒ½æŒ‡æ ‡è¡¨ (ä¿ç•™90å¤©)
CREATE TABLE IF NOT EXISTS performance_metrics (
    id BIGSERIAL NOT NULL,
    metric_time TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    metric_name VARCHAR(64) NOT NULL,            -- query_duration/write_throughput/connection_count
    metric_value DOUBLE PRECISION NOT NULL,
    metric_unit VARCHAR(32),                     -- ms/records_per_sec/count
    database_target VARCHAR(32),
    tags JSONB,                                  -- é¢å¤–ç»´åº¦æ ‡ç­¾
    PRIMARY KEY (id, metric_time)
) PARTITION BY RANGE (metric_time);

CREATE INDEX idx_perf_time ON performance_metrics USING BRIN (metric_time);
CREATE INDEX idx_perf_name_time ON performance_metrics (metric_name, metric_time DESC);

-- 3. æ•°æ®è´¨é‡æ£€æŸ¥è¡¨ (ä¿ç•™7å¤©)
CREATE TABLE IF NOT EXISTS data_quality_checks (
    id BIGSERIAL NOT NULL,
    check_time TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    check_type VARCHAR(64) NOT NULL,             -- completeness/freshness/accuracy
    data_classification VARCHAR(64) NOT NULL,
    table_name VARCHAR(128),
    expected_count BIGINT,
    actual_count BIGINT,
    missing_count BIGINT,
    completeness_rate DOUBLE PRECISION,
    freshness_delay_seconds BIGINT,
    check_result VARCHAR(32) NOT NULL,           -- pass/warning/fail
    check_details JSONB,
    PRIMARY KEY (id, check_time)
) PARTITION BY RANGE (check_time);

CREATE INDEX idx_quality_time ON data_quality_checks USING BRIN (check_time);
CREATE INDEX idx_quality_result_time ON data_quality_checks (check_result, check_time DESC);

-- 4. å‘Šè­¦è®°å½•è¡¨ (ä¿ç•™90å¤©)
CREATE TABLE IF NOT EXISTS alert_records (
    id BIGSERIAL NOT NULL,
    alert_time TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    alert_level VARCHAR(16) NOT NULL,            -- INFO/WARNING/ERROR/CRITICAL
    alert_source VARCHAR(64) NOT NULL,           -- monitoring/quality/performance
    alert_message TEXT NOT NULL,
    alert_context JSONB,
    resolved BOOLEAN DEFAULT FALSE,
    resolved_time TIMESTAMPTZ,
    resolved_by VARCHAR(64),
    PRIMARY KEY (id, alert_time)
) PARTITION BY RANGE (alert_time);

CREATE INDEX idx_alert_time ON alert_records USING BRIN (alert_time);
CREATE INDEX idx_alert_level_time ON alert_records (alert_level, alert_time DESC);
CREATE INDEX idx_alert_unresolved ON alert_records (resolved, alert_time DESC) WHERE resolved = FALSE;
```

#### pg_partmanè‡ªåŠ¨åˆ†åŒºé…ç½®
```sql
-- å®‰è£…pg_partmanæ‰©å±•
CREATE EXTENSION IF NOT EXISTS pg_partman;

-- é…ç½®operation_logsè‡ªåŠ¨åˆ†åŒº (æŒ‰æœˆ)
SELECT partman.create_parent(
    p_parent_table := 'public.operation_logs',
    p_control := 'operation_time',
    p_type := 'native',
    p_interval := '1 month',
    p_premake := 3,                              -- é¢„åˆ›å»º3ä¸ªæœˆåˆ†åŒº
    p_start_partition := '2024-10-01'
);

-- é…ç½®è‡ªåŠ¨ç»´æŠ¤
UPDATE partman.part_config
SET retention = '30 days',                       -- ä¿ç•™30å¤©
    retention_keep_table = false,                -- è¿‡æœŸååˆ é™¤è¡¨
    infinite_time_partitions = true
WHERE parent_table = 'public.operation_logs';

-- é…ç½®performance_metricsè‡ªåŠ¨åˆ†åŒº
SELECT partman.create_parent(
    p_parent_table := 'public.performance_metrics',
    p_control := 'metric_time',
    p_type := 'native',
    p_interval := '1 month',
    p_premake := 3
);

UPDATE partman.part_config
SET retention = '90 days',
    retention_keep_table = false
WHERE parent_table = 'public.performance_metrics';

-- é…ç½®data_quality_checksè‡ªåŠ¨åˆ†åŒº
SELECT partman.create_parent(
    p_parent_table := 'public.data_quality_checks',
    p_control := 'check_time',
    p_type := 'native',
    p_interval := '1 day',                       -- æŒ‰å¤©åˆ†åŒº(æ•°æ®é‡å°)
    p_premake := 7
);

UPDATE partman.part_config
SET retention = '7 days',
    retention_keep_table = false
WHERE parent_table = 'public.data_quality_checks';
```

#### pg_cronå®šæ—¶æ¸…ç†ä»»åŠ¡
```sql
-- å®‰è£…pg_cronæ‰©å±•
CREATE EXTENSION IF NOT EXISTS pg_cron;

-- é…ç½®å®šæ—¶åˆ†åŒºç»´æŠ¤ (æ¯å¤©å‡Œæ™¨4ç‚¹)
SELECT cron.schedule(
    'partman-maintenance',
    '0 4 * * *',                                 -- Cronè¡¨è¾¾å¼: æ¯å¤©4:00
    $$CALL partman.run_maintenance_proc()$$
);

-- é…ç½®å‘Šè­¦æ¸…ç† (æ¯å‘¨æ—¥å‡Œæ™¨3ç‚¹)
SELECT cron.schedule(
    'cleanup-resolved-alerts',
    '0 3 * * 0',                                 -- æ¯å‘¨æ—¥3:00
    $$DELETE FROM alert_records
      WHERE resolved = TRUE
      AND resolved_time < NOW() - INTERVAL '30 days'$$
);

-- æŸ¥çœ‹å®šæ—¶ä»»åŠ¡
SELECT * FROM cron.job;
```

#### ç›‘æ§æ•°æ®åº“è®¿é—®å±‚
```python
import psycopg2
import json
from datetime import datetime

class MonitoringDatabase:
    """ç›‘æ§æ•°æ®åº“è®¿é—®å±‚"""

    def __init__(self, connection_string):
        """
        åˆå§‹åŒ–ç›‘æ§æ•°æ®åº“è¿æ¥

        Args:
            connection_string: PostgreSQLè¿æ¥å­—ç¬¦ä¸²
                æ ¼å¼: postgresql://user:password@host:port/database
        """
        self.conn = psycopg2.connect(connection_string)
        self.conn.autocommit = True

    def log_operation(self, operation_type, database_target, table_name,
                     data_classification, record_count, success,
                     duration_ms, error_message=None, details=None):
        """è®°å½•æ“ä½œæ—¥å¿—"""
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO operation_logs (
                operation_time, operation_type, database_target, table_name,
                data_classification, record_count, operation_details,
                success, error_message, duration_ms
            ) VALUES (
                NOW(), %s, %s, %s, %s, %s, %s, %s, %s, %s
            )
        """, (
            operation_type,
            database_target,
            table_name,
            data_classification,
            record_count,
            json.dumps(details) if details else None,
            success,
            error_message,
            duration_ms
        ))
        cursor.close()

    def record_performance_metric(self, metric_name, metric_value,
                                  metric_unit, database_target=None, tags=None):
        """è®°å½•æ€§èƒ½æŒ‡æ ‡"""
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO performance_metrics (
                metric_time, metric_name, metric_value, metric_unit,
                database_target, tags
            ) VALUES (
                NOW(), %s, %s, %s, %s, %s
            )
        """, (
            metric_name,
            metric_value,
            metric_unit,
            database_target,
            json.dumps(tags) if tags else None
        ))
        cursor.close()

    def log_quality_check(self, check_type, data_classification, table_name,
                         expected_count, actual_count, check_result,
                         freshness_delay=None, details=None):
        """è®°å½•æ•°æ®è´¨é‡æ£€æŸ¥"""
        completeness_rate = (actual_count / expected_count * 100) if expected_count > 0 else 0
        missing_count = max(0, expected_count - actual_count)

        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO data_quality_checks (
                check_time, check_type, data_classification, table_name,
                expected_count, actual_count, missing_count,
                completeness_rate, freshness_delay_seconds,
                check_result, check_details
            ) VALUES (
                NOW(), %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
            )
        """, (
            check_type,
            data_classification,
            table_name,
            expected_count,
            actual_count,
            missing_count,
            completeness_rate,
            freshness_delay,
            check_result,
            json.dumps(details) if details else None
        ))
        cursor.close()

    def create_alert(self, alert_level, alert_source, alert_message, context=None):
        """åˆ›å»ºå‘Šè­¦"""
        cursor = self.conn.cursor()
        cursor.execute("""
            INSERT INTO alert_records (
                alert_time, alert_level, alert_source, alert_message, alert_context
            ) VALUES (
                NOW(), %s, %s, %s, %s
            )
        """, (
            alert_level,
            alert_source,
            alert_message,
            json.dumps(context) if context else None
        ))
        cursor.close()

        print(f"ğŸš¨ [{alert_level}] {alert_message}")

    def get_recent_failures(self, hours=24, limit=50):
        """æŸ¥è¯¢è¿‘æœŸå¤±è´¥çš„æ“ä½œ"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT operation_time, operation_type, database_target,
                   table_name, error_message
            FROM operation_logs
            WHERE success = FALSE
              AND operation_time > NOW() - INTERVAL '%s hours'
            ORDER BY operation_time DESC
            LIMIT %s
        """, (hours, limit))

        results = cursor.fetchall()
        cursor.close()
        return results

    def get_slow_queries(self, threshold_ms=5000, hours=24):
        """æŸ¥è¯¢æ…¢æ“ä½œ"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT operation_time, operation_type, database_target,
                   table_name, duration_ms
            FROM operation_logs
            WHERE duration_ms > %s
              AND operation_time > NOW() - INTERVAL '%s hours'
            ORDER BY duration_ms DESC
            LIMIT 100
        """, (threshold_ms, hours))

        results = cursor.fetchall()
        cursor.close()
        return results

# ä½¿ç”¨ç¤ºä¾‹
monitor_db = MonitoringDatabase('postgresql://postgres:password@localhost/mystocks_monitor')

# è®°å½•æ“ä½œæ—¥å¿—
import time
start = time.time()
# ... æ‰§è¡Œæ•°æ®åº“æ“ä½œ ...
duration = (time.time() - start) * 1000

monitor_db.log_operation(
    operation_type='save',
    database_target='tdengine',
    table_name='tick_data',
    data_classification='TICK_DATA',
    record_count=10000,
    success=True,
    duration_ms=duration,
    details={'batch_size': 5000}
)

# è®°å½•æ€§èƒ½æŒ‡æ ‡
monitor_db.record_performance_metric(
    metric_name='write_throughput',
    metric_value=10000 / (duration / 1000),  # records/sec
    metric_unit='records_per_sec',
    database_target='tdengine',
    tags={'table': 'tick_data'}
)

# åˆ›å»ºå‘Šè­¦
if duration > 5000:
    monitor_db.create_alert(
        alert_level='WARNING',
        alert_source='performance',
        alert_message=f'æ…¢å†™å…¥æ“ä½œ: {duration:.0f}ms',
        context={'table': 'tick_data', 'duration_ms': duration}
    )
```

---

## 8. Pythonç±»å‹ç³»ç»Ÿå†³ç­– (R8)

### Decision (å†³ç­–)
- **ç±»å‹æç¤º**: Python 3.8+ åŸç”Ÿç±»å‹æç¤º + typingæ¨¡å—
- **DataFrameéªŒè¯**: Panderaæ¡†æ¶ + è‡ªå®šä¹‰Schema
- **é™æ€æ£€æŸ¥**: mypy (ä¸¥æ ¼æ¨¡å¼)
- **è¿è¡Œæ—¶éªŒè¯**: Pydantic V2 (é…ç½®æ–‡ä»¶) + Pandera (DataFrame)

### Rationale (æŠ€æœ¯åŸå› )
1. **æ—©æœŸé”™è¯¯å‘ç°**: ç±»å‹æç¤ºè®©IDEå’Œmypyåœ¨ç¼–ç é˜¶æ®µå‘ç°é”™è¯¯
2. **DataFrameå®‰å…¨**: Panderaæä¾›DataFrame schemaéªŒè¯,é˜²æ­¢è„æ•°æ®è¿›å…¥ç³»ç»Ÿ
3. **æ–‡æ¡£ä»·å€¼**: ç±»å‹æç¤ºå³æ–‡æ¡£,æå‡ä»£ç å¯è¯»æ€§
4. **é‡æ„æ”¯æŒ**: å¼ºç±»å‹ç³»ç»Ÿè®©é‡æ„æ›´å®‰å…¨,é™ä½å›å½’é£é™©

### Alternatives Considered (è€ƒè™‘çš„å…¶ä»–æ–¹æ¡ˆ)
- âŒ **æ— ç±»å‹æç¤º**: è¿è¡Œæ—¶é”™è¯¯å¤š,ç»´æŠ¤å›°éš¾
- âŒ **ä»…Pydantic**: æ— æ³•éªŒè¯DataFrameç»“æ„
- âŒ **Great Expectations**: è¿‡äºå¤æ‚,å­¦ä¹ æ›²çº¿é™¡å³­

### Implementation Notes (å®ç°è¦ç‚¹)

#### Pandera DataFrame Schemaå®šä¹‰
```python
import pandas as pd
import pandera as pa
from pandera.typing import DataFrame, Series
from typing import Optional
from datetime import datetime

# 1. è‚¡ç¥¨æ—¥çº¿æ•°æ®Schema
class DailyKlineSchema(pa.DataFrameModel):
    """æ—¥çº¿æ•°æ®Schema"""

    symbol: Series[str] = pa.Field(
        str_matches=r'^[0-9]{6}$',               # 6ä½æ•°å­—ä»£ç 
        description="è‚¡ç¥¨ä»£ç "
    )

    trade_date: Series[pd.DatetimeTZDtype] = pa.Field(
        description="äº¤æ˜“æ—¥æœŸ"
    )

    open: Series[float] = pa.Field(
        ge=0,                                    # >= 0
        description="å¼€ç›˜ä»·"
    )

    high: Series[float] = pa.Field(
        ge=0,
        description="æœ€é«˜ä»·"
    )

    low: Series[float] = pa.Field(
        ge=0,
        description="æœ€ä½ä»·"
    )

    close: Series[float] = pa.Field(
        ge=0,
        description="æ”¶ç›˜ä»·"
    )

    volume: Series[int] = pa.Field(
        ge=0,
        description="æˆäº¤é‡"
    )

    amount: Series[float] = pa.Field(
        ge=0,
        nullable=True,
        description="æˆäº¤é¢"
    )

    @pa.dataframe_check
    def check_ohlc_logic(cls, df: pd.DataFrame) -> pd.Series:
        """éªŒè¯OHLCé€»è¾‘: high >= max(open, close), low <= min(open, close)"""
        return (
            (df['high'] >= df[['open', 'close']].max(axis=1)) &
            (df['low'] <= df[['open', 'close']].min(axis=1))
        )

    class Config:
        strict = True                            # ä¸¥æ ¼æ¨¡å¼: ä¸å…è®¸é¢å¤–åˆ—
        coerce = True                            # è‡ªåŠ¨ç±»å‹è½¬æ¢

# 2. å®æ—¶æŒä»“Schema
class PositionSchema(pa.DataFrameModel):
    """å®æ—¶æŒä»“Schema"""

    account_id: Series[str] = pa.Field(
        str_length={'min_value': 4, 'max_value': 32},
        description="è´¦æˆ·ID"
    )

    symbol: Series[str] = pa.Field(
        str_matches=r'^[0-9]{6}$',
        description="è‚¡ç¥¨ä»£ç "
    )

    quantity: Series[int] = pa.Field(
        ge=0,
        description="æŒä»“æ•°é‡"
    )

    cost_price: Series[float] = pa.Field(
        ge=0,
        description="æˆæœ¬ä»·"
    )

    current_price: Series[float] = pa.Field(
        ge=0,
        description="å½“å‰ä»·"
    )

    update_time: Series[pd.DatetimeTZDtype] = pa.Field(
        description="æ›´æ–°æ—¶é—´"
    )

    @pa.dataframe_check
    def check_quantity_lot_size(cls, df: pd.DataFrame) -> pd.Series:
        """éªŒè¯æ•°é‡ä¸º100çš„å€æ•° (Aè‚¡1æ‰‹=100è‚¡)"""
        return (df['quantity'] % 100 == 0) | (df['quantity'] == 0)

# 3. æŠ€æœ¯æŒ‡æ ‡Schema
class TechnicalIndicatorSchema(pa.DataFrameModel):
    """æŠ€æœ¯æŒ‡æ ‡Schema"""

    symbol: Series[str]
    calc_date: Series[pd.DatetimeTZDtype]
    indicator_name: Series[str] = pa.Field(isin=['MACD', 'RSI', 'BOLL', 'KDJ', 'MA'])
    indicator_value: Series[float] = pa.Field(nullable=True)

    class Config:
        strict = False                           # å…è®¸é¢å¤–åˆ—(å¦‚indicator_params)
```

#### ç±»å‹å®‰å…¨çš„æ•°æ®æ“ä½œ
```python
from typing import List, Optional, Literal
from pandera.typing import DataFrame

class TypeSafeDataManager:
    """ç±»å‹å®‰å…¨çš„æ•°æ®ç®¡ç†å™¨"""

    @pa.check_types
    def save_daily_kline(
        self,
        data: DataFrame[DailyKlineSchema],       # PanderaéªŒè¯
        classification: DataClassification
    ) -> bool:
        """
        ä¿å­˜æ—¥çº¿æ•°æ® (ç±»å‹å®‰å…¨)

        Args:
            data: æ—¥çº¿DataFrame,è‡ªåŠ¨éªŒè¯schema
            classification: æ•°æ®åˆ†ç±»æšä¸¾

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        # Panderaè‡ªåŠ¨éªŒè¯dataæ˜¯å¦ç¬¦åˆDailyKlineSchema
        # ä¸ç¬¦åˆä¼šæŠ›å‡ºSchemaErrorå¼‚å¸¸

        return self.unified_manager.save_data_by_classification(
            data,
            classification
        )

    @pa.check_types
    def load_daily_kline(
        self,
        symbol: str,
        start_date: str,
        end_date: str
    ) -> DataFrame[DailyKlineSchema]:
        """
        åŠ è½½æ—¥çº¿æ•°æ® (ç±»å‹å®‰å…¨)

        Returns:
            éªŒè¯åçš„æ—¥çº¿DataFrame
        """
        data = self.unified_manager.load_data_by_classification(
            classification=DataClassification.DAILY_KLINE,
            filters={'symbol': symbol, 'start_date': start_date, 'end_date': end_date}
        )

        # Panderaè‡ªåŠ¨éªŒè¯è¿”å›å€¼
        return data

    @pa.check_types
    def save_position(
        self,
        positions: DataFrame[PositionSchema]
    ) -> bool:
        """ä¿å­˜æŒä»“æ•°æ® (ç±»å‹å®‰å…¨)"""
        return self.unified_manager.save_data_by_classification(
            positions,
            DataClassification.REALTIME_POSITION
        )

# ä½¿ç”¨ç¤ºä¾‹
manager = TypeSafeDataManager()

# æ­£ç¡®çš„æ•°æ®
valid_data = pd.DataFrame({
    'symbol': ['000001', '600000'],
    'trade_date': pd.to_datetime(['2024-10-10', '2024-10-10'], utc=True),
    'open': [10.5, 8.3],
    'high': [11.0, 8.9],
    'low': [10.2, 8.1],
    'close': [10.8, 8.7],
    'volume': [1000000, 2000000],
    'amount': [10800000.0, 17400000.0]
})

manager.save_daily_kline(valid_data, DataClassification.DAILY_KLINE)  # âœ… é€šè¿‡

# é”™è¯¯çš„æ•°æ® (high < close)
invalid_data = pd.DataFrame({
    'symbol': ['000001'],
    'trade_date': pd.to_datetime(['2024-10-10'], utc=True),
    'open': [10.5],
    'high': [10.0],  # âŒ high < close,è¿åOHLCé€»è¾‘
    'low': [10.2],
    'close': [10.8],
    'volume': [1000000],
    'amount': [10800000.0]
})

try:
    manager.save_daily_kline(invalid_data, DataClassification.DAILY_KLINE)
except pa.errors.SchemaError as e:
    print(f"âŒ SchemaéªŒè¯å¤±è´¥: {e}")
    # è¾“å‡º: âŒ SchemaéªŒè¯å¤±è´¥: <SchemaError: Check 'check_ohlc_logic' failed>
```

#### mypyé™æ€ç±»å‹æ£€æŸ¥é…ç½®
```ini
# mypy.ini
[mypy]
python_version = 3.8
warn_return_any = True
warn_unused_configs = True
disallow_untyped_defs = True              # å¼ºåˆ¶æ‰€æœ‰å‡½æ•°æ·»åŠ ç±»å‹æç¤º
disallow_any_unimported = False
no_implicit_optional = True
warn_redundant_casts = True
warn_unused_ignores = True
warn_no_return = True
check_untyped_defs = True
strict_equality = True

# ç¬¬ä¸‰æ–¹åº“å­˜æ ¹é…ç½®
[mypy-pandas.*]
ignore_missing_imports = True

[mypy-numpy.*]
ignore_missing_imports = True

[mypy-akshare.*]
ignore_missing_imports = True

[mypy-taosws.*]
ignore_missing_imports = True
```

#### å®Œæ•´ç±»å‹æç¤ºç¤ºä¾‹
```python
from typing import List, Dict, Optional, Union, Literal, TypedDict
from datetime import datetime
from enum import Enum

# æšä¸¾ç±»å‹
class DataClassification(str, Enum):
    TICK_DATA = "TICK_DATA"
    MINUTE_KLINE = "MINUTE_KLINE"
    DAILY_KLINE = "DAILY_KLINE"
    # ... å…¶ä»–åˆ†ç±»

class DatabaseTarget(str, Enum):
    TDENGINE = "tdengine"
    POSTGRESQL = "postgresql"
    MYSQL = "mysql"
    REDIS = "redis"

# TypedDict (ç»“æ„åŒ–å­—å…¸)
class QueryFilter(TypedDict, total=False):
    """æŸ¥è¯¢è¿‡æ»¤å™¨"""
    symbol: str
    start_date: str
    end_date: str
    limit: int

class ConnectionConfig(TypedDict):
    """æ•°æ®åº“è¿æ¥é…ç½®"""
    host: str
    port: int
    user: str
    password: str
    database: str
    pool_size: int

# å®Œæ•´ç±»å‹æç¤ºçš„ç±»
class MyStocksUnifiedManager:
    """ç»Ÿä¸€æ•°æ®ç®¡ç†å™¨ (å®Œæ•´ç±»å‹æç¤º)"""

    def __init__(
        self,
        config_path: str,
        monitor_db_url: str
    ) -> None:
        self.config_path: str = config_path
        self.monitor_db_url: str = monitor_db_url
        self.connections: Dict[DatabaseTarget, Any] = {}

    @pa.check_types
    def save_data_by_classification(
        self,
        data: DataFrame[DailyKlineSchema],       # PanderaéªŒè¯
        classification: DataClassification,
        target_db: Optional[DatabaseTarget] = None
    ) -> bool:
        """
        ä¿å­˜æ•°æ® (æ ¹æ®åˆ†ç±»è‡ªåŠ¨è·¯ç”±)

        Args:
            data: æ•°æ®DataFrame
            classification: æ•°æ®åˆ†ç±»
            target_db: ç›®æ ‡æ•°æ®åº“ (Noneè¡¨ç¤ºè‡ªåŠ¨è·¯ç”±)

        Returns:
            æ˜¯å¦æˆåŠŸ

        Raises:
            ValueError: ä¸æ”¯æŒçš„æ•°æ®åˆ†ç±»
            DatabaseError: æ•°æ®åº“æ“ä½œå¤±è´¥
        """
        if target_db is None:
            target_db = self._auto_route(classification)

        return self._execute_save(data, target_db)

    def _auto_route(
        self,
        classification: DataClassification
    ) -> DatabaseTarget:
        """è‡ªåŠ¨è·¯ç”±é€»è¾‘"""
        routing_map: Dict[DataClassification, DatabaseTarget] = {
            DataClassification.TICK_DATA: DatabaseTarget.TDENGINE,
            DataClassification.DAILY_KLINE: DatabaseTarget.POSTGRESQL,
            # ... å…¶ä»–æ˜ å°„
        }

        if classification not in routing_map:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®åˆ†ç±»: {classification}")

        return routing_map[classification]

    def load_data_by_classification(
        self,
        classification: DataClassification,
        filters: Optional[QueryFilter] = None,
        limit: Optional[int] = None
    ) -> pd.DataFrame:
        """
        åŠ è½½æ•°æ® (æ ¹æ®åˆ†ç±»è‡ªåŠ¨è·¯ç”±)

        Args:
            classification: æ•°æ®åˆ†ç±»
            filters: æŸ¥è¯¢è¿‡æ»¤æ¡ä»¶
            limit: è¿”å›è®°å½•æ•°é™åˆ¶

        Returns:
            DataFrame
        """
        target_db = self._auto_route(classification)
        return self._execute_load(target_db, filters, limit)

    def batch_save(
        self,
        datasets: List[tuple[pd.DataFrame, DataClassification]]
    ) -> Dict[DataClassification, bool]:
        """
        æ‰¹é‡ä¿å­˜å¤šä¸ªæ•°æ®é›†

        Args:
            datasets: [(data1, classification1), (data2, classification2), ...]

        Returns:
            {classification: success_status}
        """
        results: Dict[DataClassification, bool] = {}

        for data, classification in datasets:
            success = self.save_data_by_classification(data, classification)
            results[classification] = success

        return results
```

#### è¿è¡Œæ—¶éªŒè¯è£…é¥°å™¨
```python
from functools import wraps
import inspect

def validate_dataframe_schema(schema_class):
    """
    è£…é¥°å™¨: éªŒè¯DataFrameå‚æ•°

    Usage:
        @validate_dataframe_schema(DailyKlineSchema)
        def process_kline(df: pd.DataFrame):
            ...
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # æŸ¥æ‰¾DataFrameç±»å‹çš„å‚æ•°
            sig = inspect.signature(func)
            bound_args = sig.bind(*args, **kwargs)

            for param_name, param_value in bound_args.arguments.items():
                if isinstance(param_value, pd.DataFrame):
                    # éªŒè¯schema
                    try:
                        schema_class.validate(param_value)
                    except pa.errors.SchemaError as e:
                        raise ValueError(
                            f"å‚æ•° '{param_name}' DataFrameéªŒè¯å¤±è´¥: {e}"
                        )

            return func(*args, **kwargs)
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@validate_dataframe_schema(DailyKlineSchema)
def calculate_moving_average(df: pd.DataFrame, window: int) -> pd.DataFrame:
    """è®¡ç®—ç§»åŠ¨å¹³å‡ (è‡ªåŠ¨éªŒè¯è¾“å…¥DataFrame)"""
    df['MA'] = df['close'].rolling(window=window).mean()
    return df
```

---

## 9. ä¾èµ–æ¸…å•

åŸºäºä¸Šè¿°8ä¸ªç ”ç©¶å†³ç­–,æœ€ç»ˆç¡®è®¤çš„Pythonä¾èµ–æ¸…å•:

### requirements.txt
```txt
# === æ ¸å¿ƒä¾èµ– ===
pandas>=2.0.0                   # æ•°æ®å¤„ç†
numpy>=1.24.0                   # æ•°å€¼è®¡ç®—
pyyaml>=6.0                     # YAMLé…ç½®è§£æ

# === æ•°æ®åº“é©±åŠ¨ ===
# TDengine
taospy>=2.7.0                   # TDengineåŸç”Ÿè¿æ¥å™¨(WebSocket)

# PostgreSQL + TimescaleDB
psycopg2-binary>=2.9.5          # PostgreSQLé©±åŠ¨
sqlalchemy>=2.0.0               # ORMæ”¯æŒ(å¯é€‰)

# MySQL/MariaDB
pymysql>=1.0.2                  # MySQLé©±åŠ¨

# Redis
redis>=4.5.0                    # Rediså®¢æˆ·ç«¯

# === æ•°æ®æºé€‚é…å™¨ ===
akshare>=1.11.0                 # ä¸»è¦æ•°æ®æº
baostock>=0.9.0                 # å†å²æ•°æ®å¤‡ä»½
tushare>=1.3.0                  # æ·±åº¦è´¢åŠ¡æ•°æ®(å¯é€‰)
efinance>=0.5.0                 # å®æ—¶è¡Œæƒ…(é€šè¿‡CustomerAdapter)

# === ç±»å‹éªŒè¯ ===
pydantic>=2.0.0                 # é…ç½®æ–‡ä»¶éªŒè¯
pandera>=0.17.0                 # DataFrame schemaéªŒè¯

# === ç›‘æ§å’Œæ—¥å¿— ===
schedule>=1.2.0                 # å®šæ—¶ä»»åŠ¡è°ƒåº¦

# === å¼€å‘å·¥å…· ===
mypy>=1.5.0                     # é™æ€ç±»å‹æ£€æŸ¥
pytest>=7.4.0                   # å•å…ƒæµ‹è¯•
pytest-cov>=4.1.0               # æµ‹è¯•è¦†ç›–ç‡

# === å¯é€‰ä¾èµ– ===
# requests>=2.31.0              # HTTPè¯·æ±‚(æ•°æ®æºå¤‡ç”¨)
# beautifulsoup4>=4.12.0        # HTMLè§£æ(çˆ¬è™«å¤‡ç”¨)
```

### ç¯å¢ƒè¦æ±‚
- **Pythonç‰ˆæœ¬**: 3.8+ (æ¨è3.10+)
- **æ“ä½œç³»ç»Ÿ**: Linux (ç”Ÿäº§ç¯å¢ƒæ¨è) / Windows / macOS
- **å†…å­˜**: æœ€ä½8GB,æ¨è16GB+
- **å­˜å‚¨**: SSDæ¨è (TDengine/TimescaleDBæ€§èƒ½ä¾èµ–ç£ç›˜IO)

### æ•°æ®åº“æœåŠ¡ç«¯ç‰ˆæœ¬
- **TDengine**: 3.0+ (æ”¯æŒWebSocketè¿æ¥)
- **PostgreSQL**: 14+ (æ”¯æŒTimescaleDB 2.x)
- **TimescaleDB**: 2.10+
- **MySQL/MariaDB**: 8.0+ / 10.6+
- **Redis**: 6.0+ (æ”¯æŒAOF+RDBæ··åˆæŒä¹…åŒ–)

### PostgreSQLæ‰©å±•è¦æ±‚
```sql
-- ç›‘æ§æ•°æ®åº“å¿…éœ€æ‰©å±•
CREATE EXTENSION IF NOT EXISTS pg_partman;
CREATE EXTENSION IF NOT EXISTS pg_cron;

-- ä¸šåŠ¡æ•°æ®åº“å¿…éœ€æ‰©å±•
CREATE EXTENSION IF NOT EXISTS timescaledb;
```

---

## 10. ç ”ç©¶æˆæœæ€»ç»“

### å…³é”®æŠ€æœ¯å†³ç­–çŸ©é˜µ

| ç ”ç©¶ä¸»é¢˜ | å†³ç­–æ–¹æ¡ˆ | æ ¸å¿ƒä¼˜åŠ¿ | é£é™©/é™åˆ¶ |
|---------|---------|---------|----------|
| **R1: TDengineé›†æˆ** | WebSocket + Super Table + ZSTDå‹ç¼© | 5-10xæ€§èƒ½æå‡,20:1å‹ç¼©æ¯” | å­¦ä¹ æ›²çº¿,éœ€WebSocketæ”¯æŒ |
| **R2: TimescaleDBé…ç½®** | 1å¤©Chunk + 30å¤©å‹ç¼© | è‡ªåŠ¨åˆ†åŒº,æŸ¥è¯¢æ€§èƒ½å¥½ | PostgreSQLä¾èµ– |
| **R3: å¤šåº“äº‹åŠ¡** | SQLite Outboxé˜Ÿåˆ— + æœ€ç»ˆä¸€è‡´æ€§ | ç®€å•å¯é ,æ•…éšœå®¹é”™ | éå¼ºä¸€è‡´æ€§ |
| **R4: YAMLæ¶æ„** | PyYAML + Pydantic V2 | ç±»å‹å®‰å…¨,IDEæ”¯æŒ | é…ç½®å¤æ‚åº¦å¢åŠ  |
| **R5: æ•°æ®æºç­–ç•¥** | Akshareä¸» + Baostock/Tushareå¤‡ | å…è´¹å…¨é¢,å¤šæºå†—ä½™ | APIç¨³å®šæ€§ä¾èµ– |
| **R6: RedisæŒä¹…åŒ–** | AOF+RDBæ··åˆ + å®šæ—¶å›ºåŒ– | æœ€å¤šä¸¢1ç§’,æ€§èƒ½å¥½ | éœ€å®šæ—¶ä»»åŠ¡ |
| **R7: ç›‘æ§æ•°æ®åº“** | PostgreSQL + pg_partman + pg_cron | JSONBå¼ºå¤§,è‡ªåŠ¨åŒ–ç»´æŠ¤ | éœ€PostgreSQLä¸“ä¸šçŸ¥è¯† |
| **R8: Pythonç±»å‹** | åŸç”Ÿç±»å‹æç¤º + Pandera + mypy | æ—©æœŸé”™è¯¯å‘ç°,DataFrameå®‰å…¨ | å¼€å‘æ—¶é—´å¢åŠ 10-15% |

### å®ªæ³•åˆè§„æ€§éªŒè¯

æ‰€æœ‰8ä¸ªç ”ç©¶å†³ç­–å‡å·²éªŒè¯ç¬¦åˆé¡¹ç›®å®ªæ³•7å¤§æ ¸å¿ƒåŸåˆ™:

âœ… **I. 5å±‚æ•°æ®åˆ†ç±»ä½“ç³»**: ç ”ç©¶æˆæœæ˜ç¡®æ”¯æŒ23ä¸ªæ•°æ®å­é¡¹çš„è·¯ç”±ç­–ç•¥
âœ… **II. é…ç½®é©±åŠ¨è®¾è®¡**: YAML + Pydanticæ¶æ„å®Œå…¨ç¬¦åˆ
âœ… **III. æ™ºèƒ½è‡ªåŠ¨è·¯ç”±**: DataStorageStrategyæ˜ å°„é€»è¾‘å·²å®šä¹‰
âœ… **IV. å¤šæ•°æ®åº“ååŒ**: 4ç§æ•°æ®åº“èŒè´£æ˜ç¡®,æŠ€æœ¯é€‰å‹åˆç†
âœ… **V. å®Œæ•´å¯è§‚æµ‹æ€§**: ç‹¬ç«‹ç›‘æ§æ•°æ®åº“ + pg_partmanè‡ªåŠ¨åŒ–
âœ… **VI. ç»Ÿä¸€è®¿é—®æ¥å£**: MyStocksUnifiedManagerç±»å‹å®‰å…¨å°è£…
âœ… **VII. å®‰å…¨ä¼˜å…ˆ**: ç¯å¢ƒå˜é‡ + .envéš”ç¦»,Redisé¿å¼€0å·åº“

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **Phase 1: Design & Contracts** (é¢„è®¡8-10å°æ—¶)
   - ç”Ÿæˆ `data-model.md` (23ä¸ªå®ä½“schemaå®šä¹‰)
   - åˆ›å»º `contracts/` ç›®å½• (3ä¸ªAPIå¥‘çº¦æ–‡æ¡£)
   - ç¼–å†™ `quickstart.md` (å®‰è£…å’Œå¿«é€Ÿå¼€å§‹æŒ‡å—)

2. **Phase 2: Tasks Generation** (é¢„è®¡2å°æ—¶)
   - è¿è¡Œ `/speckit.tasks` ç”Ÿæˆä¾èµ–æ’åºçš„ä»»åŠ¡æ¸…å•
   - éªŒè¯ä»»åŠ¡è¦†ç›–æ‰€æœ‰åŠŸèƒ½éœ€æ±‚

3. **Phase 3-4: Implementation** (é¢„è®¡40-60å°æ—¶)
   - æŒ‰ä»»åŠ¡æ¸…å•é¡ºåºå®æ–½
   - æ¯å®Œæˆ5ä¸ªä»»åŠ¡è¿è¡Œä¸€æ¬¡é›†æˆæµ‹è¯•

---

**ç ”ç©¶å®Œæˆæ—¥æœŸ**: 2025-10-11
**ç ”ç©¶æ€»è€—æ—¶**: çº¦7å°æ—¶ (8ä¸ªå¹¶è¡Œç ”ç©¶ä»»åŠ¡)
**ä¸‹ä¸€é˜¶æ®µ**: Phase 1 - Design & Contracts
