"""
Cache Manager - ç¼“å­˜è¯»å†™é€»è¾‘å®ç°
Task 2.2: å®ç°ç¼“å­˜è¯»å†™é€»è¾‘

æä¾›ç»Ÿä¸€çš„ç¼“å­˜è®¿é—®æ¥å£ï¼Œæ”¯æŒï¼š
- å•æ¡æ•°æ®è¯»å†™
- æ‰¹é‡è¯»å†™æ“ä½œ
- ç¼“å­˜å¤±æ•ˆæœºåˆ¶
- Cache-Aside æ¨¡å¼
- æ€§èƒ½ç›‘æ§

Features:
- Cache-Aside æ¨¡å¼å®ç°
- æ‰¹é‡æ“ä½œæ”¯æŒ
- ç¼“å­˜å¤±æ•ˆæœºåˆ¶
- è‡ªåŠ¨å…ƒæ•°æ®ç®¡ç†
- å®Œæ•´çš„é”™è¯¯å¤„ç†
"""

import asyncio
import time
from collections import defaultdict
from datetime import datetime, timedelta, timezone
from threading import Lock
from typing import Any, Dict, List, Optional

import structlog

from app.core.tdengine_manager import TDengineManager, get_tdengine_manager

# Rediså¤šçº§ç¼“å­˜æœåŠ¡
try:
    from src.core.cache.multi_level import CacheConfig, MultiLevelCache

    REDIS_CACHE_AVAILABLE = True
except ImportError:
    REDIS_CACHE_AVAILABLE = False
    MultiLevelCache = None
    CacheConfig = None

logger = structlog.get_logger()


class CacheManager:
    """
    ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨ - ä¸‰çº§ç¼“å­˜æ¶æ„

    å®ç° L1(å†…å­˜) -> L2(Redis) -> L3(TDengine) çš„é«˜é€Ÿé€šè·¯
    æ”¯æŒ Cache-Aside + Write-Through æ··åˆæ¨¡å¼

    Usage:
        ```python
        manager = get_cache_manager()

        # å•æ¡å¼‚æ­¥è¯»å–
        data = await manager.fetch_from_cache("000001", "fund_flow")

        # å•æ¡å¼‚æ­¥å†™å…¥
        await manager.write_to_cache("000001", "fund_flow", "1d", {"value": 100})

        # æ‰¹é‡å¼‚æ­¥è¯»å–
        results = await manager.batch_read([
            {"symbol": "000001", "data_type": "fund_flow"},
            {"symbol": "000858", "data_type": "etf"}
        ])

        # æ¸…é™¤ç¼“å­˜
        await manager.invalidate_cache(symbol="000001")

        # æ£€æŸ¥ç¼“å­˜æœ‰æ•ˆæ€§
        if await manager.is_cache_valid("000001", "fund_flow"):
            print("ç¼“å­˜æœ‰æ•ˆ")
        ```
    """

    def __init__(
        self, tdengine_manager: Optional[TDengineManager] = None, redis_cache: Optional[MultiLevelCache] = None
    ):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨

        Args:
            tdengine_manager: TDengineManager å®ä¾‹
            redis_cache: Rediså¤šçº§ç¼“å­˜æœåŠ¡å®ä¾‹
        """
        self.tdengine = tdengine_manager or get_tdengine_manager()
        self._tdengine_available = self.tdengine is not None

        # Redisç¼“å­˜æœåŠ¡ (L2)
        if redis_cache:
            self.redis_cache = redis_cache
            self._redis_available = True
        elif REDIS_CACHE_AVAILABLE:
            self.redis_cache = MultiLevelCache()
            self._redis_available = False  # éœ€è¦å¼‚æ­¥åˆå§‹åŒ–
        else:
            self.redis_cache = None
            self._redis_available = False

        self._cache_stats: Dict[str, Any] = {
            "hits": 0,
            "misses": 0,
            "reads": 0,
            "writes": 0,
            "evictions": 0,
            "batch_operations": 0,
            "total_response_time": 0.0,
        }

        # å†…å­˜ç¼“å­˜å±‚ (L1) - ä»…ä½œä¸ºRedisçš„å¿«é€Ÿç¼“å­˜
        self._memory_cache: dict[str, Any] = {}
        self._cache_ttl: dict[str, datetime] = {}
        self._cache_lock = Lock()
        self._access_patterns: defaultdict[str, list[datetime]] = defaultdict(list)

        # é…ç½®å‚æ•°
        self._max_memory_entries = 10000  # å†…å­˜ç¼“å­˜æœ€å¤§æ¡ç›®æ•°
        self._default_ttl = 300  # é»˜è®¤TTL 5åˆ†é’Ÿ
        self._tiered_ttl = {
            "tick_data": 30,  # 30ç§’
            "realtime_quote": 60,  # 1åˆ†é’Ÿ
            "minute_kline": 300,  # 5åˆ†é’Ÿ
            "fund_flow": 600,  # 10åˆ†é’Ÿ
            "etf": 1800,  # 30åˆ†é’Ÿ
            "default": 300,  # é»˜è®¤5åˆ†é’Ÿ
        }

    def _with_tdengine(self, fallback_value=None):
        """
        å®‰å…¨åœ°æ‰§è¡Œéœ€è¦ tdengine çš„æ“ä½œ

        Args:
            fallback_value: å¦‚æœ tdengine ä¸å¯ç”¨æ—¶çš„è¿”å›å€¼

        Returns:
            ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç¡®ä¿ tdengine å¯ç”¨
        """
        if self.tdengine is None:
            return fallback_value
        return self.tdengine

    # ==================== ä¸‰çº§ç¼“å­˜æ ¸å¿ƒæ“ä½œ ====================

    async def fetch_from_cache(
        self,
        symbol: str,
        data_type: str,
        timeframe: Optional[str] = None,
        days: int = 1,
    ) -> Optional[Dict[str, Any]]:
        """
        ä»ä¸‰çº§ç¼“å­˜è¯»å–æ•°æ® (L1 -> L2 -> L3)

        é‡‡ç”¨ä¸‰çº§ç¼“å­˜ç­–ç•¥ï¼šL1(å†…å­˜) -> L2(Redis) -> L3(TDengine)

        Args:
            symbol: è‚¡ç¥¨ä»£ç  (e.g., "000001")
            data_type: æ•°æ®ç±»å‹ (e.g., "fund_flow", "etf")
            timeframe: æ—¶é—´ç»´åº¦ (å¯é€‰ï¼Œe.g., "1d", "3d")
            days: å›æº¯å¤©æ•° (é»˜è®¤ 1)

        Returns:
            ç¼“å­˜æ•°æ®å­—å…¸ï¼Œæˆ– None å¦‚æœæœªæ‰¾åˆ°
        """
        start_time = time.time()
        self._cache_stats["reads"] += 1

        # è®°å½•è®¿é—®æ¨¡å¼
        self._record_access_pattern(symbol, data_type)

        try:
            # L1: å†…å­˜ç¼“å­˜ (æœ€é«˜æ€§èƒ½)
            memory_result = self._get_from_memory_cache(symbol, data_type, timeframe)
            if memory_result:
                response_time = time.time() - start_time
                self._update_performance_stats(response_time, True)
                self._cache_stats["hits"] += 1
                logger.debug(
                    "âœ… L1å†…å­˜ç¼“å­˜å‘½ä¸­",
                    symbol=symbol,
                    data_type=data_type,
                    hit_rate=self._calculate_hit_rate(),
                    response_time=response_time,
                )
                return memory_result

            # L2: Redisç¼“å­˜ (åˆ†å¸ƒå¼å…±äº«)
            cache_key = self.get_cache_key(symbol, data_type, timeframe or "1d")
            if self._redis_available and self.redis_cache:
                redis_result, found, level = await self.redis_cache.get(cache_key)
                if found:
                    response_time = time.time() - start_time
                    self._update_performance_stats(response_time, True)
                    self._cache_stats["hits"] += 1

                    # å°†æ•°æ®å›å¡«åˆ°L1å†…å­˜ç¼“å­˜
                    enriched_data = {
                        "data": redis_result,
                        "source": "redis",
                        "timestamp": datetime.now(timezone.utc).isoformat(),
                    }
                    self._add_to_memory_cache(symbol, data_type, timeframe or "1d", enriched_data)

                    logger.debug(
                        f"âœ… L2{level}ç¼“å­˜å‘½ä¸­",
                        symbol=symbol,
                        data_type=data_type,
                        hit_rate=self._calculate_hit_rate(),
                        response_time=response_time,
                    )
                    return enriched_data

            # L3: TDengineç¼“å­˜ (æŒä¹…åŒ–å­˜å‚¨)
            cache_data = None
            if self.tdengine is not None:
                cache_data = await self._write_to_tdengine(  # å¤ç”¨å¼‚æ­¥TDengineæ–¹æ³•
                    symbol=symbol,
                    data_type=data_type,
                    timeframe=timeframe or "1d",
                    data={},  # è¯»å–æ¨¡å¼
                    timestamp=None,
                )

            if cache_data:
                response_time = time.time() - start_time
                self._update_performance_stats(response_time, True)
                self._cache_stats["hits"] += 1

                # å°†æ•°æ®å›å¡«åˆ°L1+L2ç¼“å­˜
                enriched_data = {
                    "data": cache_data,
                    "source": "tdengine",
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                }
                self._add_to_memory_cache(symbol, data_type, timeframe or "1d", enriched_data)

                # å¼‚æ­¥å›å¡«åˆ°Redis (ä¸é˜»å¡å“åº”)
                if self._redis_available and self.redis_cache:
                    asyncio.create_task(
                        self.redis_cache.set(cache_key, enriched_data, ttl=self._get_tiered_ttl(data_type))
                    )

                logger.debug(
                    "âœ… L3 TDengineç¼“å­˜å‘½ä¸­",
                    symbol=symbol,
                    data_type=data_type,
                    hit_rate=self._calculate_hit_rate(),
                    response_time=response_time,
                )
                return enriched_data

            # ç¼“å­˜æœªå‘½ä¸­
            self._cache_stats["misses"] += 1
            response_time = time.time() - start_time
            self._update_performance_stats(response_time, False)

            logger.debug(
                "âš ï¸ ä¸‰çº§ç¼“å­˜å…¨éƒ¨æœªå‘½ä¸­",
                symbol=symbol,
                data_type=data_type,
                hit_rate=self._calculate_hit_rate(),
                response_time=response_time,
            )
            return None

        except Exception as e:
            response_time = time.time() - start_time
            self._update_performance_stats(response_time, False)
            logger.error(
                "âŒ ä¸‰çº§ç¼“å­˜è¯»å–å¤±è´¥",
                symbol=symbol,
                data_type=data_type,
                error=str(e),
                response_time=response_time,
            )
            return None

    async def write_to_cache(
        self,
        symbol: str,
        data_type: str,
        timeframe: str,
        data: Dict[str, Any],
        ttl_days: int = 7,
        timestamp: Optional[datetime] = None,
    ) -> bool:
        """
        å†™å…¥æ•°æ®åˆ°ä¸‰çº§ç¼“å­˜ (Write-Throughæ¨¡å¼)

        åŒæ—¶å†™å…¥L1(å†…å­˜)+L2(Redis)ï¼ŒL3(TDengine)å¼‚æ­¥å†™å…¥

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            data_type: æ•°æ®ç±»å‹
            timeframe: æ—¶é—´ç»´åº¦
            data: è¦ç¼“å­˜çš„æ•°æ®
            ttl_days: ç¼“å­˜ç”Ÿå­˜æ—¶é—´ (å¤©)
            timestamp: è‡ªå®šä¹‰æ—¶é—´æˆ³

        Returns:
            True å¦‚æœå†™å…¥æˆåŠŸï¼ŒFalse å¦åˆ™
        """
        self._cache_stats["writes"] += 1

        try:
            # éªŒè¯æ•°æ®
            is_invalid_data = data is None or not isinstance(data, dict)
            if is_invalid_data:
                logger.warning(
                    "æ— æ•ˆçš„ç¼“å­˜æ•°æ®",
                    symbol=symbol,
                    data_type=data_type,
                )
                return False

            # å¢åŠ å…ƒæ•°æ®
            enriched_data = {
                **data,
                "_cached_at": datetime.now(timezone.utc).isoformat(),
                "_ttl_days": ttl_days,
                "_cache_version": "2.0",  # å‡çº§åˆ°ä¸‰çº§ç¼“å­˜ç‰ˆæœ¬
                "_source": "market_data",
            }

            # å‡†å¤‡ç¼“å­˜æ•°æ®æ ¼å¼
            cache_data = {
                "data": data,
                "source": "cache",
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }

            cache_key = self.get_cache_key(symbol, data_type, timeframe)

            # L1: å†…å­˜ç¼“å­˜ (åŒæ­¥å†™å…¥ï¼Œæœ€é«˜ä¼˜å…ˆçº§)
            self._add_to_memory_cache(symbol, data_type, timeframe, cache_data)

            # L2: Redisç¼“å­˜ (å¼‚æ­¥å†™å…¥ï¼Œä¸é˜»å¡å“åº”)
            if self._redis_available and self.redis_cache:
                redis_ttl = ttl_days * 24 * 3600  # è½¬æ¢ä¸ºç§’
                asyncio.create_task(self.redis_cache.set(cache_key, cache_data, ttl=redis_ttl))

            # L3: TDengineç¼“å­˜ (å¼‚æ­¥å†™å…¥ï¼ŒæŒä¹…åŒ–å­˜å‚¨)
            asyncio.create_task(
                self._write_to_tdengine(
                    symbol=symbol,
                    data_type=data_type,
                    timeframe=timeframe,
                    data=enriched_data,
                    timestamp=timestamp,
                )
            )

            logger.debug(
                "âœ… ä¸‰çº§ç¼“å­˜å†™å…¥å®Œæˆ",
                symbol=symbol,
                data_type=data_type,
                ttl_days=ttl_days,
            )
            return True

        except Exception as e:
            logger.error(
                "âŒ ä¸‰çº§ç¼“å­˜å†™å…¥å¼‚å¸¸",
                symbol=symbol,
                data_type=data_type,
                error=str(e),
            )
            return False

            # å¢åŠ å…ƒæ•°æ®
            enriched_data = {
                **data,
                "_cached_at": datetime.now(timezone.utc).isoformat(),
                "_ttl_days": ttl_days,
                "_cache_version": "1.0",
                "_source": "market_data",
            }

            # é¦–å…ˆå†™å…¥å†…å­˜ç¼“å­˜ (æœ€é«˜ä¼˜å…ˆçº§)
            memory_data = {
                "data": data,
                "source": "memory",
                "timestamp": datetime.now(timezone.utc).isoformat(),
            }
            self._add_to_memory_cache(symbol, data_type, timeframe, memory_data)

            # å¹¶è¡Œå†™å…¥ TDengine (æŒä¹…åŒ–å­˜å‚¨)
            td_result = self._write_to_tdengine(
                symbol=symbol,
                data_type=data_type,
                timeframe=timeframe,
                data=enriched_data,
                timestamp=timestamp,
            )

            if td_result:
                logger.debug(
                    "âœ… æ•°æ®å·²ç¼“å­˜(å†…å­˜+TDengine)",
                    symbol=symbol,
                    data_type=data_type,
                    ttl_days=ttl_days,
                )
                return True
            else:
                logger.warning(
                    "âš ï¸ TDengineå†™å…¥å¤±è´¥ï¼Œä½†å†…å­˜ç¼“å­˜å·²æ›´æ–°",
                    symbol=symbol,
                    data_type=data_type,
                )
                return True  # å†…å­˜ç¼“å­˜æˆåŠŸå°±è®¤ä¸ºéƒ¨åˆ†æˆåŠŸ

        except Exception as e:
            logger.error(
                "âŒ ç¼“å­˜å†™å…¥å¼‚å¸¸",
                symbol=symbol,
                data_type=data_type,
                error=str(e),
            )
            return False

    async def invalidate_cache(
        self,
        symbol: Optional[str] = None,
        data_type: Optional[str] = None,
    ) -> int:
        """
        æ¸…é™¤ä¸‰çº§ç¼“å­˜ä¸­çš„ç‰¹å®šæ•°æ®

        Args:
            symbol: è‚¡ç¥¨ä»£ç  (å¯é€‰ï¼Œå¦‚æœçœç•¥åˆ™æ¸…é™¤æ‰€æœ‰ symbol)
            data_type: æ•°æ®ç±»å‹ (å¯é€‰ï¼Œå¦‚æœçœç•¥åˆ™æ¸…é™¤æ‰€æœ‰ data_type)

        Returns:
            åˆ é™¤çš„è®°å½•æ•°
        """
        total_deleted = 0

        try:
            with self._cache_lock:
                # L1: æ¸…ç†å†…å­˜ç¼“å­˜
                if symbol and data_type:
                    # æ¸…é™¤ç‰¹å®šç¬¦å·+æ•°æ®ç±»å‹çš„ç¼“å­˜
                    cache_key = self.get_cache_key(symbol, data_type)

                    if cache_key in self._memory_cache:
                        del self._memory_cache[cache_key]
                        total_deleted += 1

                    if cache_key in self._cache_ttl:
                        del self._cache_ttl[cache_key]

                    if cache_key in self._access_patterns:
                        del self._access_patterns[cache_key]

                    logger.info("ğŸ—‘ï¸ æ¸…é™¤L1å†…å­˜ç¼“å­˜", symbol=symbol, data_type=data_type)

                elif symbol:
                    # æ¸…é™¤ç‰¹å®šç¬¦å·çš„æ‰€æœ‰ç¼“å­˜
                    keys_to_delete = [key for key in self._memory_cache.keys() if key.startswith(symbol)]
                    for key in keys_to_delete:
                        del self._memory_cache[key]
                        total_deleted += 1

                        if key in self._cache_ttl:
                            del self._cache_ttl[key]
                        if key in self._access_patterns:
                            del self._access_patterns[key]

                    logger.info(
                        "ğŸ—‘ï¸ æ¸…é™¤ç¬¦å·æ‰€æœ‰L1å†…å­˜ç¼“å­˜",
                        symbol=symbol,
                        count=len(keys_to_delete),
                    )

                else:
                    # æ¸…é™¤æ‰€æœ‰å†…å­˜ç¼“å­˜
                    total_deleted = self.clear_memory_cache()
                    logger.warning("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰L1å†…å­˜ç¼“å­˜")

            # L2: æ¸…ç†Redisç¼“å­˜
            if self._redis_available and self.redis_cache:
                try:
                    if symbol and data_type:
                        cache_key = self.get_cache_key(symbol, data_type)
                        await self.redis_cache.delete(cache_key)
                        logger.info("ğŸ—‘ï¸ æ¸…é™¤L2 Redisç¼“å­˜", symbol=symbol, data_type=data_type)
                    elif symbol:
                        # åˆ é™¤æ‰€æœ‰ä»¥symbolå¼€å¤´çš„ç¼“å­˜
                        pattern = f"{symbol}:*"
                        redis_deleted = await self.redis_cache.delete_pattern(pattern)
                        total_deleted += redis_deleted
                        logger.info("ğŸ—‘ï¸ æ¸…é™¤ç¬¦å·æ‰€æœ‰L2 Redisç¼“å­˜", symbol=symbol, count=redis_deleted)
                    else:
                        await self.redis_cache.clear()
                        logger.warning("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰L2 Redisç¼“å­˜")
                except Exception as e:
                    logger.warning("L2 Redisç¼“å­˜æ¸…ç†å¤±è´¥", error=str(e))

            # L3: æ¸…ç†TDengineç¼“å­˜ï¼ˆå¼‚æ­¥ï¼‰
            if self.tdengine is not None:
                try:
                    if symbol and data_type:
                        # å¼‚æ­¥æ¸…ç†TDengineç‰¹å®šç¼“å­˜
                        asyncio.create_task(self._async_tdengine_clear(symbol, data_type))
                    elif symbol:
                        asyncio.create_task(self._async_tdengine_clear_symbol(symbol))
                    else:
                        asyncio.create_task(self._async_tdengine_clear_all())
                except Exception as e:
                    logger.warning("L3 TDengineç¼“å­˜æ¸…ç†ä»»åŠ¡åˆ›å»ºå¤±è´¥", error=str(e))

            logger.info(
                "âœ… ä¸‰çº§ç¼“å­˜æ¸…é™¤å®Œæˆ",
                symbol=symbol,
                data_type=data_type,
                total_deleted=total_deleted,
            )
            return total_deleted

        except Exception as e:
            logger.error("âŒ ç¼“å­˜æ¸…é™¤å¤±è´¥", error=str(e))
            return total_deleted

    # ==================== æ‰¹é‡æ“ä½œ ====================

    def batch_read(self, queries: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ‰¹é‡è¯»å–ç¼“å­˜ (ä¼˜åŒ–ç‰ˆ) - æ˜¾è‘—æé«˜æ€§èƒ½

        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
                {
                    "symbol": "000001",
                    "data_type": "fund_flow",
                    "timeframe": "1d",  # å¯é€‰
                    "days": 1            # å¯é€‰
                }

        Returns:
            {
                "000001:fund_flow": {...},
                "000858:etf": {...},
                ...
            }
        """
        self._cache_stats["batch_operations"] += 1
        start_time = time.time()
        results = {}
        success_count = 0

        try:
            # ä¼˜åŒ–ï¼šå¹¶å‘è¯»å–å†…å­˜ç¼“å­˜ï¼Œå…ˆå¤„ç†æœ€å¯èƒ½å‘½ä¸­çš„æ•°æ®
            _ = []  # noqa: F841 - Placeholder for memory_cache_futures (to be implemented)
            _ = []  # noqa: F841 - Placeholder for tdengine_cache_futures (to be implemented)

            # é¢„è¿‡æ»¤ï¼šé¿å…é‡å¤æŸ¥è¯¢
            unique_queries = []
            seen_keys = set()

            for query in queries:
                symbol = query.get("symbol")
                data_type = query.get("data_type")

                if not symbol or not data_type:
                    continue

                query_key = f"{symbol}:{data_type}:{query.get('timeframe', '1d')}"
                if query_key not in seen_keys:
                    seen_keys.add(query_key)
                    unique_queries.append(query)

            # æ‰¹é‡å†…å­˜ç¼“å­˜æŸ¥è¯¢
            with self._cache_lock:
                for query in unique_queries:
                    symbol = query.get("symbol")
                    data_type = query.get("data_type")
                    timeframe = query.get("timeframe", "1d")

                    # Type guards for MyPy
                    if not isinstance(symbol, str) or not isinstance(data_type, str) or not isinstance(timeframe, str):
                        continue

                    cache_key = self.get_cache_key(symbol, data_type, timeframe)

                    if cache_key in self._memory_cache:
                        # å†…å­˜ç¼“å­˜å‘½ä¸­
                        if not self._is_cache_expired(cache_key):
                            results[cache_key] = self._memory_cache[cache_key]
                            self._cache_stats["hits"] += 1
                            success_count += 1
                            self._record_access_pattern(symbol, data_type)
                        else:
                            # è¿‡æœŸäº†ï¼Œéœ€è¦åˆ é™¤
                            del self._memory_cache[cache_key]
                            del self._cache_ttl[cache_key]
                            if cache_key in self._access_patterns:
                                del self._access_patterns[cache_key]

            # å¯¹äºæœªå‘½ä¸­çš„æŸ¥è¯¢ï¼Œæ‰¹é‡TDengineæŸ¥è¯¢
            remaining_queries = []
            for query in unique_queries:
                symbol = query.get("symbol")
                data_type = query.get("data_type")
                timeframe = query.get("timeframe", "1d")

                # Type guards for MyPy
                if not isinstance(symbol, str) or not isinstance(data_type, str) or not isinstance(timeframe, str):
                    continue

                cache_key = self.get_cache_key(symbol, data_type, timeframe)
                if cache_key not in results:
                    remaining_queries.append(query)

            if remaining_queries:
                # æ‰¹é‡TDengineæŸ¥è¯¢
                for query in remaining_queries:
                    symbol = query.get("symbol")
                    data_type = query.get("data_type")
                    timeframe = query.get("timeframe", "1d")

                    # Type guards for MyPy
                    if not isinstance(symbol, str) or not isinstance(data_type, str) or not isinstance(timeframe, str):
                        continue

                    cache_key = self.get_cache_key(symbol, data_type, timeframe)

                    try:
                        if self.tdengine is not None:
                            cache_data = self.tdengine.read_cache(
                                symbol=symbol,
                                data_type=data_type,
                                timeframe=timeframe,
                                days=query.get("days", 1),
                            )
                        else:
                            cache_data = None

                        if cache_data:
                            enriched_data = {
                                "data": cache_data,
                                "source": "cache",
                                "timestamp": datetime.now(timezone.utc).isoformat(),
                            }
                            results[cache_key] = enriched_data
                            self._cache_stats["hits"] += 1
                            success_count += 1

                            # å›å¡«å†…å­˜ç¼“å­˜
                            self._add_to_memory_cache(symbol, data_type, timeframe, enriched_data)
                        else:
                            results[cache_key] = None
                            self._cache_stats["misses"] += 1

                    except Exception as e:
                        logger.warning("æ‰¹é‡è¯»å–å•é¡¹å¤±è´¥ {symbol}:{data_type}", error=str(e))
                        results[cache_key] = None
                        self._cache_stats["misses"] += 1

            response_time = time.time() - start_time
            self._update_performance_stats(response_time, success_count > 0)

            logger.info(
                "âœ… æ‰¹é‡è¯»å–å®Œæˆ",
                total=len(unique_queries),
                success=success_count,
                unique_queries=len(unique_queries),
                response_time=response_time,
                hit_rate=success_count / max(len(unique_queries), 1),
            )
            return results

        except Exception as e:
            logger.error("âŒ æ‰¹é‡è¯»å–å¤±è´¥", error=str(e))
            return results

    def batch_write(self, records: List[Dict[str, Any]], ttl_days: int = 7) -> int:
        """
        æ‰¹é‡å†™å…¥ç¼“å­˜

        Args:
            records: è®°å½•åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
                {
                    "symbol": "000001",
                    "data_type": "fund_flow",
                    "timeframe": "1d",
                    "data": {...}
                }
            ttl_days: æ‰¹é‡ TTL

        Returns:
            æˆåŠŸå†™å…¥çš„è®°å½•æ•°
        """
        count = 0

        try:
            for record in records:
                symbol = record.get("symbol")
                data_type = record.get("data_type")
                timeframe = record.get("timeframe", "1d")
                data = record.get("data", {})

                if not symbol or not data_type:
                    logger.warning("è®°å½•ç¼ºå°‘å¿…è¦å­—æ®µ", record=record)
                    continue

                if self.write_to_cache(
                    symbol=symbol,
                    data_type=data_type,
                    timeframe=timeframe,
                    data=data,
                    ttl_days=ttl_days,
                ):
                    count += 1

            logger.info(
                "âœ… æ‰¹é‡å†™å…¥å®Œæˆ",
                total=len(records),
                success=count,
            )
            return count

        except Exception as e:
            logger.error("âŒ æ‰¹é‡å†™å…¥å¤±è´¥", error=str(e))
            return count

    # ==================== ç¼“å­˜éªŒè¯ä¸æ£€æŸ¥ ====================

    async def is_cache_valid(self, symbol: str, data_type: str, max_age_days: int = 7) -> bool:
        """
        æ£€æŸ¥ä¸‰çº§ç¼“å­˜çš„æœ‰æ•ˆæ€§

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            data_type: æ•°æ®ç±»å‹
            max_age_days: æœ€å¤§ç¼“å­˜å¹´é¾„ (å¤©)

        Returns:
            True å¦‚æœç¼“å­˜æœ‰æ•ˆä¸”æœªè¿‡æœŸï¼ŒFalse å¦åˆ™
        """
        try:
            # ä¼˜å…ˆæ£€æŸ¥L1å†…å­˜ç¼“å­˜
            cache_data = self._get_from_memory_cache(symbol, data_type, "1d")
            if cache_data:
                # æ£€æŸ¥æ—¶é—´æˆ³
                if "_cached_at" in cache_data.get("data", {}):
                    cached_at_str = cache_data["data"]["_cached_at"]
                    cached_at = datetime.fromisoformat(cached_at_str)
                    if cached_at.tzinfo is None:
                        cached_at = cached_at.replace(tzinfo=timezone.utc)
                    age = datetime.now(timezone.utc) - cached_at
                    is_valid = age <= timedelta(days=max_age_days)

                    logger.debug(
                        "L1ç¼“å­˜æœ‰æ•ˆæ€§æ£€æŸ¥",
                        symbol=symbol,
                        data_type=data_type,
                        age_days=age.days,
                        valid=is_valid,
                    )
                    return is_valid
                return True

            # æ£€æŸ¥L2 Redisç¼“å­˜
            if self._redis_available and self.redis_cache:
                cache_key = self.get_cache_key(symbol, data_type, "1d")
                redis_result, found, _ = await self.redis_cache.get(cache_key)
                if found and redis_result:
                    if "_cached_at" in redis_result.get("data", {}):
                        cached_at_str = redis_result["data"]["_cached_at"]
                        cached_at = datetime.fromisoformat(cached_at_str)
                        if cached_at.tzinfo is None:
                            cached_at = cached_at.replace(tzinfo=timezone.utc)
                        age = datetime.now(timezone.utc) - cached_at
                        is_valid = age <= timedelta(days=max_age_days)

                        logger.debug(
                            "L2ç¼“å­˜æœ‰æ•ˆæ€§æ£€æŸ¥",
                            symbol=symbol,
                            data_type=data_type,
                            age_days=age.days,
                            valid=is_valid,
                        )
                        return is_valid
                    return True

            # æ£€æŸ¥L3 TDengineç¼“å­˜
            if self.tdengine is not None:
                cache_data = await self._write_to_tdengine(
                    symbol=symbol,
                    data_type=data_type,
                    timeframe="1d",
                    data={},  # è¯»å–æ¨¡å¼
                    timestamp=None,
                )
                if cache_data and "_cached_at" in cache_data.get("data", {}):
                    cached_at_str = cache_data["data"]["_cached_at"]
                    cached_at = datetime.fromisoformat(cached_at_str)
                    if cached_at.tzinfo is None:
                        cached_at = cached_at.replace(tzinfo=timezone.utc)
                    age = datetime.now(timezone.utc) - cached_at
                    is_valid = age <= timedelta(days=max_age_days)

                    logger.debug(
                        "L3ç¼“å­˜æœ‰æ•ˆæ€§æ£€æŸ¥",
                        symbol=symbol,
                        data_type=data_type,
                        age_days=age.days,
                        valid=is_valid,
                    )
                    return is_valid

            return False

        except Exception as e:
            logger.error(
                "âŒ ä¸‰çº§ç¼“å­˜æœ‰æ•ˆæ€§æ£€æŸ¥å¤±è´¥",
                symbol=symbol,
                data_type=data_type,
                error=str(e),
            )
            return False

            # æ£€æŸ¥æ—¶é—´æˆ³
            if "_cached_at" in cache_data.get("data", {}):
                cached_at_str = cache_data["data"]["_cached_at"]
                cached_at = datetime.fromisoformat(cached_at_str)
                age = datetime.now(timezone.utc) - cached_at
                is_valid = age <= timedelta(days=max_age_days)

                logger.debug(
                    "ç¼“å­˜æœ‰æ•ˆæ€§æ£€æŸ¥",
                    symbol=symbol,
                    data_type=data_type,
                    age_days=age.days,
                    valid=is_valid,
                )
                return is_valid

            return True

        except Exception as e:
            logger.error(
                "âŒ ç¼“å­˜æœ‰æ•ˆæ€§æ£€æŸ¥å¤±è´¥",
                symbol=symbol,
                error=str(e),
            )
            return False

    def get_cache_key(self, symbol: str, data_type: str, timeframe: str = "1d") -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            data_type: æ•°æ®ç±»å‹
            timeframe: æ—¶é—´ç»´åº¦

        Returns:
            ç¼“å­˜é”®å­—ç¬¦ä¸²
        """
        return f"{data_type}:{symbol}:{timeframe}".lower()

    # ==================== ç»Ÿè®¡ä¸ç›‘æ§ ====================

    def get_cache_stats(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯ (å¢å¼ºç‰ˆ)

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        hit_rate = self._calculate_hit_rate()
        avg_response_time = self._cache_stats["total_response_time"] / max(self._cache_stats["reads"], 1)

        stats = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "total_reads": self._cache_stats["reads"],
            "total_writes": self._cache_stats["writes"],
            "cache_hits": self._cache_stats["hits"],
            "cache_misses": self._cache_stats["misses"],
            "evictions": self._cache_stats["evictions"],
            "batch_operations": self._cache_stats["batch_operations"],
            "hit_rate": hit_rate,
            "hit_rate_percent": f"{hit_rate * 100:.1f}%",
            "avg_response_time_ms": round(avg_response_time * 1000, 2),
            "memory_cache_stats": self.get_memory_cache_stats(),
        }

        # æ·»åŠ å“åº”æ—¶é—´åˆ†å¸ƒç»Ÿè®¡
        if "response_time_distribution" in self._cache_stats:
            stats["response_time_distribution"] = self._cache_stats["response_time_distribution"]

        # ä» TDengine è·å–é¢å¤–ç»Ÿè®¡
        try:
            if self.tdengine is not None:
                tdengine_stats = self.tdengine.get_cache_stats()
                if tdengine_stats:
                    stats["tdengine_stats"] = tdengine_stats
        except Exception as e:
            logger.warning("æ— æ³•è·å– TDengine ç»Ÿè®¡", error=str(e))

        return stats

    def reset_stats(self) -> None:
        """é‡ç½®ç»Ÿè®¡è®¡æ•°å™¨"""
        self._cache_stats = {
            "hits": 0,
            "misses": 0,
            "reads": 0,
            "writes": 0,
        }
        logger.info("âœ… ç»Ÿè®¡è®¡æ•°å™¨å·²é‡ç½®")

    def _calculate_hit_rate(self) -> float:
        """è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡"""
        total_reads: int = self._cache_stats["reads"]
        if total_reads == 0:
            return 0.0
        hits: int = self._cache_stats["hits"]
        return float(hits) / float(total_reads)

    # ==================== å†…å­˜ç¼“å­˜å±‚ (æ›¿ä»£Redis) ====================

    def _get_from_memory_cache(self, symbol: str, data_type: str, timeframe: Optional[str]) -> Optional[Dict[str, Any]]:
        """ä»å†…å­˜ç¼“å­˜è¯»å–æ•°æ®"""
        cache_key = self.get_cache_key(symbol, data_type, timeframe or "1d")

        with self._cache_lock:
            if cache_key in self._memory_cache:
                # æ£€æŸ¥TTL
                if self._is_cache_expired(cache_key):
                    del self._memory_cache[cache_key]
                    del self._cache_ttl[cache_key]
                    return None

                # æ›´æ–°è®¿é—®ç»Ÿè®¡
                self._access_patterns[cache_key].append(datetime.now(timezone.utc))
                result: Optional[Dict[str, Any]] = self._memory_cache[cache_key]
                return result

        return None

    def _add_to_memory_cache(
        self,
        symbol: str,
        data_type: str,
        timeframe: str,
        data: Dict[str, Any],
    ) -> None:
        """å†™å…¥æ•°æ®åˆ°å†…å­˜ç¼“å­˜"""
        cache_key = self.get_cache_key(symbol, data_type, timeframe)

        with self._cache_lock:
            # æ£€æŸ¥ç¼“å­˜å¤§å°é™åˆ¶
            if len(self._memory_cache) >= self._max_memory_entries:
                self._evict_memory_cache()

            # è®¡ç®—TTL
            ttl_seconds = self._get_tiered_ttl(data_type)

            self._memory_cache[cache_key] = data
            self._cache_ttl[cache_key] = datetime.now(timezone.utc) + timedelta(seconds=ttl_seconds)
            self._access_patterns[cache_key].append(datetime.now(timezone.utc))

    def _is_cache_expired(self, cache_key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ"""
        if cache_key not in self._cache_ttl:
            return True

        return datetime.now(timezone.utc) > self._cache_ttl[cache_key]

    def _get_tiered_ttl(self, data_type: str) -> int:
        """è·å–åˆ†å±‚TTL"""
        return self._tiered_ttl.get(data_type, self._tiered_ttl["default"])

    def _evict_memory_cache(self) -> None:
        """å†…å­˜ç¼“å­˜æ·˜æ±°ç­–ç•¥ (LRU + åŸºäºè®¿é—®é¢‘ç‡)"""
        if not self._memory_cache:
            return

        # ç®€å•LRUç­–ç•¥ï¼šåˆ é™¤è®¿é—®é¢‘ç‡æœ€ä½çš„æ¡ç›®
        lru_key = None
        min_access = float("inf")

        for key, access_times in self._access_patterns.items():
            access_freq = len(access_times)
            if access_freq < min_access:
                min_access = access_freq
                lru_key = key

        if lru_key and lru_key in self._memory_cache:
            del self._memory_cache[lru_key]
            del self._cache_ttl[lru_key]
            del self._access_patterns[lru_key]
            self._cache_stats["evictions"] += 1

    def _record_access_pattern(self, symbol: str, data_type: str) -> None:
        """è®°å½•è®¿é—®æ¨¡å¼"""
        cache_key = self.get_cache_key(symbol, data_type)
        with self._cache_lock:
            self._access_patterns[cache_key].append(datetime.now(timezone.utc))

    async def _write_to_tdengine(
        self,
        symbol: str,
        data_type: str,
        timeframe: str,
        data: Dict[str, Any],
        timestamp: Optional[datetime] = None,
    ) -> bool:
        """å¼‚æ­¥å†™å…¥TDengine"""
        try:
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡ŒTDengineå†™å…¥ï¼Œé¿å…é˜»å¡
            tdengine = self.tdengine
            if tdengine is None:
                return False

            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,
                lambda: tdengine.write_cache(
                    symbol=symbol,
                    data_type=data_type,
                    timeframe=timeframe,
                    data=data,
                    timestamp=timestamp,
                ),
            )
            return result
        except Exception:
            logger.warning("TDengineå¼‚æ­¥å†™å…¥å¤±è´¥: %(e)s")
            return False

    def _update_performance_stats(self, response_time: float, hit: bool) -> None:
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        self._cache_stats["total_response_time"] += response_time

        # è®°å½•å“åº”æ—¶é—´åˆ†å¸ƒ
        if hit:
            cache_level = "memory" if response_time < 0.001 else "tdengine"
        else:
            cache_level = "miss"

        if "response_time_distribution" not in self._cache_stats:
            self._cache_stats["response_time_distribution"] = {}

        response_time_dist: Dict[str, int] = self._cache_stats["response_time_distribution"]
        response_time_dist[cache_level] = response_time_dist.get(cache_level, 0) + 1

    def get_memory_cache_stats(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ç¼“å­˜ç»Ÿè®¡"""
        with self._cache_lock:
            total_entries = len(self._memory_cache)
            total_size_mb = sum(len(str(data)) for data in self._memory_cache.values()) / (1024 * 1024)  # ä¼°ç®—å¤§å°

            # è®¡ç®—å„æ•°æ®ç±»å‹çš„åˆ†å¸ƒ
            type_distribution: defaultdict[str, int] = defaultdict(int)
            for cache_key in self._memory_cache.keys():
                parts = cache_key.split(":")
                if len(parts) >= 2:
                    data_type = parts[0]
                    type_distribution[data_type] += 1

            return {
                "total_entries": total_entries,
                "max_entries": self._max_memory_entries,
                "usage_percentage": (total_entries / self._max_memory_entries) * 100,
                "estimated_size_mb": round(total_size_mb, 2),
                "type_distribution": dict(type_distribution),
                "evictions": self._cache_stats["evictions"],
                "default_ttl_seconds": self._default_ttl,
                "tiered_ttl": self._tiered_ttl,
            }

    def clear_memory_cache(self) -> int:
        """æ¸…ç©ºå†…å­˜ç¼“å­˜"""
        with self._cache_lock:
            count = len(self._memory_cache)
            self._memory_cache.clear()
            self._cache_ttl.clear()
            self._access_patterns.clear()
            return count

    # ==================== ä¸‰çº§ç¼“å­˜è¾…åŠ©æ–¹æ³• ====================

    async def _async_tdengine_clear(self, symbol: str, data_type: str) -> None:
        """å¼‚æ­¥æ¸…ç†TDengineç‰¹å®šç¼“å­˜"""
        try:
            if self.tdengine is not None:
                # è¿™é‡Œéœ€è¦å®ç°TDengineçš„ç²¾ç¡®åˆ é™¤æ–¹æ³•
                # æš‚æ—¶ä½¿ç”¨clear_expired_cacheä½œä¸ºæ›¿ä»£
                await asyncio.get_event_loop().run_in_executor(None, lambda: self.tdengine.clear_expired_cache(days=0))
                logger.info("ğŸ—‘ï¸ L3 TDengineç¼“å­˜æ¸…ç†å®Œæˆ", symbol=symbol, data_type=data_type)
        except Exception as e:
            logger.warning("L3 TDengineç¼“å­˜æ¸…ç†å¤±è´¥", symbol=symbol, data_type=data_type, error=str(e))

    async def _async_tdengine_clear_symbol(self, symbol: str) -> None:
        """å¼‚æ­¥æ¸…ç†TDengineç‰¹å®šç¬¦å·çš„æ‰€æœ‰ç¼“å­˜"""
        try:
            if self.tdengine is not None:
                await asyncio.get_event_loop().run_in_executor(None, lambda: self.tdengine.clear_expired_cache(days=0))
                logger.info("ğŸ—‘ï¸ L3 TDengineç¬¦å·ç¼“å­˜æ¸…ç†å®Œæˆ", symbol=symbol)
        except Exception as e:
            logger.warning("L3 TDengineç¬¦å·ç¼“å­˜æ¸…ç†å¤±è´¥", symbol=symbol, error=str(e))

    async def _async_tdengine_clear_all(self) -> None:
        """å¼‚æ­¥æ¸…ç†æ‰€æœ‰TDengineç¼“å­˜"""
        try:
            if self.tdengine is not None:
                await asyncio.get_event_loop().run_in_executor(None, lambda: self.tdengine.clear_expired_cache(days=0))
                logger.warning("ğŸ—‘ï¸ L3 TDengineå…¨éƒ¨ç¼“å­˜æ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.warning("L3 TDengineå…¨éƒ¨ç¼“å­˜æ¸…ç†å¤±è´¥", error=str(e))

    def optimize_memory_cache(self) -> Dict[str, Any]:
        """ä¼˜åŒ–å†…å­˜ç¼“å­˜"""
        with self._cache_lock:
            # æ¸…ç†è¿‡æœŸæ¡ç›®
            expired_count = 0
            now = datetime.now(timezone.utc)

            expired_keys = [key for key, expire_time in self._cache_ttl.items() if now > expire_time]

            for key in expired_keys:
                if key in self._memory_cache:
                    del self._memory_cache[key]
                del self._cache_ttl[key]
                if key in self._access_patterns:
                    del self._access_patterns[key]
                expired_count += 1

            # è®°å½•ä¼˜åŒ–ç»“æœ
            stats_before = self.get_memory_cache_stats()

            return {
                "expired_entries_removed": expired_count,
                "entries_after_cleanup": len(self._memory_cache),
                "cache_usage_after": stats_before["usage_percentage"],
                "memory_freed_mb": 0,  # ç®€åŒ–å®ç°
            }

    # ==================== ç”Ÿå‘½å‘¨æœŸ ====================

    def health_check(self) -> Dict[str, Any]:
        """
        å¥åº·æ£€æŸ¥ (å¢å¼ºç‰ˆ)

        Returns:
            å¥åº·çŠ¶æ€å­—å…¸
        """
        health_status: Dict[str, Any] = {
            "overall_healthy": True,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "components": {},
            "performance_metrics": {},
            "issues": [],
        }

        try:
            # æ£€æŸ¥ TDengine è¿æ¥
            tdengine_healthy = self.tdengine.health_check() if self.tdengine is not None else False
            components: Dict[str, Any] = health_status["components"]
            components["tdengine"] = {
                "healthy": tdengine_healthy,
                "status": "OK" if tdengine_healthy else "ERROR",
            }

            if not tdengine_healthy:
                health_status["overall_healthy"] = False
                health_status["issues"].append("TDengine connection failed")

            # æ£€æŸ¥å†…å­˜ç¼“å­˜
            memory_stats = self.get_memory_cache_stats()
            memory_healthy = (
                memory_stats["usage_percentage"] < 95 and len(self._memory_cache) < self._max_memory_entries
            )

            components["memory_cache"] = {
                "healthy": memory_healthy,
                "status": "OK" if memory_healthy else "WARNING",
                "usage_percentage": memory_stats["usage_percentage"],
                "total_entries": memory_stats["total_entries"],
            }

            if not memory_healthy:
                issues: List[str] = health_status["issues"]
                issues.append("Memory cache usage high")

            # æ€§èƒ½æŒ‡æ ‡
            hit_rate = self._calculate_hit_rate()
            avg_response_time = self._cache_stats["total_response_time"] / max(self._cache_stats["reads"], 1)

            performance_healthy = hit_rate > 0.5 and avg_response_time < 1.0  # å‘½ä¸­ç‡åº”è¯¥å¤§äº50%  # å¹³å‡å“åº”æ—¶é—´å°äº1ç§’

            performance_metrics: Dict[str, Any] = {
                "hit_rate": hit_rate,
                "avg_response_time_ms": round(avg_response_time * 1000, 2),
                "performance_healthy": performance_healthy,
            }
            health_status["performance_metrics"] = performance_metrics

            if not performance_healthy:
                health_status["overall_healthy"] = False
                issues = health_status["issues"]
                if hit_rate < 0.5:
                    issues.append("Cache hit rate too low")
                if avg_response_time > 1.0:
                    issues.append("Response time too slow")

            logger.info(
                "ğŸ” ç¼“å­˜ç³»ç»Ÿå¥åº·æ£€æŸ¥å®Œæˆ",
                overall_healthy=health_status["overall_healthy"],
                issues=len(health_status.get("issues", [])),
            )

            return health_status

        except Exception as e:
            logger.error("âŒ ç¼“å­˜ç³»ç»Ÿå¥åº·æ£€æŸ¥å¤±è´¥", error=str(e))
            health_status["overall_healthy"] = False
            error_issues: List[str] = health_status["issues"]
            error_issues.append(f"Health check error: {str(e)}")
            return health_status

    def close(self) -> None:
        """å…³é—­ç¼“å­˜ç®¡ç†å™¨"""
        try:
            if self.tdengine is not None:
                self.tdengine.close()
            logger.info("âœ… ç¼“å­˜ç®¡ç†å™¨å·²å…³é—­")
        except Exception as e:
            logger.warning("å…³é—­ç¼“å­˜ç®¡ç†å™¨æ—¶å‡ºé”™", error=str(e))


# ==================== å…¨å±€å•ä¾‹ç®¡ç† ====================

_cache_manager: Optional[CacheManager] = None


async def get_cache_manager_async(
    tdengine_manager: Optional[TDengineManager] = None,
    redis_cache: Optional[MultiLevelCache] = None,
) -> CacheManager:
    """
    è·å–å¼‚æ­¥ç¼“å­˜ç®¡ç†å™¨å•ä¾‹ (æ”¯æŒRedisæ³¨å…¥)

    Args:
        tdengine_manager: TDengineManager å®ä¾‹
        redis_cache: Rediså¤šçº§ç¼“å­˜æœåŠ¡å®ä¾‹

    Returns:
        CacheManager å•ä¾‹å®ä¾‹
    """
    global _cache_manager

    if _cache_manager is None:
        _cache_manager = CacheManager(tdengine_manager, redis_cache)

        # å¦‚æœæä¾›äº†Redisç¼“å­˜ï¼Œåˆå§‹åŒ–è¿æ¥
        if redis_cache and REDIS_CACHE_AVAILABLE:
            try:
                # Redisç¼“å­˜å·²åœ¨å¤–éƒ¨åˆå§‹åŒ–ï¼Œè¿™é‡Œåªéœ€è¦éªŒè¯
                if not hasattr(redis_cache, "_redis_connected") or not redis_cache._redis_connected:
                    await redis_cache.initialize()
                _cache_manager._redis_available = True
                logger.info("âœ… Redisç¼“å­˜æœåŠ¡å·²æ³¨å…¥åˆ°ç¼“å­˜ç®¡ç†å™¨")
            except Exception as e:
                logger.warning("âš ï¸ Redisç¼“å­˜åˆå§‹åŒ–å¤±è´¥ï¼Œå°†é™çº§ä¸ºL1+L3æ¨¡å¼", error=str(e))
                _cache_manager._redis_available = False

        # æ‰§è¡Œå¥åº·æ£€æŸ¥
        try:
            health = _cache_manager.health_check()
            if not health.get("overall_healthy"):
                logger.warning("âš ï¸ ç¼“å­˜ç®¡ç†å™¨å¥åº·æ£€æŸ¥å¤±è´¥", issues=health.get("issues", []))
        except Exception as e:
            logger.warning("âš ï¸ ç¼“å­˜ç®¡ç†å™¨å¥åº·æ£€æŸ¥å¼‚å¸¸", error=str(e))

    return _cache_manager


def get_cache_manager(
    tdengine_manager: Optional[TDengineManager] = None,
) -> CacheManager:
    """
    è·å–ç¼“å­˜ç®¡ç†å™¨å•ä¾‹ (å‘åå…¼å®¹)

    æ³¨æ„: æ­¤æ–¹æ³•ä¸æ”¯æŒRedisæ³¨å…¥ã€‚å¦‚éœ€Redisæ”¯æŒï¼Œè¯·ä½¿ç”¨ get_cache_manager_async()

    Args:
        tdengine_manager: TDengineManager å®ä¾‹

    Returns:
        CacheManager å•ä¾‹å®ä¾‹
    """
    global _cache_manager

    if _cache_manager is None:
        _cache_manager = CacheManager(tdengine_manager)
        logger.warning("âš ï¸ ä½¿ç”¨åŒæ­¥ç¼“å­˜ç®¡ç†å™¨ï¼ŒRedisåŠŸèƒ½ä¸å¯ç”¨ã€‚å¦‚éœ€Redisæ”¯æŒï¼Œè¯·ä½¿ç”¨ get_cache_manager_async()")

    return _cache_manager


def reset_cache_manager() -> None:
    """é‡ç½®ç¼“å­˜ç®¡ç†å™¨ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    global _cache_manager
    if _cache_manager:
        _cache_manager.close()
    _cache_manager = None
