"""
æ•°æ®åº“æ€§èƒ½ç›‘æ§å’Œæ…¢æŸ¥è¯¢æ£€æµ‹
Database Performance Monitoring - Slow query detection and analysis

Task 14.3: æ•°æ®åº“æ€§èƒ½ä¼˜åŒ– - Performance Monitoring

åŠŸèƒ½ç‰¹æ€§:
- æŸ¥è¯¢æ€§èƒ½ç›‘æ§ï¼ˆæ—¶å»¶ã€èµ„æºï¼‰
- æ…¢æŸ¥è¯¢æ£€æµ‹å’Œå‘Šè­¦
- æŸ¥è¯¢æ‰§è¡Œè®¡åˆ’åˆ†æ
- æ—¶åºæ€§èƒ½æ•°æ®å­˜å‚¨
- æ€§èƒ½è¶‹åŠ¿åˆ†æ
- è‡ªåŠ¨å‘Šè­¦å’Œé€šçŸ¥

Author: Claude Code
Date: 2025-11-12
"""

import asyncio
import time
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional

import structlog

logger = structlog.get_logger()


class QuerySeverity(str, Enum):
    """æŸ¥è¯¢ä¸¥é‡çº§åˆ«"""

    NORMAL = "normal"  # æ­£å¸¸
    SLOW = "slow"  # æ…¢æŸ¥è¯¢
    CRITICAL = "critical"  # ä¸¥é‡


@dataclass
class QueryMetric:
    """æŸ¥è¯¢æ€§èƒ½æŒ‡æ ‡"""

    query_id: str
    sql: str
    table_name: str
    operation: str  # SELECT, INSERT, UPDATE, DELETE
    duration_ms: float
    timestamp: datetime = field(default_factory=datetime.utcnow)
    rows_affected: int = 0
    rows_scanned: int = 0
    severity: QuerySeverity = QuerySeverity.NORMAL
    error: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "query_id": self.query_id,
            "sql": self.sql[:100],  # æˆªæ–­é•¿SQL
            "table_name": self.table_name,
            "operation": self.operation,
            "duration_ms": round(self.duration_ms, 2),
            "timestamp": self.timestamp.isoformat(),
            "rows_affected": self.rows_affected,
            "rows_scanned": self.rows_scanned,
            "severity": self.severity.value,
            "error": error[:100] if (error := self.error) else None,
        }


@dataclass
class SlowQueryAlert:
    """æ…¢æŸ¥è¯¢å‘Šè­¦"""

    alert_id: str
    query_id: str
    threshold_ms: float
    actual_duration_ms: float
    excess_percent: float
    timestamp: datetime = field(default_factory=datetime.utcnow)
    sql: str = ""
    table_name: str = ""

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "alert_id": self.alert_id,
            "query_id": self.query_id,
            "threshold_ms": round(self.threshold_ms, 2),
            "actual_duration_ms": round(self.actual_duration_ms, 2),
            "excess_percent": round(self.excess_percent, 2),
            "timestamp": self.timestamp.isoformat(),
            "table_name": self.table_name,
        }


class DatabasePerformanceMonitor:
    """æ•°æ®åº“æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(
        self,
        slow_query_threshold_ms: float = 1000,
        critical_query_threshold_ms: float = 5000,
        metrics_retention_hours: int = 24,
        alert_retention_hours: int = 7,
    ):
        """
        åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨

        Args:
            slow_query_threshold_ms: æ…¢æŸ¥è¯¢é˜ˆå€¼ï¼ˆæ¯«ç§’ï¼‰
            critical_query_threshold_ms: ä¸¥é‡æŸ¥è¯¢é˜ˆå€¼ï¼ˆæ¯«ç§’ï¼‰
            metrics_retention_hours: æŒ‡æ ‡ä¿ç•™æ—¶é—´ï¼ˆå°æ—¶ï¼‰
            alert_retention_hours: å‘Šè­¦ä¿ç•™æ—¶é—´ï¼ˆå°æ—¶ï¼‰
        """
        self.slow_query_threshold_ms = slow_query_threshold_ms
        self.critical_query_threshold_ms = critical_query_threshold_ms
        self.metrics_retention_hours = metrics_retention_hours
        self.alert_retention_hours = alert_retention_hours

        # æ€§èƒ½æŒ‡æ ‡
        self.query_metrics: List[QueryMetric] = []
        self.slow_query_alerts: List[SlowQueryAlert] = []

        # ç»Ÿè®¡
        self.total_queries = 0
        self.total_slow_queries = 0
        self.total_critical_queries = 0
        self.total_errors = 0

        # æŒ‰è¡¨ç»Ÿè®¡
        self.table_stats: Dict[str, Dict[str, Any]] = {}

        # ç›‘æ§ä»»åŠ¡
        self.cleanup_task: Optional[asyncio.Task] = None

        logger.info(
            "âœ… Database Performance Monitor initialized",
            slow_threshold_ms=slow_query_threshold_ms,
            critical_threshold_ms=critical_query_threshold_ms,
        )

    async def start_monitoring(self) -> None:
        """å¯åŠ¨ç›‘æ§"""
        if not self.cleanup_task or self.cleanup_task.done():
            self.cleanup_task = asyncio.create_task(self._cleanup_loop())

        logger.info("âœ… Performance monitoring started")

    async def stop_monitoring(self) -> None:
        """åœæ­¢ç›‘æ§"""
        if self.cleanup_task and not self.cleanup_task.done():
            self.cleanup_task.cancel()
            try:
                await self.cleanup_task
            except asyncio.CancelledError:
                pass

        logger.info("âœ… Performance monitoring stopped")

    async def _cleanup_loop(self) -> None:
        """å®šæœŸæ¸…ç†å¾ªç¯"""
        while True:
            try:
                # æ¯å°æ—¶æ¸…ç†ä¸€æ¬¡
                await asyncio.sleep(3600)
                await self._cleanup_old_metrics()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error("âŒ Error in cleanup loop", error=str(e))

    async def _cleanup_old_metrics(self) -> None:
        """æ¸…ç†è¿‡æœŸæŒ‡æ ‡"""
        try:
            now = datetime.utcnow()
            cutoff_metrics = now - timedelta(hours=self.metrics_retention_hours)
            cutoff_alerts = now - timedelta(hours=self.alert_retention_hours)

            # æ¸…ç†æŒ‡æ ‡
            initial_count = len(self.query_metrics)
            self.query_metrics = [m for m in self.query_metrics if m.timestamp > cutoff_metrics]
            removed_metrics = initial_count - len(self.query_metrics)

            # æ¸…ç†å‘Šè­¦
            alert_count = len(self.slow_query_alerts)
            self.slow_query_alerts = [a for a in self.slow_query_alerts if a.timestamp > cutoff_alerts]
            removed_alerts = alert_count - len(self.slow_query_alerts)

            if removed_metrics > 0 or removed_alerts > 0:
                logger.info(
                    "ğŸ§¹ Cleaned up metrics and alerts",
                    removed_metrics=removed_metrics,
                    removed_alerts=removed_alerts,
                )

        except Exception as e:
            logger.error("âŒ Error cleaning up metrics", error=str(e))

    def record_query(
        self,
        sql: str,
        table_name: str,
        operation: str,
        duration_ms: float,
        rows_affected: int = 0,
        rows_scanned: int = 0,
        error: Optional[str] = None,
    ) -> Optional[SlowQueryAlert]:
        """
        è®°å½•æŸ¥è¯¢æ€§èƒ½

        Args:
            sql: SQLè¯­å¥
            table_name: è¡¨å
            operation: æ“ä½œç±»å‹ï¼ˆSELECT/INSERT/UPDATE/DELETEï¼‰
            duration_ms: æ‰§è¡Œæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
            rows_affected: å½±å“è¡Œæ•°
            rows_scanned: æ‰«æè¡Œæ•°
            error: é”™è¯¯ä¿¡æ¯

        Returns:
            å¦‚æœæ˜¯æ…¢æŸ¥è¯¢ï¼Œè¿”å›SlowQueryAlertå¯¹è±¡
        """
        try:
            query_id = f"query_{int(time.time() * 1000000)}"

            # åˆ¤æ–­æŸ¥è¯¢ä¸¥é‡çº§åˆ«
            if duration_ms >= self.critical_query_threshold_ms:
                severity = QuerySeverity.CRITICAL
            elif duration_ms >= self.slow_query_threshold_ms:
                severity = QuerySeverity.SLOW
            else:
                severity = QuerySeverity.NORMAL

            # åˆ›å»ºæŒ‡æ ‡å¯¹è±¡
            metric = QueryMetric(
                query_id=query_id,
                sql=sql,
                table_name=table_name,
                operation=operation,
                duration_ms=duration_ms,
                rows_affected=rows_affected,
                rows_scanned=rows_scanned,
                severity=severity,
                error=error,
            )

            self.query_metrics.append(metric)
            self.total_queries += 1

            # æ›´æ–°è¡¨ç»Ÿè®¡
            if table_name not in self.table_stats:
                self.table_stats[table_name] = {
                    "total_queries": 0,
                    "total_duration_ms": 0,
                    "avg_duration_ms": 0,
                    "slow_queries": 0,
                    "critical_queries": 0,
                }

            stats = self.table_stats[table_name]
            stats["total_queries"] += 1
            stats["total_duration_ms"] += duration_ms
            stats["avg_duration_ms"] = stats["total_duration_ms"] / stats["total_queries"]

            # å¦‚æœæ˜¯æ…¢æŸ¥è¯¢æˆ–é”™è¯¯ï¼Œç”Ÿæˆå‘Šè­¦
            alert = None
            if severity == QuerySeverity.SLOW:
                self.total_slow_queries += 1
                stats["slow_queries"] += 1

                excess_percent = (duration_ms - self.slow_query_threshold_ms) / self.slow_query_threshold_ms * 100

                alert = SlowQueryAlert(
                    alert_id=f"alert_{query_id}",
                    query_id=query_id,
                    threshold_ms=self.slow_query_threshold_ms,
                    actual_duration_ms=duration_ms,
                    excess_percent=excess_percent,
                    sql=sql[:200],
                    table_name=table_name,
                )

                self.slow_query_alerts.append(alert)

                logger.warning(
                    "âš ï¸ Slow query detected",
                    table=table_name,
                    operation=operation,
                    duration_ms=round(duration_ms, 2),
                    threshold_ms=self.slow_query_threshold_ms,
                    excess_percent=round(excess_percent, 1),
                )

            elif severity == QuerySeverity.CRITICAL:
                self.total_critical_queries += 1
                stats["critical_queries"] += 1

                excess_percent = (
                    (duration_ms - self.critical_query_threshold_ms) / self.critical_query_threshold_ms * 100
                )

                alert = SlowQueryAlert(
                    alert_id=f"alert_{query_id}",
                    query_id=query_id,
                    threshold_ms=self.critical_query_threshold_ms,
                    actual_duration_ms=duration_ms,
                    excess_percent=excess_percent,
                    sql=sql[:200],
                    table_name=table_name,
                )

                self.slow_query_alerts.append(alert)

                logger.critical(
                    "âŒ CRITICAL slow query detected",
                    table=table_name,
                    operation=operation,
                    duration_ms=round(duration_ms, 2),
                    threshold_ms=self.critical_query_threshold_ms,
                    excess_percent=round(excess_percent, 1),
                )

            if error:
                self.total_errors += 1
                logger.error(
                    "âŒ Query error",
                    table=table_name,
                    operation=operation,
                    error=error[:100],
                )

            return alert

        except Exception as e:
            logger.error("âŒ Error recording query metric", error=str(e))
            return None

    def get_slow_queries(self, limit: int = 100) -> List[Dict[str, Any]]:
        """è·å–æ…¢æŸ¥è¯¢åˆ—è¡¨"""
        slow_queries = [m for m in self.query_metrics if m.severity != QuerySeverity.NORMAL]
        # æŒ‰æ—¶é—´å€’åºæ’åˆ—
        slow_queries.sort(key=lambda x: x.timestamp, reverse=True)
        return [q.to_dict() for q in slow_queries[:limit]]

    def get_recent_alerts(self, limit: int = 100) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘å‘Šè­¦"""
        recent = sorted(self.slow_query_alerts, key=lambda x: x.timestamp, reverse=True)
        return [a.to_dict() for a in recent[:limit]]

    def get_table_performance(self) -> Dict[str, Dict[str, Any]]:
        """è·å–è¡¨çº§æ€§èƒ½ç»Ÿè®¡"""
        return {
            table: {
                **stats,
                "avg_duration_ms": round(stats["avg_duration_ms"], 2),
            }
            for table, stats in self.table_stats.items()
        }

    def get_monitoring_stats(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§ç»Ÿè®¡"""
        return {
            "total_queries": self.total_queries,
            "total_slow_queries": self.total_slow_queries,
            "total_critical_queries": self.total_critical_queries,
            "total_errors": self.total_errors,
            "slow_query_rate": ((self.total_slow_queries / max(1, self.total_queries)) * 100),
            "error_rate": (self.total_errors / max(1, self.total_queries)) * 100,
            "metrics_count": len(self.query_metrics),
            "alerts_count": len(self.slow_query_alerts),
            "tables_monitored": len(self.table_stats),
            "timestamp": datetime.utcnow().isoformat(),
        }

    def get_comprehensive_report(self) -> Dict[str, Any]:
        """è·å–ç»¼åˆæŠ¥å‘Š"""
        return {
            "summary": self.get_monitoring_stats(),
            "table_performance": self.get_table_performance(),
            "recent_slow_queries": self.get_slow_queries(10),
            "recent_alerts": self.get_recent_alerts(10),
            "timestamp": datetime.utcnow().isoformat(),
        }


# å…¨å±€å•ä¾‹
_performance_monitor: Optional[DatabasePerformanceMonitor] = None


def get_performance_monitor(
    slow_query_threshold_ms: float = 1000,
    critical_query_threshold_ms: float = 5000,
    metrics_retention_hours: int = 24,
    alert_retention_hours: int = 7,
) -> DatabasePerformanceMonitor:
    """è·å–æ€§èƒ½ç›‘æ§å™¨å•ä¾‹"""
    global _performance_monitor
    if _performance_monitor is None:
        _performance_monitor = DatabasePerformanceMonitor(
            slow_query_threshold_ms=slow_query_threshold_ms,
            critical_query_threshold_ms=critical_query_threshold_ms,
            metrics_retention_hours=metrics_retention_hours,
            alert_retention_hours=alert_retention_hours,
        )
    return _performance_monitor


def reset_performance_monitor() -> None:
    """é‡ç½®æ€§èƒ½ç›‘æ§å™¨ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰"""
    global _performance_monitor
    _performance_monitor = None
