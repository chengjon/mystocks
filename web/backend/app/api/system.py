"""
ç³»ç»Ÿç®¡ç†APIç«¯ç‚¹ (Phase 2.4 - System & Monitoring Module Alignment)
æä¾›ç³»ç»Ÿè®¾ç½®ã€æ•°æ®åº“è¿æ¥æµ‹è¯•ã€è¿è¡Œæ—¥å¿—æŸ¥è¯¢ã€ç³»ç»ŸçŠ¶æ€ç›‘æ§ç­‰åŠŸèƒ½

ç‰ˆæœ¬: 2.4.0
æ—¥æœŸ: 2025-12-24
æ›´æ–°å†…å®¹:
- æ·»åŠ  /api/system/status ç»¼åˆçŠ¶æ€ç«¯ç‚¹
- æ·»åŠ ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡æ”¶é›†
- ä½¿ç”¨ç»Ÿä¸€å“åº”æ ¼å¼
- æ·»åŠ PydanticéªŒè¯æ¨¡å‹
- Phase 2.4.3: æ·»åŠ  /api/system/logs/stream SSEæ—¥å¿—æµç«¯ç‚¹
"""

import os
import psutil
import logging
import time
import asyncio
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional

import psycopg2
import taos
from fastapi import APIRouter, HTTPException, Query, Request
from pydantic import BaseModel, Field
from sse_starlette.sse import EventSourceResponse

# å¯¼å…¥ç»Ÿä¸€å“åº”æ ¼å¼
from app.core.responses import (
    ErrorCodes,
    ResponseMessages,
    create_error_response,
    create_success_response,
    create_health_response,
)

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# Mockæ•°æ®æ”¯æŒ
use_mock = os.getenv("USE_MOCK_DATA", "false").lower() == "true"

router = APIRouter()


# ============================================================================
# ç³»ç»Ÿå¯åŠ¨æ—¶é—´è¿½è¸ª (ç”¨äºè¿è¡Œæ—¶é—´è®¡ç®—)
# ============================================================================
# åº”ç”¨å¯åŠ¨æ—¶è®°å½•æ—¶é—´ï¼ˆåœ¨main.pyæˆ–app_factory.pyä¸­è®¾ç½®ï¼‰
_app_start_time = time.time()


# ============================================================================
# Pydantic éªŒè¯æ¨¡å‹
# ============================================================================


class SystemStatus(str, Enum):
    """ç³»ç»ŸçŠ¶æ€æšä¸¾"""

    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"


class ComponentStatus(BaseModel):
    """ç»„ä»¶çŠ¶æ€æ¨¡å‹"""

    name: str = Field(..., description="ç»„ä»¶åç§°")
    status: str = Field(..., description="çŠ¶æ€ (healthy/unhealthy/unknown)")
    message: Optional[str] = Field(None, description="çŠ¶æ€æ¶ˆæ¯")
    response_time_ms: Optional[int] = Field(None, description="å“åº”æ—¶é—´(æ¯«ç§’)")
    details: Optional[Dict[str, Any]] = Field(None, description="è¯¦ç»†ä¿¡æ¯")


class PerformanceMetrics(BaseModel):
    """æ€§èƒ½æŒ‡æ ‡æ¨¡å‹"""

    cpu_percent: float = Field(..., description="CPUä½¿ç”¨ç‡(%)")
    memory_percent: float = Field(..., description="å†…å­˜ä½¿ç”¨ç‡(%)")
    memory_used_mb: float = Field(..., description="å·²ä½¿ç”¨å†…å­˜(MB)")
    memory_total_mb: float = Field(..., description="æ€»å†…å­˜(MB)")
    disk_percent: float = Field(..., description="ç£ç›˜ä½¿ç”¨ç‡(%)")
    disk_used_gb: float = Field(..., description="å·²ä½¿ç”¨ç£ç›˜(GB)")
    disk_total_gb: float = Field(..., description="æ€»ç£ç›˜(GB)")
    uptime_seconds: float = Field(..., description="ç³»ç»Ÿè¿è¡Œæ—¶é—´(ç§’)")
    uptime_formatted: str = Field(..., description="ç³»ç»Ÿè¿è¡Œæ—¶é—´(æ ¼å¼åŒ–)")


class RequestStatistics(BaseModel):
    """è¯·æ±‚ç»Ÿè®¡æ¨¡å‹"""

    total_requests: int = Field(..., description="æ€»è¯·æ±‚æ•°")
    requests_per_minute: float = Field(..., description="æ¯åˆ†é’Ÿè¯·æ±‚æ•°")
    avg_response_time_ms: float = Field(..., description="å¹³å‡å“åº”æ—¶é—´(æ¯«ç§’)")
    error_rate: float = Field(..., description="é”™è¯¯ç‡(%)")
    active_connections: int = Field(..., description="æ´»è·ƒè¿æ¥æ•°")


class SystemStatusResponse(BaseModel):
    """ç³»ç»ŸçŠ¶æ€å“åº”æ¨¡å‹"""

    overall_status: str = Field(..., description="æ•´ä½“çŠ¶æ€")
    uptime: str = Field(..., description="ç³»ç»Ÿè¿è¡Œæ—¶é—´")
    components: Dict[str, ComponentStatus] = Field(..., description="ç»„ä»¶çŠ¶æ€")
    performance: PerformanceMetrics = Field(..., description="æ€§èƒ½æŒ‡æ ‡")
    statistics: RequestStatistics = Field(..., description="è¯·æ±‚ç»Ÿè®¡")
    alerts: List[Dict[str, Any]] = Field(..., description="æ´»è·ƒå‘Šè­¦")
    timestamp: str = Field(..., description="æ›´æ–°æ—¶é—´")


@router.get("/health")
async def system_health():
    """
    ç³»ç»Ÿå¥åº·æ£€æŸ¥ç«¯ç‚¹ (åŒæ•°æ®åº“æ¶æ„: TDengine + PostgreSQL)

    è¿”å›ç»Ÿä¸€æ ¼å¼çš„å¥åº·æ£€æŸ¥å“åº”:
    - æ•°æ®åº“è¿æ¥çŠ¶æ€
    - ç³»ç»Ÿè¿è¡Œæ—¶é—´
    - æœåŠ¡çŠ¶æ€

    Phase 2.4.4: æ›´æ–°ä¸ºç»Ÿä¸€å“åº”æ ¼å¼
    """
    try:
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥çŠ¶æ€
        db_status = {}
        overall_status = "healthy"
        health_details = {}

        # PostgreSQL æ£€æŸ¥
        try:
            conn = psycopg2.connect(
                host=os.getenv("POSTGRESQL_HOST", "192.168.123.104"),
                port=int(os.getenv("POSTGRESQL_PORT", "5438")),
                user=os.getenv("POSTGRESQL_USER", "postgres"),
                password=os.getenv("POSTGRESQL_PASSWORD"),
                database=os.getenv("POSTGRESQL_DATABASE", "mystocks"),
                connect_timeout=5,
            )
            conn.close()
            db_status["postgresql"] = "healthy"
            health_details["postgresql"] = "è¿æ¥æˆåŠŸ"
        except Exception as e:
            db_status["postgresql"] = "unhealthy"
            health_details["postgresql"] = f"è¿æ¥å¤±è´¥: {str(e)}"
            overall_status = "unhealthy"

        # TDengine æ£€æŸ¥
        try:
            conn = taos.connect(
                host=os.getenv("TDENGINE_HOST", "192.168.123.104"),
                port=int(os.getenv("TDENGINE_PORT", "6030")),
                user=os.getenv("TDENGINE_USER", "root"),
                password=os.getenv("TDENGINE_PASSWORD", "taosdata"),
                database=os.getenv("TDENGINE_DATABASE", "market_data"),
            )
            conn.close()
            db_status["tdengine"] = "healthy"
            health_details["tdengine"] = "è¿æ¥æˆåŠŸ"
        except Exception as e:
            db_status["tdengine"] = "unhealthy"
            health_details["tdengine"] = f"è¿æ¥å¤±è´¥: {str(e)}"
            if overall_status == "healthy":
                overall_status = "degraded"

        # è®¡ç®—è¿è¡Œæ—¶é—´
        uptime_seconds = time.time() - _app_start_time
        uptime_formatted = _format_uptime(uptime_seconds)

        return create_health_response(
            service="system",
            status=overall_status,
            details={
                "databases": db_status,
                "architecture": "dual-database",
                "uptime": uptime_formatted,
                "uptime_seconds": round(uptime_seconds, 2),
                "version": "2.4.0",
                "health_details": health_details,
            },
        )

    except Exception as e:
        logger.error(f"[SYSTEM] Health check failed: {str(e)}")
        return create_health_response(
            service="system",
            status="unhealthy",
            details={"error": str(e)},
        )


# ============================================================================
# ç³»ç»ŸçŠ¶æ€ç«¯ç‚¹ (Phase 2.4.1 - æ–°å¢)
# ============================================================================


def _format_uptime(seconds: float) -> str:
    """æ ¼å¼åŒ–è¿è¡Œæ—¶é—´"""
    days = int(seconds // 86400)
    hours = int((seconds % 86400) // 3600)
    minutes = int((seconds % 3600) // 60)

    if days > 0:
        return f"{days}å¤© {hours}å°æ—¶ {minutes}åˆ†é’Ÿ"
    elif hours > 0:
        return f"{hours}å°æ—¶ {minutes}åˆ†é’Ÿ"
    else:
        return f"{minutes}åˆ†é’Ÿ"


def _get_performance_metrics() -> PerformanceMetrics:
    """è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
    try:
        # CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=0.1)

        # å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        memory_used_mb = memory.used / (1024 * 1024)
        memory_total_mb = memory.total / (1024 * 1024)

        # ç£ç›˜ä½¿ç”¨æƒ…å†µ
        disk = psutil.disk_usage("/")
        disk_percent = disk.percent
        disk_used_gb = disk.used / (1024 * 1024 * 1024)
        disk_total_gb = disk.total / (1024 * 1024 * 1024)

        # ç³»ç»Ÿè¿è¡Œæ—¶é—´
        uptime_seconds = time.time() - _app_start_time
        uptime_formatted = _format_uptime(uptime_seconds)

        return PerformanceMetrics(
            cpu_percent=round(cpu_percent, 2),
            memory_percent=round(memory_percent, 2),
            memory_used_mb=round(memory_used_mb, 2),
            memory_total_mb=round(memory_total_mb, 2),
            disk_percent=round(disk_percent, 2),
            disk_used_gb=round(disk_used_gb, 2),
            disk_total_gb=round(disk_total_gb, 2),
            uptime_seconds=round(uptime_seconds, 2),
            uptime_formatted=uptime_formatted,
        )
    except Exception as e:
        # è¿”å›é»˜è®¤å€¼å½“psutilä¸å¯ç”¨æ—¶
        return PerformanceMetrics(
            cpu_percent=0.0,
            memory_percent=0.0,
            memory_used_mb=0.0,
            memory_total_mb=0.0,
            disk_percent=0.0,
            disk_used_gb=0.0,
            disk_total_gb=0.0,
            uptime_seconds=0.0,
            uptime_formatted="æœªçŸ¥",
        )


def _get_database_status() -> Dict[str, ComponentStatus]:
    """è·å–æ•°æ®åº“ç»„ä»¶çŠ¶æ€"""
    components = {}

    # PostgreSQL çŠ¶æ€æ£€æŸ¥
    start_time = time.time()
    try:
        conn = psycopg2.connect(
            host=os.getenv("POSTGRESQL_HOST", "192.168.123.104"),
            port=int(os.getenv("POSTGRESQL_PORT", "5438")),
            user=os.getenv("POSTGRESQL_USER", "postgres"),
            password=os.getenv("POSTGRESQL_PASSWORD"),
            database=os.getenv("POSTGRESQL_DATABASE", "mystocks"),
            connect_timeout=5,
        )
        cursor = conn.cursor()
        cursor.execute("SELECT version()")
        version = cursor.fetchone()[0]
        cursor.close()
        conn.close()

        response_time = int((time.time() - start_time) * 1000)
        components["postgresql"] = ComponentStatus(
            name="PostgreSQL",
            status="healthy",
            message=f"è¿æ¥æˆåŠŸ (ç‰ˆæœ¬: {version.split(',')[0]})",
            response_time_ms=response_time,
            details={
                "host": os.getenv("POSTGRESQL_HOST"),
                "port": int(os.getenv("POSTGRESQL_PORT", "5438")),
                "database": os.getenv("POSTGRESQL_DATABASE"),
            },
        )
    except Exception as e:
        components["postgresql"] = ComponentStatus(
            name="PostgreSQL",
            status="unhealthy",
            message=f"è¿æ¥å¤±è´¥: {str(e)}",
            response_time_ms=None,
            details={
                "host": os.getenv("POSTGRESQL_HOST"),
                "port": int(os.getenv("POSTGRESQL_PORT", "5438")),
            },
        )

    # TDengine çŠ¶æ€æ£€æŸ¥
    start_time = time.time()
    try:
        conn = taos.connect(
            host=os.getenv("TDENGINE_HOST", "192.168.123.104"),
            port=int(os.getenv("TDENGINE_PORT", "6030")),
            user=os.getenv("TDENGINE_USER", "root"),
            password=os.getenv("TDENGINE_PASSWORD", "taosdata"),
            database=os.getenv("TDENGINE_DATABASE", "market_data"),
        )
        result = conn.query("SELECT SERVER_VERSION()")
        version = result.fetch_all()[0][0] if result else "unknown"
        conn.close()

        response_time = int((time.time() - start_time) * 1000)
        components["tdengine"] = ComponentStatus(
            name="TDengine",
            status="healthy",
            message=f"è¿æ¥æˆåŠŸ (ç‰ˆæœ¬: {version})",
            response_time_ms=response_time,
            details={
                "host": os.getenv("TDENGINE_HOST"),
                "port": int(os.getenv("TDENGINE_PORT", "6030")),
                "database": os.getenv("TDENGINE_DATABASE"),
            },
        )
    except Exception as e:
        components["tdengine"] = ComponentStatus(
            name="TDengine",
            status="unhealthy",
            message=f"è¿æ¥å¤±è´¥: {str(e)}",
            response_time_ms=None,
            details={
                "host": os.getenv("TDENGINE_HOST"),
                "port": int(os.getenv("TDENGINE_PORT", "6030")),
            },
        )

    return components


def _get_adapter_status() -> Dict[str, ComponentStatus]:
    """è·å–æ•°æ®é€‚é…å™¨çŠ¶æ€"""
    components = {}

    try:
        from app.core.adapter_loader import check_all_adapters

        adapter_health = check_all_adapters()

        for adapter_name, is_healthy in adapter_health.items():
            components[adapter_name] = ComponentStatus(
                name=adapter_name.capitalize(),
                status="healthy" if is_healthy else "unhealthy",
                message="è¿è¡Œæ­£å¸¸" if is_healthy else "æœåŠ¡å¼‚å¸¸",
                response_time_ms=None,
                details={"type": "data_adapter"},
            )
    except Exception:
        # å¦‚æœé€‚é…å™¨æ£€æŸ¥å¤±è´¥ï¼Œè¿”å›é»˜è®¤çŠ¶æ€
        default_adapters = ["akshare", "tdx", "financial"]
        for name in default_adapters:
            components[name] = ComponentStatus(
                name=name.capitalize(),
                status="unknown",
                message="çŠ¶æ€æ£€æŸ¥ä¸å¯ç”¨",
                response_time_ms=None,
                details={"type": "data_adapter"},
            )

    return components


def _get_request_statistics() -> RequestStatistics:
    """è·å–è¯·æ±‚ç»Ÿè®¡ä¿¡æ¯ (æ¨¡æ‹Ÿæ•°æ®)"""
    # TODO: å®é™…åº”è¯¥ä»ä¸­é—´ä»¶æˆ–æ—¥å¿—ä¸­è·å–
    return RequestStatistics(
        total_requests=15420,
        requests_per_minute=125.5,
        avg_response_time_ms=45.2,
        error_rate=0.12,
        active_connections=8,
    )


def _get_active_alerts() -> List[Dict[str, Any]]:
    """è·å–æ´»è·ƒå‘Šè­¦ (æ¨¡æ‹Ÿæ•°æ®)"""
    # TODO: å®é™…åº”è¯¥ä»ç›‘æ§æ•°æ®åº“è·å–
    return [
        {
            "id": 1,
            "level": "warning",
            "message": "TDengineæŸ¥è¯¢å“åº”æ—¶é—´è¶…è¿‡3ç§’",
            "component": "tdengine",
            "created_at": (datetime.now() - timedelta(minutes=2)).isoformat(),
        },
        {
            "id": 2,
            "level": "info",
            "message": "ç³»ç»Ÿèµ„æºä½¿ç”¨æ­£å¸¸",
            "component": "system",
            "created_at": (datetime.now() - timedelta(minutes=10)).isoformat(),
        },
    ]


@router.get("/status")
async def get_system_status():
    """
    è·å–ç³»ç»Ÿç»¼åˆçŠ¶æ€ (Phase 2.4.1 - æ–°å¢)

    è¿”å›å®Œæ•´çš„ç³»ç»ŸçŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬:
    - æ•´ä½“çŠ¶æ€è¯„ä¼°
    - ç³»ç»Ÿè¿è¡Œæ—¶é—´
    - å„ç»„ä»¶å¥åº·çŠ¶æ€ (æ•°æ®åº“ã€é€‚é…å™¨ã€æœåŠ¡ç­‰)
    - æ€§èƒ½æŒ‡æ ‡ (CPUã€å†…å­˜ã€ç£ç›˜)
    - è¯·æ±‚ç»Ÿè®¡ (è¯·æ±‚æ•°ã€å“åº”æ—¶é—´ã€é”™è¯¯ç‡)
    - æ´»è·ƒå‘Šè­¦åˆ—è¡¨

    **æ•´ä½“çŠ¶æ€åˆ¤å®šè§„åˆ™**:
    - `healthy`: æ‰€æœ‰æ ¸å¿ƒç»„ä»¶å¥åº·ï¼Œæ— ä¸¥é‡å‘Šè­¦
    - `degraded`: éƒ¨åˆ†ç»„ä»¶å¼‚å¸¸æˆ–å­˜åœ¨è­¦å‘Šçº§åˆ«å‘Šè­¦
    - `unhealthy`: æ ¸å¿ƒç»„ä»¶ä¸å¯ç”¨æˆ–å­˜åœ¨ä¸¥é‡å‘Šè­¦

    **æ ¸å¿ƒç»„ä»¶**:
    - PostgreSQLæ•°æ®åº“
    - TDengineæ—¶åºæ•°æ®åº“

    **ç¤ºä¾‹è¯·æ±‚**:
        GET /api/system/status

    **ç¤ºä¾‹å“åº”**:
    ```json
    {
        "success": true,
        "message": "ç³»ç»ŸçŠ¶æ€è·å–æˆåŠŸ",
        "data": {
            "overall_status": "healthy",
            "uptime": "2å¤© 5å°æ—¶ 30åˆ†é’Ÿ",
            "components": {
                "postgresql": {
                    "name": "PostgreSQL",
                    "status": "healthy",
                    "message": "è¿æ¥æˆåŠŸ",
                    "response_time_ms": 15
                },
                "tdengine": {
                    "name": "TDengine",
                    "status": "healthy",
                    "message": "è¿æ¥æˆåŠŸ",
                    "response_time_ms": 8
                }
            },
            "performance": {
                "cpu_percent": 15.5,
                "memory_percent": 45.2,
                "uptime_formatted": "2å¤© 5å°æ—¶ 30åˆ†é’Ÿ"
            },
            "statistics": {
                "total_requests": 15420,
                "requests_per_minute": 125.5,
                "avg_response_time_ms": 45.2,
                "error_rate": 0.12
            },
            "alerts": [],
            "timestamp": "2025-12-24T10:30:00"
        }
    }
    """
    try:
        # è·å–å„ç»„ä»¶çŠ¶æ€
        db_components = _get_database_status()
        adapter_components = _get_adapter_status()

        # åˆå¹¶æ‰€æœ‰ç»„ä»¶
        all_components = {**db_components, **adapter_components}

        # è®¡ç®—æ•´ä½“çŠ¶æ€
        core_components = ["postgresql", "tdengine"]
        core_healthy = sum(
            1
            for name in core_components
            if all_components.get(name) and all_components.get(name).status == "healthy"
        )

        if core_healthy == len(core_components):
            overall_status = "healthy"
        elif core_healthy > 0:
            overall_status = "degraded"
        else:
            overall_status = "unhealthy"

        # è·å–æ€§èƒ½æŒ‡æ ‡
        performance = _get_performance_metrics()

        # è·å–è¯·æ±‚ç»Ÿè®¡
        statistics = _get_request_statistics()

        # è·å–æ´»è·ƒå‘Šè­¦
        alerts = _get_active_alerts()

        # æ„å»ºå“åº”æ•°æ®
        status_data = SystemStatusResponse(
            overall_status=overall_status,
            uptime=performance.uptime_formatted,
            components=all_components,
            performance=performance,
            statistics=statistics,
            alerts=alerts,
            timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        )

        return create_success_response(
            data=status_data.model_dump(),
            message=f"ç³»ç»ŸçŠ¶æ€: {overall_status.upper()}",
        )

    except Exception as e:
        logger.error(f"[SYSTEM] Failed to get system status: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=create_error_response(
                ErrorCodes.INTERNAL_SERVER_ERROR,
                f"è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {str(e)}",
            ).model_dump(mode="json"),
        )


@router.get("/adapters/health")
async def get_adapters_health():
    """
    ğŸš€ é€‚é…å™¨å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼ˆæ–°å¢ï¼‰

    æ£€æŸ¥æ‰€æœ‰æ•°æ®é€‚é…å™¨çš„å¥åº·çŠ¶æ€ï¼š
    - akshare: AkShareé€‚é…å™¨
    - tdx: é€šè¾¾ä¿¡é€‚é…å™¨
    - financial: è´¢åŠ¡æ•°æ®é€‚é…å™¨

    è¿”å›:
    - æ¯ä¸ªé€‚é…å™¨çš„å¥åº·çŠ¶æ€
    - æœ€åæ£€æŸ¥æ—¶é—´
    - é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰

    ç”¨äºç›‘æ§å’Œè‡ªåŠ¨é™çº§
    """
    try:
        from datetime import datetime

        from app.core.adapter_loader import (
            check_all_adapters,
            get_adapter_health_status,
        )

        # æ‰§è¡Œå¥åº·æ£€æŸ¥
        health_results = check_all_adapters()

        # è·å–è¯¦ç»†çŠ¶æ€
        detailed_status = {}
        for adapter_name, is_healthy in health_results.items():
            status_info = get_adapter_health_status(adapter_name)
            detailed_status[adapter_name] = {
                "healthy": is_healthy,
                "status": status_info.get("status", "unknown"),
                "error": status_info.get("error"),
                "last_check": datetime.now().isoformat(),
            }

        # è®¡ç®—æ€»ä½“å¥åº·åº¦
        total_adapters = len(health_results)
        healthy_adapters = sum(1 for h in health_results.values() if h)
        overall_healthy = healthy_adapters == total_adapters

        return {
            "overall_status": "healthy" if overall_healthy else "degraded",
            "healthy_count": healthy_adapters,
            "total_count": total_adapters,
            "adapters": detailed_status,
            "timestamp": datetime.now().isoformat(),
            "message": f"{healthy_adapters}/{total_adapters} é€‚é…å™¨æ­£å¸¸è¿è¡Œ",
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é€‚é…å™¨å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")


@router.get("/datasources")
async def get_datasources():
    """
    è·å–å·²é…ç½®çš„æ•°æ®æºåˆ—è¡¨

    è¿”å›æ‰€æœ‰å¯ç”¨çš„æ•°æ®æºé…ç½®ä¿¡æ¯
    """
    datasources = [
        {
            "id": "tdx",
            "name": "é€šè¾¾ä¿¡(TDX)",
            "type": "realtime",
            "status": "active",
            "description": "å®æ—¶è¡Œæƒ…å’Œå¤šå‘¨æœŸKçº¿æ•°æ®",
            "features": ["å®æ—¶è¡Œæƒ…", "åˆ†é’ŸKçº¿", "æ—¥Kçº¿"],
        },
        {
            "id": "akshare",
            "name": "AkShare",
            "type": "historical",
            "status": "active",
            "description": "å†å²æ•°æ®å’Œè´¢åŠ¡æ•°æ®",
            "features": ["å†å²è¡Œæƒ…", "è´¢åŠ¡æŠ¥è¡¨", "å®è§‚æ•°æ®"],
        },
        {
            "id": "financial",
            "name": "Financial Adapter",
            "type": "comprehensive",
            "status": "active",
            "description": "ç»¼åˆè´¢åŠ¡æ•°æ®é€‚é…å™¨",
            "features": ["å®æ—¶è¡Œæƒ…", "è´¢åŠ¡æ•°æ®", "æŒ‡æ•°æ•°æ®"],
        },
        {
            "id": "baostock",
            "name": "BaoStock",
            "type": "historical",
            "status": "available",
            "description": "å†å²æ•°æ®å¤‡ç”¨æ•°æ®æº",
            "features": ["å†å²è¡Œæƒ…", "å¤æƒæ•°æ®"],
        },
    ]

    return {
        "success": True,
        "data": datasources,
        "total": len(datasources),
        "timestamp": datetime.now().isoformat(),
    }


class ConnectionTestRequest(BaseModel):
    """æ•°æ®åº“è¿æ¥æµ‹è¯•è¯·æ±‚"""

    db_type: str
    host: str
    port: int


class ConnectionTestResponse(BaseModel):
    """æ•°æ®åº“è¿æ¥æµ‹è¯•å“åº”"""

    success: bool
    message: Optional[str] = None
    error: Optional[str] = None


@router.post("/test-connection", response_model=ConnectionTestResponse)
async def test_database_connection(request: ConnectionTestRequest):
    """
    æµ‹è¯•æ•°æ®åº“è¿æ¥ (åŒæ•°æ®åº“æ¶æ„)

    æ”¯æŒçš„æ•°æ®åº“ç±»å‹:
    - postgresql: PostgreSQL (ä¸»æ•°æ®åº“)
    - tdengine: TDengine (æ—¶åºæ•°æ®åº“)
    """
    db_type = request.db_type.lower()
    host = request.host
    port = request.port

    try:
        if db_type == "postgresql":
            # æµ‹è¯• PostgreSQL è¿æ¥ - è¿æ¥åˆ°é»˜è®¤çš„ postgres æ•°æ®åº“
            connection = None
            cursor = None
            try:
                connection = psycopg2.connect(
                    host=host,
                    port=port,
                    user=os.getenv("POSTGRESQL_USER", "postgres"),
                    password=os.getenv("POSTGRESQL_PASSWORD"),
                    database="postgres",  # è¿æ¥åˆ°é»˜è®¤æ•°æ®åº“
                    connect_timeout=5,
                )
                # æ‰§è¡Œç®€å•æŸ¥è¯¢æµ‹è¯•
                cursor = connection.cursor()
                cursor.execute("SELECT version()")
                version = cursor.fetchone()[0].split(",")[0]

                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ mystocks ç›¸å…³æ•°æ®åº“
                cursor.execute(
                    "SELECT datname FROM pg_database WHERE datname LIKE 'mystocks%'"
                )
                databases = cursor.fetchall()
                db_list = [db[0] for db in databases] if databases else []

                if db_list:
                    return ConnectionTestResponse(
                        success=True,
                        message=f"PostgreSQL è¿æ¥æˆåŠŸ ({version})ï¼Œå‘ç°æ•°æ®åº“: {', '.join(db_list)}",
                    )
                else:
                    return ConnectionTestResponse(
                        success=True,
                        message=f"PostgreSQL è¿æ¥æˆåŠŸ ({version})ï¼Œä½†æœªå‘ç° mystocks ç›¸å…³æ•°æ®åº“",
                    )
            except psycopg2.Error:
                raise
            finally:
                # ç¡®ä¿è¿æ¥è¢«å…³é—­ï¼Œé˜²æ­¢è¿æ¥æ³„æ¼
                if cursor is not None:
                    try:
                        cursor.close()
                    except Exception:
                        pass
                if connection is not None:
                    try:
                        connection.close()
                    except Exception:
                        pass

        elif db_type == "tdengine":
            # æµ‹è¯• TDengine è¿æ¥
            connection = None
            cursor = None
            try:
                connection = taos.connect(
                    host=host,
                    port=port,
                    user=os.getenv("TDENGINE_USER", "root"),
                    password=os.getenv("TDENGINE_PASSWORD", "taosdata"),
                    config="/etc/taos",
                    timeout=5000,
                )
                # æ‰§è¡Œç®€å•æŸ¥è¯¢æµ‹è¯•
                cursor = connection.cursor()
                cursor.execute("SELECT SERVER_VERSION()")
                result = cursor.fetchone()
                version = result[0] if result and len(result) > 0 else "æœªçŸ¥ç‰ˆæœ¬"

                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ mystocks ç›¸å…³æ•°æ®åº“
                cursor.execute("SHOW DATABASES")
                databases = cursor.fetchall()
                db_list = (
                    [db[0] for db in databases if db and "mystocks" in db[0].lower()]
                    if databases
                    else []
                )

                if db_list:
                    return ConnectionTestResponse(
                        success=True,
                        message=f"TDengine è¿æ¥æˆåŠŸ (ç‰ˆæœ¬: {version})ï¼Œå‘ç°æ•°æ®åº“: {', '.join(db_list)}",
                    )
                else:
                    return ConnectionTestResponse(
                        success=True,
                        message=f"TDengine è¿æ¥æˆåŠŸ (ç‰ˆæœ¬: {version})ï¼Œä½†æœªå‘ç° mystocks ç›¸å…³æ•°æ®åº“",
                    )
            except Exception as e:
                # TDengine å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
                error_msg = str(e)
                if (
                    "Unable to establish connection" in error_msg
                    or "Connection refused" in error_msg
                ):
                    return ConnectionTestResponse(
                        success=False,
                        error=f"æ— æ³•è¿æ¥åˆ° TDengine æœåŠ¡å™¨ ({host}:{port})ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ",
                    )
                raise
            finally:
                # ç¡®ä¿è¿æ¥è¢«å…³é—­ï¼Œé˜²æ­¢è¿æ¥æ³„æ¼
                if cursor is not None:
                    try:
                        cursor.close()
                    except Exception:
                        pass
                if connection is not None:
                    try:
                        connection.close()
                    except Exception:
                        pass

        else:
            return ConnectionTestResponse(
                success=False,
                error=f"ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {db_type}ï¼Œä»…æ”¯æŒ postgresql å’Œ tdengine",
            )

    except psycopg2.OperationalError as e:
        error_msg = str(e)
        if "could not connect to server" in error_msg:
            return ConnectionTestResponse(
                success=False,
                error=f"æ— æ³•è¿æ¥åˆ° PostgreSQL æœåŠ¡å™¨ ({host}:{port})ï¼Œè¯·æ£€æŸ¥åœ°å€å’Œç«¯å£æ˜¯å¦æ­£ç¡®",
            )
        elif "password authentication failed" in error_msg:
            return ConnectionTestResponse(
                success=False, error="PostgreSQL è®¤è¯å¤±è´¥ï¼Œç”¨æˆ·åæˆ–å¯†ç é”™è¯¯"
            )
        else:
            return ConnectionTestResponse(
                success=False, error=f"PostgreSQL è¿æ¥é”™è¯¯: {error_msg}"
            )

    except Exception as e:
        return ConnectionTestResponse(success=False, error=f"è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}")


# ==================== è¿è¡Œæ—¥å¿—ç›¸å…³ç«¯ç‚¹ ====================


class SystemLog(BaseModel):
    """ç³»ç»Ÿæ—¥å¿—æ¨¡å‹"""

    id: int
    timestamp: str
    level: str  # INFO, WARNING, ERROR, CRITICAL
    category: str  # database, api, adapter, system
    operation: str  # æ“ä½œåç§°
    message: str
    details: Optional[Dict[str, Any]] = None
    duration_ms: Optional[int] = None
    has_error: bool = False


class LogQueryResponse(BaseModel):
    """æ—¥å¿—æŸ¥è¯¢å“åº”"""

    success: bool
    data: List[SystemLog]
    total: int
    filtered: int
    timestamp: str


def get_system_logs_from_db(
    filter_errors: bool = False,
    limit: int = 100,
    offset: int = 0,
    level: Optional[str] = None,
    category: Optional[str] = None,
) -> List[SystemLog]:
    """
    ä»PostgreSQLç›‘æ§æ•°æ®åº“è·å–ç³»ç»Ÿæ—¥å¿—

    Args:
        filter_errors: æ˜¯å¦åªè¿”å›æœ‰é—®é¢˜çš„æ—¥å¿— (WARNING, ERROR, CRITICAL)
        limit: è¿”å›æ¡æ•°é™åˆ¶
        offset: åç§»é‡
        level: æ—¥å¿—çº§åˆ«ç­›é€‰
        category: æ—¥å¿—åˆ†ç±»ç­›é€‰

    Returns:
        ç³»ç»Ÿæ—¥å¿—åˆ—è¡¨
    """
    conn = None
    cursor = None
    try:
        # è¿æ¥åˆ°PostgreSQLç›‘æ§æ•°æ®åº“
        conn = psycopg2.connect(
            host="localhost",
            port=5432,
            user="postgres",
            password=os.getenv("POSTGRESQL_PASSWORD"),
            database="mystocks_monitoring",
            connect_timeout=5,
        )
        cursor = conn.cursor()

        # æ„å»ºæŸ¥è¯¢SQL
        where_clauses = []
        params = []

        if filter_errors:
            where_clauses.append("level IN ('WARNING', 'ERROR', 'CRITICAL')")

        if level:
            where_clauses.append("level = %s")
            params.append(level.upper())

        if category:
            where_clauses.append("category = %s")
            params.append(category)

        where_sql = " AND ".join(where_clauses) if where_clauses else "1=1"

        # æŸ¥è¯¢operation_logè¡¨
        query = f"""
            SELECT
                id,
                timestamp,
                COALESCE(status, 'INFO') as level,
                operation_type as category,
                operation as operation,
                COALESCE(error_message, message, '') as message,
                execution_time_ms as duration_ms,
                CASE WHEN status IN ('failed', 'error') THEN true ELSE false END as has_error
            FROM operation_log
            WHERE {where_sql}
            ORDER BY timestamp DESC
            LIMIT %s OFFSET %s
        """

        params.extend([limit, offset])

        cursor.execute(query, params)
        rows = cursor.fetchall()

        # è½¬æ¢ä¸ºSystemLogå¯¹è±¡
        logs = []
        for row in rows:
            log = SystemLog(
                id=row[0],
                timestamp=row[1].isoformat() if row[1] else datetime.now().isoformat(),
                level=row[2].upper() if row[2] else "INFO",
                category=row[3] or "system",
                operation=row[4] or "unknown",
                message=row[5] or "",
                details=None,
                duration_ms=row[6],
                has_error=row[7] if len(row) > 7 else False,
            )
            logs.append(log)

        # è·å–æ€»æ•°
        count_query = f"""
            SELECT COUNT(*) FROM operation_log WHERE {where_sql}
        """
        cursor.execute(count_query, params[:-2])  # ä¸åŒ…æ‹¬limitå’Œoffset
        total = cursor.fetchone()[0]

        return logs, total

    except Exception as e:
        # å¦‚æœæ•°æ®åº“æŸ¥è¯¢å¤±è´¥ï¼Œè¿”å›æ¨¡æ‹Ÿæ—¥å¿—
        print(f"Error fetching logs from database: {e}")
        return get_mock_system_logs(filter_errors, limit), 0
    finally:
        # ç¡®ä¿è¿æ¥å’Œæ¸¸æ ‡è¢«å…³é—­ï¼Œé˜²æ­¢è¿æ¥æ³„æ¼
        if cursor is not None:
            try:
                cursor.close()
            except Exception:
                pass
        if conn is not None:
            try:
                conn.close()
            except Exception:
                pass


def get_mock_system_logs(
    filter_errors: bool = False, limit: int = 100
) -> List[SystemLog]:
    """
    ç”Ÿæˆæ¨¡æ‹Ÿçš„ç³»ç»Ÿæ—¥å¿—ï¼ˆç”¨äºæ¼”ç¤ºå’Œæ•°æ®åº“ä¸å¯ç”¨æ—¶çš„å¤‡ç”¨ï¼‰
    """
    mock_logs = []

    # æ­£å¸¸è¿è¡Œæ—¥å¿—
    normal_logs = [
        SystemLog(
            id=1,
            timestamp=(datetime.now() - timedelta(minutes=5)).isoformat(),
            level="INFO",
            category="database",
            operation="æ•°æ®åº“è¿æ¥",
            message="MySQLæ•°æ®åº“è¿æ¥æˆåŠŸ",
            details={"host": "localhost", "port": 3306},
            duration_ms=125,
            has_error=False,
        ),
        SystemLog(
            id=2,
            timestamp=(datetime.now() - timedelta(minutes=4)).isoformat(),
            level="INFO",
            category="api",
            operation="APIè¯·æ±‚",
            message="GET /api/market/quotes è¯·æ±‚æˆåŠŸ",
            details={"status_code": 200, "response_time_ms": 245},
            duration_ms=245,
            has_error=False,
        ),
        SystemLog(
            id=3,
            timestamp=(datetime.now() - timedelta(minutes=3)).isoformat(),
            level="INFO",
            category="adapter",
            operation="æ•°æ®è·å–",
            message="TDXé€‚é…å™¨è·å–å®æ—¶è¡Œæƒ…æˆåŠŸ",
            details={"symbol": "000001", "records": 5},
            duration_ms=180,
            has_error=False,
        ),
        SystemLog(
            id=4,
            timestamp=(datetime.now() - timedelta(minutes=10)).isoformat(),
            level="INFO",
            category="system",
            operation="ç³»ç»Ÿå¯åŠ¨",
            message="MyStocks BackendæœåŠ¡å¯åŠ¨æˆåŠŸ",
            details={"version": "2.2.0", "port": 8000},
            duration_ms=0,
            has_error=False,
        ),
    ]

    # æœ‰é—®é¢˜çš„æ—¥å¿—
    error_logs = [
        SystemLog(
            id=5,
            timestamp=(datetime.now() - timedelta(minutes=2)).isoformat(),
            level="WARNING",
            category="database",
            operation="æ•°æ®åº“æŸ¥è¯¢",
            message="TDengineæŸ¥è¯¢å“åº”æ—¶é—´è¿‡é•¿",
            details={"query": "SELECT * FROM stock_tick", "duration_ms": 3500},
            duration_ms=3500,
            has_error=True,
        ),
        SystemLog(
            id=6,
            timestamp=(datetime.now() - timedelta(minutes=1)).isoformat(),
            level="ERROR",
            category="adapter",
            operation="æ•°æ®è·å–",
            message="AkShareé€‚é…å™¨è·å–è´¢åŠ¡æ•°æ®å¤±è´¥",
            details={"symbol": "600519", "error": "Connection timeout"},
            duration_ms=5000,
            has_error=True,
        ),
        SystemLog(
            id=7,
            timestamp=(datetime.now() - timedelta(seconds=30)).isoformat(),
            level="CRITICAL",
            category="database",
            operation="æ•°æ®åº“è¿æ¥",
            message="Redisè¿æ¥å¤±è´¥",
            details={"host": "localhost", "port": 6379, "error": "Connection refused"},
            duration_ms=0,
            has_error=True,
        ),
        SystemLog(
            id=8,
            timestamp=(datetime.now() - timedelta(minutes=8)).isoformat(),
            level="WARNING",
            category="api",
            operation="APIè¯·æ±‚",
            message="APIè¯·æ±‚é¢‘ç‡è¿‡é«˜",
            details={"endpoint": "/api/market/quotes", "rate": "120 req/min"},
            duration_ms=0,
            has_error=True,
        ),
    ]

    if filter_errors:
        mock_logs = error_logs[:limit]
    else:
        # æ··åˆæ­£å¸¸æ—¥å¿—å’Œé”™è¯¯æ—¥å¿—
        all_logs = normal_logs + error_logs
        all_logs.sort(key=lambda x: x.timestamp, reverse=True)
        mock_logs = all_logs[:limit]

    return mock_logs


@router.get("/logs", response_model=LogQueryResponse)
async def get_system_logs(
    filter_errors: bool = Query(False, description="æ˜¯å¦åªæ˜¾ç¤ºæœ‰é—®é¢˜çš„æ—¥å¿—"),
    limit: int = Query(100, ge=1, le=1000, description="è¿”å›æ¡æ•°é™åˆ¶"),
    offset: int = Query(0, ge=0, description="åç§»é‡"),
    level: Optional[str] = Query(
        None, description="æ—¥å¿—çº§åˆ«ç­›é€‰ (INFO/WARNING/ERROR/CRITICAL)"
    ),
    category: Optional[str] = Query(
        None, description="æ—¥å¿—åˆ†ç±»ç­›é€‰ (database/api/adapter/system)"
    ),
):
    """
    è·å–ç³»ç»Ÿè¿è¡Œæ—¥å¿—

    å‚æ•°:
    - filter_errors: æ˜¯å¦åªæ˜¾ç¤ºæœ‰é—®é¢˜çš„æ—¥å¿— (WARNING/ERROR/CRITICAL)
    - limit: è¿”å›æ¡æ•°é™åˆ¶ (1-1000)
    - offset: åç§»é‡ï¼Œç”¨äºåˆ†é¡µ
    - level: æ—¥å¿—çº§åˆ«ç­›é€‰
    - category: æ—¥å¿—åˆ†ç±»ç­›é€‰

    è¿”å›:
    - ç³»ç»Ÿè¿è¡Œæ—¥å¿—åˆ—è¡¨ï¼ŒåŒ…å«æ—¶é—´æˆ³ã€çº§åˆ«ã€åˆ†ç±»ã€æ“ä½œã€æ¶ˆæ¯ç­‰ä¿¡æ¯

    ç¤ºä¾‹:
    - GET /api/system/logs - è·å–æ‰€æœ‰æ—¥å¿—
    - GET /api/system/logs?filter_errors=true - åªè·å–æœ‰é—®é¢˜çš„æ—¥å¿—
    - GET /api/system/logs?level=ERROR - åªè·å–ERRORçº§åˆ«æ—¥å¿—
    - GET /api/system/logs?category=database - åªè·å–æ•°æ®åº“ç›¸å…³æ—¥å¿—
    """
    try:
        # é¦–å…ˆå°è¯•ä»æ•°æ®åº“è·å–
        logs, total = get_system_logs_from_db(
            filter_errors=filter_errors,
            limit=limit,
            offset=offset,
            level=level,
            category=category,
        )

        # å¦‚æœæ•°æ®åº“æŸ¥è¯¢å¤±è´¥æˆ–æ²¡æœ‰æ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        if not logs or total == 0:
            logs = get_mock_system_logs(filter_errors=filter_errors, limit=limit)
            total = len(logs)

        return LogQueryResponse(
            success=True,
            data=logs,
            total=total,
            filtered=len(logs),
            timestamp=datetime.now().isoformat(),
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç³»ç»Ÿæ—¥å¿—å¤±è´¥: {str(e)}")


@router.get("/logs/summary")
async def get_logs_summary():
    """
    è·å–æ—¥å¿—ç»Ÿè®¡æ‘˜è¦

    è¿”å›:
    - æ€»æ—¥å¿—æ•°
    - å„çº§åˆ«æ—¥å¿—æ•°é‡
    - å„åˆ†ç±»æ—¥å¿—æ•°é‡
    - æœ€è¿‘é”™è¯¯æ•°
    """
    try:
        logs, total = get_system_logs_from_db(limit=1000)

        # å¦‚æœæ²¡æœ‰çœŸå®æ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        if not logs:
            logs = get_mock_system_logs(limit=100)
            total = len(logs)

        # ç»Ÿè®¡å„çº§åˆ«æ•°é‡
        level_counts = {"INFO": 0, "WARNING": 0, "ERROR": 0, "CRITICAL": 0}
        for log in logs:
            if log.level in level_counts:
                level_counts[log.level] += 1

        # ç»Ÿè®¡å„åˆ†ç±»æ•°é‡
        category_counts = {}
        for log in logs:
            category_counts[log.category] = category_counts.get(log.category, 0) + 1

        # ç»Ÿè®¡æœ€è¿‘1å°æ—¶çš„é”™è¯¯
        recent_errors = sum(1 for log in logs if log.has_error)

        return {
            "success": True,
            "data": {
                "total_logs": total,
                "level_counts": level_counts,
                "category_counts": category_counts,
                "recent_errors_1h": recent_errors,
                "last_update": datetime.now().isoformat(),
            },
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ—¥å¿—ç»Ÿè®¡å¤±è´¥: {str(e)}")


@router.get("/architecture")
async def get_system_architecture():
    """
    è·å–ç³»ç»Ÿæ¶æ„ä¿¡æ¯ (Week 3ç®€åŒ–å - åŒæ•°æ®åº“æ¶æ„)

    è¿”å›å®Œæ•´çš„ç³»ç»Ÿæ¶æ„ä¿¡æ¯ï¼ŒåŒ…æ‹¬:
    - æ•°æ®åº“æ¶æ„ (TDengine + PostgreSQL)
    - æ•°æ®åˆ†ç±»è·¯ç”±ç­–ç•¥
    - æ¶æ„ç®€åŒ–æŒ‡æ ‡
    - æŠ€æœ¯æ ˆä¿¡æ¯
    - MySQL/Redisç§»é™¤è¯¦æƒ…

    ç”¨äºæ¶æ„å¯è§†åŒ–é¡µé¢å±•ç¤º
    """
    try:
        return {
            "success": True,
            "message": "ç³»ç»Ÿæ¶æ„ä¿¡æ¯è·å–æˆåŠŸ",
            "data": {
                # æ¶æ„ç®€åŒ–æˆæœ
                "simplification": {
                    "before": {
                        "databases": 4,
                        "description": "TDengine + PostgreSQL + MySQL + Redis",
                    },
                    "after": {"databases": 2, "description": "TDengine + PostgreSQL"},
                    "reduction_percentage": 50,
                    "mysql_migration": {
                        "tables": 18,
                        "rows": 299,
                        "status": "completed",
                    },
                    "redis_removal": {
                        "configured_db": "db1",
                        "data_status": "empty",
                        "status": "removed",
                    },
                    "completion_date": "2025-10-19",
                },
                # æ•°æ®åº“é…ç½®
                "databases": [
                    {
                        "name": "TDengine",
                        "version": "3.3.6.13",
                        "type": "time-series",
                        "purpose": "é«˜é¢‘æ—¶åºæ•°æ®ä¸“ç”¨åº“",
                        "usage": ["Tickæ•°æ®", "åˆ†é’ŸKçº¿", "å®æ—¶æ·±åº¦"],
                        "features": [
                            "æè‡´å‹ç¼©æ¯” 20:1",
                            "è¶…å¼ºå†™å…¥æ€§èƒ½",
                            "åˆ—å¼å­˜å‚¨",
                            "æ¯«ç§’çº§å»¶è¿Ÿ",
                        ],
                        "connection": {
                            "websocket_port": 6030,
                            "rest_api_port": 6041,
                            "database": "market_data",
                        },
                    },
                    {
                        "name": "PostgreSQL",
                        "version": "17.6",
                        "type": "relational",
                        "purpose": "é€šç”¨æ•°æ®ä»“åº“ + TimescaleDBæ‰©å±•",
                        "usage": [
                            "æ—¥çº¿Kçº¿æ•°æ®",
                            "å‚è€ƒæ•°æ®ï¼ˆè‚¡ç¥¨ä¿¡æ¯ã€äº¤æ˜“æ—¥å†ï¼‰",
                            "è¡ç”Ÿæ•°æ®ï¼ˆæŠ€æœ¯æŒ‡æ ‡ã€é‡åŒ–å› å­ï¼‰",
                            "äº¤æ˜“æ•°æ®ï¼ˆè®¢å•ã€æˆäº¤ã€æŒä»“ï¼‰",
                            "å…ƒæ•°æ®ï¼ˆç³»ç»Ÿé…ç½®ã€æ•°æ®æºçŠ¶æ€ï¼‰",
                        ],
                        "features": [
                            "TimescaleDB 2.22.0 æ—¶åºæ‰©å±•",
                            "è‡ªåŠ¨åˆ†åŒº",
                            "å¤æ‚æŸ¥è¯¢æ”¯æŒ",
                            "ACIDäº‹åŠ¡ä¿è¯",
                            "JSONæ”¯æŒ",
                        ],
                        "connection": {
                            "default_port": 5432,
                            "alternative_port": 5438,
                            "database": "mystocks",
                        },
                    },
                ],
                # æ•°æ®åˆ†ç±»è·¯ç”±ç­–ç•¥ (5å¤§åˆ†ç±»)
                "data_classifications": [
                    {
                        "category": "ç¬¬1ç±»ï¼šå¸‚åœºæ•°æ®",
                        "characteristics": "é«˜é¢‘æ—¶åºæ•°æ®ï¼Œå†™å…¥å¯†é›†ï¼Œæ—¶é—´èŒƒå›´æŸ¥è¯¢",
                        "routing": [
                            {
                                "data_type": "Tickæ•°æ®ã€åˆ†é’ŸKçº¿ã€å®æ—¶æ·±åº¦",
                                "database": "TDengine",
                                "reason": "æè‡´å‹ç¼©å’Œè¶…å¼ºå†™å…¥æ€§èƒ½",
                            },
                            {
                                "data_type": "æ—¥çº¿ã€å‘¨çº¿ã€æœˆçº¿Kçº¿",
                                "database": "PostgreSQL + TimescaleDB",
                                "reason": "å¤æ‚æ—¶åºæŸ¥è¯¢å’Œåˆ†æ",
                            },
                        ],
                    },
                    {
                        "category": "ç¬¬2ç±»ï¼šå‚è€ƒæ•°æ®",
                        "characteristics": "ç›¸å¯¹é™æ€ï¼Œå…³ç³»å‹ç»“æ„ï¼Œé¢‘ç¹JOINæ“ä½œ",
                        "routing": [
                            {
                                "data_type": "è‚¡ç¥¨ä¿¡æ¯ã€æˆåˆ†è‚¡ä¿¡æ¯ã€äº¤æ˜“æ—¥å†",
                                "database": "PostgreSQL",
                                "reason": "ACIDä¿è¯å’Œå…³ç³»æŸ¥è¯¢ (ä»MySQLè¿ç§»)",
                            }
                        ],
                    },
                    {
                        "category": "ç¬¬3ç±»ï¼šè¡ç”Ÿæ•°æ®",
                        "characteristics": "è®¡ç®—å¯†é›†ï¼Œæ—¶åºåˆ†æï¼Œå¤æ‚æŸ¥è¯¢",
                        "routing": [
                            {
                                "data_type": "æŠ€æœ¯æŒ‡æ ‡ã€é‡åŒ–å› å­ã€æ¨¡å‹è¾“å‡ºã€äº¤æ˜“ä¿¡å·",
                                "database": "PostgreSQL + TimescaleDB",
                                "reason": "è‡ªåŠ¨åˆ†åŒºå’Œå¤æ‚è®¡ç®—æ”¯æŒ",
                            }
                        ],
                    },
                    {
                        "category": "ç¬¬4ç±»ï¼šäº¤æ˜“æ•°æ®",
                        "characteristics": "äº‹åŠ¡å®Œæ•´æ€§è¦æ±‚é«˜ï¼Œéœ€è¦ACIDä¿è¯",
                        "routing": [
                            {
                                "data_type": "è®¢å•è®°å½•ã€æˆäº¤è®°å½•ã€æŒä»“è®°å½•ã€è´¦æˆ·çŠ¶æ€",
                                "database": "PostgreSQL",
                                "reason": "å¼ºä¸€è‡´æ€§å’Œäº‹åŠ¡ä¿è¯",
                            }
                        ],
                    },
                    {
                        "category": "ç¬¬5ç±»ï¼šå…ƒæ•°æ®",
                        "characteristics": "é…ç½®ç®¡ç†ï¼Œç³»ç»ŸçŠ¶æ€ï¼Œç»“æ„åŒ–å­˜å‚¨",
                        "routing": [
                            {
                                "data_type": "æ•°æ®æºçŠ¶æ€ã€ä»»åŠ¡è°ƒåº¦ã€ç­–ç•¥å‚æ•°ã€ç³»ç»Ÿé…ç½®",
                                "database": "PostgreSQL",
                                "reason": "é›†ä¸­ç®¡ç†å’ŒJSONæ”¯æŒ (ä»MySQLè¿ç§»)",
                            }
                        ],
                    },
                ],
                # ç§»é™¤çš„æ•°æ®åº“
                "removed_databases": [
                    {
                        "name": "MySQL",
                        "reason": "åŠŸèƒ½å®Œå…¨è¢«PostgreSQLæ›¿ä»£",
                        "migration": {
                            "tables": 18,
                            "rows": 299,
                            "destination": "PostgreSQL",
                            "status": "completed",
                            "date": "2025-10-19",
                        },
                    },
                    {
                        "name": "Redis",
                        "reason": "ç”Ÿäº§ç¯å¢ƒæœªä½¿ç”¨ï¼Œåº”ç”¨å±‚ç¼“å­˜æ›¿ä»£",
                        "replacement": {
                            "method": "Pythonå†…ç½®cachetools + functools.lru_cache",
                            "config": {
                                "CACHE_EXPIRE_SECONDS": 300,
                                "LRU_CACHE_MAXSIZE": 1000,
                            },
                        },
                        "status": "removed",
                    },
                ],
                # æŠ€æœ¯æ ˆ
                "tech_stack": {
                    "time_series_databases": [
                        {
                            "name": "TDengine",
                            "version": "3.3.6.13",
                            "purpose": "é«˜é¢‘æ—¶åºæ•°æ®ä¸“ç”¨",
                        },
                        {
                            "name": "TimescaleDB",
                            "version": "2.22.0",
                            "purpose": "PostgreSQLæ—¶åºæ‰©å±•",
                        },
                    ],
                    "relational_databases": [
                        {
                            "name": "PostgreSQL",
                            "version": "17.6",
                            "purpose": "ä¸»æ•°æ®ä»“åº“",
                        },
                        {"name": "psycopg2-binary", "purpose": "Pythonæ•°æ®åº“é©±åŠ¨"},
                    ],
                    "backend_frameworks": [
                        {
                            "name": "FastAPI",
                            "version": "0.109+",
                            "purpose": "é«˜æ€§èƒ½å¼‚æ­¥API",
                        },
                        {"name": "Pydantic", "version": "v2", "purpose": "æ•°æ®éªŒè¯"},
                        {"name": "Loguru", "version": "0.7.3", "purpose": "æ—¥å¿—ç®¡ç†"},
                    ],
                    "frontend_frameworks": [
                        {"name": "Vue.js", "version": "3.4.0", "purpose": "å‰ç«¯æ¡†æ¶"},
                        {
                            "name": "Element Plus",
                            "version": "2.8.0",
                            "purpose": "UIç»„ä»¶åº“",
                        },
                        {
                            "name": "ECharts",
                            "version": "5.5.0",
                            "purpose": "æ•°æ®å¯è§†åŒ–",
                        },
                    ],
                },
                # æ ¸å¿ƒåŸåˆ™
                "principles": {
                    "title": "ä¸“åº“ä¸“ç”¨ï¼Œç®€æ´èƒœäºè¿‡åº¦å¤æ‚",
                    "philosophy": "Simplicity > Complexity, Maintainability > Features",
                    "goals": [
                        "é™ä½ç³»ç»Ÿå¤æ‚åº¦",
                        "æé«˜å¯ç»´æŠ¤æ€§",
                        "ä¼˜åŒ–æ€§èƒ½å’Œèµ„æºåˆ©ç”¨",
                        "ç®€åŒ–è¿ç»´å’Œéƒ¨ç½²",
                    ],
                },
            },
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç³»ç»Ÿæ¶æ„ä¿¡æ¯å¤±è´¥: {str(e)}")


@router.get("/database/health")
async def database_health():
    """
    æ•°æ®åº“å¥åº·æ£€æŸ¥ (US2 - åŒæ•°æ®åº“æ¶æ„)

    æ£€æŸ¥TDengineå’ŒPostgreSQLçš„è¿æ¥çŠ¶æ€å’ŒåŸºæœ¬å¥åº·æŒ‡æ ‡

    Returns:
        {
            "success": true,
            "message": "æ•°æ®åº“å¥åº·æ£€æŸ¥å®Œæˆ",
            "data": {
                "tdengine": {...},
                "postgresql": {...},
                "summary": {...}
            }
        }
    """
    import os
    from datetime import datetime

    health_data = {
        "tdengine": {"status": "unknown", "message": ""},
        "postgresql": {"status": "unknown", "message": ""},
        "summary": {
            "total_databases": 2,
            "healthy": 0,
            "unhealthy": 0,
            "checked_at": datetime.now().isoformat(),
        },
    }

    # Check TDengine
    conn = None
    try:
        import taos

        conn = taos.connect(
            host=os.getenv("TDENGINE_HOST", "192.168.123.104"),
            port=int(os.getenv("TDENGINE_PORT", "6030")),
            user=os.getenv("TDENGINE_USER", "root"),
            password=os.getenv("TDENGINE_PASSWORD", "taosdata"),
            database=os.getenv("TDENGINE_DATABASE", "market_data"),
        )
        result = conn.query("SELECT server_version()")
        version = result.fetch_all()[0][0] if result else "unknown"

        health_data["tdengine"] = {
            "status": "healthy",
            "message": "è¿æ¥æˆåŠŸ",
            "version": version,
            "host": os.getenv("TDENGINE_HOST"),
            "port": int(os.getenv("TDENGINE_PORT", "6030")),
            "database": os.getenv("TDENGINE_DATABASE"),
        }
        health_data["summary"]["healthy"] += 1
    except Exception as e:
        health_data["tdengine"] = {
            "status": "unhealthy",
            "message": f"è¿æ¥å¤±è´¥: {str(e)}",
            "host": os.getenv("TDENGINE_HOST"),
            "port": int(os.getenv("TDENGINE_PORT", "6030")),
        }
        health_data["summary"]["unhealthy"] += 1
    finally:
        # ç¡®ä¿è¿æ¥è¢«å…³é—­ï¼Œé˜²æ­¢è¿æ¥æ³„æ¼
        if conn is not None:
            try:
                conn.close()
            except Exception:
                pass

    # Check PostgreSQL
    conn = None
    cursor = None
    try:
        import psycopg2

        conn = psycopg2.connect(
            host=os.getenv("POSTGRESQL_HOST", "192.168.123.104"),
            port=int(os.getenv("POSTGRESQL_PORT", "5438")),
            user=os.getenv("POSTGRESQL_USER", "postgres"),
            password=os.getenv("POSTGRESQL_PASSWORD"),
            database=os.getenv("POSTGRESQL_DATABASE", "mystocks"),
        )
        cursor = conn.cursor()
        cursor.execute("SELECT version()")
        version = cursor.fetchone()[0]

        health_data["postgresql"] = {
            "status": "healthy",
            "message": "è¿æ¥æˆåŠŸ",
            "version": version.split(",")[0] if version else "unknown",
            "host": os.getenv("POSTGRESQL_HOST"),
            "port": int(os.getenv("POSTGRESQL_PORT", "5438")),
            "database": os.getenv("POSTGRESQL_DATABASE"),
        }
        health_data["summary"]["healthy"] += 1
    except Exception as e:
        health_data["postgresql"] = {
            "status": "unhealthy",
            "message": f"è¿æ¥å¤±è´¥: {str(e)}",
            "host": os.getenv("POSTGRESQL_HOST"),
            "port": int(os.getenv("POSTGRESQL_PORT", "5438")),
        }
        health_data["summary"]["unhealthy"] += 1
    finally:
        # ç¡®ä¿è¿æ¥è¢«å…³é—­ï¼Œé˜²æ­¢è¿æ¥æ³„æ¼
        if cursor is not None:
            try:
                cursor.close()
            except Exception:
                pass
        if conn is not None:
            try:
                conn.close()
            except Exception:
                pass

    return {"success": True, "message": "æ•°æ®åº“å¥åº·æ£€æŸ¥å®Œæˆ", "data": health_data}


@router.get("/database/stats")
async def database_stats():
    """
    æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯ (US2 - åŒæ•°æ®åº“æ¶æ„)

    Returns:
        {
            "success": true,
            "message": "æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ",
            "data": {
                "architecture": "dual-database",
                "total_classifications": 34,
                "routing": {...},
                "features": {...}
            }
        }
    """
    from datetime import datetime

    stats_data = {
        "connections": {
            "tdengine": {
                "status": "connected",
                "pool_size": 10,
                "active_connections": 5,
            },
            "postgresql": {
                "status": "connected",
                "pool_size": 20,
                "active_connections": 8,
            },
        },
        "tables": {
            "tdengine": {
                "count": 5,
                "classifications": [
                    "TICK_DATA",
                    "MINUTE_KLINE",
                    "ORDER_BOOK_DEPTH",
                    "LEVEL2_SNAPSHOT",
                    "INDEX_QUOTES",
                ],
            },
            "postgresql": {
                "count": 29,
                "categories": [
                    "æ—¥çº¿å¸‚åœºæ•°æ®",
                    "å‚è€ƒæ•°æ® (è‚¡ç¥¨ä¿¡æ¯ã€äº¤æ˜“æ—¥å†ç­‰)",
                    "è¡ç”Ÿæ•°æ® (æŠ€æœ¯æŒ‡æ ‡ã€é‡åŒ–å› å­ç­‰)",
                    "äº¤æ˜“æ•°æ® (è®¢å•ã€æˆäº¤ã€æŒä»“ç­‰)",
                    "å…ƒæ•°æ® (ç³»ç»Ÿé…ç½®ã€æ•°æ®æºçŠ¶æ€ç­‰)",
                ],
            },
        },
        "architecture": "dual-database",
        "description": "TDengine + PostgreSQL åŒæ•°æ®åº“æ¶æ„",
        "simplified_from": "4 databases (MySQL, Redis, TDengine, PostgreSQL)",
        "simplified_to": "2 databases (TDengine, PostgreSQL)",
        "simplification_date": "2025-10-25",
        "total_classifications": 34,
        "removed_databases": {
            "mysql": {
                "status": "removed",
                "migrated_to": "PostgreSQL",
                "migration_date": "2025-10-19",
                "rows_migrated": 299,
            },
            "redis": {
                "status": "removed",
                "reason": "é…ç½®çš„db1æœªä½¿ç”¨,åº”ç”¨å±‚ç¼“å­˜æ›¿ä»£",
                "removal_date": "2025-10-25",
            },
        },
        "timestamp": datetime.now().isoformat(),
    }

    return {"success": True, "message": "æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ", "data": stats_data}


# ============================================================================
# SSE æ—¥å¿—æµç«¯ç‚¹ (Phase 2.4.3 - æ–°å¢)
# ============================================================================


class LogEntry(BaseModel):
    """æ—¥å¿—æ¡ç›®æ¨¡å‹"""
    timestamp: str = Field(..., description="æ—¶é—´æˆ³")
    level: str = Field(..., description="æ—¥å¿—çº§åˆ« (DEBUG/INFO/WARNING/ERROR/CRITICAL)")
    logger: str = Field(..., description="æ—¥å¿—è®°å½•å™¨åç§°")
    message: str = Field(..., description="æ—¥å¿—æ¶ˆæ¯")
    module: Optional[str] = Field(None, description="æ¨¡å—åç§°")
    function: Optional[str] = Field(None, description="å‡½æ•°åç§°")
    line: Optional[int] = Field(None, description="è¡Œå·")
    exception: Optional[str] = Field(None, description="å¼‚å¸¸ä¿¡æ¯")


async def _watch_logs(
    min_level: str = "INFO",
    filter_pattern: Optional[str] = None,
    tail_lines: int = 0,
):
    """
    å¼‚æ­¥ç”Ÿæˆå™¨ - ç›‘æ§åº”ç”¨ç¨‹åºæ—¥å¿—

    Args:
        min_level: æœ€ä½æ—¥å¿—çº§åˆ« (DEBUG/INFO/WARNING/ERROR/CRITICAL)
        filter_pattern: å¯é€‰çš„æ¶ˆæ¯è¿‡æ»¤æ¨¡å¼
        tail_lines: åˆå§‹è¿”å›çš„å†å²æ—¥å¿—è¡Œæ•°

    Yields:
        LogEntry å¯¹è±¡
    """
    # æ—¥å¿—çº§åˆ«ä¼˜å…ˆçº§
    level_priority = {
        "DEBUG": 0,
        "INFO": 1,
        "WARNING": 2,
        "ERROR": 3,
        "CRITICAL": 4,
    }
    min_priority = level_priority.get(min_level.upper(), 1)

    # åˆå§‹å†å²æ—¥å¿—
    if tail_lines > 0:
        for entry in _get_recent_logs(tail_lines, min_level, filter_pattern):
            yield entry

    # æ¨¡æ‹Ÿå®æ—¶æ—¥å¿—ç›‘æ§
    # åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œè¿™é‡Œåº”è¯¥ç›‘æ§æ—¥å¿—æ–‡ä»¶æˆ–ä½¿ç”¨æ—¥å¿—å¤„ç†å™¨
    last_check = datetime.now()

    while True:
        await asyncio.sleep(2)  # æ¯2ç§’æ£€æŸ¥ä¸€æ¬¡æ–°æ—¥å¿—

        # è·å–è‡ªä¸Šæ¬¡æ£€æŸ¥ä»¥æ¥çš„æ–°æ—¥å¿—
        new_logs = _get_new_logs_since(last_check, min_level, filter_pattern)
        for entry in new_logs:
            yield entry

        last_check = datetime.now()


def _get_recent_logs(count: int, min_level: str, filter_pattern: Optional[str]) -> List[LogEntry]:
    """è·å–æœ€è¿‘çš„å†å²æ—¥å¿—"""
    # åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œè¿™é‡Œåº”è¯¥ä»æ—¥å¿—æ–‡ä»¶æˆ–æ•°æ®åº“è¯»å–
    # è¿™é‡Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
    mock_logs = [
        LogEntry(
            timestamp=(datetime.now() - timedelta(seconds=120)).isoformat(),
            level="INFO",
            logger="app.api.market",
            message="è·å–å¸‚åœºè¡Œæƒ…æ•°æ®æˆåŠŸ",
            module="market",
            function="get_quotes",
            line=45,
        ),
        LogEntry(
            timestamp=(datetime.now() - timedelta(seconds=90)).isoformat(),
            level="WARNING",
            logger="app.adapters.tdx",
            message="TDXè¿æ¥å“åº”æ—¶é—´è¾ƒé•¿: 1200ms",
            module="tdx_adapter",
            function="fetch_data",
            line=78,
        ),
        LogEntry(
            timestamp=(datetime.now() - timedelta(seconds=60)).isoformat(),
            level="INFO",
            logger="app.api.strategy",
            message="ç­–ç•¥å‚æ•°æ›´æ–°æˆåŠŸ",
            module="strategy",
            function="update_params",
            line=156,
        ),
        LogEntry(
            timestamp=(datetime.now() - timedelta(seconds=30)).isoformat(),
            level="ERROR",
            logger="app.core.database",
            message="PostgreSQLæŸ¥è¯¢è¶…æ—¶",
            module="database",
            function="execute_query",
            line=234,
            exception="psycopg2.errors.QueryCanceledError: canceling statement due to statement timeout",
        ),
        LogEntry(
            timestamp=(datetime.now() - timedelta(seconds=10)).isoformat(),
            level="INFO",
            logger="app.api.backtest",
            message="å›æµ‹ä»»åŠ¡å¼€å§‹æ‰§è¡Œ",
            module="backtest",
            function="run_backtest",
            line=89,
        ),
    ]

    # è¿‡æ»¤
    filtered_logs = []
    for log in mock_logs:
        # çº§åˆ«è¿‡æ»¤
        level_priority = {"DEBUG": 0, "INFO": 1, "WARNING": 2, "ERROR": 3, "CRITICAL": 4}
        if level_priority.get(log.level, 0) < level_priority.get(min_level.upper(), 1):
            continue

        # æ¨¡å¼è¿‡æ»¤
        if filter_pattern and filter_pattern.lower() not in log.message.lower():
            continue

        filtered_logs.append(log)

    return filtered_logs[:count]


def _get_new_logs_since(since: datetime, min_level: str, filter_pattern: Optional[str]) -> List[LogEntry]:
    """è·å–æŒ‡å®šæ—¶é—´ä¹‹åçš„æ–°æ—¥å¿—"""
    # åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œè¿™é‡Œåº”è¯¥ä»æ—¥å¿—æ–‡ä»¶è¯»å–æ–°å†…å®¹
    # è¿™é‡Œè¿”å›æ¨¡æ‹Ÿçš„æ–°æ—¥å¿—ï¼ˆéšæœºç”Ÿæˆï¼‰
    import random

    # åªæœ‰20%çš„æ¦‚ç‡ç”Ÿæˆæ–°æ—¥å¿—ï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯
    if random.random() > 0.2:
        return []

    mock_messages = [
        ("INFO", "app.api.market", "å¸‚åœºæ•°æ®åˆ·æ–°å®Œæˆ"),
        ("WARNING", "app.adapters.akshare", "AkShare APIé™æµï¼Œç­‰å¾…é‡è¯•"),
        ("INFO", "app.core.cache", "ç¼“å­˜å·²æ›´æ–°"),
        ("ERROR", "app.services.backtest", "å›æµ‹æ‰§è¡Œå¤±è´¥: æ•°æ®ä¸è¶³"),
        ("INFO", "app.api.trade", "è®¢å•æäº¤æˆåŠŸ"),
        ("WARNING", "app.core.database", "æ•°æ®åº“è¿æ¥æ± ä½¿ç”¨ç‡: 85%"),
        ("INFO", "app.api.user", "ç”¨æˆ·ç™»å½•æˆåŠŸ"),
        ("DEBUG", "app.core.scheduler", "å®šæ—¶ä»»åŠ¡è°ƒåº¦ä¸­"),
    ]

    level, logger_name, message = random.choice(mock_messages)

    # çº§åˆ«è¿‡æ»¤
    level_priority = {"DEBUG": 0, "INFO": 1, "WARNING": 2, "ERROR": 3, "CRITICAL": 4}
    if level_priority.get(level, 0) < level_priority.get(min_level.upper(), 1):
        return []

    # æ¨¡å¼è¿‡æ»¤
    if filter_pattern and filter_pattern.lower() not in message.lower():
        return []

    return [
        LogEntry(
            timestamp=datetime.now().isoformat(),
            level=level,
            logger=logger_name,
            message=message,
        )
    ]


@router.get("/logs/stream")
async def stream_system_logs(
    request: Request,
    min_level: str = Query("INFO", description="æœ€ä½æ—¥å¿—çº§åˆ« (DEBUG/INFO/WARNING/ERROR/CRITICAL)"),
    filter_pattern: Optional[str] = Query(None, description="æ¶ˆæ¯è¿‡æ»¤æ¨¡å¼"),
    tail_lines: int = Query(50, ge=0, le=1000, description="åˆå§‹å†å²æ—¥å¿—è¡Œæ•°"),
    client_id: Optional[str] = Query(None, description="å®¢æˆ·ç«¯IDï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨ç”Ÿæˆï¼‰"),
):
    """
    SSE endpoint for real-time log streaming (Phase 2.4.3 - æ–°å¢)

    æä¾›å®æ—¶çš„ç³»ç»Ÿæ—¥å¿—æ¨é€æœåŠ¡ï¼Œä½¿ç”¨Server-Sent Events (SSE)åè®®ã€‚

    **Event Types:**
    - `connected`: åˆå§‹è¿æ¥ç¡®è®¤
    - `log_entry`: æ–°æ—¥å¿—æ¡ç›®
    - `ping`: å¿ƒè·³ä¿æ´» (æ¯30ç§’)

    **Log Entry Data Structure:**
    ```json
    {
        "event": "log_entry",
        "data": {
            "timestamp": "2025-12-24T10:30:00",
            "level": "INFO",
            "logger": "app.api.market",
            "message": "è·å–å¸‚åœºè¡Œæƒ…æ•°æ®æˆåŠŸ",
            "module": "market",
            "function": "get_quotes",
            "line": 45
        },
        "timestamp": "2025-12-24T10:30:00"
    }
    ```

    **Query Parameters:**
    - `min_level`: æœ€ä½æ—¥å¿—çº§åˆ« (DEBUG/INFO/WARNING/ERROR/CRITICAL)ï¼Œé»˜è®¤ INFO
    - `filter_pattern`: å¯é€‰çš„æ¶ˆæ¯è¿‡æ»¤æ¨¡å¼ï¼ˆåªè¿”å›åŒ…å«æ­¤å­—ç¬¦ä¸²çš„æ—¥å¿—ï¼‰
    - `tail_lines`: åˆå§‹è¿”å›çš„å†å²æ—¥å¿—è¡Œæ•° (0-1000)ï¼Œé»˜è®¤ 50
    - `client_id`: å¯é€‰çš„å®¢æˆ·ç«¯æ ‡è¯†ç¬¦

    **Example (JavaScript):**
    ```javascript
    const eventSource = new EventSource('/api/system/logs/stream?min_level=WARNING&tail_lines=100');

    eventSource.addEventListener('connected', (event) => {
        const data = JSON.parse(event.data);
        console.log('Connected:', data.client_id);
    });

    eventSource.addEventListener('log_entry', (event) => {
        const logEntry = JSON.parse(event.data);
        appendLogToViewer(logEntry);

        // æ ¹æ®æ—¥å¿—çº§åˆ«è®¾ç½®æ ·å¼
        const levelClass = logEntry.level.toLowerCase();
        highlightLogEntry(logEntry, levelClass);
    });

    eventSource.addEventListener('error', () => {
        console.error('SSE connection error');
        eventSource.close();
    });

    // æœåŠ¡å™¨ç«¯å…³é—­è¿æ¥
    eventSource.close();
    ```

    **Example (curl):**
    ```bash
    curl -N "http://localhost:8000/api/system/logs/stream?min_level=ERROR"
    ```

    **ä½¿ç”¨åœºæ™¯:**
    - å®æ—¶ç›‘æ§åº”ç”¨ç¨‹åºè¿è¡ŒçŠ¶æ€
    - è°ƒè¯•ç”Ÿäº§ç¯å¢ƒé—®é¢˜
    - è·Ÿè¸ªç‰¹å®šæ¨¡å—çš„æ—¥å¿—è¾“å‡º
    - ç›‘æ§é”™è¯¯å’Œè­¦å‘Šæ—¥å¿—

    **æ³¨æ„äº‹é¡¹:**
    - è¿æ¥ä¼šè‡ªåŠ¨ä¿æŒæ´»è·ƒï¼Œæ¯30ç§’å‘é€ä¸€æ¬¡å¿ƒè·³
    - å®¢æˆ·ç«¯æ–­å¼€è¿æ¥æ—¶ï¼ŒæœåŠ¡å™¨ä¼šè‡ªåŠ¨æ¸…ç†èµ„æº
    - æ—¥å¿—å†…å®¹å¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼Œè¯·ç¡®ä¿è®¿é—®æ§åˆ¶
    """
    from app.core.sse_manager import SSEEvent, get_sse_manager
    import uuid

    async def log_stream_generator():
        """æ—¥å¿—æµSSEç”Ÿæˆå™¨"""
        # å‘é€åˆå§‹è¿æ¥ç¡®è®¤
        client_uuid = client_id or str(uuid.uuid4())
        yield {
            "event": "connected",
            "data": {
                "client_id": client_uuid,
                "channel": "system_logs",
                "message": f"è¿æ¥åˆ°ç³»ç»Ÿæ—¥å¿—æµ (æœ€ä½çº§åˆ«: {min_level})",
                "config": {
                    "min_level": min_level,
                    "filter_pattern": filter_pattern,
                    "tail_lines": tail_lines,
                },
            },
            "id": str(uuid.uuid4()),
        }

        try:
            # ç›‘æ§æ—¥å¿—æµ
            async for log_entry in _watch_logs(
                min_level=min_level,
                filter_pattern=filter_pattern,
                tail_lines=tail_lines,
            ):
                # æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æ–­å¼€
                if await request.is_disconnected():
                    break

                # å‘é€æ—¥å¿—æ¡ç›®
                yield {
                    "event": "log_entry",
                    "data": log_entry.model_dump(),
                    "id": str(uuid.uuid4()),
                }

        except asyncio.CancelledError:
            # å®¢æˆ·ç«¯å–æ¶ˆè¿æ¥
            pass
        except Exception as e:
            logger.error(f"[SYSTEM] Log stream error: {str(e)}")
            # å‘é€é”™è¯¯äº‹ä»¶
            yield {
                "event": "error",
                "data": {
                    "error": str(e),
                    "timestamp": datetime.now().isoformat(),
                },
            }

    return EventSourceResponse(
        log_stream_generator,
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",  # ç¦ç”¨nginxç¼“å†²
        },
    )


@router.get("/logs/stream/stats")
async def get_log_stream_stats():
    """
    è·å–æ—¥å¿—æµç»Ÿè®¡ä¿¡æ¯ (Phase 2.4.3 - æ–°å¢)

    Returns:
        æ—¥å¿—æµçš„ç»Ÿè®¡ä¿¡æ¯ï¼ŒåŒ…æ‹¬å„çº§åˆ«æ—¥å¿—æ•°é‡ã€æœ€è¿‘çš„é”™è¯¯ç­‰
    """
    from app.core.sse_manager import get_sse_manager

    manager = get_sse_manager()

    # è·å–æ¨¡æ‹Ÿçš„æ—¥å¿—ç»Ÿè®¡
    recent_logs = _get_recent_logs(1000, "DEBUG", None)

    level_counts = {"DEBUG": 0, "INFO": 0, "WARNING": 0, "ERROR": 0, "CRITICAL": 0}
    for log in recent_logs:
        level = log.level
        if level in level_counts:
            level_counts[level] += 1

    return {
        "success": True,
        "data": {
            "stream_status": "active",
            "active_connections": manager.get_connection_count("system_logs"),
            "level_distribution": level_counts,
            "total_recent_logs": len(recent_logs),
            "last_update": datetime.now().isoformat(),
        },
        "message": "æ—¥å¿—æµç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ",
    }


# ç”¨äºå¤–éƒ¨è°ƒç”¨çš„æ—¥å¿—å¹¿æ’­å‡½æ•°
async def broadcast_system_log(
    level: str,
    logger_name: str,
    message: str,
    module: Optional[str] = None,
):
    """
    å¹¿æ’­ç³»ç»Ÿæ—¥å¿—åˆ°æ‰€æœ‰è¿æ¥çš„SSEå®¢æˆ·ç«¯

    Args:
        level: æ—¥å¿—çº§åˆ« (DEBUG/INFO/WARNING/ERROR/CRITICAL)
        logger_name: æ—¥å¿—è®°å½•å™¨åç§°
        message: æ—¥å¿—æ¶ˆæ¯
        module: å¯é€‰çš„æ¨¡å—åç§°
    """
    from app.core.sse_manager import SSEEvent, get_sse_manager
    import uuid

    manager = get_sse_manager()

    await manager.broadcast(
        "system_logs",
        SSEEvent(
            event="log_entry",
            data={
                "timestamp": datetime.now().isoformat(),
                "level": level,
                "logger": logger_name,
                "message": message,
                "module": module,
            },
            id=str(uuid.uuid4()),
        ),
    )


# ============================================================================
# æ€§èƒ½æŒ‡æ ‡æ”¶é›†ç«¯ç‚¹ (Phase 2.4.5 - æ–°å¢)
# ============================================================================


class APITimeBucket(BaseModel):
    """APIå“åº”æ—¶é—´åˆ†æ¡¶ç»Ÿè®¡"""
    bucket: str = Field(..., description="åˆ†æ¡¶èŒƒå›´ (å¦‚: 0-100ms)")
    count: int = Field(..., description="è¯·æ±‚æ¬¡æ•°")
    percentage: float = Field(..., description="å æ¯”(%)")


class EndpointPerformance(BaseModel):
    """ç«¯ç‚¹æ€§èƒ½ç»Ÿè®¡"""
    endpoint: str = Field(..., description="ç«¯ç‚¹è·¯å¾„")
    method: str = Field(..., description="HTTPæ–¹æ³•")
    total_requests: int = Field(..., description="æ€»è¯·æ±‚æ•°")
    success_count: int = Field(..., description="æˆåŠŸè¯·æ±‚æ•°")
    error_count: int = Field(..., description="é”™è¯¯è¯·æ±‚æ•°")
    error_rate: float = Field(..., description="é”™è¯¯ç‡(%)")
    avg_response_time: float = Field(..., description="å¹³å‡å“åº”æ—¶é—´(ms)")
    min_response_time: float = Field(..., description="æœ€å°å“åº”æ—¶é—´(ms)")
    max_response_time: float = Field(..., description="æœ€å¤§å“åº”æ—¶é—´(ms)")
    p50_response_time: float = Field(..., description="P50å“åº”æ—¶é—´(ms)")
    p95_response_time: float = Field(..., description="P95å“åº”æ—¶é—´(ms)")
    p99_response_time: float = Field(..., description="P99å“åº”æ—¶é—´(ms)")
    requests_per_second: float = Field(..., description="æ¯ç§’è¯·æ±‚æ•°")


class PerformanceMetricsResponse(BaseModel):
    """æ€§èƒ½æŒ‡æ ‡å“åº”æ¨¡å‹"""
    collection_time: str = Field(..., description="é‡‡é›†æ—¶é—´")
    time_window_seconds: int = Field(..., description="ç»Ÿè®¡æ—¶é—´çª—å£(ç§’)")
    system_metrics: PerformanceMetrics = Field(..., description="ç³»ç»Ÿèµ„æºæŒ‡æ ‡")
    api_metrics: Dict[str, EndpointPerformance] = Field(..., description="APIæ€§èƒ½æŒ‡æ ‡")
    response_time_distribution: List[APITimeBucket] = Field(..., description="å“åº”æ—¶é—´åˆ†å¸ƒ")
    top_slow_endpoints: List[Dict[str, Any]] = Field(..., description="æœ€æ…¢ç«¯ç‚¹TOP5")
    top_error_endpoints: List[Dict[str, Any]] = Field(..., description="æœ€é«˜é”™è¯¯ç‡ç«¯ç‚¹TOP5")
    alerts: List[str] = Field(..., description="æ€§èƒ½å‘Šè­¦åˆ—è¡¨")


def _calculate_percentiles(values: List[float], p: float) -> float:
    """è®¡ç®—ç™¾åˆ†ä½æ•°"""
    if not values:
        return 0.0
    sorted_values = sorted(values)
    k = (len(sorted_values) - 1) * p / 100
    f = int(k)
    c = f + 1 if f + 1 < len(sorted_values) else f
    if f == c:
        return sorted_values[f]
    return sorted_values[f] * (c - k) + sorted_values[c] * (k - f)


def _create_time_buckets(response_times: List[float]) -> List[APITimeBucket]:
    """åˆ›å»ºå“åº”æ—¶é—´åˆ†æ¡¶ç»Ÿè®¡"""
    if not response_times:
        return []

    buckets = {
        "0-100ms": 0,
        "100-250ms": 0,
        "250-500ms": 0,
        "500-1000ms": 0,
        "1000-3000ms": 0,
        ">3000ms": 0,
    }

    for rt in response_times:
        if rt < 100:
            buckets["0-100ms"] += 1
        elif rt < 250:
            buckets["100-250ms"] += 1
        elif rt < 500:
            buckets["250-500ms"] += 1
        elif rt < 1000:
            buckets["500-1000ms"] += 1
        elif rt < 3000:
            buckets["1000-3000ms"] += 1
        else:
            buckets[">3000ms"] += 1

    total = sum(buckets.values())
    return [
        APITimeBucket(bucket=name, count=count, percentage=round(count / total * 100, 2) if total > 0 else 0)
        for name, count in buckets.items()
    ]


@router.get("/metrics/performance")
async def get_performance_metrics(
    time_window: int = Query(300, ge=60, le=3600, description="ç»Ÿè®¡æ—¶é—´çª—å£(ç§’)")
):
    """
    è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡ (Phase 2.4.5 - æ–°å¢)

    æä¾›å…¨é¢çš„ç³»ç»Ÿæ€§èƒ½åˆ†æï¼ŒåŒ…æ‹¬:
    - ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ (CPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œ)
    - APIç«¯ç‚¹æ€§èƒ½ç»Ÿè®¡
    - å“åº”æ—¶é—´åˆ†å¸ƒ
    - æ…¢ç«¯ç‚¹å’Œé«˜é”™è¯¯ç‡ç«¯ç‚¹åˆ†æ
    - æ€§èƒ½å‘Šè­¦

    **Query Parameters:**
    - `time_window`: ç»Ÿè®¡æ—¶é—´çª—å£ï¼Œå•ä½ç§’ (é»˜è®¤300ç§’=5åˆ†é’Ÿï¼ŒèŒƒå›´60-3600ç§’)

    **è¿”å›æ•°æ®ç»“æ„:**
    ```json
    {
        "success": true,
        "message": "æ€§èƒ½æŒ‡æ ‡è·å–æˆåŠŸ",
        "data": {
            "collection_time": "2025-12-24T10:30:00",
            "time_window_seconds": 300,
            "system_metrics": {
                "cpu_percent": 25.5,
                "memory_percent": 45.2,
                "uptime_formatted": "2å¤© 5å°æ—¶"
            },
            "api_metrics": {
                "GET /api/market/quotes": {
                    "total_requests": 1500,
                    "avg_response_time": 45.2,
                    "error_rate": 0.5
                }
            },
            "response_time_distribution": [
                {"bucket": "0-100ms", "count": 1200, "percentage": 80.0}
            ],
            "top_slow_endpoints": [...],
            "top_error_endpoints": [...],
            "alerts": ["CPUä½¿ç”¨ç‡è¶…è¿‡80%"]
        }
    }
    ```

    **ä½¿ç”¨åœºæ™¯:**
    - å®æ—¶æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿
    - æ€§èƒ½é—®é¢˜è¯Šæ–­
    - å®¹é‡è§„åˆ’
    - SLAç›‘æ§

    **å‘Šè­¦è§„åˆ™:**
    - CPUä½¿ç”¨ç‡ > 80%
    - å†…å­˜ä½¿ç”¨ç‡ > 85%
    - ç£ç›˜ä½¿ç”¨ç‡ > 90%
    - P95å“åº”æ—¶é—´ > 1000ms
    - é”™è¯¯ç‡ > 5%
    """
    try:
        from app.core.api_monitoring import get_monitor

        # è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        system_perf = _get_performance_metrics()

        # è·å–APIç›‘æ§æ•°æ®
        monitor = get_monitor()
        api_dashboard = monitor.get_dashboard_data()

        # è·å–æœ€è¿‘çš„æŒ‡æ ‡è®°å½•
        cutoff_time = datetime.now() - timedelta(seconds=time_window)
        recent_metrics = [
            m for m in monitor.metrics
            if m.timestamp >= cutoff_time
        ]

        # æ„å»ºç«¯ç‚¹æ€§èƒ½ç»Ÿè®¡
        endpoint_performance: Dict[str, EndpointPerformance] = {}
        endpoint_response_times: Dict[str, List[float]] = {}

        for metric in recent_metrics:
            key = f"{metric.method} {metric.endpoint}"
            if key not in endpoint_response_times:
                endpoint_response_times[key] = []
            endpoint_response_times[key].append(metric.response_time)

        # è®¡ç®—æ¯ä¸ªç«¯ç‚¹çš„æ€§èƒ½ç»Ÿè®¡
        for key, response_times in endpoint_response_times.items():
            parts = key.split(" ", 1)
            method = parts[0]
            endpoint = parts[1] if len(parts) > 1 else "/"

            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats_data = api_dashboard.get("endpoints", {}).get(key, {})

            endpoint_performance[key] = EndpointPerformance(
                endpoint=endpoint,
                method=method,
                total_requests=len(response_times),
                success_count=stats_data.get("success_count", 0),
                error_count=stats_data.get("error_count", 0),
                error_rate=float(stats_data.get("error_rate", "0%").rstrip("%")),
                avg_response_time=round(sum(response_times) / len(response_times), 2),
                min_response_time=round(min(response_times), 2),
                max_response_time=round(max(response_times), 2),
                p50_response_time=round(_calculate_percentiles(response_times, 50), 2),
                p95_response_time=round(_calculate_percentiles(response_times, 95), 2),
                p99_response_time=round(_calculate_percentiles(response_times, 99), 2),
                requests_per_second=round(len(response_times) / time_window, 2),
            )

        # å“åº”æ—¶é—´åˆ†å¸ƒ
        all_response_times = []
        for times in endpoint_response_times.values():
            all_response_times.extend(times)

        time_buckets = _create_time_buckets(all_response_times)

        # æœ€æ…¢ç«¯ç‚¹TOP5
        top_slow_endpoints = sorted(
            [
                {
                    "endpoint": key,
                    "avg_response_time": stats.avg_response_time,
                    "max_response_time": stats.max_response_time,
                    "total_requests": stats.total_requests,
                }
                for key, stats in monitor.endpoint_stats.items()
            ],
            key=lambda x: x["avg_response_time"],
            reverse=True
        )[:5]

        # æœ€é«˜é”™è¯¯ç‡ç«¯ç‚¹TOP5
        top_error_endpoints = sorted(
            [
                {
                    "endpoint": key,
                    "error_rate": stats.error_rate,
                    "error_count": stats.error_count,
                    "total_requests": stats.total_requests,
                }
                for key, stats in monitor.endpoint_stats.items()
            ],
            key=lambda x: x["error_rate"],
            reverse=True
        )[:5]

        # ç”Ÿæˆå‘Šè­¦
        alerts = []
        if system_perf.cpu_percent > 80:
            alerts.append(f"CPUä½¿ç”¨ç‡è¿‡é«˜: {system_perf.cpu_percent}%")
        if system_perf.memory_percent > 85:
            alerts.append(f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {system_perf.memory_percent}%")
        if system_perf.disk_percent > 90:
            alerts.append(f"ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜: {system_perf.disk_percent}%")

        # æ£€æŸ¥P95å“åº”æ—¶é—´
        if all_response_times:
            p95_response = _calculate_percentiles(all_response_times, 95)
            if p95_response > 1000:
                alerts.append(f"P95å“åº”æ—¶é—´è¿‡é•¿: {round(p95_response, 0)}ms")

        # æ£€æŸ¥é”™è¯¯ç‡
        if recent_metrics:
            error_count = sum(1 for m in recent_metrics if m.status_code >= 400)
            error_rate = error_count / len(recent_metrics) * 100
            if error_rate > 5:
                alerts.append(f"APIé”™è¯¯ç‡è¿‡é«˜: {round(error_rate, 2)}%")

        # æ„å»ºå“åº”æ•°æ®
        performance_data = PerformanceMetricsResponse(
            collection_time=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            time_window_seconds=time_window,
            system_metrics=system_perf,
            api_metrics=endpoint_performance,
            response_time_distribution=time_buckets,
            top_slow_endpoints=top_slow_endpoints,
            top_error_endpoints=top_error_endpoints,
            alerts=alerts,
        )

        return create_success_response(
            data=performance_data.model_dump(),
            message=f"æ€§èƒ½æŒ‡æ ‡è·å–æˆåŠŸ (æ—¶é—´çª—å£: {time_window}ç§’)",
        )

    except Exception as e:
        logger.error(f"[SYSTEM] Failed to get performance metrics: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=create_error_response(
                ErrorCodes.INTERNAL_SERVER_ERROR,
                f"è·å–æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {str(e)}",
            ).model_dump(mode="json"),
        )


@router.get("/metrics/performance/summary")
async def get_performance_summary():
    """
    è·å–æ€§èƒ½æŒ‡æ ‡æ‘˜è¦ (Phase 2.4.5 - æ–°å¢)

    Returns:
        ç®€åŒ–çš„æ€§èƒ½æ‘˜è¦ï¼Œç”¨äºå¿«é€Ÿå¥åº·æ£€æŸ¥
    """
    try:
        from app.core.api_monitoring import get_monitor

        # ç³»ç»ŸæŒ‡æ ‡
        system_perf = _get_performance_metrics()

        # APIå¥åº·æ£€æŸ¥
        monitor = get_monitor()
        health_check = monitor.get_health_check()

        return create_success_response(
            data={
                "system": {
                    "status": health_check["status"],
                    "cpu_percent": system_perf.cpu_percent,
                    "memory_percent": system_perf.memory_percent,
                    "uptime": system_perf.uptime_formatted,
                },
                "api": {
                    "status": health_check["status"],
                    "avg_response_time_ms": health_check.get("avg_response_time_ms", 0),
                    "error_rate": health_check.get("error_rate", "0%"),
                },
                "overall_status": (
                    "healthy"
                    if health_check["status"] == "healthy" and system_perf.cpu_percent < 80
                    else "warning"
                ),
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            },
            message="æ€§èƒ½æ‘˜è¦è·å–æˆåŠŸ",
        )

    except Exception as e:
        logger.error(f"[SYSTEM] Failed to get performance summary: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=create_error_response(
                ErrorCodes.INTERNAL_SERVER_ERROR,
                f"è·å–æ€§èƒ½æ‘˜è¦å¤±è´¥: {str(e)}",
            ).model_dump(mode="json"),
        )


__all__ = ["router", "broadcast_system_log"]
