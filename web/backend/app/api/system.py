"""
ç³»ç»Ÿç®¡ç†APIç«¯ç‚¹
æä¾›ç³»ç»Ÿè®¾ç½®ã€æ•°æ®åº“è¿æ¥æµ‹è¯•ã€è¿è¡Œæ—¥å¿—æŸ¥è¯¢ç­‰åŠŸèƒ½
"""

import os
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

import psycopg2
import taos
from fastapi import APIRouter, HTTPException, Query
from pydantic import BaseModel

# Mockæ•°æ®æ”¯æŒ
use_mock = os.getenv("USE_MOCK_DATA", "false").lower() == "true"

router = APIRouter()


@router.get("/health")
async def system_health():
    """
    ç³»ç»Ÿå¥åº·æ£€æŸ¥ç«¯ç‚¹ (åŒæ•°æ®åº“æ¶æ„: TDengine + PostgreSQL)

    è¿”å›:
    - æ•°æ®åº“è¿æ¥çŠ¶æ€
    - ç³»ç»Ÿè¿è¡Œæ—¶é—´
    - æœåŠ¡çŠ¶æ€
    """
    if use_mock:
        # Mockæ•°æ®ï¼šè¿”å›æ¨¡æ‹Ÿå¥åº·çŠ¶æ€
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "databases": {"postgresql": "healthy", "tdengine": "healthy"},
            "service": "mystocks-web-api",
            "version": "2.2.0",
            "mock_mode": True,
            "architecture": "dual-database",
            "uptime": "2å¤© 14å°æ—¶ 23åˆ†é’Ÿ",
        }

    try:
        from app.core.database import db_service

        # æ£€æŸ¥æ•°æ®åº“è¿æ¥ (ä»… PostgreSQL å’Œ TDengine)
        db_status = {
            "postgresql": "unknown",
            "tdengine": "unknown",
        }

        # ç®€å•æ£€æŸ¥ - å°è¯•æŸ¥è¯¢
        try:
            db_service.query_stocks_basic(limit=1)
            db_status["postgresql"] = "healthy"
        except Exception:
            pass

        try:
            # æ£€æŸ¥ TDengine
            # TODO: æ·»åŠ  TDengine å¥åº·æ£€æŸ¥
            db_status["tdengine"] = "healthy"
        except Exception:
            pass

        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "databases": db_status,
            "service": "mystocks-web-api",
            "version": "2.2.0",
            "architecture": "dual-database",
        }
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"ç³»ç»Ÿå¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")


@router.get("/adapters/health")
async def get_adapters_health():
    """
    ğŸš€ é€‚é…å™¨å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼ˆæ–°å¢ï¼‰

    æ£€æŸ¥æ‰€æœ‰æ•°æ®é€‚é…å™¨çš„å¥åº·çŠ¶æ€ï¼š
    - akshare: AkShareé€‚é…å™¨
    - tdx: é€šè¾¾ä¿¡é€‚é…å™¨
    - financial: è´¢åŠ¡æ•°æ®é€‚é…å™¨

    è¿”å›:
    - æ¯ä¸ªé€‚é…å™¨çš„å¥åº·çŠ¶æ€
    - æœ€åæ£€æŸ¥æ—¶é—´
    - é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰

    ç”¨äºç›‘æ§å’Œè‡ªåŠ¨é™çº§
    """
    try:
        from datetime import datetime

        from app.core.adapter_loader import (
            check_all_adapters,
            get_adapter_health_status,
        )

        # æ‰§è¡Œå¥åº·æ£€æŸ¥
        health_results = check_all_adapters()

        # è·å–è¯¦ç»†çŠ¶æ€
        detailed_status = {}
        for adapter_name, is_healthy in health_results.items():
            status_info = get_adapter_health_status(adapter_name)
            detailed_status[adapter_name] = {
                "healthy": is_healthy,
                "status": status_info.get("status", "unknown"),
                "error": status_info.get("error"),
                "last_check": datetime.now().isoformat(),
            }

        # è®¡ç®—æ€»ä½“å¥åº·åº¦
        total_adapters = len(health_results)
        healthy_adapters = sum(1 for h in health_results.values() if h)
        overall_healthy = healthy_adapters == total_adapters

        return {
            "overall_status": "healthy" if overall_healthy else "degraded",
            "healthy_count": healthy_adapters,
            "total_count": total_adapters,
            "adapters": detailed_status,
            "timestamp": datetime.now().isoformat(),
            "message": f"{healthy_adapters}/{total_adapters} é€‚é…å™¨æ­£å¸¸è¿è¡Œ",
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é€‚é…å™¨å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}")


@router.get("/datasources")
async def get_datasources():
    """
    è·å–å·²é…ç½®çš„æ•°æ®æºåˆ—è¡¨

    è¿”å›æ‰€æœ‰å¯ç”¨çš„æ•°æ®æºé…ç½®ä¿¡æ¯
    """
    datasources = [
        {
            "id": "tdx",
            "name": "é€šè¾¾ä¿¡(TDX)",
            "type": "realtime",
            "status": "active",
            "description": "å®æ—¶è¡Œæƒ…å’Œå¤šå‘¨æœŸKçº¿æ•°æ®",
            "features": ["å®æ—¶è¡Œæƒ…", "åˆ†é’ŸKçº¿", "æ—¥Kçº¿"],
        },
        {
            "id": "akshare",
            "name": "AkShare",
            "type": "historical",
            "status": "active",
            "description": "å†å²æ•°æ®å’Œè´¢åŠ¡æ•°æ®",
            "features": ["å†å²è¡Œæƒ…", "è´¢åŠ¡æŠ¥è¡¨", "å®è§‚æ•°æ®"],
        },
        {
            "id": "financial",
            "name": "Financial Adapter",
            "type": "comprehensive",
            "status": "active",
            "description": "ç»¼åˆè´¢åŠ¡æ•°æ®é€‚é…å™¨",
            "features": ["å®æ—¶è¡Œæƒ…", "è´¢åŠ¡æ•°æ®", "æŒ‡æ•°æ•°æ®"],
        },
        {
            "id": "baostock",
            "name": "BaoStock",
            "type": "historical",
            "status": "available",
            "description": "å†å²æ•°æ®å¤‡ç”¨æ•°æ®æº",
            "features": ["å†å²è¡Œæƒ…", "å¤æƒæ•°æ®"],
        },
    ]

    return {
        "success": True,
        "data": datasources,
        "total": len(datasources),
        "timestamp": datetime.now().isoformat(),
    }


class ConnectionTestRequest(BaseModel):
    """æ•°æ®åº“è¿æ¥æµ‹è¯•è¯·æ±‚"""

    db_type: str
    host: str
    port: int


class ConnectionTestResponse(BaseModel):
    """æ•°æ®åº“è¿æ¥æµ‹è¯•å“åº”"""

    success: bool
    message: Optional[str] = None
    error: Optional[str] = None


@router.post("/test-connection", response_model=ConnectionTestResponse)
async def test_database_connection(request: ConnectionTestRequest):
    """
    æµ‹è¯•æ•°æ®åº“è¿æ¥ (åŒæ•°æ®åº“æ¶æ„)

    æ”¯æŒçš„æ•°æ®åº“ç±»å‹:
    - postgresql: PostgreSQL (ä¸»æ•°æ®åº“)
    - tdengine: TDengine (æ—¶åºæ•°æ®åº“)
    """
    db_type = request.db_type.lower()
    host = request.host
    port = request.port

    try:
        if db_type == "postgresql":
            # æµ‹è¯• PostgreSQL è¿æ¥ - è¿æ¥åˆ°é»˜è®¤çš„ postgres æ•°æ®åº“
            connection = None
            cursor = None
            try:
                connection = psycopg2.connect(
                    host=host,
                    port=port,
                    user="postgres",
                    password="c790414J",
                    database="postgres",  # è¿æ¥åˆ°é»˜è®¤æ•°æ®åº“
                    connect_timeout=5,
                )
                # æ‰§è¡Œç®€å•æŸ¥è¯¢æµ‹è¯•
                cursor = connection.cursor()
                cursor.execute("SELECT version()")
                version = cursor.fetchone()[0].split(",")[0]

                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ mystocks ç›¸å…³æ•°æ®åº“
                cursor.execute(
                    "SELECT datname FROM pg_database WHERE datname LIKE 'mystocks%'"
                )
                databases = cursor.fetchall()
                db_list = [db[0] for db in databases] if databases else []

                if db_list:
                    return ConnectionTestResponse(
                        success=True,
                        message=f"PostgreSQL è¿æ¥æˆåŠŸ ({version})ï¼Œå‘ç°æ•°æ®åº“: {', '.join(db_list)}",
                    )
                else:
                    return ConnectionTestResponse(
                        success=True,
                        message=f"PostgreSQL è¿æ¥æˆåŠŸ ({version})ï¼Œä½†æœªå‘ç° mystocks ç›¸å…³æ•°æ®åº“",
                    )
            except psycopg2.Error:
                raise
            finally:
                # ç¡®ä¿è¿æ¥è¢«å…³é—­ï¼Œé˜²æ­¢è¿æ¥æ³„æ¼
                if cursor is not None:
                    try:
                        cursor.close()
                    except Exception:
                        pass
                if connection is not None:
                    try:
                        connection.close()
                    except Exception:
                        pass

        elif db_type == "tdengine":
            # æµ‹è¯• TDengine è¿æ¥
            connection = None
            cursor = None
            try:
                connection = taos.connect(
                    host=host,
                    port=port,
                    user="root",
                    password="taosdata",
                    config="/etc/taos",
                    timeout=5000,
                )
                # æ‰§è¡Œç®€å•æŸ¥è¯¢æµ‹è¯•
                cursor = connection.cursor()
                cursor.execute("SELECT SERVER_VERSION()")
                result = cursor.fetchone()
                version = result[0] if result and len(result) > 0 else "æœªçŸ¥ç‰ˆæœ¬"

                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ mystocks ç›¸å…³æ•°æ®åº“
                cursor.execute("SHOW DATABASES")
                databases = cursor.fetchall()
                db_list = (
                    [db[0] for db in databases if db and "mystocks" in db[0].lower()]
                    if databases
                    else []
                )

                if db_list:
                    return ConnectionTestResponse(
                        success=True,
                        message=f"TDengine è¿æ¥æˆåŠŸ (ç‰ˆæœ¬: {version})ï¼Œå‘ç°æ•°æ®åº“: {', '.join(db_list)}",
                    )
                else:
                    return ConnectionTestResponse(
                        success=True,
                        message=f"TDengine è¿æ¥æˆåŠŸ (ç‰ˆæœ¬: {version})ï¼Œä½†æœªå‘ç° mystocks ç›¸å…³æ•°æ®åº“",
                    )
            except Exception as e:
                # TDengine å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
                error_msg = str(e)
                if (
                    "Unable to establish connection" in error_msg
                    or "Connection refused" in error_msg
                ):
                    return ConnectionTestResponse(
                        success=False,
                        error=f"æ— æ³•è¿æ¥åˆ° TDengine æœåŠ¡å™¨ ({host}:{port})ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ",
                    )
                raise
            finally:
                # ç¡®ä¿è¿æ¥è¢«å…³é—­ï¼Œé˜²æ­¢è¿æ¥æ³„æ¼
                if cursor is not None:
                    try:
                        cursor.close()
                    except Exception:
                        pass
                if connection is not None:
                    try:
                        connection.close()
                    except Exception:
                        pass

        else:
            return ConnectionTestResponse(
                success=False,
                error=f"ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {db_type}ï¼Œä»…æ”¯æŒ postgresql å’Œ tdengine",
            )

    except psycopg2.OperationalError as e:
        error_msg = str(e)
        if "could not connect to server" in error_msg:
            return ConnectionTestResponse(
                success=False,
                error=f"æ— æ³•è¿æ¥åˆ° PostgreSQL æœåŠ¡å™¨ ({host}:{port})ï¼Œè¯·æ£€æŸ¥åœ°å€å’Œç«¯å£æ˜¯å¦æ­£ç¡®",
            )
        elif "password authentication failed" in error_msg:
            return ConnectionTestResponse(
                success=False, error="PostgreSQL è®¤è¯å¤±è´¥ï¼Œç”¨æˆ·åæˆ–å¯†ç é”™è¯¯"
            )
        else:
            return ConnectionTestResponse(
                success=False, error=f"PostgreSQL è¿æ¥é”™è¯¯: {error_msg}"
            )

    except Exception as e:
        return ConnectionTestResponse(success=False, error=f"è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}")


# ==================== è¿è¡Œæ—¥å¿—ç›¸å…³ç«¯ç‚¹ ====================


class SystemLog(BaseModel):
    """ç³»ç»Ÿæ—¥å¿—æ¨¡å‹"""

    id: int
    timestamp: str
    level: str  # INFO, WARNING, ERROR, CRITICAL
    category: str  # database, api, adapter, system
    operation: str  # æ“ä½œåç§°
    message: str
    details: Optional[Dict[str, Any]] = None
    duration_ms: Optional[int] = None
    has_error: bool = False


class LogQueryResponse(BaseModel):
    """æ—¥å¿—æŸ¥è¯¢å“åº”"""

    success: bool
    data: List[SystemLog]
    total: int
    filtered: int
    timestamp: str


def get_system_logs_from_db(
    filter_errors: bool = False,
    limit: int = 100,
    offset: int = 0,
    level: Optional[str] = None,
    category: Optional[str] = None,
) -> List[SystemLog]:
    """
    ä»PostgreSQLç›‘æ§æ•°æ®åº“è·å–ç³»ç»Ÿæ—¥å¿—

    Args:
        filter_errors: æ˜¯å¦åªè¿”å›æœ‰é—®é¢˜çš„æ—¥å¿— (WARNING, ERROR, CRITICAL)
        limit: è¿”å›æ¡æ•°é™åˆ¶
        offset: åç§»é‡
        level: æ—¥å¿—çº§åˆ«ç­›é€‰
        category: æ—¥å¿—åˆ†ç±»ç­›é€‰

    Returns:
        ç³»ç»Ÿæ—¥å¿—åˆ—è¡¨
    """
    conn = None
    cursor = None
    try:
        # è¿æ¥åˆ°PostgreSQLç›‘æ§æ•°æ®åº“
        conn = psycopg2.connect(
            host="localhost",
            port=5432,
            user="postgres",
            password="c790414J",
            database="mystocks_monitoring",
            connect_timeout=5,
        )
        cursor = conn.cursor()

        # æ„å»ºæŸ¥è¯¢SQL
        where_clauses = []
        params = []

        if filter_errors:
            where_clauses.append("level IN ('WARNING', 'ERROR', 'CRITICAL')")

        if level:
            where_clauses.append("level = %s")
            params.append(level.upper())

        if category:
            where_clauses.append("category = %s")
            params.append(category)

        where_sql = " AND ".join(where_clauses) if where_clauses else "1=1"

        # æŸ¥è¯¢operation_logè¡¨
        query = f"""
            SELECT
                id,
                timestamp,
                COALESCE(status, 'INFO') as level,
                operation_type as category,
                operation as operation,
                COALESCE(error_message, message, '') as message,
                execution_time_ms as duration_ms,
                CASE WHEN status IN ('failed', 'error') THEN true ELSE false END as has_error
            FROM operation_log
            WHERE {where_sql}
            ORDER BY timestamp DESC
            LIMIT %s OFFSET %s
        """

        params.extend([limit, offset])

        cursor.execute(query, params)
        rows = cursor.fetchall()

        # è½¬æ¢ä¸ºSystemLogå¯¹è±¡
        logs = []
        for row in rows:
            log = SystemLog(
                id=row[0],
                timestamp=row[1].isoformat() if row[1] else datetime.now().isoformat(),
                level=row[2].upper() if row[2] else "INFO",
                category=row[3] or "system",
                operation=row[4] or "unknown",
                message=row[5] or "",
                details=None,
                duration_ms=row[6],
                has_error=row[7] if len(row) > 7 else False,
            )
            logs.append(log)

        # è·å–æ€»æ•°
        count_query = f"""
            SELECT COUNT(*) FROM operation_log WHERE {where_sql}
        """
        cursor.execute(count_query, params[:-2])  # ä¸åŒ…æ‹¬limitå’Œoffset
        total = cursor.fetchone()[0]

        return logs, total

    except Exception as e:
        # å¦‚æœæ•°æ®åº“æŸ¥è¯¢å¤±è´¥ï¼Œè¿”å›æ¨¡æ‹Ÿæ—¥å¿—
        print(f"Error fetching logs from database: {e}")
        return get_mock_system_logs(filter_errors, limit), 0
    finally:
        # ç¡®ä¿è¿æ¥å’Œæ¸¸æ ‡è¢«å…³é—­ï¼Œé˜²æ­¢è¿æ¥æ³„æ¼
        if cursor is not None:
            try:
                cursor.close()
            except Exception:
                pass
        if conn is not None:
            try:
                conn.close()
            except Exception:
                pass


def get_mock_system_logs(
    filter_errors: bool = False, limit: int = 100
) -> List[SystemLog]:
    """
    ç”Ÿæˆæ¨¡æ‹Ÿçš„ç³»ç»Ÿæ—¥å¿—ï¼ˆç”¨äºæ¼”ç¤ºå’Œæ•°æ®åº“ä¸å¯ç”¨æ—¶çš„å¤‡ç”¨ï¼‰
    """
    mock_logs = []

    # æ­£å¸¸è¿è¡Œæ—¥å¿—
    normal_logs = [
        SystemLog(
            id=1,
            timestamp=(datetime.now() - timedelta(minutes=5)).isoformat(),
            level="INFO",
            category="database",
            operation="æ•°æ®åº“è¿æ¥",
            message="MySQLæ•°æ®åº“è¿æ¥æˆåŠŸ",
            details={"host": "localhost", "port": 3306},
            duration_ms=125,
            has_error=False,
        ),
        SystemLog(
            id=2,
            timestamp=(datetime.now() - timedelta(minutes=4)).isoformat(),
            level="INFO",
            category="api",
            operation="APIè¯·æ±‚",
            message="GET /api/market/quotes è¯·æ±‚æˆåŠŸ",
            details={"status_code": 200, "response_time_ms": 245},
            duration_ms=245,
            has_error=False,
        ),
        SystemLog(
            id=3,
            timestamp=(datetime.now() - timedelta(minutes=3)).isoformat(),
            level="INFO",
            category="adapter",
            operation="æ•°æ®è·å–",
            message="TDXé€‚é…å™¨è·å–å®æ—¶è¡Œæƒ…æˆåŠŸ",
            details={"symbol": "000001", "records": 5},
            duration_ms=180,
            has_error=False,
        ),
        SystemLog(
            id=4,
            timestamp=(datetime.now() - timedelta(minutes=10)).isoformat(),
            level="INFO",
            category="system",
            operation="ç³»ç»Ÿå¯åŠ¨",
            message="MyStocks BackendæœåŠ¡å¯åŠ¨æˆåŠŸ",
            details={"version": "2.2.0", "port": 8000},
            duration_ms=0,
            has_error=False,
        ),
    ]

    # æœ‰é—®é¢˜çš„æ—¥å¿—
    error_logs = [
        SystemLog(
            id=5,
            timestamp=(datetime.now() - timedelta(minutes=2)).isoformat(),
            level="WARNING",
            category="database",
            operation="æ•°æ®åº“æŸ¥è¯¢",
            message="TDengineæŸ¥è¯¢å“åº”æ—¶é—´è¿‡é•¿",
            details={"query": "SELECT * FROM stock_tick", "duration_ms": 3500},
            duration_ms=3500,
            has_error=True,
        ),
        SystemLog(
            id=6,
            timestamp=(datetime.now() - timedelta(minutes=1)).isoformat(),
            level="ERROR",
            category="adapter",
            operation="æ•°æ®è·å–",
            message="AkShareé€‚é…å™¨è·å–è´¢åŠ¡æ•°æ®å¤±è´¥",
            details={"symbol": "600519", "error": "Connection timeout"},
            duration_ms=5000,
            has_error=True,
        ),
        SystemLog(
            id=7,
            timestamp=(datetime.now() - timedelta(seconds=30)).isoformat(),
            level="CRITICAL",
            category="database",
            operation="æ•°æ®åº“è¿æ¥",
            message="Redisè¿æ¥å¤±è´¥",
            details={"host": "localhost", "port": 6379, "error": "Connection refused"},
            duration_ms=0,
            has_error=True,
        ),
        SystemLog(
            id=8,
            timestamp=(datetime.now() - timedelta(minutes=8)).isoformat(),
            level="WARNING",
            category="api",
            operation="APIè¯·æ±‚",
            message="APIè¯·æ±‚é¢‘ç‡è¿‡é«˜",
            details={"endpoint": "/api/market/quotes", "rate": "120 req/min"},
            duration_ms=0,
            has_error=True,
        ),
    ]

    if filter_errors:
        mock_logs = error_logs[:limit]
    else:
        # æ··åˆæ­£å¸¸æ—¥å¿—å’Œé”™è¯¯æ—¥å¿—
        all_logs = normal_logs + error_logs
        all_logs.sort(key=lambda x: x.timestamp, reverse=True)
        mock_logs = all_logs[:limit]

    return mock_logs


@router.get("/logs", response_model=LogQueryResponse)
async def get_system_logs(
    filter_errors: bool = Query(False, description="æ˜¯å¦åªæ˜¾ç¤ºæœ‰é—®é¢˜çš„æ—¥å¿—"),
    limit: int = Query(100, ge=1, le=1000, description="è¿”å›æ¡æ•°é™åˆ¶"),
    offset: int = Query(0, ge=0, description="åç§»é‡"),
    level: Optional[str] = Query(
        None, description="æ—¥å¿—çº§åˆ«ç­›é€‰ (INFO/WARNING/ERROR/CRITICAL)"
    ),
    category: Optional[str] = Query(
        None, description="æ—¥å¿—åˆ†ç±»ç­›é€‰ (database/api/adapter/system)"
    ),
):
    """
    è·å–ç³»ç»Ÿè¿è¡Œæ—¥å¿—

    å‚æ•°:
    - filter_errors: æ˜¯å¦åªæ˜¾ç¤ºæœ‰é—®é¢˜çš„æ—¥å¿— (WARNING/ERROR/CRITICAL)
    - limit: è¿”å›æ¡æ•°é™åˆ¶ (1-1000)
    - offset: åç§»é‡ï¼Œç”¨äºåˆ†é¡µ
    - level: æ—¥å¿—çº§åˆ«ç­›é€‰
    - category: æ—¥å¿—åˆ†ç±»ç­›é€‰

    è¿”å›:
    - ç³»ç»Ÿè¿è¡Œæ—¥å¿—åˆ—è¡¨ï¼ŒåŒ…å«æ—¶é—´æˆ³ã€çº§åˆ«ã€åˆ†ç±»ã€æ“ä½œã€æ¶ˆæ¯ç­‰ä¿¡æ¯

    ç¤ºä¾‹:
    - GET /api/system/logs - è·å–æ‰€æœ‰æ—¥å¿—
    - GET /api/system/logs?filter_errors=true - åªè·å–æœ‰é—®é¢˜çš„æ—¥å¿—
    - GET /api/system/logs?level=ERROR - åªè·å–ERRORçº§åˆ«æ—¥å¿—
    - GET /api/system/logs?category=database - åªè·å–æ•°æ®åº“ç›¸å…³æ—¥å¿—
    """
    try:
        # é¦–å…ˆå°è¯•ä»æ•°æ®åº“è·å–
        logs, total = get_system_logs_from_db(
            filter_errors=filter_errors,
            limit=limit,
            offset=offset,
            level=level,
            category=category,
        )

        # å¦‚æœæ•°æ®åº“æŸ¥è¯¢å¤±è´¥æˆ–æ²¡æœ‰æ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        if not logs or total == 0:
            logs = get_mock_system_logs(filter_errors=filter_errors, limit=limit)
            total = len(logs)

        return LogQueryResponse(
            success=True,
            data=logs,
            total=total,
            filtered=len(logs),
            timestamp=datetime.now().isoformat(),
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç³»ç»Ÿæ—¥å¿—å¤±è´¥: {str(e)}")


@router.get("/logs/summary")
async def get_logs_summary():
    """
    è·å–æ—¥å¿—ç»Ÿè®¡æ‘˜è¦

    è¿”å›:
    - æ€»æ—¥å¿—æ•°
    - å„çº§åˆ«æ—¥å¿—æ•°é‡
    - å„åˆ†ç±»æ—¥å¿—æ•°é‡
    - æœ€è¿‘é”™è¯¯æ•°
    """
    try:
        logs, total = get_system_logs_from_db(limit=1000)

        # å¦‚æœæ²¡æœ‰çœŸå®æ•°æ®ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        if not logs:
            logs = get_mock_system_logs(limit=100)
            total = len(logs)

        # ç»Ÿè®¡å„çº§åˆ«æ•°é‡
        level_counts = {"INFO": 0, "WARNING": 0, "ERROR": 0, "CRITICAL": 0}
        for log in logs:
            if log.level in level_counts:
                level_counts[log.level] += 1

        # ç»Ÿè®¡å„åˆ†ç±»æ•°é‡
        category_counts = {}
        for log in logs:
            category_counts[log.category] = category_counts.get(log.category, 0) + 1

        # ç»Ÿè®¡æœ€è¿‘1å°æ—¶çš„é”™è¯¯
        recent_errors = sum(1 for log in logs if log.has_error)

        return {
            "success": True,
            "data": {
                "total_logs": total,
                "level_counts": level_counts,
                "category_counts": category_counts,
                "recent_errors_1h": recent_errors,
                "last_update": datetime.now().isoformat(),
            },
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ—¥å¿—ç»Ÿè®¡å¤±è´¥: {str(e)}")


@router.get("/architecture")
async def get_system_architecture():
    """
    è·å–ç³»ç»Ÿæ¶æ„ä¿¡æ¯ (Week 3ç®€åŒ–å - åŒæ•°æ®åº“æ¶æ„)

    è¿”å›å®Œæ•´çš„ç³»ç»Ÿæ¶æ„ä¿¡æ¯ï¼ŒåŒ…æ‹¬:
    - æ•°æ®åº“æ¶æ„ (TDengine + PostgreSQL)
    - æ•°æ®åˆ†ç±»è·¯ç”±ç­–ç•¥
    - æ¶æ„ç®€åŒ–æŒ‡æ ‡
    - æŠ€æœ¯æ ˆä¿¡æ¯
    - MySQL/Redisç§»é™¤è¯¦æƒ…

    ç”¨äºæ¶æ„å¯è§†åŒ–é¡µé¢å±•ç¤º
    """
    try:
        return {
            "success": True,
            "message": "ç³»ç»Ÿæ¶æ„ä¿¡æ¯è·å–æˆåŠŸ",
            "data": {
                # æ¶æ„ç®€åŒ–æˆæœ
                "simplification": {
                    "before": {
                        "databases": 4,
                        "description": "TDengine + PostgreSQL + MySQL + Redis",
                    },
                    "after": {"databases": 2, "description": "TDengine + PostgreSQL"},
                    "reduction_percentage": 50,
                    "mysql_migration": {
                        "tables": 18,
                        "rows": 299,
                        "status": "completed",
                    },
                    "redis_removal": {
                        "configured_db": "db1",
                        "data_status": "empty",
                        "status": "removed",
                    },
                    "completion_date": "2025-10-19",
                },
                # æ•°æ®åº“é…ç½®
                "databases": [
                    {
                        "name": "TDengine",
                        "version": "3.3.6.13",
                        "type": "time-series",
                        "purpose": "é«˜é¢‘æ—¶åºæ•°æ®ä¸“ç”¨åº“",
                        "usage": ["Tickæ•°æ®", "åˆ†é’ŸKçº¿", "å®æ—¶æ·±åº¦"],
                        "features": [
                            "æè‡´å‹ç¼©æ¯” 20:1",
                            "è¶…å¼ºå†™å…¥æ€§èƒ½",
                            "åˆ—å¼å­˜å‚¨",
                            "æ¯«ç§’çº§å»¶è¿Ÿ",
                        ],
                        "connection": {
                            "websocket_port": 6030,
                            "rest_api_port": 6041,
                            "database": "market_data",
                        },
                    },
                    {
                        "name": "PostgreSQL",
                        "version": "17.6",
                        "type": "relational",
                        "purpose": "é€šç”¨æ•°æ®ä»“åº“ + TimescaleDBæ‰©å±•",
                        "usage": [
                            "æ—¥çº¿Kçº¿æ•°æ®",
                            "å‚è€ƒæ•°æ®ï¼ˆè‚¡ç¥¨ä¿¡æ¯ã€äº¤æ˜“æ—¥å†ï¼‰",
                            "è¡ç”Ÿæ•°æ®ï¼ˆæŠ€æœ¯æŒ‡æ ‡ã€é‡åŒ–å› å­ï¼‰",
                            "äº¤æ˜“æ•°æ®ï¼ˆè®¢å•ã€æˆäº¤ã€æŒä»“ï¼‰",
                            "å…ƒæ•°æ®ï¼ˆç³»ç»Ÿé…ç½®ã€æ•°æ®æºçŠ¶æ€ï¼‰",
                        ],
                        "features": [
                            "TimescaleDB 2.22.0 æ—¶åºæ‰©å±•",
                            "è‡ªåŠ¨åˆ†åŒº",
                            "å¤æ‚æŸ¥è¯¢æ”¯æŒ",
                            "ACIDäº‹åŠ¡ä¿è¯",
                            "JSONæ”¯æŒ",
                        ],
                        "connection": {
                            "default_port": 5432,
                            "alternative_port": 5438,
                            "database": "mystocks",
                        },
                    },
                ],
                # æ•°æ®åˆ†ç±»è·¯ç”±ç­–ç•¥ (5å¤§åˆ†ç±»)
                "data_classifications": [
                    {
                        "category": "ç¬¬1ç±»ï¼šå¸‚åœºæ•°æ®",
                        "characteristics": "é«˜é¢‘æ—¶åºæ•°æ®ï¼Œå†™å…¥å¯†é›†ï¼Œæ—¶é—´èŒƒå›´æŸ¥è¯¢",
                        "routing": [
                            {
                                "data_type": "Tickæ•°æ®ã€åˆ†é’ŸKçº¿ã€å®æ—¶æ·±åº¦",
                                "database": "TDengine",
                                "reason": "æè‡´å‹ç¼©å’Œè¶…å¼ºå†™å…¥æ€§èƒ½",
                            },
                            {
                                "data_type": "æ—¥çº¿ã€å‘¨çº¿ã€æœˆçº¿Kçº¿",
                                "database": "PostgreSQL + TimescaleDB",
                                "reason": "å¤æ‚æ—¶åºæŸ¥è¯¢å’Œåˆ†æ",
                            },
                        ],
                    },
                    {
                        "category": "ç¬¬2ç±»ï¼šå‚è€ƒæ•°æ®",
                        "characteristics": "ç›¸å¯¹é™æ€ï¼Œå…³ç³»å‹ç»“æ„ï¼Œé¢‘ç¹JOINæ“ä½œ",
                        "routing": [
                            {
                                "data_type": "è‚¡ç¥¨ä¿¡æ¯ã€æˆåˆ†è‚¡ä¿¡æ¯ã€äº¤æ˜“æ—¥å†",
                                "database": "PostgreSQL",
                                "reason": "ACIDä¿è¯å’Œå…³ç³»æŸ¥è¯¢ (ä»MySQLè¿ç§»)",
                            }
                        ],
                    },
                    {
                        "category": "ç¬¬3ç±»ï¼šè¡ç”Ÿæ•°æ®",
                        "characteristics": "è®¡ç®—å¯†é›†ï¼Œæ—¶åºåˆ†æï¼Œå¤æ‚æŸ¥è¯¢",
                        "routing": [
                            {
                                "data_type": "æŠ€æœ¯æŒ‡æ ‡ã€é‡åŒ–å› å­ã€æ¨¡å‹è¾“å‡ºã€äº¤æ˜“ä¿¡å·",
                                "database": "PostgreSQL + TimescaleDB",
                                "reason": "è‡ªåŠ¨åˆ†åŒºå’Œå¤æ‚è®¡ç®—æ”¯æŒ",
                            }
                        ],
                    },
                    {
                        "category": "ç¬¬4ç±»ï¼šäº¤æ˜“æ•°æ®",
                        "characteristics": "äº‹åŠ¡å®Œæ•´æ€§è¦æ±‚é«˜ï¼Œéœ€è¦ACIDä¿è¯",
                        "routing": [
                            {
                                "data_type": "è®¢å•è®°å½•ã€æˆäº¤è®°å½•ã€æŒä»“è®°å½•ã€è´¦æˆ·çŠ¶æ€",
                                "database": "PostgreSQL",
                                "reason": "å¼ºä¸€è‡´æ€§å’Œäº‹åŠ¡ä¿è¯",
                            }
                        ],
                    },
                    {
                        "category": "ç¬¬5ç±»ï¼šå…ƒæ•°æ®",
                        "characteristics": "é…ç½®ç®¡ç†ï¼Œç³»ç»ŸçŠ¶æ€ï¼Œç»“æ„åŒ–å­˜å‚¨",
                        "routing": [
                            {
                                "data_type": "æ•°æ®æºçŠ¶æ€ã€ä»»åŠ¡è°ƒåº¦ã€ç­–ç•¥å‚æ•°ã€ç³»ç»Ÿé…ç½®",
                                "database": "PostgreSQL",
                                "reason": "é›†ä¸­ç®¡ç†å’ŒJSONæ”¯æŒ (ä»MySQLè¿ç§»)",
                            }
                        ],
                    },
                ],
                # ç§»é™¤çš„æ•°æ®åº“
                "removed_databases": [
                    {
                        "name": "MySQL",
                        "reason": "åŠŸèƒ½å®Œå…¨è¢«PostgreSQLæ›¿ä»£",
                        "migration": {
                            "tables": 18,
                            "rows": 299,
                            "destination": "PostgreSQL",
                            "status": "completed",
                            "date": "2025-10-19",
                        },
                    },
                    {
                        "name": "Redis",
                        "reason": "ç”Ÿäº§ç¯å¢ƒæœªä½¿ç”¨ï¼Œåº”ç”¨å±‚ç¼“å­˜æ›¿ä»£",
                        "replacement": {
                            "method": "Pythonå†…ç½®cachetools + functools.lru_cache",
                            "config": {
                                "CACHE_EXPIRE_SECONDS": 300,
                                "LRU_CACHE_MAXSIZE": 1000,
                            },
                        },
                        "status": "removed",
                    },
                ],
                # æŠ€æœ¯æ ˆ
                "tech_stack": {
                    "time_series_databases": [
                        {
                            "name": "TDengine",
                            "version": "3.3.6.13",
                            "purpose": "é«˜é¢‘æ—¶åºæ•°æ®ä¸“ç”¨",
                        },
                        {
                            "name": "TimescaleDB",
                            "version": "2.22.0",
                            "purpose": "PostgreSQLæ—¶åºæ‰©å±•",
                        },
                    ],
                    "relational_databases": [
                        {
                            "name": "PostgreSQL",
                            "version": "17.6",
                            "purpose": "ä¸»æ•°æ®ä»“åº“",
                        },
                        {"name": "psycopg2-binary", "purpose": "Pythonæ•°æ®åº“é©±åŠ¨"},
                    ],
                    "backend_frameworks": [
                        {
                            "name": "FastAPI",
                            "version": "0.109+",
                            "purpose": "é«˜æ€§èƒ½å¼‚æ­¥API",
                        },
                        {"name": "Pydantic", "version": "v2", "purpose": "æ•°æ®éªŒè¯"},
                        {"name": "Loguru", "version": "0.7.3", "purpose": "æ—¥å¿—ç®¡ç†"},
                    ],
                    "frontend_frameworks": [
                        {"name": "Vue.js", "version": "3.4.0", "purpose": "å‰ç«¯æ¡†æ¶"},
                        {
                            "name": "Element Plus",
                            "version": "2.8.0",
                            "purpose": "UIç»„ä»¶åº“",
                        },
                        {
                            "name": "ECharts",
                            "version": "5.5.0",
                            "purpose": "æ•°æ®å¯è§†åŒ–",
                        },
                    ],
                },
                # æ ¸å¿ƒåŸåˆ™
                "principles": {
                    "title": "ä¸“åº“ä¸“ç”¨ï¼Œç®€æ´èƒœäºè¿‡åº¦å¤æ‚",
                    "philosophy": "Simplicity > Complexity, Maintainability > Features",
                    "goals": [
                        "é™ä½ç³»ç»Ÿå¤æ‚åº¦",
                        "æé«˜å¯ç»´æŠ¤æ€§",
                        "ä¼˜åŒ–æ€§èƒ½å’Œèµ„æºåˆ©ç”¨",
                        "ç®€åŒ–è¿ç»´å’Œéƒ¨ç½²",
                    ],
                },
            },
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç³»ç»Ÿæ¶æ„ä¿¡æ¯å¤±è´¥: {str(e)}")


@router.get("/database/health")
async def database_health():
    """
    æ•°æ®åº“å¥åº·æ£€æŸ¥ (US2 - åŒæ•°æ®åº“æ¶æ„)

    æ£€æŸ¥TDengineå’ŒPostgreSQLçš„è¿æ¥çŠ¶æ€å’ŒåŸºæœ¬å¥åº·æŒ‡æ ‡

    Returns:
        {
            "success": true,
            "message": "æ•°æ®åº“å¥åº·æ£€æŸ¥å®Œæˆ",
            "data": {
                "tdengine": {...},
                "postgresql": {...},
                "summary": {...}
            }
        }
    """
    import os
    from datetime import datetime

    health_data = {
        "tdengine": {"status": "unknown", "message": ""},
        "postgresql": {"status": "unknown", "message": ""},
        "summary": {
            "total_databases": 2,
            "healthy": 0,
            "unhealthy": 0,
            "checked_at": datetime.now().isoformat(),
        },
    }

    # Check TDengine
    conn = None
    try:
        import taos

        conn = taos.connect(
            host=os.getenv("TDENGINE_HOST", "192.168.123.104"),
            port=int(os.getenv("TDENGINE_PORT", "6030")),
            user=os.getenv("TDENGINE_USER", "root"),
            password=os.getenv("TDENGINE_PASSWORD", "taosdata"),
            database=os.getenv("TDENGINE_DATABASE", "market_data"),
        )
        result = conn.query("SELECT server_version()")
        version = result.fetch_all()[0][0] if result else "unknown"

        health_data["tdengine"] = {
            "status": "healthy",
            "message": "è¿æ¥æˆåŠŸ",
            "version": version,
            "host": os.getenv("TDENGINE_HOST"),
            "port": int(os.getenv("TDENGINE_PORT", "6030")),
            "database": os.getenv("TDENGINE_DATABASE"),
        }
        health_data["summary"]["healthy"] += 1
    except Exception as e:
        health_data["tdengine"] = {
            "status": "unhealthy",
            "message": f"è¿æ¥å¤±è´¥: {str(e)}",
            "host": os.getenv("TDENGINE_HOST"),
            "port": int(os.getenv("TDENGINE_PORT", "6030")),
        }
        health_data["summary"]["unhealthy"] += 1
    finally:
        # ç¡®ä¿è¿æ¥è¢«å…³é—­ï¼Œé˜²æ­¢è¿æ¥æ³„æ¼
        if conn is not None:
            try:
                conn.close()
            except Exception:
                pass

    # Check PostgreSQL
    conn = None
    cursor = None
    try:
        import psycopg2

        conn = psycopg2.connect(
            host=os.getenv("POSTGRESQL_HOST", "192.168.123.104"),
            port=int(os.getenv("POSTGRESQL_PORT", "5438")),
            user=os.getenv("POSTGRESQL_USER", "postgres"),
            password=os.getenv("POSTGRESQL_PASSWORD"),
            database=os.getenv("POSTGRESQL_DATABASE", "mystocks"),
        )
        cursor = conn.cursor()
        cursor.execute("SELECT version()")
        version = cursor.fetchone()[0]

        health_data["postgresql"] = {
            "status": "healthy",
            "message": "è¿æ¥æˆåŠŸ",
            "version": version.split(",")[0] if version else "unknown",
            "host": os.getenv("POSTGRESQL_HOST"),
            "port": int(os.getenv("POSTGRESQL_PORT", "5438")),
            "database": os.getenv("POSTGRESQL_DATABASE"),
        }
        health_data["summary"]["healthy"] += 1
    except Exception as e:
        health_data["postgresql"] = {
            "status": "unhealthy",
            "message": f"è¿æ¥å¤±è´¥: {str(e)}",
            "host": os.getenv("POSTGRESQL_HOST"),
            "port": int(os.getenv("POSTGRESQL_PORT", "5438")),
        }
        health_data["summary"]["unhealthy"] += 1
    finally:
        # ç¡®ä¿è¿æ¥è¢«å…³é—­ï¼Œé˜²æ­¢è¿æ¥æ³„æ¼
        if cursor is not None:
            try:
                cursor.close()
            except Exception:
                pass
        if conn is not None:
            try:
                conn.close()
            except Exception:
                pass

    return {"success": True, "message": "æ•°æ®åº“å¥åº·æ£€æŸ¥å®Œæˆ", "data": health_data}


@router.get("/database/stats")
async def database_stats():
    """
    æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯ (US2 - åŒæ•°æ®åº“æ¶æ„)

    Returns:
        {
            "success": true,
            "message": "æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ",
            "data": {
                "architecture": "dual-database",
                "total_classifications": 34,
                "routing": {...},
                "features": {...}
            }
        }
    """
    from datetime import datetime

    stats_data = {
        "connections": {
            "tdengine": {
                "status": "connected",
                "pool_size": 10,
                "active_connections": 5,
            },
            "postgresql": {
                "status": "connected",
                "pool_size": 20,
                "active_connections": 8,
            },
        },
        "tables": {
            "tdengine": {
                "count": 5,
                "classifications": [
                    "TICK_DATA",
                    "MINUTE_KLINE",
                    "ORDER_BOOK_DEPTH",
                    "LEVEL2_SNAPSHOT",
                    "INDEX_QUOTES",
                ],
            },
            "postgresql": {
                "count": 29,
                "categories": [
                    "æ—¥çº¿å¸‚åœºæ•°æ®",
                    "å‚è€ƒæ•°æ® (è‚¡ç¥¨ä¿¡æ¯ã€äº¤æ˜“æ—¥å†ç­‰)",
                    "è¡ç”Ÿæ•°æ® (æŠ€æœ¯æŒ‡æ ‡ã€é‡åŒ–å› å­ç­‰)",
                    "äº¤æ˜“æ•°æ® (è®¢å•ã€æˆäº¤ã€æŒä»“ç­‰)",
                    "å…ƒæ•°æ® (ç³»ç»Ÿé…ç½®ã€æ•°æ®æºçŠ¶æ€ç­‰)",
                ],
            },
        },
        "architecture": "dual-database",
        "description": "TDengine + PostgreSQL åŒæ•°æ®åº“æ¶æ„",
        "simplified_from": "4 databases (MySQL, Redis, TDengine, PostgreSQL)",
        "simplified_to": "2 databases (TDengine, PostgreSQL)",
        "simplification_date": "2025-10-25",
        "total_classifications": 34,
        "removed_databases": {
            "mysql": {
                "status": "removed",
                "migrated_to": "PostgreSQL",
                "migration_date": "2025-10-19",
                "rows_migrated": 299,
            },
            "redis": {
                "status": "removed",
                "reason": "é…ç½®çš„db1æœªä½¿ç”¨,åº”ç”¨å±‚ç¼“å­˜æ›¿ä»£",
                "removal_date": "2025-10-25",
            },
        },
        "timestamp": datetime.now().isoformat(),
    }

    return {"success": True, "message": "æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ", "data": stats_data}
