#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•æ•°æ®ç®¡é“

æä¾›æµ‹è¯•æ•°æ®çš„ETLæµç¨‹ã€æ•°æ®è½¬æ¢ã€éªŒè¯å’Œä¼˜åŒ–åŠŸèƒ½ï¼Œç¡®ä¿æµ‹è¯•æ•°æ®çš„é«˜æ•ˆæµåŠ¨å’Œç®¡ç†ã€‚
"""

import asyncio
import json
import logging
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
import pandas as pd
from concurrent.futures import ThreadPoolExecutor

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class PipelineStage(Enum):
    """ç®¡é“å¤„ç†é˜¶æ®µæšä¸¾"""

    INGESTION = "ingestion"  # æ•°æ®æ‘„å…¥
    VALIDATION = "validation"  # æ•°æ®éªŒè¯
    TRANSFORMATION = "transformation"  # æ•°æ®è½¬æ¢
    AGGREGATION = "aggregation"  # æ•°æ®èšåˆ
    OPTIMIZATION = "optimization"  # æ•°æ®ä¼˜åŒ–
    EXPORT = "export"  # æ•°æ®å¯¼å‡º


class DataQuality(Enum):
    """æ•°æ®è´¨é‡çº§åˆ«æšä¸¾"""

    EXCELLENT = "excellent"
    GOOD = "good"
    FAIR = "fair"
    POOR = "poor"
    INVALID = "invalid"


@dataclass
class PipelineConfig:
    """ç®¡é“é…ç½®ç±»"""

    # åŸºç¡€é…ç½®
    name: str
    description: str

    # æ€§èƒ½é…ç½®
    max_workers: int = 4
    batch_size: int = 1000
    timeout_seconds: int = 300

    # è´¨é‡é…ç½®
    quality_threshold: float = 0.8
    validation_enabled: bool = True
    auto_repair: bool = True

    # å­˜å‚¨é…ç½®
    storage_path: str = "data/pipeline"
    cache_enabled: bool = True
    cache_ttl: int = 3600

    # ç›‘æ§é…ç½®
    enable_monitoring: bool = True
    metrics_interval: int = 60

    # é”™è¯¯å¤„ç†é…ç½®
    fail_fast: bool = False
    max_retries: int = 3
    retry_delay: float = 1.0


@dataclass
class DataBatch:
    """æ•°æ®æ‰¹æ¬¡ç±»"""

    id: str
    source: str
    data: List[Dict[str, Any]]
    metadata: Dict[str, Any]
    created_at: datetime = field(default_factory=datetime.now)
    quality_score: float = 0.0
    processing_stage: PipelineStage = PipelineStage.INGESTION

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "id": self.id,
            "source": self.source,
            "data": self.data,
            "metadata": self.metadata,
            "created_at": self.created_at.isoformat(),
            "quality_score": self.quality_score,
            "processing_stage": self.processing_stage.value,
        }


@dataclass
class PipelineMetrics:
    """ç®¡é“æ€§èƒ½æŒ‡æ ‡"""

    total_records: int = 0
    processed_records: int = 0
    failed_records: int = 0
    batches_processed: int = 0
    total_duration: float = 0.0
    avg_processing_time: float = 0.0
    quality_score: float = 0.0
    throughput_records_per_second: float = 0.0

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "total_records": self.total_records,
            "processed_records": self.processed_records,
            "failed_records": self.failed_records,
            "batches_processed": self.batches_processed,
            "total_duration": self.total_duration,
            "avg_processing_time": self.avg_processing_time,
            "quality_score": self.quality_score,
            "throughput_records_per_second": self.throughput_records_per_second,
        }


class TestDataValidator:
    """æµ‹è¯•æ•°æ®éªŒè¯å™¨"""

    def __init__(self):
        self.validation_rules = {
            "market_data": self._validate_market_data,
            "trading_data": self._validate_trading_data,
            "performance_data": self._validate_performance_data,
            "ai_data": self._validate_ai_data,
        }

    def validate_batch(self, batch: DataBatch) -> Tuple[bool, List[str]]:
        """éªŒè¯æ•°æ®æ‰¹æ¬¡"""
        errors = []

        # åŸºç¡€ç»“æ„éªŒè¯
        if not batch.data:
            errors.append("æ•°æ®ä¸ºç©º")
            return False, errors

        # æ•°æ®ç±»å‹éªŒè¯
        for i, record in enumerate(batch.data[:100]):  # åªæ£€æŸ¥å‰100æ¡è®°å½•
            if not isinstance(record, dict):
                errors.append(f"ç¬¬{i}æ¡è®°å½•ä¸æ˜¯å­—å…¸ç±»å‹")
                continue

            # æ ¹æ®æºç±»å‹åº”ç”¨ç‰¹å®šéªŒè¯è§„åˆ™
            validator = self.validation_rules.get(batch.source)
            if validator:
                record_errors = validator(record)
                errors.extend(record_errors)

        quality_score = self._calculate_quality_score(batch.data)
        batch.quality_score = quality_score

        passed = quality_score >= 0.7 and len(errors) == 0
        return passed, errors

    def _validate_market_data(self, record: Dict[str, Any]) -> List[str]:
        """éªŒè¯å¸‚åœºæ•°æ®æ ¼å¼"""
        errors = []

        # å¿…éœ€å­—æ®µæ£€æŸ¥
        required_fields = ["symbol", "date", "open", "high", "low", "close", "volume"]
        missing_fields = [f for f in required_fields if f not in record]
        if missing_fields:
            errors.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")

        # æ•°æ®ç±»å‹æ£€æŸ¥
        if "symbol" in record and not isinstance(record["symbol"], str):
            errors.append("symbol å¿…é¡»æ˜¯å­—ç¬¦ä¸²")

        if "volume" in record and not isinstance(record["volume"], (int, float)):
            errors.append("volume å¿…é¡»æ˜¯æ•°å­—")

        # æ•°å€¼èŒƒå›´æ£€æŸ¥
        numeric_fields = ["open", "high", "low", "close"]
        for field in numeric_fields:
            if field in record:
                value = record[field]
                if isinstance(value, (int, float)):
                    if value <= 0:
                        errors.append(f"{field} å¿…é¡»å¤§äº0")
                    if value > 1000000:  # å¼‚å¸¸é«˜ä»·æ£€æŸ¥
                        errors.append(f"{field} æ•°å€¼å¼‚å¸¸é«˜")

        return errors

    def _validate_trading_data(self, record: Dict[str, Any]) -> List[str]:
        """éªŒè¯äº¤æ˜“æ•°æ®æ ¼å¼"""
        errors = []

        # å¿…éœ€å­—æ®µæ£€æŸ¥
        required_fields = ["symbol", "quantity", "price", "timestamp"]
        missing_fields = [f for f in required_fields if f not in record]
        if missing_fields:
            errors.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")

        # æ•°å€¼èŒƒå›´æ£€æŸ¥
        if "quantity" in record and isinstance(record["quantity"], (int, float)):
            if record["quantity"] <= 0:
                errors.append("quantity å¿…é¡»å¤§äº0")
            if record["quantity"] > 1000000:
                errors.append("quantity æ•°å€¼å¼‚å¸¸å¤§")

        if "price" in record and isinstance(record["price"], (int, float)):
            if record["price"] <= 0:
                errors.append("price å¿…é¡»å¤§äº0")
            if record["price"] > 1000000:
                errors.append("price æ•°å€¼å¼‚å¸¸é«˜")

        return errors

    def _validate_performance_data(self, record: Dict[str, Any]) -> List[str]:
        """éªŒè¯æ€§èƒ½æ•°æ®æ ¼å¼"""
        errors = []

        # å¿…éœ€å­—æ®µæ£€æŸ¥
        required_fields = ["test_name", "duration", "status"]
        missing_fields = [f for f in required_fields if f not in record]
        if missing_fields:
            errors.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")

        # çŠ¶æ€å€¼æ£€æŸ¥
        if "status" in record and record["status"] not in [
            "passed",
            "failed",
            "skipped",
        ]:
            errors.append("status å¿…é¡»æ˜¯ passed/failed/skipped ä¹‹ä¸€")

        return errors

    def _validate_ai_data(self, record: Dict[str, Any]) -> List[str]:
        """éªŒè¯AIæ•°æ®æ ¼å¼"""
        errors = []

        # å¿…éœ€å­—æ®µæ£€æŸ¥
        required_fields = ["symbol", "timestamp", "prediction", "confidence"]
        missing_fields = [f for f in required_fields if f not in record]
        if missing_fields:
            errors.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")

        # ç½®ä¿¡åº¦èŒƒå›´æ£€æŸ¥
        if "confidence" in record and isinstance(record["confidence"], (int, float)):
            if not (0 <= record["confidence"] <= 1):
                errors.append("confidence å¿…é¡»åœ¨0-1ä¹‹é—´")

        return errors

    def _calculate_quality_score(self, data: List[Dict[str, Any]]) -> float:
        """è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°"""
        if not data:
            return 0.0

        score = 100.0
        total_records = len(data)

        # æ£€æŸ¥è®°å½•å®Œæ•´æ€§
        for record in data[:100]:  # åªæ£€æŸ¥å‰100æ¡è®°å½•
            if not isinstance(record, dict):
                score -= 5
                continue

            # åŸºç¡€å­—æ®µå­˜åœ¨æ€§æ£€æŸ¥
            if not record:
                score -= 10
                continue

            # ç©ºå€¼æ¯”ä¾‹æ£€æŸ¥
            empty_fields = sum(1 for v in record.values() if v is None or v == "")
            if empty_fields > len(record) * 0.5:  # ç©ºå€¼è¶…è¿‡50%
                score -= 20
                break

        # æ£€æŸ¥é‡å¤è®°å½•
        if total_records > 1:
            seen = set()
            duplicates = 0
            for record in data:
                record_str = json.dumps(record, sort_keys=True)
                if record_str in seen:
                    duplicates += 1
                seen.add(record_str)

            duplicate_ratio = duplicates / total_records
            score -= duplicate_ratio * 30

        return max(0.0, min(100.0, score))


class DataTransformer:
    """æ•°æ®è½¬æ¢å™¨"""

    def __init__(self):
        self.transformers = {
            "normalize": self._normalize_data,
            "aggregate": self._aggregate_data,
            "filter": self._filter_data,
            "enrich": self._enrich_data,
            "optimize": self._optimize_data,
        }

    def transform(self, batch: DataBatch, transformations: List[str]) -> DataBatch:
        """åº”ç”¨æ•°æ®è½¬æ¢"""
        transformed_data = batch.data.copy()

        for transform_name in transformations:
            transformer = self.transformers.get(transform_name)
            if transformer:
                try:
                    transformed_data = transformer(transformed_data, batch.metadata)
                    logger.info(f"åº”ç”¨è½¬æ¢: {transform_name}")
                except Exception as e:
                    logger.error(f"è½¬æ¢å¤±è´¥ {transform_name}: {str(e)}")
                    continue

        # åˆ›å»ºæ–°çš„æ‰¹æ¬¡
        new_batch = DataBatch(
            id=f"{batch.id}_transformed",
            source=batch.source,
            data=transformed_data,
            metadata=batch.metadata.copy(),
            quality_score=batch.quality_score,
        )
        new_batch.processing_stage = PipelineStage.TRANSFORMATION

        return new_batch

    def _normalize_data(
        self, data: List[Dict[str, Any]], metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """æ ‡å‡†åŒ–æ•°æ®æ ¼å¼"""
        normalized = []

        for record in data:
            # æ ‡å‡†åŒ–æ—¶é—´æˆ³æ ¼å¼
            if "date" in record:
                record["timestamp"] = record["date"]

            # æ ‡å‡†åŒ–ä»·æ ¼å­—æ®µ
            price_fields = ["open", "high", "low", "close"]
            for field in price_fields:
                if field in record and isinstance(record[field], str):
                    try:
                        record[field] = float(record[field].replace(",", ""))
                    except:
                        pass

            # æ ‡å‡†åŒ–æˆäº¤é‡
            if "volume" in record and isinstance(record["volume"], str):
                try:
                    record["volume"] = int(record["volume"].replace(",", ""))
                except:
                    pass

            normalized.append(record)

        return normalized

    def _aggregate_data(
        self, data: List[Dict[str, Any]], metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """èšåˆæ•°æ®"""
        if not data:
            return []

        # æŒ‰è‚¡ç¥¨ä»£ç åˆ†ç»„èšåˆ
        grouped = {}
        for record in data:
            symbol = record.get("symbol", "unknown")
            if symbol not in grouped:
                grouped[symbol] = []
            grouped[symbol].append(record)

        aggregated = []
        for symbol, records in grouped.items():
            if len(records) == 1:
                aggregated.append(records[0])
                continue

            # è®¡ç®—èšåˆæŒ‡æ ‡
            agg_record = {
                "symbol": symbol,
                "record_count": len(records),
                "timestamp": datetime.now().isoformat(),
                "aggregated": True,
            }

            # è®¡ç®—ä»·æ ¼èšåˆ
            price_fields = ["open", "high", "low", "close"]
            for field in price_fields:
                values = [r.get(field) for r in records if r.get(field) is not None]
                if values:
                    agg_record[f"avg_{field}"] = sum(values) / len(values)
                    agg_record[f"max_{field}"] = max(values)
                    agg_record[f"min_{field}"] = min(values)

            # è®¡ç®—æˆäº¤é‡èšåˆ
            volumes = [r.get("volume") for r in records if r.get("volume") is not None]
            if volumes:
                agg_record["total_volume"] = sum(volumes)
                agg_record["avg_volume"] = sum(volumes) / len(volumes)

            aggregated.append(agg_record)

        return aggregated

    def _filter_data(
        self, data: List[Dict[str, Any]], metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """è¿‡æ»¤æ•°æ®"""
        filtered = []

        for record in data:
            # è¿‡æ»¤æ— æ•ˆæ•°æ®
            if not record:
                continue

            # è¿‡æ»¤ç©ºè®°å½•
            if all(v is None or v == "" for v in record.values()):
                continue

            # è¿‡æ»¤å¼‚å¸¸ä»·æ ¼ï¼ˆæ ¹æ®å…ƒæ•°æ®ä¸­çš„èŒƒå›´é…ç½®ï¼‰
            price_ranges = metadata.get("price_ranges", {})
            for field, range_config in price_ranges.items():
                if field in record and isinstance(record[field], (int, float)):
                    min_val, max_val = range_config
                    if not (min_val <= record[field] <= max_val):
                        break
            else:
                filtered.append(record)

        return filtered

    def _enrich_data(
        self, data: List[Dict[str, Any]], metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ä¸°å¯Œæ•°æ®"""
        enriched = []

        for record in data:
            enriched_record = record.copy()

            # æ·»åŠ å¤„ç†æ—¶é—´æˆ³
            enriched_record["processed_at"] = datetime.now().isoformat()

            # æ·»åŠ æ•°æ®æ¥æº
            enriched_record["source"] = metadata.get("source", "unknown")

            # æ·»åŠ è´¨é‡åˆ†æ•°
            enriched_record["quality_score"] = metadata.get("quality_score", 0.0)

            enriched.append(enriched_record)

        return enriched

    def _optimize_data(
        self, data: List[Dict[str, Any]], metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–æ•°æ®ç»“æ„"""
        optimized = []

        for record in data:
            optimized_record = {}

            # åªä¿ç•™å¿…è¦å­—æ®µ
            essential_fields = metadata.get(
                "essential_fields", ["id", "symbol", "timestamp"]
            )
            for field in essential_fields:
                if field in record:
                    optimized_record[field] = record[field]

            # å‹ç¼©æ•°å€¼ç²¾åº¦
            for key, value in optimized_record.items():
                if isinstance(value, float):
                    optimized_record[key] = round(value, 4)
                elif isinstance(value, str) and len(value) > 100:
                    # æˆªæ–­è¿‡é•¿çš„å­—ç¬¦ä¸²
                    optimized_record[key] = value[:100] + "..."

            optimized.append(optimized_record)

        return optimized


class TestDataPipeline:
    """æµ‹è¯•æ•°æ®ç®¡é“ä¸»ç±»"""

    def __init__(self, config: PipelineConfig):
        self.config = config
        self.validator = TestDataValidator()
        self.transformer = DataTransformer()
        self.metrics = PipelineMetrics()
        self.batches: Dict[str, DataBatch] = {}
        self.processing_queue = asyncio.Queue()
        self.executor = ThreadPoolExecutor(max_workers=config.max_workers)

        # åˆ›å»ºå­˜å‚¨ç›®å½•
        self.storage_path = Path(config.storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)

        logger.info(f"åˆå§‹åŒ–æµ‹è¯•æ•°æ®ç®¡é“: {config.name}")

    async def ingest_data(
        self,
        source: str,
        data: List[Dict[str, Any]],
        metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """æ‘„å…¥æ•°æ®"""
        batch_id = f"{source}_{int(time.time())}"
        batch_metadata = metadata or {}
        batch_metadata.update(
            {
                "source": source,
                "ingestion_time": datetime.now().isoformat(),
                "record_count": len(data),
            }
        )

        batch = DataBatch(
            id=batch_id, source=source, data=data, metadata=batch_metadata
        )

        # å°†æ‰¹æ¬¡åŠ å…¥å¤„ç†é˜Ÿåˆ—
        await self.processing_queue.put(batch)
        self.batches[batch_id] = batch

        logger.info(f"æ•°æ®å·²æ‘„å…¥: {batch_id} ({len(data)} æ¡è®°å½•)")
        return batch_id

    async def process_pipeline(self) -> List[str]:
        """å¤„ç†ç®¡é“ä¸­çš„æ‰€æœ‰æ‰¹æ¬¡"""
        processed_batches = []

        while not self.processing_queue.empty():
            batch = await self.processing_queue.get()

            try:
                # å¤„ç†æ‰¹æ¬¡
                result_batch = await self._process_batch(batch)
                processed_batches.append(result_batch.id)

                # æ›´æ–°æŒ‡æ ‡
                self.metrics.batches_processed += 1
                self.metrics.processed_records += len(result_batch.data)

                logger.info(f"æ‰¹æ¬¡å¤„ç†å®Œæˆ: {batch.id}")

            except Exception as e:
                logger.error(f"æ‰¹æ¬¡å¤„ç†å¤±è´¥ {batch.id}: {str(e)}")
                self.metrics.failed_records += len(batch.data)
                if self.config.fail_fast:
                    break

        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        if self.metrics.batches_processed > 0:
            self.metrics.avg_processing_time = (
                self.metrics.total_duration / self.metrics.batches_processed
            )

        return processed_batches

    async def _process_batch(self, batch: DataBatch) -> DataBatch:
        """å¤„ç†å•ä¸ªæ•°æ®æ‰¹æ¬¡"""
        start_time = time.time()

        try:
            # é˜¶æ®µ1: éªŒè¯
            if self.config.validation_enabled:
                passed, errors = self.validator.validate_batch(batch)
                if not passed and self.config.auto_repair:
                    batch = self._auto_repair_batch(batch, errors)
                    # é‡æ–°éªŒè¯
                    passed, errors = self.validator.validate_batch(batch)

                if not passed:
                    logger.warning(f"æ‰¹æ¬¡éªŒè¯å¤±è´¥ {batch.id}: {errors}")
                    # åˆ›å»ºå¤±è´¥æ‰¹æ¬¡ä½†ç»§ç»­å¤„ç†
                    failed_batch = DataBatch(
                        id=f"{batch.id}_failed",
                        source=batch.source,
                        data=[],  # å¤±è´¥æ‰¹æ¬¡ä¸ºç©º
                        metadata=batch.metadata,
                        quality_score=batch.quality_score,
                    )
                    return failed_batch

            # é˜¶æ®µ2: è½¬æ¢
            transformations = self.config.metadata.get("transformations", [])
            if transformations:
                batch = self.transformer.transform(batch, transformations)

            # é˜¶æ®µ3: ä¼˜åŒ–
            self._optimize_batch(batch)

            # é˜¶æ®µ4: å­˜å‚¨
            await self._store_batch(batch)

            # æ›´æ–°å¤„ç†çŠ¶æ€
            batch.processing_stage = PipelineStage.EXPORT

            processing_time = time.time() - start_time
            self.metrics.total_duration += processing_time

            logger.info(f"æ‰¹æ¬¡å¤„ç†æˆåŠŸ {batch.id}: {processing_time:.2f}ç§’")

            return batch

        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"æ‰¹æ¬¡å¤„ç†å¼‚å¸¸ {batch.id}: {str(e)}")

            # åˆ›å»ºé”™è¯¯æ‰¹æ¬¡
            error_batch = DataBatch(
                id=f"{batch.id}_error",
                source=batch.source,
                data=[],
                metadata=batch.metadata,
                quality_score=0.0,
            )
            return error_batch

    def _auto_repair_batch(self, batch: DataBatch, errors: List[str]) -> DataBatch:
        """è‡ªåŠ¨ä¿®å¤æ‰¹æ¬¡æ•°æ®"""
        repaired_data = []

        for record in batch.data:
            repaired_record = record.copy()

            # ä¿®å¤å¸¸è§é—®é¢˜
            if not isinstance(repaired_record, dict):
                continue

            # ä¿®å¤ç¼ºå¤±çš„å¿…éœ€å­—æ®µ
            essential_fields = ["symbol", "timestamp", "value"]
            for field in essential_fields:
                if field not in repaired_record:
                    repaired_record[field] = None

            # ä¿®å¤æ•°æ®ç±»å‹
            if "volume" in repaired_record and isinstance(
                repaired_record["volume"], str
            ):
                try:
                    repaired_record["volume"] = int(
                        repaired_record["volume"].replace(",", "")
                    )
                except:
                    repaired_record["volume"] = 0

            repaired_data.append(repaired_record)

        repaired_batch = DataBatch(
            id=f"{batch.id}_repaired",
            source=batch.source,
            data=repaired_data,
            metadata=batch.metadata,
            quality_score=batch.quality_score,
        )

        logger.info(
            f"è‡ªåŠ¨ä¿®å¤æ‰¹æ¬¡: {batch.id} ({len(batch.data)} -> {len(repaired_data)})"
        )
        return repaired_batch

    def _optimize_batch(self, batch: DataBatch):
        """ä¼˜åŒ–æ‰¹æ¬¡æ•°æ®"""
        # å»é‡
        unique_data = []
        seen = set()

        for record in batch.data:
            record_str = json.dumps(record, sort_keys=True)
            if record_str not in seen:
                seen.add(record_str)
                unique_data.append(record)

        batch.data = unique_data
        logger.info(f"æ‰¹æ¬¡æ•°æ®å»é‡: {batch.id} ({len(batch.data)} æ¡å”¯ä¸€è®°å½•)")

    async def _store_batch(self, batch: DataBatch):
        """å­˜å‚¨æ‰¹æ¬¡æ•°æ®"""
        try:
            # åˆ›å»ºå­˜å‚¨æ–‡ä»¶
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{batch.source}_{batch.id}_{timestamp}.json"
            filepath = self.storage_path / filename

            # åºåˆ—åŒ–æ•°æ®
            serialized = {"batch_info": batch.to_dict(), "data": batch.data}

            # å†™å…¥æ–‡ä»¶
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(serialized, f, indent=2, ensure_ascii=False)

            logger.info(f"æ‰¹æ¬¡å·²å­˜å‚¨: {filepath}")

        except Exception as e:
            logger.error(f"æ‰¹æ¬¡å­˜å‚¨å¤±è´¥ {batch.id}: {str(e)}")

    async def export_data(
        self, batch_ids: List[str], output_path: str, format: str = "json"
    ) -> bool:
        """å¯¼å‡ºæ•°æ®"""
        try:
            export_data = []

            for batch_id in batch_ids:
                if batch_id in self.batches:
                    batch = self.batches[batch_id]
                    export_data.extend(batch.data)

            if not export_data:
                logger.warning("æ²¡æœ‰å¯å¯¼å‡ºçš„æ•°æ®")
                return False

            # æ ¹æ®æ ¼å¼å¯¼å‡º
            output_file = Path(output_path)

            if format == "json":
                with open(output_file, "w", encoding="utf-8") as f:
                    json.dump(export_data, f, indent=2, ensure_ascii=False)

            elif format == "csv":
                df = pd.DataFrame(export_data)
                df.to_csv(output_file, index=False)

            elif format == "parquet":
                df = pd.DataFrame(export_data)
                df.to_parquet(output_file)

            else:
                logger.error(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format}")
                return False

            logger.info(f"æ•°æ®å¯¼å‡ºå®Œæˆ: {output_file} ({len(export_data)} æ¡è®°å½•)")
            return True

        except Exception as e:
            logger.error(f"æ•°æ®å¯¼å‡ºå¤±è´¥: {str(e)}")
            return False

    def get_metrics(self) -> PipelineMetrics:
        """è·å–ç®¡é“æ€§èƒ½æŒ‡æ ‡"""
        return self.metrics

    def get_batch_info(self, batch_id: str) -> Optional[Dict[str, Any]]:
        """è·å–æ‰¹æ¬¡ä¿¡æ¯"""
        if batch_id in self.batches:
            return self.batches[batch_id].to_dict()
        return None

    def list_batches(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰æ‰¹æ¬¡"""
        return [batch.to_dict() for batch in self.batches.values()]

    async def clear_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        self.batches.clear()
        self.processing_queue = asyncio.Queue()

        # æ¸…ç†å­˜å‚¨ç›®å½•ä¸­çš„æ—§æ–‡ä»¶
        cutoff_time = datetime.now() - timedelta(days=7)
        for file_path in self.storage_path.glob("*.json"):
            if datetime.fromtimestamp(file_path.stat().st_mtime) < cutoff_time:
                file_path.unlink()
                logger.info(f"æ¸…ç†æ—§æ–‡ä»¶: {file_path}")

        logger.info("ç¼“å­˜æ¸…ç†å®Œæˆ")


# ç®¡é“å·¥å‚ç±»
class PipelineFactory:
    """ç®¡é“å·¥å‚ç±»"""

    @staticmethod
    def create_market_data_pipeline() -> TestDataPipeline:
        """åˆ›å»ºå¸‚åœºæ•°æ®ç®¡é“"""
        config = PipelineConfig(
            name="market_data_pipeline",
            description="å¸‚åœºæ•°æ®å¤„ç†ç®¡é“",
            max_workers=8,
            batch_size=5000,
            quality_threshold=0.85,
            enable_monitoring=True,
        )

        # å¸‚åœºæ•°æ®ç‰¹å®šçš„é…ç½®
        config.metadata = {
            "transformations": ["normalize", "aggregate"],
            "essential_fields": [
                "symbol",
                "timestamp",
                "open",
                "high",
                "low",
                "close",
                "volume",
            ],
            "price_ranges": {
                "open": (0, 1000000),
                "high": (0, 1000000),
                "low": (0, 1000000),
                "close": (0, 1000000),
            },
        }

        return TestDataPipeline(config)

    @staticmethod
    def create_trading_data_pipeline() -> TestDataPipeline:
        """åˆ›å»ºäº¤æ˜“æ•°æ®ç®¡é“"""
        config = PipelineConfig(
            name="trading_data_pipeline",
            description="äº¤æ˜“æ•°æ®å¤„ç†ç®¡é“",
            max_workers=6,
            batch_size=2000,
            quality_threshold=0.9,
            auto_repair=True,
        )

        config.metadata = {
            "transformations": ["normalize", "filter"],
            "essential_fields": ["symbol", "quantity", "price", "timestamp"],
            "volume_ranges": {"quantity": (0, 1000000), "price": (0, 100000)},
        }

        return TestDataPipeline(config)

    @staticmethod
    def create_performance_data_pipeline() -> TestDataPipeline:
        """åˆ›å»ºæ€§èƒ½æ•°æ®ç®¡é“"""
        config = PipelineConfig(
            name="performance_data_pipeline",
            description="æ€§èƒ½æ•°æ®å¤„ç†ç®¡é“",
            max_workers=4,
            batch_size=1000,
            quality_threshold=0.95,
            fail_fast=True,
        )

        config.metadata = {
            "transformations": ["normalize", "optimize"],
            "essential_fields": ["test_name", "duration", "status", "timestamp"],
        }

        return TestDataPipeline(config)


# ä½¿ç”¨ç¤ºä¾‹
async def demo_data_pipeline():
    """æ¼”ç¤ºæ•°æ®ç®¡é“åŠŸèƒ½"""
    print("ğŸš€ æ¼”ç¤ºæµ‹è¯•æ•°æ®ç®¡é“åŠŸèƒ½")

    # åˆ›å»ºå¸‚åœºæ•°æ®ç®¡é“
    market_pipeline = PipelineFactory.create_market_data_pipeline()

    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    test_data = [
        {
            "symbol": "AAPL",
            "date": "2024-12-12",
            "open": 150.0,
            "high": 155.0,
            "low": 149.0,
            "close": 152.0,
            "volume": 1000000,
        }
        for _ in range(100)
    ]

    # æ‘„å…¥æ•°æ®
    batch_id = await market_pipeline.ingest_data("market_data", test_data)
    print(f"âœ“ æ•°æ®æ‘„å…¥å®Œæˆ: {batch_id}")

    # å¤„ç†ç®¡é“
    processed_batches = await market_pipeline.process_pipeline()
    print(f"âœ“ å¤„ç†å®Œæˆ {len(processed_batches)} ä¸ªæ‰¹æ¬¡")

    # è·å–æŒ‡æ ‡
    metrics = market_pipeline.get_metrics()
    print(f"ğŸ“Š ç®¡é“æŒ‡æ ‡: {metrics.to_dict()}")

    # å¯¼å‡ºæ•°æ®
    await market_pipeline.export_data(
        processed_batches, "exported_market_data.json", "json"
    )
    print("âœ“ æ•°æ®å¯¼å‡ºå®Œæˆ")

    # æ¸…ç†
    await market_pipeline.clear_cache()


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(demo_data_pipeline())
