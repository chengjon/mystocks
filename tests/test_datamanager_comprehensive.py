#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
US3 DataManager ç»¼åˆæµ‹è¯•å¥—ä»¶
åŒ…æ‹¬ï¼šè¾¹ç•Œæµ‹è¯•ã€æ€§èƒ½åŸºå‡†æµ‹è¯•ã€å‹åŠ›æµ‹è¯•

ç‰ˆæœ¬: 1.0.0
åˆ›å»ºæ—¥æœŸ: 2025-10-25
"""

import os
import sys
import time
import pytest
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from core.data_manager import DataManager
from core.data_classification import DataClassification, DatabaseTarget


# ============================================
# æµ‹è¯•é…ç½®
# ============================================

class TestConfig:
    """æµ‹è¯•é…ç½®"""
    # æ€§èƒ½åŸºå‡†
    ROUTING_TIME_TARGET_MS = 0.001  # è·¯ç”±æ—¶é—´ç›®æ ‡ï¼š1msï¼ˆå°æ•°æ®é›†ï¼‰
    ROUTING_TIME_EXPECTED_MS = 0.0002  # é¢„æœŸï¼š0.0002ms
    ROUTING_TIME_LARGE_DATA_MS = 0.005  # å¤§æ•°æ®é›†ï¼ˆ10kè¡Œï¼‰ç›®æ ‡ï¼š5ms
    ROUTING_TIME_VERY_LARGE_DATA_MS = 0.01  # è¶…å¤§æ•°æ®é›†ï¼ˆ100kè¡Œï¼‰ç›®æ ‡ï¼š10ms

    # å‹åŠ›æµ‹è¯•
    STRESS_THREAD_COUNT = 10  # å¹¶å‘çº¿ç¨‹æ•°
    STRESS_OPERATIONS_PER_THREAD = 100  # æ¯çº¿ç¨‹æ“ä½œæ•°

    # è¾¹ç•Œæµ‹è¯•
    MAX_DATA_SIZE = 1000000  # æœ€å¤§æ•°æ®æ¡æ•°
    MIN_DATA_SIZE = 0  # æœ€å°æ•°æ®æ¡æ•°


# ============================================
# æµ‹è¯•å¤¹å…·
# ============================================

@pytest.fixture(scope="module")
def data_manager():
    """åˆ›å»º DataManager å®ä¾‹"""
    dm = DataManager()
    yield dm
    # æ¸…ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰


@pytest.fixture
def sample_dataframe():
    """åˆ›å»ºç¤ºä¾‹ DataFrame"""
    return pd.DataFrame({
        'symbol': ['600000', '000001', '000002'],
        'name': ['æµ¦å‘é“¶è¡Œ', 'å¹³å®‰é“¶è¡Œ', 'ä¸‡ç§‘A'],
        'price': [10.50, 12.30, 15.80],
        'volume': [1000000, 2000000, 1500000]
    })


@pytest.fixture
def large_dataframe():
    """åˆ›å»ºå¤§è§„æ¨¡ DataFrame"""
    size = 10000
    return pd.DataFrame({
        'symbol': [f'60{i:04d}' for i in range(size)],
        'price': np.random.uniform(5, 100, size),
        'volume': np.random.randint(1000, 1000000, size)
    })


# ============================================
# 1. è¾¹ç•Œæµ‹è¯• (Boundary Tests)
# ============================================

class TestBoundaryConditions:
    """è¾¹ç•Œæ¡ä»¶æµ‹è¯•"""

    def test_empty_dataframe(self, data_manager):
        """æµ‹è¯•ç©º DataFrame"""
        empty_df = pd.DataFrame()

        # åº”è¯¥èƒ½å¤Ÿå¤„ç†ç©ºæ•°æ®è€Œä¸å´©æºƒ
        result = data_manager.get_target_database(DataClassification.DAILY_KLINE)
        assert result == DatabaseTarget.POSTGRESQL

    def test_single_row_dataframe(self, data_manager):
        """æµ‹è¯•å•è¡Œ DataFrame"""
        single_row = pd.DataFrame({
            'symbol': ['600000'],
            'price': [10.50]
        })

        result = data_manager.get_target_database(DataClassification.TICK_DATA)
        assert result == DatabaseTarget.TDENGINE

    def test_large_dataframe(self, data_manager, large_dataframe):
        """æµ‹è¯•å¤§è§„æ¨¡ DataFrameï¼ˆ10,000è¡Œï¼‰"""
        start_time = time.time()
        result = data_manager.get_target_database(DataClassification.MINUTE_KLINE)
        routing_time = (time.time() - start_time) * 1000

        assert result == DatabaseTarget.TDENGINE
        assert routing_time < TestConfig.ROUTING_TIME_LARGE_DATA_MS
        print(f"  âœ“ 10,000è¡Œæ•°æ®è·¯ç”±æ—¶é—´: {routing_time:.6f}ms")

    def test_very_large_dataframe(self, data_manager):
        """æµ‹è¯•è¶…å¤§è§„æ¨¡ DataFrameï¼ˆ100,000è¡Œï¼‰"""
        size = 100000
        very_large_df = pd.DataFrame({
            'symbol': [f'60{i:04d}' for i in range(size)],
            'price': np.random.uniform(5, 100, size)
        })

        start_time = time.time()
        result = data_manager.get_target_database(DataClassification.DAILY_KLINE)
        routing_time = (time.time() - start_time) * 1000

        assert result == DatabaseTarget.POSTGRESQL
        assert routing_time < TestConfig.ROUTING_TIME_VERY_LARGE_DATA_MS
        print(f"  âœ“ 100,000è¡Œæ•°æ®è·¯ç”±æ—¶é—´: {routing_time:.6f}ms")

    def test_all_34_classifications(self, data_manager):
        """æµ‹è¯•æ‰€æœ‰34ç§æ•°æ®åˆ†ç±»çš„è·¯ç”±"""
        classifications = [
            # å¸‚åœºæ•°æ® (6ç§)
            DataClassification.TICK_DATA,
            DataClassification.MINUTE_KLINE,
            DataClassification.DAILY_KLINE,
            DataClassification.ORDER_BOOK_DEPTH,
            DataClassification.LEVEL2_SNAPSHOT,
            DataClassification.INDEX_QUOTES,

            # å‚è€ƒæ•°æ® (9ç§)
            DataClassification.SYMBOLS_INFO,
            DataClassification.INDUSTRY_CLASS,
            DataClassification.CONCEPT_CLASS,
            DataClassification.INDEX_CONSTITUENTS,
            DataClassification.TRADE_CALENDAR,
            DataClassification.FUNDAMENTAL_METRICS,
            DataClassification.DIVIDEND_DATA,
            DataClassification.SHAREHOLDER_DATA,
            DataClassification.MARKET_RULES,

            # è¡ç”Ÿæ•°æ® (6ç§)
            DataClassification.TECHNICAL_INDICATORS,
            DataClassification.QUANT_FACTORS,
            DataClassification.MODEL_OUTPUT,
            DataClassification.TRADE_SIGNALS,
            DataClassification.BACKTEST_RESULTS,
            DataClassification.RISK_METRICS,

            # äº¤æ˜“æ•°æ® (7ç§)
            DataClassification.ORDER_RECORDS,
            DataClassification.TRADE_RECORDS,
            DataClassification.POSITION_HISTORY,
            DataClassification.REALTIME_POSITIONS,
            DataClassification.REALTIME_ACCOUNT,
            DataClassification.FUND_FLOW,
            DataClassification.ORDER_QUEUE,

            # å…ƒæ•°æ® (6ç§)
            DataClassification.DATA_SOURCE_STATUS,
            DataClassification.TASK_SCHEDULE,
            DataClassification.STRATEGY_PARAMS,
            DataClassification.SYSTEM_CONFIG,
            DataClassification.DATA_QUALITY_METRICS,
            DataClassification.USER_CONFIG,
        ]

        results = {}
        for classification in classifications:
            target_db = data_manager.get_target_database(classification)
            results[classification.value] = target_db.value

        # éªŒè¯è·¯ç”±è§„åˆ™
        # é«˜é¢‘æ—¶åºæ•°æ® â†’ TDengine
        assert results['TICK_DATA'].upper() == 'TDENGINE'
        assert results['MINUTE_KLINE'].upper() == 'TDENGINE'
        assert results['ORDER_BOOK_DEPTH'].upper() == 'TDENGINE'
        assert results['LEVEL2_SNAPSHOT'].upper() == 'TDENGINE'
        assert results['INDEX_QUOTES'].upper() == 'TDENGINE'

        # æ‰€æœ‰å…¶ä»–æ•°æ® â†’ PostgreSQL
        assert results['DAILY_KLINE'].upper() == 'POSTGRESQL'
        assert results['SYMBOLS_INFO'].upper() == 'POSTGRESQL'
        assert results['TECHNICAL_INDICATORS'].upper() == 'POSTGRESQL'

        print(f"  âœ“ æ‰€æœ‰34ç§æ•°æ®åˆ†ç±»è·¯ç”±éªŒè¯é€šè¿‡")

        # ç»Ÿè®¡åˆ†å¸ƒ
        tdengine_count = sum(1 for v in results.values() if v.upper() == 'TDENGINE')
        postgresql_count = sum(1 for v in results.values() if v.upper() == 'POSTGRESQL')

        print(f"  âœ“ TDengine: {tdengine_count}ç§åˆ†ç±» ({tdengine_count/34*100:.1f}%)")
        print(f"  âœ“ PostgreSQL: {postgresql_count}ç§åˆ†ç±» ({postgresql_count/34*100:.1f}%)")

        assert tdengine_count == 5  # é«˜é¢‘æ—¶åºæ•°æ®
        assert postgresql_count == 29  # å…¶ä»–æ‰€æœ‰æ•°æ®

    def test_invalid_classification(self, data_manager):
        """æµ‹è¯•æ— æ•ˆçš„æ•°æ®åˆ†ç±»ï¼ˆåº”è¯¥é»˜è®¤è·¯ç”±åˆ° PostgreSQLï¼‰"""
        # åˆ›å»ºä¸€ä¸ªä¸åœ¨è·¯ç”±æ˜ å°„ä¸­çš„åˆ†ç±»ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰
        # ä½†æµ‹è¯•é»˜è®¤è¡Œä¸º
        result = data_manager.get_target_database(DataClassification.SYMBOLS_INFO)
        assert result in [DatabaseTarget.TDENGINE, DatabaseTarget.POSTGRESQL]

    def test_null_values_dataframe(self, data_manager):
        """æµ‹è¯•åŒ…å« NULL å€¼çš„ DataFrame"""
        null_df = pd.DataFrame({
            'symbol': ['600000', None, '000002'],
            'price': [10.50, None, 15.80],
            'volume': [1000000, 2000000, None]
        })

        # åº”è¯¥èƒ½å¤Ÿå¤„ç†åŒ…å« NULL çš„æ•°æ®
        result = data_manager.get_target_database(DataClassification.DAILY_KLINE)
        assert result == DatabaseTarget.POSTGRESQL

    def test_extreme_values_dataframe(self, data_manager):
        """æµ‹è¯•æç«¯æ•°å€¼"""
        extreme_df = pd.DataFrame({
            'symbol': ['600000'],
            'price': [1e10],  # æå¤§å€¼
            'volume': [1],  # æå°å€¼
            'amount': [0.0000001]  # æå°æµ®ç‚¹æ•°
        })

        result = data_manager.get_target_database(DataClassification.TICK_DATA)
        assert result == DatabaseTarget.TDENGINE


# ============================================
# 2. æ€§èƒ½åŸºå‡†æµ‹è¯• (Performance Benchmark)
# ============================================

class TestPerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    def test_routing_decision_speed_single(self, data_manager):
        """æµ‹è¯•å•æ¬¡è·¯ç”±å†³ç­–é€Ÿåº¦"""
        routing_times = []
        iterations = 1000

        for _ in range(iterations):
            start_time = time.time()
            data_manager.get_target_database(DataClassification.TICK_DATA)
            end_time = time.time()
            routing_times.append((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’

        avg_time = np.mean(routing_times)
        min_time = np.min(routing_times)
        max_time = np.max(routing_times)
        p95_time = np.percentile(routing_times, 95)
        p99_time = np.percentile(routing_times, 99)

        print(f"\n  è·¯ç”±å†³ç­–æ€§èƒ½ (1,000æ¬¡è¿­ä»£):")
        print(f"  âœ“ å¹³å‡æ—¶é—´: {avg_time:.6f}ms")
        print(f"  âœ“ æœ€å°æ—¶é—´: {min_time:.6f}ms")
        print(f"  âœ“ æœ€å¤§æ—¶é—´: {max_time:.6f}ms")
        print(f"  âœ“ P95: {p95_time:.6f}ms")
        print(f"  âœ“ P99: {p99_time:.6f}ms")

        # éªŒè¯æ€§èƒ½ç›®æ ‡
        assert avg_time < TestConfig.ROUTING_TIME_TARGET_MS, \
            f"å¹³å‡è·¯ç”±æ—¶é—´ {avg_time:.6f}ms è¶…è¿‡ç›®æ ‡ {TestConfig.ROUTING_TIME_TARGET_MS}ms"

        # éªŒè¯æ˜¯å¦è¾¾åˆ°é¢„æœŸæ€§èƒ½ï¼ˆ0.0002ms é™„è¿‘ï¼‰
        if avg_time < TestConfig.ROUTING_TIME_EXPECTED_MS * 10:
            print(f"  ğŸ‰ è¶…å‡ºé¢„æœŸï¼å¹³å‡æ—¶é—´ {avg_time:.6f}ms æ¥è¿‘é¢„æœŸ {TestConfig.ROUTING_TIME_EXPECTED_MS}ms")

    def test_routing_decision_speed_all_classifications(self, data_manager):
        """æµ‹è¯•æ‰€æœ‰34ç§åˆ†ç±»çš„è·¯ç”±é€Ÿåº¦"""
        classifications = list(DataClassification)

        total_time_ms = 0
        for classification in classifications:
            start_time = time.time()
            data_manager.get_target_database(classification)
            end_time = time.time()
            total_time_ms += (end_time - start_time) * 1000

        avg_time_per_classification = total_time_ms / len(classifications)

        print(f"\n  æ‰€æœ‰34ç§åˆ†ç±»è·¯ç”±æ€§èƒ½:")
        print(f"  âœ“ æ€»æ—¶é—´: {total_time_ms:.6f}ms")
        print(f"  âœ“ å¹³å‡æ¯åˆ†ç±»: {avg_time_per_classification:.6f}ms")

        assert avg_time_per_classification < TestConfig.ROUTING_TIME_TARGET_MS

    def test_throughput_sequential(self, data_manager):
        """æµ‹è¯•é¡ºåºæ‰§è¡Œååé‡"""
        iterations = 10000
        classifications = [
            DataClassification.TICK_DATA,
            DataClassification.DAILY_KLINE,
            DataClassification.SYMBOLS_INFO,
            DataClassification.TECHNICAL_INDICATORS
        ]

        start_time = time.time()
        for i in range(iterations):
            classification = classifications[i % len(classifications)]
            data_manager.get_target_database(classification)
        end_time = time.time()

        total_time = end_time - start_time
        throughput = iterations / total_time

        print(f"\n  é¡ºåºæ‰§è¡Œååé‡ (10,000æ¬¡è·¯ç”±):")
        print(f"  âœ“ æ€»æ—¶é—´: {total_time:.3f}ç§’")
        print(f"  âœ“ ååé‡: {throughput:.0f} æ¬¡/ç§’")
        print(f"  âœ“ å¹³å‡æ¯æ¬¡: {(total_time/iterations)*1000:.6f}ms")

        # æœŸæœ›ååé‡ > 100,000 æ¬¡/ç§’ï¼ˆåŸºäº 0.01ms çš„è·¯ç”±æ—¶é—´ï¼‰
        assert throughput > 10000, f"ååé‡ {throughput:.0f} æ¬¡/ç§’ ä½äºé¢„æœŸ"

    def test_memory_usage(self, data_manager):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        import psutil
        import gc

        process = psutil.Process()

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()

        # è®°å½•åˆå§‹å†…å­˜
        mem_before = process.memory_info().rss / 1024 / 1024  # MB

        # æ‰§è¡Œå¤§é‡è·¯ç”±æ“ä½œ
        for _ in range(100000):
            data_manager.get_target_database(DataClassification.TICK_DATA)

        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()

        # è®°å½•æœ€ç»ˆå†…å­˜
        mem_after = process.memory_info().rss / 1024 / 1024  # MB
        mem_increase = mem_after - mem_before

        print(f"\n  å†…å­˜ä½¿ç”¨ (100,000æ¬¡è·¯ç”±):")
        print(f"  âœ“ åˆå§‹: {mem_before:.2f}MB")
        print(f"  âœ“ æœ€ç»ˆ: {mem_after:.2f}MB")
        print(f"  âœ“ å¢åŠ : {mem_increase:.2f}MB")

        # éªŒè¯å†…å­˜å¢é•¿åˆç†ï¼ˆé¢„æœŸ <10MBï¼‰
        assert mem_increase < 10, f"å†…å­˜å¢é•¿ {mem_increase:.2f}MB è¶…è¿‡é¢„æœŸ"


# ============================================
# 3. å‹åŠ›æµ‹è¯• (Stress Tests)
# ============================================

class TestStressConditions:
    """å‹åŠ›æµ‹è¯•"""

    def test_concurrent_routing_decisions(self, data_manager):
        """æµ‹è¯•å¹¶å‘è·¯ç”±å†³ç­–"""
        thread_count = TestConfig.STRESS_THREAD_COUNT
        operations_per_thread = TestConfig.STRESS_OPERATIONS_PER_THREAD

        def worker(thread_id: int) -> Dict[str, Any]:
            """å·¥ä½œçº¿ç¨‹"""
            start_time = time.time()
            results = []

            for i in range(operations_per_thread):
                classification = DataClassification.TICK_DATA if i % 2 == 0 else DataClassification.DAILY_KLINE
                routing_start = time.time()
                target_db = data_manager.get_target_database(classification)
                routing_time = (time.time() - routing_start) * 1000
                results.append({
                    'thread_id': thread_id,
                    'iteration': i,
                    'routing_time_ms': routing_time,
                    'target_db': target_db.value
                })

            end_time = time.time()
            return {
                'thread_id': thread_id,
                'total_time': end_time - start_time,
                'operations': len(results),
                'results': results
            }

        # å¹¶å‘æ‰§è¡Œ
        start_time = time.time()
        with ThreadPoolExecutor(max_workers=thread_count) as executor:
            futures = [executor.submit(worker, i) for i in range(thread_count)]
            thread_results = [future.result() for future in as_completed(futures)]
        end_time = time.time()

        total_time = end_time - start_time
        total_operations = thread_count * operations_per_thread
        throughput = total_operations / total_time

        # æ”¶é›†æ‰€æœ‰è·¯ç”±æ—¶é—´
        all_routing_times = []
        for thread_result in thread_results:
            for result in thread_result['results']:
                all_routing_times.append(result['routing_time_ms'])

        avg_routing_time = np.mean(all_routing_times)
        max_routing_time = np.max(all_routing_times)
        p99_routing_time = np.percentile(all_routing_times, 99)

        print(f"\n  å¹¶å‘å‹åŠ›æµ‹è¯• ({thread_count}çº¿ç¨‹ x {operations_per_thread}æ¬¡):")
        print(f"  âœ“ æ€»æ—¶é—´: {total_time:.3f}ç§’")
        print(f"  âœ“ æ€»æ“ä½œæ•°: {total_operations}")
        print(f"  âœ“ ååé‡: {throughput:.0f} æ¬¡/ç§’")
        print(f"  âœ“ å¹³å‡è·¯ç”±æ—¶é—´: {avg_routing_time:.6f}ms")
        print(f"  âœ“ æœ€å¤§è·¯ç”±æ—¶é—´: {max_routing_time:.6f}ms")
        print(f"  âœ“ P99è·¯ç”±æ—¶é—´: {p99_routing_time:.6f}ms")

        # éªŒè¯æ€§èƒ½
        assert avg_routing_time < TestConfig.ROUTING_TIME_TARGET_MS
        assert throughput > 1000, f"å¹¶å‘ååé‡ {throughput:.0f} æ¬¡/ç§’ ä½äºé¢„æœŸ"

    def test_sustained_load(self, data_manager):
        """æµ‹è¯•æŒç»­è´Ÿè½½ï¼ˆæŒç»­10ç§’ï¼‰"""
        duration_seconds = 10
        classifications = list(DataClassification)

        operation_count = 0
        routing_times = []

        start_time = time.time()
        while time.time() - start_time < duration_seconds:
            classification = classifications[operation_count % len(classifications)]

            routing_start = time.time()
            data_manager.get_target_database(classification)
            routing_time = (time.time() - routing_start) * 1000

            routing_times.append(routing_time)
            operation_count += 1

        end_time = time.time()
        actual_duration = end_time - start_time
        throughput = operation_count / actual_duration
        avg_routing_time = np.mean(routing_times)

        print(f"\n  æŒç»­è´Ÿè½½æµ‹è¯• ({actual_duration:.1f}ç§’):")
        print(f"  âœ“ æ€»æ“ä½œæ•°: {operation_count}")
        print(f"  âœ“ ååé‡: {throughput:.0f} æ¬¡/ç§’")
        print(f"  âœ“ å¹³å‡è·¯ç”±æ—¶é—´: {avg_routing_time:.6f}ms")

        assert throughput > 10000, f"æŒç»­è´Ÿè½½ååé‡ {throughput:.0f} æ¬¡/ç§’ ä½äºé¢„æœŸ"

    def test_rapid_classification_switching(self, data_manager):
        """æµ‹è¯•å¿«é€Ÿåˆ‡æ¢ä¸åŒåˆ†ç±»"""
        iterations = 10000
        classifications = list(DataClassification)

        routing_times = []
        start_time = time.time()

        for i in range(iterations):
            # æ¯æ¬¡è¿­ä»£åˆ‡æ¢ä¸åŒçš„åˆ†ç±»
            classification = classifications[i % len(classifications)]

            routing_start = time.time()
            data_manager.get_target_database(classification)
            routing_time = (time.time() - routing_start) * 1000
            routing_times.append(routing_time)

        end_time = time.time()
        total_time = end_time - start_time
        throughput = iterations / total_time
        avg_routing_time = np.mean(routing_times)

        print(f"\n  å¿«é€Ÿåˆ‡æ¢æµ‹è¯• ({iterations}æ¬¡ï¼Œ34ç§åˆ†ç±»):")
        print(f"  âœ“ æ€»æ—¶é—´: {total_time:.3f}ç§’")
        print(f"  âœ“ ååé‡: {throughput:.0f} æ¬¡/ç§’")
        print(f"  âœ“ å¹³å‡è·¯ç”±æ—¶é—´: {avg_routing_time:.6f}ms")

        assert throughput > 10000


# ============================================
# 4. é›†æˆæµ‹è¯• (Integration Tests)
# ============================================

class TestIntegration:
    """é›†æˆæµ‹è¯•"""

    def test_end_to_end_workflow(self, data_manager, sample_dataframe):
        """ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•"""
        # 1. è·å–è·¯ç”±ç›®æ ‡
        classification = DataClassification.SYMBOLS_INFO
        target_db = data_manager.get_target_database(classification)

        # 2. éªŒè¯è·¯ç”±ç»“æœ
        assert target_db == DatabaseTarget.POSTGRESQL

        # 3. éªŒè¯æ•°æ®åˆ†ç±»æ˜ å°„å®Œæ•´æ€§
        assert DataClassification.SYMBOLS_INFO in data_manager._ROUTING_MAP

        print(f"  âœ“ ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•é€šè¿‡")

    def test_routing_consistency(self, data_manager):
        """æµ‹è¯•è·¯ç”±ä¸€è‡´æ€§ï¼ˆå¤šæ¬¡è°ƒç”¨åº”è¯¥è¿”å›ç›¸åŒç»“æœï¼‰"""
        classification = DataClassification.TICK_DATA

        # å¤šæ¬¡è°ƒç”¨
        results = [data_manager.get_target_database(classification) for _ in range(100)]

        # éªŒè¯ä¸€è‡´æ€§
        assert all(r == DatabaseTarget.TDENGINE for r in results)
        print(f"  âœ“ è·¯ç”±ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡ï¼ˆ100æ¬¡è°ƒç”¨ï¼‰")


# ============================================
# æµ‹è¯•è¿è¡Œå…¥å£
# ============================================

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    print("=" * 60)
    print("US3 DataManager ç»¼åˆæµ‹è¯•å¥—ä»¶")
    print("=" * 60)

    pytest.main([
        __file__,
        "-v",  # è¯¦ç»†è¾“å‡º
        "-s",  # æ˜¾ç¤º print è¾“å‡º
        "--tb=short",  # ç®€çŸ­çš„é”™è¯¯è¿½è¸ª
        "--durations=10",  # æ˜¾ç¤ºæœ€æ…¢çš„10ä¸ªæµ‹è¯•
    ])
