"""
æµ‹è¯•æ•°æ®ç®¡ç†æ¨¡å—

æä¾›å…¨é¢çš„æµ‹è¯•æ•°æ®ç®¡ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®ç”Ÿæˆå’Œç®¡ç†
- æ•°æ®è´¨é‡åˆ†æ
- æ•°æ®ä¼˜åŒ–å’Œå‹ç¼©
- æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†
"""

from .test_data_optimizer import (
    DataQualityMetrics,
    CompressionResult,
    DataOptimizationStrategy,
    TestDataOptimizer,
)

# å¯¼å‡ºä¸»è¦ç±»å’Œå‡½æ•°
__all__ = [
    "TestDataOptimizer",
    "DataQualityMetrics",
    "CompressionResult",
    "DataOptimizationStrategy",
    # ä¾¿æ·å‡½æ•°
    "create_data_optimization_session",
    "optimize_test_data_profile",
    "analyze_data_quality",
]


def create_data_optimization_session(data_manager=None) -> TestDataOptimizer:
    """åˆ›å»ºæ•°æ®ä¼˜åŒ–ä¼šè¯"""
    from ..ai.test_data_manager import AITestDataManager

    if data_manager is None:
        data_manager = AITestDataManager()

    return TestDataOptimizer(data_manager)


async def optimize_test_data_profile(profile_name: str, data_manager=None) -> dict:
    """ä¼˜åŒ–æµ‹è¯•æ•°æ®æ¡£æ¡ˆ

    Args:
        profile_name: æ•°æ®æ¡£æ¡ˆåç§°
        data_manager: æ•°æ®ç®¡ç†å™¨å®ä¾‹

    Returns:
        ä¼˜åŒ–ç»“æœå­—å…¸
    """
    optimizer = create_data_optimization_session(data_manager)
    return await optimizer.optimize_test_data(profile_name)


async def analyze_data_quality(profile_name: str, data_manager=None) -> dict:
    """åˆ†ææ•°æ®è´¨é‡

    Args:
        profile_name: æ•°æ®æ¡£æ¡ˆåç§°
        data_manager: æ•°æ®ç®¡ç†å™¨å®ä¾‹

    Returns:
        è´¨é‡åˆ†æç»“æœ
    """
    from ..ai.test_data_manager import AITestDataManager

    if data_manager is None:
        data_manager = AITestDataManager()

    optimizer = TestDataOptimizer(data_manager)
    metrics = await optimizer.analyze_data_quality(profile_name)

    return {
        "profile_name": profile_name,
        "metrics": metrics.__dict__,
        "quality_grade": _get_quality_grade(metrics.overall_quality),
        "recommendations": _get_quality_recommendations(metrics),
    }


def _get_quality_grade(overall_quality: float) -> str:
    """è·å–è´¨é‡ç­‰çº§"""
    if overall_quality >= 0.9:
        return "ä¼˜ç§€"
    elif overall_quality >= 0.8:
        return "è‰¯å¥½"
    elif overall_quality >= 0.7:
        return "ä¸­ç­‰"
    elif overall_quality >= 0.6:
        return "å¾…æ”¹è¿›"
    else:
        return "è¾ƒå·®"


def _get_quality_recommendations(metrics: DataQualityMetrics) -> list:
    """è·å–è´¨é‡å»ºè®®"""
    recommendations = []

    if metrics.completeness_score < 0.8:
        recommendations.append("æ•°æ®å®Œæ•´æ€§ä¸è¶³ï¼Œå»ºè®®è¡¥å……ç¼ºå¤±å­—æ®µ")

    if metrics.consistency_score < 0.8:
        recommendations.append("æ•°æ®ä¸€è‡´æ€§è¾ƒå·®ï¼Œå»ºè®®ç»Ÿä¸€æ•°æ®æ ¼å¼")

    if metrics.accuracy_score < 0.8:
        recommendations.append("æ•°æ®å‡†ç¡®æ€§éœ€è¦æå‡ï¼Œå»ºè®®æ·»åŠ éªŒè¯è§„åˆ™")

    if metrics.timeliness_score < 0.8:
        recommendations.append("æ•°æ®æ—¶æ•ˆæ€§æœ‰å¾…æé«˜ï¼Œå»ºè®®ä¼˜åŒ–æ›´æ–°é¢‘ç‡")

    if metrics.duplicate_ratio > 0.1:
        recommendations.append("æ£€æµ‹åˆ°é‡å¤æ•°æ®ï¼Œå»ºè®®è¿›è¡Œå»é‡å¤„ç†")

    return recommendations


# é¡¹ç›®ç¤ºä¾‹ç”¨æ³•
def create_my_stocks_data_context():
    """åˆ›å»ºMyStocksé¡¹ç›®æ•°æ®ä¸Šä¸‹æ–‡"""
    return {
        "project_name": "MyStocks",
        "data_profiles": [
            {
                "name": "market_data",
                "description": "å¸‚åœºæµ‹è¯•æ•°æ®",
                "size": 1000,
                "constraints": {
                    "symbols": ["AAPL", "GOOGL", "MSFT"],
                    "date_range": "2025-01-01:2025-12-12",
                    "data_types": ["price", "volume", "timestamp"],
                },
            },
            {
                "name": "trading_data",
                "description": "äº¤æ˜“æµ‹è¯•æ•°æ®",
                "size": 500,
                "constraints": {
                    "account_types": ["cash", "margin"],
                    "order_types": ["market", "limit"],
                    "status_codes": [0, 1, 2, 3],
                },
            },
            {
                "name": "user_data",
                "description": "ç”¨æˆ·æµ‹è¯•æ•°æ®",
                "size": 100,
                "constraints": {
                    "user_roles": ["admin", "trader", "viewer"],
                    "account_status": ["active", "inactive", "suspended"],
                },
            },
        ],
        "optimization_goals": {
            "storage_reduction": 0.3,  # 30%å­˜å‚¨ç©ºé—´å‡å°‘
            "quality_improvement": 0.2,  # 20%è´¨é‡æå‡
            "performance_optimization": True,
        },
    }


async def demo_data_optimization():
    """æ¼”ç¤ºæ•°æ®ä¼˜åŒ–åŠŸèƒ½"""
    print("ğŸ“Š æµ‹è¯•æ•°æ®ä¼˜åŒ–æ¼”ç¤º")

    # 1. åˆ›å»ºæ•°æ®ä¸Šä¸‹æ–‡
    context = create_my_stocks_data_context()

    # 2. åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = create_data_optimization_session()

    # 3. ä¼˜åŒ–æ¯ä¸ªæ•°æ®æ¡£æ¡ˆ
    results = {}
    for profile in context["data_profiles"]:
        print(f"\nğŸ”§ ä¼˜åŒ–æ•°æ®æ¡£æ¡ˆ: {profile['name']}")
        result = await optimizer.optimize_test_data(profile["name"])
        results[profile["name"]] = result

        # æ˜¾ç¤ºä¼˜åŒ–ç»“æœ
        if "quality_improvement" in result:
            print(f"  è´¨é‡æ”¹è¿›: {result['quality_improvement']:.2%}")
        if "compression_ratio" in result:
            print(f"  å‹ç¼©æ¯”ç‡: {result['compression_ratio']:.2%}")

    # 4. æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
    print("\n=== ä¼˜åŒ–ç»Ÿè®¡ ===")
    stats = await optimizer.get_optimization_statistics()
    print(f"æ€»ä¼˜åŒ–æ¬¡æ•°: {stats['total_optimizations']}")
    print(f"æˆåŠŸç‡: {stats['success_rate']:.2%}")
    print(f"å¹³å‡è´¨é‡æ”¹è¿›: {stats['average_quality_improvement']:.2%}")

    # 5. æ¸…ç†ç¼“å­˜
    cleanup_result = await optimizer.cleanup_optimization_cache()
    print(f"ç¼“å­˜æ¸…ç†: æ¸…ç†äº† {cleanup_result['cleaned_entries']} ä¸ªæ¡ç›®")


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    import asyncio

    asyncio.run(demo_data_optimization())
