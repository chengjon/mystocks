# Context

MyStocks 项目当前使用的数据源管理模块（`src/core/data_source/`）和数据治理模块（`src/governance/`）存在性能瓶颈和可靠性问题。现有实现采用简单的 LRU 缓存、基础的健康检查和同步串行的批量处理方式，无法满足生产环境的高并发、高可用需求。

## 关键约束

1. **同步代码库**: 现有 `DataSourceManagerV2` 为同步代码，引入 `async/await` 会导致"传染性改造"风险
2. **FastAPI + Gunicorn/Uvicorn**: 生产环境使用多 worker 模式，需要考虑线程安全
3. **现有监控体系**: 已集成 Prometheus，使用 `/metrics` 端点，避免端口冲突
4. **向后兼容**: 不破坏现有 API 接口，保持平滑升级

## Goals / Non-Goals

### Goals（目标）
1. 提升数据源调用的性能和可靠性（响应时间 500ms → 100ms）
2. 降低 API 调用成本（节约 70%）
3. 增强系统可观测性（全面的监控指标）
4. 保证线程安全（Gunicorn 多 worker 环境）

### Non-Goals（非目标）
1. 全面迁移到异步架构（保留同步代码，使用线程池）
2. 重写整个数据源管理系统（增量优化）
3. 引入新的数据库或中间件（使用现有技术栈）

## Decisions

### 决策 1: 使用 threading.Lock 而非 asyncio

**选择**: 使用 `threading.Lock` 和后台线程预刷新，而非 `asyncio` 异步刷新

**原因**:
- 现有 `DataSourceManagerV2` 为同步代码，引入 async 会导致整个调用链改造
- `asyncio` 在同步代码库中会导致"async 传染"，所有调用方都需要 await
- `threading.Thread` 更适合同步代码库，Python 3.12+ 优化了 GIL 性能
- 同步代码的调试和堆栈跟踪更清晰

**替代方案**:
- ❌ `asyncio.create_task()`: 需要改造整个调用链为异步
- ❌ 完全重写为异步: 工作量巨大，风险高
- ✅ `threading.Thread` + `threading.Lock`: 最小化改动，风险低

**权衡**:
- 优势: 适合同步代码库、易于调试、改造风险低
- 劣势: GIL 限制、线程开销（但 I/O 密集型场景影响小）

---

### 决策 2: 线程安全的熔断器实现

**选择**: 使用 `threading.Lock` 保护熔断器状态转换

**原因**:
- Gunicorn 多 worker 模式下，非线程安全的熔断器会导致状态竞争
- 状态转换需要原子操作，防止误触发熔断
- 使用 `RLock` 可重入锁，避免死锁

**实现细节**:
- 仅在状态转换时加锁，执行调用时释放锁（避免阻塞）
- 状态检查和状态转换在同一锁内完成
- 使用 `time.time()` 而非 `datetime.now()`（性能优化）

**替代方案**:
- ❌ 无锁实现: 多线程环境下会导致状态不一致
- ❌ 分布式锁: 引入 Redis 等外部依赖，增加复杂度
- ✅ 进程内锁: 足够简单且有效（单进程场景）

---

### 决策 3: 集成现有 FastAPI /metrics 端点

**选择**: 复用现有 FastAPI 应用的 `/metrics` 端点，而非启动独立的 HTTP Server

**原因**:
- 避免端口冲突（9091 端口可能被其他服务占用）
- 统一运维入口（所有监控指标通过 FastAPI 暴露）
- 简化部署（无需管理额外的进程）

**实现细节**:
- 使用 `prometheus_client` 的全局 Registry
- 在 FastAPI 中添加 `/metrics` 路由：`return Response(generate_latest(REGISTRY), media_type=CONTENT_TYPE_LATEST)`
- 避免重复注册（使用 `REGISTRY.register()` 或直接使用全局指标）

**替代方案**:
- ❌ `start_http_server(9091)`: 端口冲突、进程管理复杂
- ❌ 独立的 Prometheus Exporter: 增加部署复杂度
- ✅ FastAPI 集成: 简化运维、统一入口

---

### 决策 4: 调用层并发（GovernanceDataFetcher）

**选择**: 在 `GovernanceDataFetcher` 层使用 `ThreadPoolExecutor` 并发调用，`DataSourceManager` 保持同步

**原因**:
- 保持 `DataSourceManager` 同步 API，不破坏现有接口
- 在调用层进行并发优化，符合分层架构原则
- `ThreadPoolExecutor` 非常适合 I/O 密集型场景（网络请求）

**实现细节**:
- `max_workers=10`: 限制并发线程数，避免资源耗尽
- `future.result(timeout=30)`: 超时控制，防止单个请求卡住
- 异常隔离: 每个 future 的异常不影响其他 future
- 优雅关闭: 实现 `shutdown()` 方法清理线程池

**替代方案**:
- ❌ 修改 `DataSourceManager` 为异步: 破坏现有接口
- ❌ 使用 `asyncio`: 不适合同步代码库
- ✅ `ThreadPoolExecutor`: 最小侵入性、最大兼容性

---

### 决策 5: SmartCache 使用线程池限制后台刷新

**选择**: 使用 `ThreadPoolExecutor(max_workers=5)` 限制并发刷新线程数

**原因**:
- 防止大量缓存同时过期时创建过多后台线程
- 避免刷新风暴（资源耗尽、性能下降）
- 线程池复用线程，减少创建/销毁开销

**实现细节**:
- `max_workers=5`: 最多 5 个并发刷新任务
- 使用 `daemon=True`: 后台线程，主线程退出时自动结束
- 使用 `set()` 记录正在刷新的 key，防止重复刷新

**替代方案**:
- ❌ 每次刷新创建新线程: 线程数爆炸、资源耗尽
- ❌ 无限制的线程池: 可能导致资源耗尽
- ✅ 固定大小线程池: 平衡性能和资源

---

## Risks / Trade-offs

### 风险 1: 线程安全风险

**风险描述**: 多线程环境下，共享状态的竞争可能导致数据不一致

**缓解措施**:
- 使用 `threading.Lock` 和 `RLock` 保护所有共享状态
- 编写并发单元测试（使用 `pytest` 的 `fixture` 和 `threading` 模拟并发）
- 代码审查：确保所有状态访问都在锁保护下
- 性能测试：验证锁不会成为性能瓶颈

**监控指标**:
- 锁竞争次数（如果竞争频繁，考虑优化锁粒度）
- 线程数（线程池的活跃线程数）

---

### 风险 2: 刷新风暴

**风险描述**: 大量缓存同时过期时，创建过多后台线程导致资源耗尽

**缓解措施**:
- 使用 `ThreadPoolExecutor(max_workers=5)` 限制并发数
- 使用 `set()` 记录正在刷新的 key，防止重复刷新
- 添加监控：记录后台刷新失败率

**监控指标**:
- 后台刷新线程池的队列长度
- 刷新失败率（目标 < 5%）
- 平均刷新耗时

---

### 风险 3: 锁竞争导致性能下降

**风险描述**: 高并发下，锁可能成为性能瓶颈

**缓解措施**:
- 使用 `RLock` 可重入锁，减少锁持有时间
- 仅在必要时加锁（状态转换、缓存访问）
- 将长时间操作（如 API 调用）移出锁保护范围
- 性能测试：对比有锁和无锁的性能差异

**监控指标**:
- 锁等待时间（如果等待时间过长，考虑优化锁粒度）
- P95/P99 延迟（如果锁导致延迟增加，考虑无锁数据结构）

---

### 风险 4: 缓存一致性问题

**风险描述**: 软过期策略可能导致短暂的数据不一致

**缓解措施**:
- 在返回旧数据时记录警告日志
- 提供配置选项：硬过期（过期直接返回 None）或软过期（返回旧数据）
- 数据质量验证：在 `DataQualityValidator` 中检测异常数据

**监控指标**:
- 缓存命中率（目标 > 80%）
- 缓存过期时返回旧数据的次数

---

### 权衡 1: 性能 vs 简单性

**权衡**: 使用线程池会引入一定的复杂度，但换来更好的性能

**决策**: 选择线程池（性能 > 简单性）

**原因**:
- 量化交易系统对性能要求高
- 线程池是成熟的并发模型，风险可控
- 清晰的文档和测试可以降低维护成本

---

### 权衡 2: 成本 vs 实时性

**权衡**: 缓存 TTL 设置过长可能影响数据实时性

**决策**: 默认 TTL = 1 小时（平衡成本和实时性）

**原因**:
- 大部分量化场景不需要秒级实时数据
- 1 小时 TTL 可以显著降低 API 成本（70% 节约）
- 提供配置选项，允许用户根据场景调整 TTL

---

## Migration Plan

### Phase 1: 核心稳定性（1-2 周）

**步骤**:
1. 实现 `SmartCache`（替换 `LRUCache`）
   - 编写单元测试（包括并发测试）
   - 在测试环境验证功能正确性
   - 性能测试：对比优化前后的缓存命中率

2. 实现 `CircuitBreaker`
   - 编写单元测试（验证状态转换）
   - 集成到 `DataSourceManagerV2._call_endpoint`
   - 在测试环境模拟故障场景

3. 实现 `DataQualityValidator`
   - 编写 100+ 测试用例（覆盖各种异常场景）
   - 集成到 `GPUValidator`
   - 在测试环境验证数据质量检查

**验收标准**:
- [ ] 所有单元测试通过
- [ ] 并发测试通过（模拟 100 个并发请求）
- [ ] API 调用成本降低 40%
- [ ] 响应时间减少 50%（500ms → 250ms）

**回滚计划**:
- 保留原有 `LRUCache` 实现，通过配置切换
- 如果 SmartCache 出现问题，立即切换回原实现

---

### Phase 2: 能力提升（1 个月）

**步骤**:
1. 实现 `SmartRouter`
   - 多维度路由决策（性能、成本、负载、地域）
   - A/B 测试：对比新旧路由策略的性能差异

2. 集成 Prometheus 监控
   - 添加指标埋点到 `DataSourceManagerV2._call_endpoint`
   - 更新 Grafana 仪表板
   - 配置告警规则（成功率 < 95%、P95 延迟 > 500ms）

3. 实现 `BatchProcessor`
   - 在 `GovernanceDataFetcher` 层使用 `ThreadPoolExecutor`
   - 性能测试：对比优化前后的吞吐量

**验收标准**:
- [ ] Prometheus 指标可查询
- [ ] Grafana 仪表板正常显示
- [ ] 批量获取性能提升 3-5 倍
- [ ] P95 延迟 < 200ms

**回滚计划**:
- 保留原有的串行处理逻辑
- 通过配置切换并发/串行模式

---

### Phase 3: 高级特性（2-3 个月）

**步骤**:
1. 实现 `DataLineageTracker`（可选）
   - 设计血缘数据模型
   - 实现 Neo4j 存储逻辑
   - 提供血缘查询 API

2. 实现 `AdaptiveRateLimiter`（可选）
   - 基于错误率动态调整速率
   - 配置告警：速率过低/过高时通知

**验收标准**:
- [ ] 数据血缘追踪功能可用
- [ ] 自适应限流正常运行
- [ ] 系统可用性达到 99.9%

---

## Open Questions

1. **缓存 TTL 的默认值**: 1 小时是否合适？是否需要根据数据类型动态调整？
   - **建议**: 日线数据 TTL = 1 小时，分钟数据 TTL = 5 分钟，实时数据 TTL = 30 秒

2. **熔断器阈值**: 连续失败 5 次是否合理？是否需要根据数据源特性调整？
   - **建议**: 免费数据源阈值 = 3 次（更快熔断），付费数据源阈值 = 10 次（更宽容）

3. **线程池大小**: `max_workers=10` 是否合适？是否需要根据 CPU 核心数动态调整？
   - **建议**: `max_workers = min(32, (os.cpu_count() or 1) * 4)`（参考 `concurrent.futures` 默认值）

4. **Prometheus 指标保留时间**: 需要保留多长时间的监控数据？
   - **建议**: 延迟指标保留 30 天，成本指标保留 90 天，数据质量指标保留 180 天

5. **是否需要实现缓存预热**: 系统启动时是否需要预加载热门数据？
   - **建议**: Phase 2 实现，预加载 Top 100 热门股票的日线数据

---

## Implementation Notes

### 线程安全最佳实践

1. **锁的粒度**: 尽量缩小锁的范围，仅保护必要的代码
2. **避免死锁**: 按固定顺序获取多个锁，使用 `RLock` 可重入锁
3. **锁超时**: 考虑使用 `with lock` 而非 `lock.acquire()`（自动释放锁）
4. **线程本地存储**: 对于不需要共享的数据，使用 `threading.local()`

### 性能优化建议

1. **缓存预热**: 系统启动时预加载热门数据
2. **批量操作**: 合并多个请求，减少网络往返
3. **连接池**: 复用 HTTP 连接，减少握手开销
4. **数据压缩**: 对于大量数据，使用 `gzip` 压缩

### 监控和告警

1. **关键指标**:
   - 缓存命中率 > 80%
   - API 成功率 > 95%
   - P95 延迟 < 200ms
   - 熔断器开启率 < 5%

2. **告警规则**:
   - 成功率 < 95% 持续 5 分钟 → 立即告警
   - P95 延迟 > 500ms 持续 5 分钟 → 警告告警
   - 熔断器开启率 > 10% → 立即告警

3. **日志记录**:
   - 熔断器状态转换（CLOSED → OPEN → HALF_OPEN → CLOSED）
   - 缓存刷新失败
   - 数据质量检查失败

---

## Testing Strategy

### 单元测试

- **SmartCache**: 测试 TTL 过期、并发访问、LRU 淘汰
- **CircuitBreaker**: 测试状态转换、并发熔断、半开恢复
- **DataQualityValidator**: 测试各种异常场景（极端价格、异常成交量）

### 集成测试

- 测试 `DataSourceManagerV2` 集成 `SmartCache` 和 `CircuitBreaker`
- 测试 `GovernanceDataFetcher` 批量获取功能
- 测试 Prometheus 指标采集

### 性能测试

- 基准测试：对比优化前后的性能（延迟、吞吐量、成本）
- 压力测试：模拟 1000 个并发请求
- 稳定性测试：长时间运行（24 小时）验证无内存泄漏

### 并发测试

- 使用 `pytest` + `threading` 模拟多线程并发
- 使用 `locust` 进行负载测试
- 验证线程安全（无数据竞争、无死锁）
