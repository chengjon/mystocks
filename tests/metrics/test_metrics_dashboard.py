#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MyStocks æµ‹è¯•è´¨é‡æŒ‡æ ‡ä»ªè¡¨ç›˜

æä¾›å®æ—¶è´¨é‡æŒ‡æ ‡ç›‘æ§ã€å¯è§†åŒ–å±•ç¤ºå’Œäº¤äº’å¼åˆ†æåŠŸèƒ½ã€‚
"""

import asyncio
import json
import time
import statistics
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, field
from enum import Enum
import numpy as np

from .test_quality_metrics import TestQualityMetrics, TestSuiteMetrics, TestResult


class AlertLevel(Enum):
    """å‘Šè­¦çº§åˆ«"""

    CRITICAL = "critical"  # ä¸¥é‡
    HIGH = "high"  # é«˜
    MEDIUM = "medium"  # ä¸­ç­‰
    LOW = "low"  # ä½
    INFO = "info"  # ä¿¡æ¯


class MetricStatus(Enum):
    """æŒ‡æ ‡çŠ¶æ€"""

    HEALTHY = "healthy"  # å¥åº·
    WARNING = "warning"  # è­¦å‘Š
    CRITICAL = "critical"  # ä¸¥é‡
    UNKNOWN = "unknown"  # æœªçŸ¥


@dataclass
class QualityAlert:
    """è´¨é‡å‘Šè­¦"""

    id: str
    metric_name: str
    current_value: float
    threshold_value: float
    alert_level: AlertLevel
    message: str
    timestamp: datetime
    is_resolved: bool = False
    resolution_time: Optional[datetime] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class MetricVisualization:
    """æŒ‡æ ‡å¯è§†åŒ–æ•°æ®"""

    metric_name: str
    chart_type: str  # "line", "bar", "pie", "gauge", "heatmap"
    data: List[Union[float, Dict[str, Any]]]
    labels: List[str]
    colors: List[str]
    options: Dict[str, Any] = field(default_factory=dict)


class TestMetricsMonitor:
    """æµ‹è¯•æŒ‡æ ‡ç›‘æ§å™¨"""

    def __init__(self):
        self.quality_metrics = TestQualityMetrics()
        self.alerts: List[QualityAlert] = []
        self.monitoring_config = {
            "check_interval": 60,  # ç§’
            "alert_thresholds": {
                "quality_score": {"critical": 60, "high": 75, "medium": 85, "low": 95},
                "pass_rate": {"critical": 85, "high": 90, "medium": 95, "low": 98},
                "coverage": {"critical": 70, "high": 80, "medium": 90, "low": 95},
            },
        }
        self.monitoring_history = []
        self.is_monitoring = False

    async def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§"""
        print("ğŸ”´ å¯åŠ¨è´¨é‡æŒ‡æ ‡ç›‘æ§...")
        self.is_monitoring = True

        while self.is_monitoring:
            try:
                await self._check_quality_metrics()
                await asyncio.sleep(self.monitoring_config["check_interval"])
            except Exception as e:
                print(f"âŒ ç›‘æ§å¼‚å¸¸: {e}")
                await asyncio.sleep(60)  # å‡ºé”™åç­‰å¾…60ç§’å†é‡è¯•

    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        print("ğŸŸ¢ åœæ­¢è´¨é‡æŒ‡æ ‡ç›‘æ§")
        self.is_monitoring = False

    async def _check_quality_metrics(self):
        """æ£€æŸ¥è´¨é‡æŒ‡æ ‡"""
        print(f"ğŸ“Š æ‰§è¡Œè´¨é‡æŒ‡æ ‡æ£€æŸ¥ ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})")

        # ç”Ÿæˆæ¨¡æ‹Ÿæµ‹è¯•ç»“æœç”¨äºç›‘æ§
        test_results = self._generate_monitoring_test_results()
        code_files = ["src/main.py", "src/services.py", "src/utils.py"]

        # è®¡ç®—è´¨é‡æŒ‡æ ‡
        suite_metrics = self.quality_metrics.calculate_test_suite_metrics(
            test_results, code_files
        )

        if suite_metrics:
            # ç”Ÿæˆå¯è§†åŒ–æ•°æ®
            visualizations = self._generate_visualizations(suite_metrics)

            # æ£€æŸ¥å‘Šè­¦
            alerts = self._check_alerts(suite_metrics)

            # è®°å½•ç›‘æ§å†å²
            self._record_monitoring_event(suite_metrics, visualizations, alerts)

            # æ‰“å°ç›‘æ§æ‘˜è¦
            self._print_monitoring_summary(suite_metrics, alerts)

    def _generate_monitoring_test_results(self) -> List[TestResult]:
        """ç”Ÿæˆç”¨äºç›‘æ§çš„æµ‹è¯•ç»“æœ"""
        test_results = []
        num_tests = np.random.randint(50, 200)

        for i in range(num_tests):
            # æ¨¡æ‹Ÿæµ‹è¯•çŠ¶æ€åˆ†å¸ƒ
            status_prob = np.random.random()
            if status_prob < 0.85:  # 85% é€šè¿‡
                status = "passed"
                duration = np.random.exponential(1.0)  # æŒ‡æ•°åˆ†å¸ƒ
            elif status_prob < 0.92:  # 7% å¤±è´¥
                status = "failed"
                duration = np.random.uniform(0.5, 5.0)
            elif status_prob < 0.97:  # 5% é”™è¯¯
                status = "error"
                duration = np.random.uniform(0.3, 3.0)
            else:  # 3% è·³è¿‡
                status = "skipped"
                duration = np.random.uniform(0.1, 0.5)

            test_result = TestResult(
                test_id=f"monitor_test_{i + 1:03d}",
                test_name=f"Monitoring Test {i + 1}",
                status=status,
                duration=duration,
                timestamp=datetime.now()
                - timedelta(seconds=np.random.randint(0, 3600)),
                error_message=f"Monitor error {i}"
                if status in ["failed", "error"]
                else None,
                metadata={
                    "error_type": np.random.choice(
                        ["assertion", "timeout", "network", "unknown"]
                    ),
                    "category": np.random.choice(
                        ["unit", "integration", "e2e", "performance"]
                    ),
                },
            )
            test_results.append(test_result)

        return test_results

    def _generate_visualizations(
        self, suite_metrics: TestSuiteMetrics
    ) -> Dict[str, MetricVisualization]:
        """ç”Ÿæˆå¯è§†åŒ–æ•°æ®"""
        visualizations = {}

        # æµ‹è¯•ç»“æœé¥¼å›¾
        visualizations["test_results_pie"] = MetricVisualization(
            metric_name="æµ‹è¯•ç»“æœåˆ†å¸ƒ",
            chart_type="pie",
            data=[
                suite_metrics.passed_tests,
                suite_metrics.failed_tests,
                suite_metrics.skipped_tests,
                suite_metrics.error_tests,
            ],
            labels=["é€šè¿‡", "å¤±è´¥", "è·³è¿‡", "é”™è¯¯"],
            colors=["#28a745", "#dc3545", "#ffc107", "#6c757d"],
            options={"title": "æµ‹è¯•ç»“æœåˆ†å¸ƒ", "legend_position": "bottom"},
        )

        # è´¨é‡å¾—åˆ†è¶‹åŠ¿å›¾
        if len(self.quality_metrics.quality_history) >= 2:
            recent_history = self.quality_metrics.quality_history[-20:]
            visualizations["quality_trend"] = MetricVisualization(
                metric_name="è´¨é‡å¾—åˆ†è¶‹åŠ¿",
                chart_type="line",
                data=[m.quality_score for m in recent_history],
                labels=[m.timestamp.strftime("%m-%d %H:%M") for m in recent_history],
                colors=["#007bff"],
                options={
                    "title": "è´¨é‡å¾—åˆ†è¶‹åŠ¿",
                    "y_axis_label": "å¾—åˆ†",
                    "x_axis_label": "æ—¶é—´",
                },
            )

        # è¦†ç›–ç‡è¿›åº¦æ¡
        visualizations["coverage_gauge"] = MetricVisualization(
            metric_name="ä»£ç è¦†ç›–ç‡",
            chart_type="gauge",
            data=[suite_metrics.coverage_percentage],
            labels=["è¦†ç›–ç‡"],
            colors=["#17a2b8"],
            options={"title": "ä»£ç è¦†ç›–ç‡", "max_value": 100, "min_value": 0},
        )

        # æ€§èƒ½æŒ‡æ ‡æŸ±çŠ¶å›¾
        visualizations["performance_bar"] = MetricVisualization(
            metric_name="æ€§èƒ½æŒ‡æ ‡",
            chart_type="bar",
            data=[
                suite_metrics.average_duration * 1000,  # è½¬æ¢ä¸ºæ¯«ç§’
                suite_metrics.total_duration * 1000,
                suite_metrics.reliability_score,
                suite_metrics.performance_score,
            ],
            labels=["å¹³å‡è€—æ—¶(ms)", "æ€»è€—æ—¶(ms)", "å¯é æ€§å¾—åˆ†", "æ€§èƒ½å¾—åˆ†"],
            colors=["#fd7e14", "#6f42c1", "#20c997", "#e83e8c"],
            options={"title": "æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”"},
        )

        # æŒ‰ç±»åˆ«åˆ†ç»„çš„æµ‹è¯•çƒ­åŠ›å›¾
        category_counts = {}
        for result in suite_metrics.test_results:
            category = result.metadata.get("category", "unknown")
            category_counts[category] = category_counts.get(category, 0) + 1

        visualizations["category_heatmap"] = MetricVisualization(
            metric_name="æŒ‰ç±»åˆ«åˆ†ç»„çš„æµ‹è¯•",
            chart_type="heatmap",
            data=[{"name": k, "value": v} for k, v in category_counts.items()],
            labels=list(category_counts.keys()),
            colors=["#fff3cd", "#ffeaa7", "#fab1a0", "#e17055"],
            options={"title": "æµ‹è¯•ç±»åˆ«åˆ†å¸ƒ"},
        )

        return visualizations

    def _check_alerts(self, suite_metrics: TestSuiteMetrics) -> List[QualityAlert]:
        """æ£€æŸ¥è´¨é‡æŒ‡æ ‡å¹¶ç”Ÿæˆå‘Šè­¦"""
        new_alerts = []
        thresholds = self.monitoring_config["alert_thresholds"]

        # æ£€æŸ¥è´¨é‡å¾—åˆ†
        self._check_metric_alert(
            suite_metrics.quality_score,
            thresholds["quality_score"],
            "ç»¼åˆè´¨é‡å¾—åˆ†",
            suite_metrics,
            new_alerts,
        )

        # æ£€æŸ¥é€šè¿‡ç‡
        self._check_metric_alert(
            suite_metrics.pass_rate,
            thresholds["pass_rate"],
            "æµ‹è¯•é€šè¿‡ç‡",
            suite_metrics,
            new_alerts,
        )

        # æ£€æŸ¥è¦†ç›–ç‡
        self._check_metric_alert(
            suite_metrics.coverage_percentage,
            thresholds["coverage"],
            "ä»£ç è¦†ç›–ç‡",
            suite_metrics,
            new_alerts,
        )

        # æ£€æŸ¥å¼‚å¸¸
        self._check_anomaly_alerts(suite_metrics, new_alerts)

        # æ·»åŠ åˆ°å‘Šè­¦åˆ—è¡¨
        self.alerts.extend(new_alerts)

        # æ¸…ç†å·²è§£å†³çš„å‘Šè­¦
        self._cleanup_resolved_alerts()

        return new_alerts

    def _check_metric_alert(
        self,
        value: float,
        thresholds: Dict[str, float],
        metric_name: str,
        suite_metrics: TestSuiteMetrics,
        alerts: List[QualityAlert],
    ):
        """æ£€æŸ¥å•ä¸ªæŒ‡æ ‡çš„å‘Šè­¦"""
        if value < thresholds["critical"]:
            level = AlertLevel.CRITICAL
            message = f"{metric_name} ä¸¥é‡ä¸è¶³ ({value:.1f} < {thresholds['critical']})"
        elif value < thresholds["high"]:
            level = AlertLevel.HIGH
            message = f"{metric_name} è¾ƒä½ ({value:.1f} < {thresholds['high']})"
        elif value < thresholds["medium"]:
            level = AlertLevel.MEDIUM
            message = f"{metric_name} æ¥è¿‘é˜ˆå€¼ ({value:.1f} < {thresholds['medium']})"
        elif value < thresholds["low"]:
            level = AlertLevel.LOW
            message = f"{metric_name} æ¥è¿‘ç›®æ ‡ ({value:.1f} < {thresholds['low']})"
        else:
            return

        alert = QualityAlert(
            id=f"alert_{int(time.time())}_{len(self.alerts)}",
            metric_name=metric_name,
            current_value=value,
            threshold_value=thresholds.get("low", 100),
            alert_level=level,
            message=message,
            timestamp=datetime.now(),
            metadata={"suite_name": suite_metrics.suite_name, "thresholds": thresholds},
        )
        alerts.append(alert)

    def _check_anomaly_alerts(
        self, suite_metrics: TestSuiteMetrics, alerts: List[QualityAlert]
    ):
        """æ£€æŸ¥å¼‚å¸¸å‘Šè­¦"""
        # æ£€æŸ¥æµ‹è¯•æ‰§è¡Œæ—¶é—´å¼‚å¸¸
        durations = [r.duration for r in suite_metrics.test_results]
        if durations:
            avg_duration = statistics.mean(durations)
            std_duration = statistics.stdev(durations) if len(durations) > 1 else 0

            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸æ…¢çš„æµ‹è¯•
            slow_tests = [
                r
                for r in suite_metrics.test_results
                if r.duration > avg_duration + 3 * std_duration
            ]
            if slow_tests:
                slow_test_ids = [t.test_id for t in slow_tests[:3]]
                alert = QualityAlert(
                    id=f"anomaly_slow_{int(time.time())}",
                    metric_name="æ€§èƒ½å¼‚å¸¸",
                    current_value=len(slow_tests),
                    threshold_value=0,
                    alert_level=AlertLevel.MEDIUM,
                    message=f"æ£€æµ‹åˆ° {len(slow_tests)} ä¸ªæ‰§è¡Œå¼‚å¸¸ç¼“æ…¢çš„æµ‹è¯•",
                    timestamp=datetime.now(),
                    metadata={"slow_test_ids": slow_test_ids},
                )
                alerts.append(alert)

    def _cleanup_resolved_alerts(self):
        """æ¸…ç†å·²è§£å†³çš„å‘Šè­¦"""
        self.alerts = [
            alert
            for alert in self.alerts
            if not alert.is_resolved
            or (alert.is_resolved and (datetime.now() - alert.resolution_time).days < 7)
        ]

    def _record_monitoring_event(
        self,
        suite_metrics: TestSuiteMetrics,
        visualizations: Dict,
        alerts: List[QualityAlert],
    ):
        """è®°å½•ç›‘æ§äº‹ä»¶"""
        event = {
            "timestamp": datetime.now(),
            "suite_metrics": suite_metrics.__dict__,
            "visualizations": {k: v.__dict__ for k, v in visualizations.items()},
            "alert_count": len(alerts),
            "alert_levels": {
                alert.alert_level.value: len(
                    [a for a in alerts if a.alert_level == alert.alert_level]
                )
                for alert in alerts
            },
        }
        self.monitoring_history.append(event)

        # åªä¿ç•™æœ€è¿‘100æ¡è®°å½•
        if len(self.monitoring_history) > 100:
            self.monitoring_history = self.monitoring_history[-100:]

    def _print_monitoring_summary(
        self, suite_metrics: TestSuiteMetrics, alerts: List[QualityAlert]
    ):
        """æ‰“å°ç›‘æ§æ‘˜è¦"""
        print("\nğŸ“Š è´¨é‡æŒ‡æ ‡ç›‘æ§æ‘˜è¦")
        print(f"   â”Œâ”€ ç»¼åˆè´¨é‡å¾—åˆ†: {suite_metrics.quality_score:.1f}")
        print(f"   â”œâ”€ æµ‹è¯•é€šè¿‡ç‡: {suite_metrics.pass_rate:.1f}%")
        print(f"   â”œâ”€ ä»£ç è¦†ç›–ç‡: {suite_metrics.coverage_percentage:.1f}%")
        print(f"   â”œâ”€ å¹³å‡è€—æ—¶: {suite_metrics.average_duration:.2f}s")
        print(f"   â”œâ”€ æ€»æµ‹è¯•æ•°: {suite_metrics.total_tests}")
        print(f"   â””â”€ æ´»è·ƒå‘Šè­¦: {len(alerts)}")

        if alerts:
            print("   âš ï¸  å‘Šè­¦è¯¦æƒ…:")
            for alert in alerts[:3]:  # æ˜¾ç¤ºå‰3ä¸ªå‘Šè­¦
                print(f"      â€¢ {alert.alert_level.value.upper()}: {alert.message}")

    def get_monitoring_dashboard(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§ä»ªè¡¨ç›˜æ•°æ®"""
        if not self.monitoring_history:
            return {"status": "no_data"}

        latest_event = self.monitoring_history[-1]
        latest_metrics = TestSuiteMetrics(**latest_event["suite_metrics"])

        return {
            "current_status": {
                "quality_score": latest_metrics.quality_score,
                "pass_rate": latest_metrics.pass_rate,
                "coverage_percentage": latest_metrics.coverage_percentage,
                "total_tests": latest_metrics.total_tests,
                "average_duration": latest_metrics.average_duration,
                "timestamp": latest_event["timestamp"].isoformat(),
            },
            "alerts": {
                "total": len(self.alerts),
                "by_level": {
                    level.value: len([a for a in self.alerts if a.alert_level == level])
                    for level in AlertLevel
                },
                "active": len([a for a in self.alerts if not a.is_resolved]),
                "recent_alerts": [a.__dict__ for a in self.alerts[-5:]],
            },
            "historical_trends": {
                "timestamps": [
                    e["timestamp"].strftime("%Y-%m-%d %H:%M")
                    for e in self.monitoring_history[-20:]
                ],
                "quality_scores": [
                    e["suite_metrics"]["quality_score"]
                    for e in self.monitoring_history[-20:]
                ],
                "pass_rates": [
                    e["suite_metrics"]["pass_rate"]
                    for e in self.monitoring_history[-20:]
                ],
                "coverage_percentages": [
                    e["suite_metrics"]["coverage_percentage"]
                    for e in self.monitoring_history[-20:]
                ],
            },
            "visualizations": {
                name: viz.__dict__
                for name, viz in latest_event.get("visualizations", {}).items()
            },
            "system_health": self._assess_system_health(),
        }

    def _assess_system_health(self) -> Dict[str, Any]:
        """è¯„ä¼°ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        if not self.monitoring_history:
            return {"status": "unknown"}

        # åŸºäºæœ€è¿‘çš„ç›‘æ§æ•°æ®è¯„ä¼°
        recent_events = self.monitoring_history[-10:]  # æœ€è¿‘10æ¬¡æ£€æŸ¥

        avg_quality = statistics.mean(
            [e["suite_metrics"]["quality_score"] for e in recent_events]
        )
        avg_pass_rate = statistics.mean(
            [e["suite_metrics"]["pass_rate"] for e in recent_events]
        )
        avg_coverage = statistics.mean(
            [e["suite_metrics"]["coverage_percentage"] for e in recent_events]
        )

        # è®¡ç®—ç¨³å®šæ€§
        quality_stability = (
            100
            - statistics.stdev(
                [e["suite_metrics"]["quality_score"] for e in recent_events]
            )
            if len(recent_events) > 1
            else 100
        )

        # ç¡®å®šå¥åº·çŠ¶æ€
        if (
            avg_quality >= 90
            and avg_pass_rate >= 95
            and avg_coverage >= 85
            and quality_stability >= 90
        ):
            health_status = "excellent"
        elif (
            avg_quality >= 80
            and avg_pass_rate >= 90
            and avg_coverage >= 80
            and quality_stability >= 80
        ):
            health_status = "good"
        elif (
            avg_quality >= 70
            and avg_pass_rate >= 85
            and avg_coverage >= 75
            and quality_stability >= 70
        ):
            health_status = "fair"
        else:
            health_status = "poor"

        return {
            "status": health_status,
            "average_quality": round(avg_quality, 2),
            "average_pass_rate": round(avg_pass_rate, 2),
            "average_coverage": round(avg_coverage, 2),
            "stability": round(quality_stability, 2),
            "recommendations": self._generate_health_recommendations(
                health_status, avg_quality, avg_pass_rate, avg_coverage
            ),
        }

    def _generate_health_recommendations(
        self, status: str, quality: float, pass_rate: float, coverage: float
    ) -> List[str]:
        """ç”Ÿæˆå¥åº·çŠ¶æ€å»ºè®®"""
        recommendations = []

        if status == "excellent":
            recommendations.append("ç³»ç»Ÿå¥åº·çŠ¶æ€è‰¯å¥½ï¼Œç»§ç»­ä¿æŒæœ€ä½³å®è·µ")
            recommendations.append("è€ƒè™‘å¼•å…¥æ›´å¤šè‡ªåŠ¨åŒ–æµ‹è¯•å’Œé«˜çº§åŠŸèƒ½")
        elif status == "good":
            recommendations.append("ç³»ç»Ÿå¥åº·çŠ¶æ€è‰¯å¥½ï¼Œä»æœ‰æ”¹è¿›ç©ºé—´")
            if coverage < 85:
                recommendations.append("å»ºè®®æé«˜ä»£ç è¦†ç›–ç‡")
        elif status == "fair":
            recommendations.append("ç³»ç»Ÿå¥åº·çŠ¶æ€ä¸€èˆ¬ï¼Œéœ€è¦å…³æ³¨")
            if pass_rate < 90:
                recommendations.append("å»ºè®®ä¿®å¤å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹")
            if quality < 80:
                recommendations.append("å»ºè®®ä¼˜åŒ–æµ‹è¯•è´¨é‡å’Œå¯é æ€§")
        else:  # poor
            recommendations.append("ç³»ç»Ÿå¥åº·çŠ¶æ€è¾ƒå·®ï¼Œéœ€è¦ç«‹å³æ”¹è¿›")
            recommendations.append("å»ºè®®è¿›è¡Œå…¨é¢çš„è´¨é‡è¯„ä¼°å’Œä¼˜åŒ–")

        return recommendations

    def export_monitoring_report(
        self, format: str = "json", file_path: str = None
    ) -> str:
        """å¯¼å‡ºç›‘æ§æŠ¥å‘Š"""
        dashboard_data = self.get_monitoring_dashboard()

        if format == "json":
            output = json.dumps(
                dashboard_data, ensure_ascii=False, indent=2, default=str
            )
        elif format == "html":
            output = self._generate_html_monitoring_report(dashboard_data)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}")

        if file_path:
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(output)
            print(f"âœ… ç›‘æ§æŠ¥å‘Šå·²ä¿å­˜åˆ°: {file_path}")
            return file_path
        else:
            return output

    def _generate_html_monitoring_report(self, dashboard_data: Dict[str, Any]) -> str:
        """ç”ŸæˆHTMLæ ¼å¼çš„ç›‘æ§æŠ¥å‘Š"""
        current = dashboard_data["current_status"]
        alerts = dashboard_data["alerts"]
        health = dashboard_data["system_health"]

        html_template = """
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>æµ‹è¯•è´¨é‡ç›‘æ§æŠ¥å‘Š</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; background: #f8f9fa; }
                .header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; margin-bottom: 20px; }
                .metric-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 15px; margin-bottom: 20px; }
                .metric-card { background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
                .metric-value { font-size: 2em; font-weight: bold; color: #007bff; }
                .metric-label { color: #6c757d; font-size: 0.9em; }
                .alert-section { background: white; padding: 20px; border-radius: 8px; margin-bottom: 20px; }
                .health-status { padding: 10px 20px; border-radius: 5px; color: white; text-align: center; }
                .excellent { background-color: #28a745; }
                .good { background-color: #17a2b8; }
                .fair { background-color: #ffc107; color: #212529; }
                .poor { background-color: #dc3545; }
                .alert-list { max-height: 300px; overflow-y: auto; }
                .alert-item { padding: 10px; margin: 5px 0; border-radius: 5px; }
                .critical { background-color: #f8d7da; border-left: 4px solid #dc3545; }
                .high { background-color: #fff3cd; border-left: 4px solid #ffc107; }
                .medium { background-color: #d1ecf1; border-left: 4px solid #17a2b8; }
                .low { background-color: #d4edda; border-left: 4px solid #28a745; }
                table { width: 100%; border-collapse: collapse; }
                th, td { border: 1px solid #dee2e6; padding: 8px; text-align: left; }
                th { background-color: #f8f9fa; }
            </style>
        </head>
        <body>
            <div class="header">
                <h1>æµ‹è¯•è´¨é‡ç›‘æ§æŠ¥å‘Š</h1>
                <p>ç”Ÿæˆæ—¶é—´: {generated_at}</p>
                <div class="health-status {health_class}">{health_status}</div>
            </div>

            <div class="metric-grid">
                <div class="metric-card">
                    <div class="metric-value">{quality_score}</div>
                    <div class="metric-label">ç»¼åˆè´¨é‡å¾—åˆ†</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{pass_rate}%</div>
                    <div class="metric-label">æµ‹è¯•é€šè¿‡ç‡</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{coverage}%</div>
                    <div class="metric-label">ä»£ç è¦†ç›–ç‡</div>
                </div>
                <div class="metric-card">
                    <div class="metric-value">{duration}s</div>
                    <div class="metric-label">å¹³å‡è€—æ—¶</div>
                </div>
            </div>

            <div class="alert-section">
                <h2>å‘Šè­¦æ¦‚è§ˆ</h2>
                <table>
                    <tr><th>ä¸¥é‡çº§åˆ«</th><th>æ•°é‡</th><th>æ´»è·ƒå‘Šè­¦</th></tr>
                    {alert_table}
                </table>
                <div class="alert-list">
                    {recent_alerts}
                </div>
            </div>

            <div class="alert-section">
                <h2>å¥åº·å»ºè®®</h2>
                <ul>
                    {recommendations}
                </ul>
            </div>
        </body>
        </html>
        """

        # ç”Ÿæˆå‘Šè­¦è¡¨æ ¼
        alert_table = ""
        for level, count in alerts["by_level"].items():
            active_count = len(
                [a for a in alerts["recent_alerts"] if a.get("alert_level") == level]
            )
            alert_table += f"<tr><td>{level.upper()}</td><td>{count}</td><td>{active_count}</td></tr>"

        # ç”Ÿæˆæœ€è¿‘å‘Šè­¦
        recent_alerts = ""
        for alert in alerts["recent_alerts"][:5]:
            level_class = alert.get("alert_level", "low")
            recent_alerts += f"""
            <div class="alert-item {level_class}">
                <strong>{level_class.upper()}</strong> - {alert.get("message", "Unknown alert")}
                <br><small>{alert.get("timestamp", "")}</small>
            </div>
            """

        # ç”Ÿæˆå»ºè®®
        recommendations_html = ""
        for rec in health.get("recommendations", []):
            recommendations_html += f"<li>{rec}</li>"

        return html_template.format(
            generated_at=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            health_class=health["status"],
            health_status=f"ç³»ç»Ÿå¥åº·çŠ¶æ€: {health['status'].upper()}",
            quality_score=current["quality_score"],
            pass_rate=current["pass_rate"],
            coverage=current["coverage_percentage"],
            duration=round(current["average_duration"], 2),
            alert_table=alert_table,
            recent_alerts=recent_alerts,
            recommendations=recommendations_html,
        )


async def demo_metrics_dashboard():
    """æ¼”ç¤ºè´¨é‡æŒ‡æ ‡ä»ªè¡¨ç›˜"""
    print("ğŸš€ æ¼”ç¤ºè´¨é‡æŒ‡æ ‡ä»ªè¡¨ç›˜åŠŸèƒ½")

    # åˆ›å»ºç›‘æ§å™¨
    monitor = TestMetricsMonitor()

    # æ¨¡æ‹Ÿä¸€äº›åˆå§‹æ•°æ®
    print("ğŸ“Š æ·»åŠ åˆå§‹ç›‘æ§æ•°æ®...")
    for i in range(3):
        test_results = monitor._generate_monitoring_test_results()
        suite_metrics = monitor.quality_metrics.calculate_test_suite_metrics(
            test_results, ["src/main.py", "src/services.py"]
        )
        if suite_metrics:
            visualizations = monitor._generate_visualizations(suite_metrics)
            alerts = monitor._check_alerts(suite_metrics)
            monitor._record_monitoring_event(suite_metrics, visualizations, alerts)
            print(f"   âœ“ æ·»åŠ ç›‘æ§æ‰¹æ¬¡ {i + 1}")

    # è·å–ä»ªè¡¨ç›˜æ•°æ®
    dashboard_data = monitor.get_monitoring_dashboard()
    print(f"\nğŸ“ˆ ç³»ç»Ÿå¥åº·çŠ¶æ€: {dashboard_data['system_health']['status']}")
    print(f"ğŸ”” æ´»è·ƒå‘Šè­¦æ•°: {dashboard_data['alerts']['active']}")

    # å¯¼å‡ºæŠ¥å‘Š
    html_file = monitor.export_monitoring_report(
        "html", "/tmp/test_metrics_dashboard.html"
    )
    print(f"ğŸ“„ ç›‘æ§æŠ¥å‘Šå·²ä¿å­˜: {html_file}")

    # å¯åŠ¨å®æ—¶ç›‘æ§ï¼ˆæ¼”ç¤º5ç§’ååœæ­¢ï¼‰
    print("\nğŸ”´ å¯åŠ¨å®æ—¶ç›‘æ§æ¼”ç¤º...")
    monitor_task = asyncio.create_task(monitor.start_monitoring())

    # ç­‰å¾…5ç§’
    await asyncio.sleep(5)

    # åœæ­¢ç›‘æ§
    monitor.stop_monitoring()
    await monitor_task

    print("âœ… ç›‘æ§æ¼”ç¤ºå®Œæˆ")


if __name__ == "__main__":
    asyncio.run(demo_metrics_dashboard())
