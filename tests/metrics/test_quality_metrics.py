#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MyStocks æµ‹è¯•è´¨é‡æŒ‡æ ‡ç³»ç»Ÿ

æä¾›å…¨é¢çš„æµ‹è¯•è´¨é‡è¯„ä¼°ã€åº¦é‡å’Œåˆ†æåŠŸèƒ½ï¼Œæ”¯æŒå¤šç»´åº¦è´¨é‡æŒ‡æ ‡è®¡ç®—å’Œä¼˜åŒ–å»ºè®®ã€‚
"""

import json
import statistics
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional

import numpy as np


class MetricCategory(Enum):
    """è´¨é‡æŒ‡æ ‡ç±»åˆ«"""

    COVERAGE = "coverage"  # è¦†ç›–ç‡æŒ‡æ ‡
    RELIABILITY = "reliability"  # å¯é æ€§æŒ‡æ ‡
    PERFORMANCE = "performance"  # æ€§èƒ½æŒ‡æ ‡
    MAINTAINABILITY = "maintainability"  # å¯ç»´æŠ¤æ€§æŒ‡æ ‡
    USABILITY = "usability"  # å¯ç”¨æ€§æŒ‡æ ‡
    SECURITY = "security"  # å®‰å…¨æ€§æŒ‡æ ‡


class MetricWeight(Enum):
    """æŒ‡æ ‡æƒé‡"""

    CRITICAL = 0.4  # å…³é”®æƒé‡ 40%
    HIGH = 0.3  # é«˜æƒé‡ 30%
    MEDIUM = 0.2  # ä¸­ç­‰æƒé‡ 20%
    LOW = 0.1  # ä½æƒé‡ 10%


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç»“æ„"""

    test_id: str
    test_name: str
    status: str  # "passed", "failed", "skipped", "error"
    duration: float
    timestamp: datetime
    error_message: Optional[str] = None
    stack_trace: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class QualityMetric:
    """è´¨é‡æŒ‡æ ‡å®šä¹‰"""

    name: str
    category: MetricCategory
    description: str
    unit: str
    weight: MetricWeight
    formula: str
    target_value: float
    min_value: float
    max_value: float
    is_higher_better: bool = True


@dataclass
class TestSuiteMetrics:
    """æµ‹è¯•å¥—ä»¶æŒ‡æ ‡"""

    suite_name: str
    total_tests: int
    passed_tests: int
    failed_tests: int
    skipped_tests: int
    error_tests: int
    pass_rate: float
    average_duration: float
    total_duration: float
    coverage_percentage: float
    reliability_score: float
    performance_score: float
    quality_score: float
    timestamp: datetime
    test_results: List[TestResult] = field(default_factory=list)


class TestCoverageAnalyzer:
    """æµ‹è¯•è¦†ç›–ç‡åˆ†æå™¨"""

    def __init__(self):
        self.coverage_data = {}
        self.metrics = {}

    def analyze_code_coverage(self, test_results: List[TestResult], code_files: List[str]) -> Dict[str, Any]:
        """åˆ†æä»£ç è¦†ç›–ç‡"""
        print("ğŸ” åˆ†æä»£ç è¦†ç›–ç‡...")

        # æ¨¡æ‹Ÿè¦†ç›–ç‡è®¡ç®—
        total_lines = 0
        covered_lines = 0
        coverage_by_file = {}

        for file_path in code_files:
            total_lines += 1000  # å‡è®¾æ¯ä¸ªæ–‡ä»¶1000è¡Œ
            covered_lines += int(1000 * np.random.uniform(0.6, 0.95))
            coverage_by_file[file_path] = round(np.random.uniform(0.7, 0.95), 3) * 100

        # æµ‹è¯•è¦†ç›–ç‡
        test_coverage_rate = (
            len([r for r in test_results if r.status == "passed"]) / len(test_results) * 100 if test_results else 0
        )

        coverage_metrics = {
            "total_lines_covered": covered_lines,
            "total_lines": total_lines,
            "overall_coverage_percentage": round(covered_lines / total_lines * 100, 2),
            "test_coverage_rate": round(test_coverage_rate, 2),
            "coverage_by_file": coverage_by_file,
            "coverage_trend": self._calculate_coverage_trend(),
            "missing_coverage_areas": self._identify_missing_coverage(test_results, code_files),
        }

        return coverage_metrics

    def _calculate_coverage_trend(self) -> Dict[str, Any]:
        """è®¡ç®—è¦†ç›–ç‡è¶‹åŠ¿"""
        # æ¨¡æ‹Ÿè¶‹åŠ¿æ•°æ®
        dates = [datetime.now() - timedelta(days=i) for i in range(30)]
        coverage_rates = [np.random.uniform(70, 95) for _ in dates]

        return {
            "dates": [d.strftime("%Y-%m-%d") for d in dates],
            "coverage_rates": coverage_rates,
            "trend_direction": "improving" if coverage_rates[-1] > coverage_rates[0] else "declining",
            "average_coverage": round(statistics.mean(coverage_rates), 2),
        }

    def _identify_missing_coverage(self, test_results: List[TestResult], code_files: List[str]) -> List[Dict[str, Any]]:
        """è¯†åˆ«è¦†ç›–ç‡ä¸è¶³çš„åŒºåŸŸ"""
        missing_areas = []

        # æ¨¡æ‹Ÿè¯†åˆ«æœªè¦†ç›–çš„ä»£ç åŒºåŸŸ
        for i, file_path in enumerate(code_files[:3]):  # åªåˆ†æå‰3ä¸ªæ–‡ä»¶
            missing_areas.append(
                {
                    "file": file_path,
                    "uncovered_functions": [f"function_{j}" for j in range(np.random.randint(1, 4))],
                    "uncovered_branches": np.random.randint(5, 15),
                    "suggested_tests": [f"Test_{file_path.split('/')[-1]}_{j}" for j in range(1, 3)],
                }
            )

        return missing_areas


class TestReliabilityAnalyzer:
    """æµ‹è¯•å¯é æ€§åˆ†æå™¨"""

    def __init__(self):
        self.reliability_history = []

    def analyze_reliability(self, test_results: List[TestResult]) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•å¯é æ€§"""
        print("ğŸ” åˆ†ææµ‹è¯•å¯é æ€§...")

        if not test_results:
            return {"status": "no_data"}

        # è®¡ç®—åŸºæœ¬å¯é æ€§æŒ‡æ ‡
        passed_count = len([r for r in test_results if r.status == "passed"])
        total_count = len(test_results)
        pass_rate = passed_count / total_count * 100

        # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
        stability_score = self._calculate_stability(test_results)

        # è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡
        consistency_score = self._calculate_consistency(test_results)

        # è®¡ç®—é”™è¯¯åˆ†å¸ƒ
        error_distribution = self._analyze_error_distribution(test_results)

        # è®¡ç®—å¯é æ€§è¶‹åŠ¿
        reliability_trend = self._calculate_reliability_trend(test_results)

        reliability_metrics = {
            "pass_rate": round(pass_rate, 2),
            "stability_score": round(stability_score, 2),
            "consistency_score": round(consistency_score, 2),
            "reliability_score": round((pass_rate + stability_score + consistency_score) / 3, 2),
            "error_distribution": error_distribution,
            "reliability_trend": reliability_trend,
            "failure_analysis": self._analyze_failures(test_results),
            "reliability_recommendations": self._generate_reliability_recommendations(
                pass_rate, stability_score, consistency_score
            ),
        }

        return reliability_metrics

    def _calculate_stability(self, test_results: List[TestResult]) -> float:
        """è®¡ç®—ç¨³å®šæ€§å¾—åˆ†"""
        # åŸºäºæµ‹è¯•æ‰§è¡Œæ—¶é—´çš„ç¨³å®šæ€§
        durations = [r.duration for r in test_results]
        if len(durations) < 2:
            return 100.0

        # è®¡ç®—å˜å¼‚ç³»æ•°
        mean_duration = statistics.mean(durations)
        std_duration = statistics.stdev(durations)
        cv = std_duration / mean_duration if mean_duration > 0 else 0

        # ç¨³å®šæ€§è¯„åˆ†ï¼ˆå˜å¼‚ç³»æ•°è¶Šå°è¶Šç¨³å®šï¼‰
        stability = max(0, 100 - cv * 100)
        return stability

    def _calculate_consistency(self, test_results: List[TestResult]) -> float:
        """è®¡ç®—ä¸€è‡´æ€§å¾—åˆ†"""
        # åŸºäºæµ‹è¯•é€šè¿‡ç‡çš„ä¸€è‡´æ€§
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„åˆ†æ
        return len([r for r in test_results if r.status == "passed"]) / len(test_results) * 100

    def _analyze_error_distribution(self, test_results: List[TestResult]) -> Dict[str, Any]:
        """åˆ†æé”™è¯¯åˆ†å¸ƒ"""
        errors = [r for r in test_results if r.status in ["failed", "error"]]

        error_types = {}
        for error in errors:
            error_type = error.metadata.get("error_type", "unknown")
            error_types[error_type] = error_types.get(error_type, 0) + 1

        return {
            "total_errors": len(errors),
            "error_types": error_types,
            "error_rate": round(len(errors) / len(test_results) * 100, 2) if test_results else 0,
        }

    def _calculate_reliability_trend(self, test_results: List[TestResult]) -> Dict[str, Any]:
        """è®¡ç®—å¯é æ€§è¶‹åŠ¿"""
        # æ¨¡æ‹Ÿè¶‹åŠ¿æ•°æ®
        dates = [datetime.now() - timedelta(hours=i) for i in range(24)]
        reliability_rates = [np.random.uniform(85, 98) for _ in dates]

        return {
            "dates": [d.strftime("%Y-%m-%d %H:%M") for d in dates],
            "reliability_rates": reliability_rates,
            "trend_direction": "improving" if reliability_rates[-1] > reliability_rates[0] else "declining",
            "average_reliability": round(statistics.mean(reliability_rates), 2),
        }

    def _analyze_failures(self, test_results: List[TestResult]) -> List[Dict[str, Any]]:
        """åˆ†æå¤±è´¥æ¨¡å¼"""
        failures = [r for r in test_results if r.status in ["failed", "error"]]

        failure_patterns = []
        for failure in failures[:5]:  # åˆ†æå‰5ä¸ªå¤±è´¥
            pattern = {
                "test_id": failure.test_id,
                "test_name": failure.test_name,
                "error_type": failure.metadata.get("error_type", "unknown"),
                "error_message": failure.error_message,
                "failure_count": failure.metadata.get("failure_count", 1),
                "first_seen": failure.timestamp.isoformat(),
            }
            failure_patterns.append(pattern)

        return failure_patterns

    def _generate_reliability_recommendations(
        self, pass_rate: float, stability_score: float, consistency_score: float
    ) -> List[str]:
        """ç”Ÿæˆå¯é æ€§æ”¹è¿›å»ºè®®"""
        recommendations = []

        if pass_rate < 90:
            recommendations.append(f"æµ‹è¯•é€šè¿‡ç‡è¾ƒä½ ({pass_rate:.1f}%)ï¼Œå»ºè®®ä¿®å¤å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹")

        if stability_score < 85:
            recommendations.append(f"æµ‹è¯•æ‰§è¡Œä¸ç¨³å®š (ç¨³å®šæ€§å¾—åˆ†: {stability_score:.1f})ï¼Œå»ºè®®ä¼˜åŒ–æµ‹è¯•æ€§èƒ½")

        if consistency_score < 90:
            recommendations.append(f"æµ‹è¯•ä¸€è‡´æ€§è¾ƒå·® (ä¸€è‡´æ€§å¾—åˆ†: {consistency_score:.1f})ï¼Œå»ºè®®ç»Ÿä¸€æµ‹è¯•æ ‡å‡†")

        if pass_rate >= 95 and stability_score >= 90 and consistency_score >= 95:
            recommendations.append("æµ‹è¯•å¯é æ€§è‰¯å¥½ï¼Œç»§ç»­ä¿æŒ")

        return recommendations


class TestPerformanceAnalyzer:
    """æµ‹è¯•æ€§èƒ½åˆ†æå™¨"""

    def __init__(self):
        self.performance_benchmarks = {}

    def analyze_performance(self, test_results: List[TestResult]) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•æ€§èƒ½"""
        print("ğŸ” åˆ†ææµ‹è¯•æ€§èƒ½...")

        if not test_results:
            return {"status": "no_data"}

        # åŸºæœ¬æ€§èƒ½æŒ‡æ ‡
        durations = [r.duration for r in test_results]
        avg_duration = statistics.mean(durations)
        max_duration = max(durations)
        min_duration = min(durations)

        # æ€§èƒ½åˆ†å¸ƒ
        performance_distribution = self._analyze_performance_distribution(durations)

        # æ€§èƒ½è¶‹åŠ¿
        performance_trend = self._calculate_performance_trend(test_results)

        # èµ„æºä½¿ç”¨åˆ†æ
        resource_usage = self._analyze_resource_usage(test_results)

        # æ€§èƒ½ç“¶é¢ˆè¯†åˆ«
        performance_bottlenecks = self._identify_performance_bottlenecks(test_results)

        performance_metrics = {
            "average_duration_ms": round(avg_duration * 1000, 2),
            "max_duration_ms": round(max_duration * 1000, 2),
            "min_duration_ms": round(min_duration * 1000, 2),
            "median_duration_ms": round(statistics.median(durations) * 1000, 2),
            "performance_distribution": performance_distribution,
            "performance_trend": performance_trend,
            "resource_usage": resource_usage,
            "performance_bottlenecks": performance_bottlenecks,
            "performance_score": self._calculate_performance_score(avg_duration),
            "performance_recommendations": self._generate_performance_recommendations(
                avg_duration, performance_distribution
            ),
        }

        return performance_metrics

    def _analyze_performance_distribution(self, durations: List[float]) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½åˆ†å¸ƒ"""
        if not durations:
            return {}

        # åˆ†ä½æ•°åˆ†æ
        percentiles = {
            "p25": statistics.quantiles(durations, n=4)[0],
            "p50": statistics.median(durations),
            "p75": statistics.quantiles(durations, n=4)[2],
            "p90": np.percentile(durations, 90),
            "p95": np.percentile(durations, 95),
            "p99": np.percentile(durations, 99),
        }

        # è½¬æ¢ä¸ºæ¯«ç§’
        return {k: round(v * 1000, 2) for k, v in percentiles.items()}

    def _calculate_performance_trend(self, test_results: List[TestResult]) -> Dict[str, Any]:
        """è®¡ç®—æ€§èƒ½è¶‹åŠ¿"""
        # æŒ‰æ—¶é—´æ’åº
        sorted_results = sorted(test_results, key=lambda x: x.timestamp)

        # åˆ†ç»„è®¡ç®—æ¯å°æ—¶çš„å¹³å‡æ€§èƒ½
        hourly_performance = {}
        for result in sorted_results:
            hour_key = result.timestamp.strftime("%Y-%m-%d %H:00")
            if hour_key not in hourly_performance:
                hourly_performance[hour_key] = []
            hourly_performance[hour_key].append(result.duration)

        # è®¡ç®—æ¯å°æ—¶çš„å¹³å‡å€¼
        trend_data = {}
        for hour, durations in hourly_performance.items():
            trend_data[hour] = statistics.mean(durations)

        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        dates = list(trend_data.keys())
        performance_rates = list(trend_data.values())

        return {
            "dates": dates,
            "performance_rates": performance_rates,
            "trend_direction": "improving" if performance_rates[-1] < performance_rates[0] else "declining",
            "average_performance": round(statistics.mean(performance_rates), 3),
        }

    def _analyze_resource_usage(self, test_results: List[TestResult]) -> Dict[str, Any]:
        """åˆ†æèµ„æºä½¿ç”¨æƒ…å†µ"""
        # æ¨¡æ‹Ÿèµ„æºä½¿ç”¨æ•°æ®
        return {
            "cpu_usage": round(np.random.uniform(30, 70), 1),
            "memory_usage_mb": round(np.random.uniform(100, 500), 1),
            "disk_io_mb": round(np.random.uniform(10, 50), 1),
            "network_io_mb": round(np.random.uniform(5, 30), 1),
            "resource_efficiency": round(np.random.uniform(70, 95), 1),
        }

    def _identify_performance_bottlenecks(self, test_results: List[TestResult]) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        # æ‰¾å‡ºæ‰§è¡Œæ—¶é—´æœ€é•¿çš„æµ‹è¯•
        sorted_results = sorted(test_results, key=lambda x: x.duration, reverse=True)

        bottlenecks = []
        for result in sorted_results[:3]:  # åˆ†æå‰3ä¸ªæœ€æ…¢çš„æµ‹è¯•
            bottleneck = {
                "test_id": result.test_id,
                "test_name": result.test_name,
                "duration_ms": round(result.duration * 1000, 2),
                "duration_percentage": round(result.duration / sum(r.duration for r in test_results) * 100, 2),
                "suggested_optimization": self._suggest_optimization(result),
            }
            bottlenecks.append(bottleneck)

        return bottlenecks

    def _suggest_optimization(self, test_result: TestResult) -> str:
        """å»ºè®®ä¼˜åŒ–æ–¹æ¡ˆ"""
        if test_result.duration > 10:  # è¶…è¿‡10ç§’
            return "è€ƒè™‘å¹¶è¡ŒåŒ–æˆ–ç¼“å­˜ä¼˜åŒ–"
        elif test_result.duration > 5:
            return "è€ƒè™‘ç®—æ³•ä¼˜åŒ–æˆ–å‡å°‘IOæ“ä½œ"
        else:
            return "æ€§èƒ½è‰¯å¥½ï¼Œå¯ä»¥è¿›ä¸€æ­¥å¾®è°ƒ"

    def _calculate_performance_score(self, avg_duration: float) -> float:
        """è®¡ç®—æ€§èƒ½å¾—åˆ†"""
        # åŸºäºå¹³å‡æ‰§è¡Œæ—¶é—´çš„è¯„åˆ†ï¼ˆæœŸæœ› < 1ç§’ï¼‰
        if avg_duration <= 1:
            return 100
        elif avg_duration <= 3:
            return 90
        elif avg_duration <= 5:
            return 75
        elif avg_duration <= 10:
            return 60
        else:
            return max(0, 50 - (avg_duration - 10) * 5)

    def _generate_performance_recommendations(self, avg_duration: float, distribution: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½æ”¹è¿›å»ºè®®"""
        recommendations = []

        if avg_duration > 3:
            recommendations.append(f"å¹³å‡æ‰§è¡Œæ—¶é—´è¾ƒé•¿ ({avg_duration:.2f}s)ï¼Œå»ºè®®ä¼˜åŒ–æµ‹è¯•é€»è¾‘")

        if distribution.get("p95", 0) > 10:
            recommendations.append("95%åˆ†ä½æ•°æ‰§è¡Œæ—¶é—´è¿‡é•¿ï¼Œå»ºè®®ä¼˜åŒ–æç«¯æƒ…å†µ")

        if distribution.get("p99", 0) > 20:
            recommendations.append("99%åˆ†ä½æ•°æ‰§è¡Œæ—¶é—´è¿‡é•¿ï¼Œå­˜åœ¨æ€§èƒ½å¼‚å¸¸")

        recommendations.append("è€ƒè™‘ä½¿ç”¨å¼‚æ­¥æµ‹è¯•æé«˜å¹¶è¡Œæ‰§è¡Œæ•ˆç‡")

        return recommendations


class TestQualityMetrics:
    """æµ‹è¯•è´¨é‡æŒ‡æ ‡ä¸»ç±»"""

    def __init__(self):
        self.coverage_analyzer = TestCoverageAnalyzer()
        self.reliability_analyzer = TestReliabilityAnalyzer()
        self.performance_analyzer = TestPerformanceAnalyzer()
        self.metrics_registry = self._initialize_metrics_registry()
        self.quality_history = []

    def _initialize_metrics_registry(self) -> Dict[str, QualityMetric]:
        """åˆå§‹åŒ–æŒ‡æ ‡æ³¨å†Œè¡¨"""
        return {
            # è¦†ç›–ç‡æŒ‡æ ‡
            "test_coverage": QualityMetric(
                name="æµ‹è¯•è¦†ç›–ç‡",
                category=MetricCategory.COVERAGE,
                description="æµ‹è¯•ç”¨ä¾‹å¯¹ä»£ç çš„è¦†ç›–ç¨‹åº¦",
                unit="%",
                weight=MetricWeight.CRITICAL,
                formula="covered_lines / total_lines * 100",
                target_value=90.0,
                min_value=0.0,
                max_value=100.0,
                is_higher_better=True,
            ),
            "function_coverage": QualityMetric(
                name="å‡½æ•°è¦†ç›–ç‡",
                category=MetricCategory.COVERAGE,
                description="æµ‹è¯•ç”¨ä¾‹å¯¹å‡½æ•°çš„è¦†ç›–ç¨‹åº¦",
                unit="%",
                weight=MetricWeight.HIGH,
                formula="covered_functions / total_functions * 100",
                target_value=95.0,
                min_value=0.0,
                max_value=100.0,
                is_higher_better=True,
            ),
            # å¯é æ€§æŒ‡æ ‡
            "pass_rate": QualityMetric(
                name="æµ‹è¯•é€šè¿‡ç‡",
                category=MetricCategory.RELIABILITY,
                description="æµ‹è¯•ç”¨ä¾‹é€šè¿‡çš„æ¯”ä¾‹",
                unit="%",
                weight=MetricWeight.CRITICAL,
                formula="passed_tests / total_tests * 100",
                target_value=98.0,
                min_value=0.0,
                max_value=100.0,
                is_higher_better=True,
            ),
            "stability_score": QualityMetric(
                name="ç¨³å®šæ€§å¾—åˆ†",
                category=MetricCategory.RELIABILITY,
                description="æµ‹è¯•æ‰§è¡Œç»“æœçš„ç¨³å®šæ€§",
                unit="åˆ†",
                weight=MetricWeight.HIGH,
                formula="100 - coefficient_of_variation",
                target_value=90.0,
                min_value=0.0,
                max_value=100.0,
                is_higher_better=True,
            ),
            # æ€§èƒ½æŒ‡æ ‡
            "test_execution_time": QualityMetric(
                name="æµ‹è¯•æ‰§è¡Œæ—¶é—´",
                category=MetricCategory.PERFORMANCE,
                description="æµ‹è¯•å¥—ä»¶çš„å¹³å‡æ‰§è¡Œæ—¶é—´",
                unit="ç§’",
                weight=MetricWeight.MEDIUM,
                formula="total_duration / total_tests",
                target_value=2.0,
                min_value=0.0,
                max_value=60.0,
                is_higher_better=False,
            ),
            "concurrent_performance": QualityMetric(
                name="å¹¶å‘æ€§èƒ½",
                category=MetricCategory.PERFORMANCE,
                description="æµ‹è¯•å¹¶å‘æ‰§è¡Œçš„æ€§èƒ½è¡¨ç°",
                unit="req/s",
                weight=MetricWeight.MEDIUM,
                formula="successful_requests / time_seconds",
                target_value=100.0,
                min_value=0.0,
                max_value=1000.0,
                is_higher_better=True,
            ),
            # å¯ç»´æŠ¤æ€§æŒ‡æ ‡
            "test_maintainability": QualityMetric(
                name="å¯ç»´æŠ¤æ€§å¾—åˆ†",
                category=MetricCategory.MAINTAINABILITY,
                description="æµ‹è¯•ä»£ç çš„å¯ç»´æŠ¤ç¨‹åº¦",
                unit="åˆ†",
                weight=MetricWeight.MEDIUM,
                formula="åŸºäºä»£ç å¤æ‚åº¦å’Œè€¦åˆåº¦çš„è¯„åˆ†",
                target_value=85.0,
                min_value=0.0,
                max_value=100.0,
                is_higher_better=True,
            ),
            # å¯ç”¨æ€§æŒ‡æ ‡
            "test_usability": QualityMetric(
                name="æµ‹è¯•å¯ç”¨æ€§",
                category=MetricCategory.USABILITY,
                description="æµ‹è¯•æ¡†æ¶å’Œå·¥å…·çš„æ˜“ç”¨æ€§",
                unit="åˆ†",
                weight=MetricWeight.LOW,
                formula="åŸºäºç”¨æˆ·åé¦ˆå’Œå·¥å…·æ˜“ç”¨æ€§",
                target_value=80.0,
                min_value=0.0,
                max_value=100.0,
                is_higher_better=True,
            ),
            # å®‰å…¨æ€§æŒ‡æ ‡
            "test_security": QualityMetric(
                name="æµ‹è¯•å®‰å…¨æ€§",
                category=MetricCategory.SECURITY,
                description="æµ‹è¯•è¿‡ç¨‹ä¸­çš„å®‰å…¨æ€§ä¿éšœ",
                unit="åˆ†",
                weight=MetricWeight.HIGH,
                formula="å®‰å…¨æµ‹è¯•è¦†ç›–ç‡ + æ¼æ´æ£€æµ‹èƒ½åŠ›",
                target_value=90.0,
                min_value=0.0,
                max_value=100.0,
                is_higher_better=True,
            ),
        }

    def calculate_test_suite_metrics(
        self, test_results: List[TestResult], code_files: List[str] = None
    ) -> TestSuiteMetrics:
        """è®¡ç®—æµ‹è¯•å¥—ä»¶è´¨é‡æŒ‡æ ‡"""
        print("ğŸ¯ è®¡ç®—æµ‹è¯•å¥—ä»¶è´¨é‡æŒ‡æ ‡...")

        if not test_results:
            print("âš ï¸  æ²¡æœ‰æµ‹è¯•ç»“æœæ•°æ®")
            return None

        # åŸºæœ¬ç»Ÿè®¡
        total_tests = len(test_results)
        passed_tests = len([r for r in test_results if r.status == "passed"])
        failed_tests = len([r for r in test_results if r.status == "failed"])
        skipped_tests = len([r for r in test_results if r.status == "skipped"])
        error_tests = len([r for r in test_results if r.status == "error"])

        pass_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        average_duration = statistics.mean([r.duration for r in test_results])
        total_duration = sum([r.duration for r in test_results])

        # åˆ†æè¦†ç›–ç‡
        coverage_metrics = self.coverage_analyzer.analyze_code_coverage(test_results, code_files or [])
        coverage_percentage = coverage_metrics.get("overall_coverage_percentage", 0)

        # åˆ†æå¯é æ€§
        reliability_metrics = self.reliability_analyzer.analyze_reliability(test_results)
        reliability_score = reliability_metrics.get("reliability_score", 0)

        # åˆ†ææ€§èƒ½
        performance_metrics = self.performance_analyzer.analyze_performance(test_results)
        performance_score = performance_metrics.get("performance_score", 0)

        # è®¡ç®—ç»¼åˆè´¨é‡å¾—åˆ†
        quality_score = self._calculate_quality_score(coverage_percentage, reliability_score, performance_score)

        # åˆ›å»ºæµ‹è¯•å¥—ä»¶æŒ‡æ ‡
        suite_metrics = TestSuiteMetrics(
            suite_name="comprehensive_test_suite",
            total_tests=total_tests,
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            skipped_tests=skipped_tests,
            error_tests=error_tests,
            pass_rate=round(pass_rate, 2),
            average_duration=round(average_duration, 3),
            total_duration=round(total_duration, 2),
            coverage_percentage=round(coverage_percentage, 2),
            reliability_score=round(reliability_score, 2),
            performance_score=round(performance_score, 2),
            quality_score=round(quality_score, 2),
            timestamp=datetime.now(),
            test_results=test_results,
        )

        # ä¿å­˜åˆ°å†å²è®°å½•
        self.quality_history.append(suite_metrics)

        return suite_metrics

    def _calculate_quality_score(self, coverage: float, reliability: float, performance: float) -> float:
        """è®¡ç®—ç»¼åˆè´¨é‡å¾—åˆ†"""
        # ä½¿ç”¨åŠ æƒå¹³å‡
        weights = {"coverage": 0.3, "reliability": 0.4, "performance": 0.3}

        quality_score = (
            coverage * weights["coverage"] + reliability * weights["reliability"] + performance * weights["performance"]
        )

        return round(quality_score, 2)

    def generate_quality_report(self, suite_metrics: TestSuiteMetrics) -> Dict[str, Any]:
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        print("ğŸ“Š ç”Ÿæˆè´¨é‡æŠ¥å‘Š...")

        # åˆ†æå„ç»´åº¦æŒ‡æ ‡
        coverage_analysis = self.coverage_analyzer.analyze_code_coverage(
            suite_metrics.test_results, ["sample_file1.py", "sample_file2.py"]
        )
        reliability_analysis = self.reliability_analyzer.analyze_reliability(suite_metrics.test_results)
        performance_analysis = self.performance_analyzer.analyze_performance(suite_metrics.test_results)

        # ç”Ÿæˆå»ºè®®
        quality_recommendations = self._generate_quality_recommendations(suite_metrics)

        # è¯†åˆ«æ”¹è¿›æœºä¼š
        improvement_opportunities = self._identify_improvement_opportunities(
            coverage_analysis, reliability_analysis, performance_analysis
        )

        # è´¨é‡è¶‹åŠ¿åˆ†æ
        quality_trend = self._analyze_quality_trend()

        quality_report = {
            "report_summary": {
                "suite_name": suite_metrics.suite_name,
                "generated_at": suite_metrics.timestamp.isoformat(),
                "quality_score": suite_metrics.quality_score,
                "overall_rating": self._get_quality_rating(suite_metrics.quality_score),
                "total_test_executions": suite_metrics.total_tests,
            },
            "metrics_by_category": {
                "coverage": coverage_analysis,
                "reliability": reliability_analysis,
                "performance": performance_analysis,
            },
            "detailed_metrics": {
                "test_results": {
                    "total": suite_metrics.total_tests,
                    "passed": suite_metrics.passed_tests,
                    "failed": suite_metrics.failed_tests,
                    "skipped": suite_metrics.skipped_tests,
                    "errors": suite_metrics.error_tests,
                    "pass_rate": suite_metrics.pass_rate,
                },
                "performance": {
                    "average_duration_ms": suite_metrics.average_duration * 1000,
                    "total_duration_ms": suite_metrics.total_duration * 1000,
                },
                "coverage": {"percentage": suite_metrics.coverage_percentage},
            },
            "quality_recommendations": quality_recommendations,
            "improvement_opportunities": improvement_opportunities,
            "quality_trend": quality_trend,
            "metric_definitions": {name: metric.__dict__ for name, metric in self.metrics_registry.items()},
            "benchmark_comparison": self._compare_with_benchmarks(suite_metrics),
        }

        return quality_report

    def _generate_quality_recommendations(self, suite_metrics: TestSuiteMetrics) -> List[Dict[str, Any]]:
        """ç”Ÿæˆè´¨é‡æ”¹è¿›å»ºè®®"""
        recommendations = []

        # åŸºäºé€šè¿‡ç‡çš„å»ºè®®
        if suite_metrics.pass_rate < 95:
            recommendations.append(
                {
                    "priority": "high",
                    "category": "reliability",
                    "issue": f"æµ‹è¯•é€šè¿‡ç‡è¾ƒä½ ({suite_metrics.pass_rate:.1f}%)",
                    "recommendation": "ä¿®å¤å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹ï¼Œæé«˜æµ‹è¯•è´¨é‡",
                    "estimated_effort": "medium",
                }
            )

        # åŸºäºè¦†ç›–ç‡çš„å»ºè®®
        if suite_metrics.coverage_percentage < 85:
            recommendations.append(
                {
                    "priority": "medium",
                    "category": "coverage",
                    "issue": f"ä»£ç è¦†ç›–ç‡ä¸è¶³ ({suite_metrics.coverage_percentage:.1f}%)",
                    "recommendation": "å¢åŠ æµ‹è¯•ç”¨ä¾‹ä»¥æé«˜è¦†ç›–ç‡",
                    "estimated_effort": "high",
                }
            )

        # åŸºäºæ€§èƒ½çš„å»ºè®®
        if suite_metrics.average_duration > 5:
            recommendations.append(
                {
                    "priority": "low",
                    "category": "performance",
                    "issue": f"æµ‹è¯•æ‰§è¡Œæ—¶é—´è¾ƒé•¿ ({suite_metrics.average_duration:.2f}s)",
                    "recommendation": "ä¼˜åŒ–æµ‹è¯•é€»è¾‘ï¼Œæé«˜æ‰§è¡Œæ•ˆç‡",
                    "estimated_effort": "low",
                }
            )

        # åŸºäºç»¼åˆè´¨é‡çš„å»ºè®®
        if suite_metrics.quality_score < 80:
            recommendations.append(
                {
                    "priority": "high",
                    "category": "overall",
                    "issue": f"æ•´ä½“è´¨é‡è¯„åˆ†è¾ƒä½ ({suite_metrics.quality_score:.1f})",
                    "recommendation": "å…¨é¢æå‡æµ‹è¯•è´¨é‡",
                    "estimated_effort": "high",
                }
            )

        return recommendations

    def _identify_improvement_opportunities(
        self, coverage: Dict, reliability: Dict, performance: Dict
    ) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ”¹è¿›æœºä¼š"""
        opportunities = []

        # è¦†ç›–ç‡æ”¹è¿›æœºä¼š
        if coverage.get("overall_coverage_percentage", 0) < 90:
            opportunities.append(
                {
                    "area": "coverage",
                    "potential_improvement": 90 - coverage.get("overall_coverage_percentage", 0),
                    "priority": "medium",
                    "description": "æé«˜ä»£ç è¦†ç›–ç‡",
                    "estimated_effort": "medium",
                }
            )

        # å¯é æ€§æ”¹è¿›æœºä¼š
        if reliability.get("pass_rate", 0) < 98:
            opportunities.append(
                {
                    "area": "reliability",
                    "potential_improvement": 98 - reliability.get("pass_rate", 0),
                    "priority": "high",
                    "description": "æé«˜æµ‹è¯•å¯é æ€§",
                    "estimated_effort": "medium",
                }
            )

        # æ€§èƒ½æ”¹è¿›æœºä¼š
        if performance.get("average_duration_ms", 0) > 2000:
            opportunities.append(
                {
                    "area": "performance",
                    "potential_improvement": performance.get("average_duration_ms", 0) - 2000,
                    "priority": "low",
                    "description": "ä¼˜åŒ–æµ‹è¯•æ‰§è¡Œæ€§èƒ½",
                    "estimated_effort": "low",
                }
            )

        return opportunities

    def _analyze_quality_trend(self) -> Dict[str, Any]:
        """åˆ†æè´¨é‡è¶‹åŠ¿"""
        if len(self.quality_history) < 2:
            return {"status": "insufficient_data"}

        # è·å–æœ€è¿‘10æ¬¡çš„ç»“æœ
        recent_history = self.quality_history[-10:]

        dates = [m.timestamp.strftime("%Y-%m-%d") for m in recent_history]
        quality_scores = [m.quality_score for m in recent_history]

        trend_direction = "improving" if quality_scores[-1] > quality_scores[0] else "declining"

        return {
            "dates": dates,
            "quality_scores": quality_scores,
            "trend_direction": trend_direction,
            "average_score": round(statistics.mean(quality_scores), 2),
            "score_change": round(quality_scores[-1] - quality_scores[0], 2),
            "volatility": round(statistics.stdev(quality_scores) if len(quality_scores) > 1 else 0, 2),
        }

    def _get_quality_rating(self, quality_score: float) -> str:
        """è·å–è´¨é‡è¯„çº§"""
        if quality_score >= 95:
            return "excellent"
        elif quality_score >= 85:
            return "good"
        elif quality_score >= 75:
            return "fair"
        elif quality_score >= 60:
            return "poor"
        else:
            return "critical"

    def _compare_with_benchmarks(self, suite_metrics: TestSuiteMetrics) -> Dict[str, Any]:
        """ä¸è¡Œä¸šæ ‡å‡†åŸºå‡†å¯¹æ¯”"""
        # æ¨¡æ‹Ÿè¡Œä¸šæ ‡å‡†æ•°æ®
        industry_benchmarks = {
            "coverage": 85.0,
            "pass_rate": 95.0,
            "performance_score": 80.0,
            "quality_score": 82.0,
        }

        comparison = {}
        for metric, benchmark in industry_benchmarks.items():
            if metric == "coverage":
                actual = suite_metrics.coverage_percentage
            elif metric == "pass_rate":
                actual = suite_metrics.pass_rate
            elif metric == "performance_score":
                actual = suite_metrics.performance_score
            else:
                actual = suite_metrics.quality_score

            comparison[metric] = {
                "actual": round(actual, 2),
                "benchmark": benchmark,
                "difference": round(actual - benchmark, 2),
                "status": "above" if actual > benchmark else ("below" if actual < benchmark else "meets"),
            }

        return comparison

    def export_metrics(
        self,
        suite_metrics: TestSuiteMetrics,
        format: str = "json",
        file_path: str = None,
    ) -> str:
        """å¯¼å‡ºè´¨é‡æŒ‡æ ‡"""
        print(f"ğŸ“¤ å¯¼å‡ºè´¨é‡æŒ‡æ ‡ ({format})...")

        quality_report = self.generate_quality_report(suite_metrics)

        if format == "json":
            output = json.dumps(quality_report, ensure_ascii=False, indent=2, default=str)
        elif format == "html":
            output = self._generate_html_report(quality_report)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}")

        # ä¿å­˜åˆ°æ–‡ä»¶
        if file_path:
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(output)
            print(f"âœ… è´¨é‡æŒ‡æ ‡å·²ä¿å­˜åˆ°: {file_path}")
            return file_path
        else:
            return output

    def _generate_html_report(self, quality_report: Dict[str, Any]) -> str:
        """ç”ŸæˆHTMLæ ¼å¼çš„è´¨é‡æŠ¥å‘Š"""
        html_template = """
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>æµ‹è¯•è´¨é‡æŠ¥å‘Š</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                .header { background: #f0f0f0; padding: 20px; border-radius: 5px; }
                .metric { margin: 10px 0; padding: 10px; border: 1px solid #ddd; border-radius: 3px; }
                .recommendation { background: #fff3cd; padding: 10px; margin: 5px 0; border-radius: 3px; }
                .excellent { color: green; font-weight: bold; }
                .good { color: blue; font-weight: bold; }
                .fair { color: orange; font-weight: bold; }
                .poor { color: red; font-weight: bold; }
                table { width: 100%; border-collapse: collapse; margin: 10px 0; }
                th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
                th { background-color: #f2f2f2; }
            </style>
        </head>
        <body>
            <div class="header">
                <h1>æµ‹è¯•è´¨é‡æŠ¥å‘Š</h1>
                <p>ç”Ÿæˆæ—¶é—´: {generated_at}</p>
                <p>ç»¼åˆè´¨é‡å¾—åˆ†: <span class="{rating}">{quality_score}</span></p>
            </div>

            <div class="metric">
                <h2>æµ‹è¯•ç»“æœç»Ÿè®¡</h2>
                <table>
                    <tr><th>æ€»æµ‹è¯•æ•°</th><th>é€šè¿‡</th><th>å¤±è´¥</th><th>è·³è¿‡</th><th>é”™è¯¯</th></tr>
                    <tr><td>{total_tests}</td><td>{passed_tests}</td><td>{failed_tests}</td><td>{skipped_tests}</td><td>{error_tests}</td></tr>
                </table>
            </div>

            <div class="metric">
                <h2>è¦†ç›–ç‡åˆ†æ</h2>
                <p>æ€»ä½“è¦†ç›–ç‡: {coverage_percentage}%</p>
            </div>

            <div class="metric">
                <h2>æ”¹è¿›å»ºè®®</h2>
                {recommendations}
            </div>
        </body>
        </html>
        """

        # æ ¼å¼åŒ–å»ºè®®
        recommendations_html = ""
        for rec in quality_report.get("quality_recommendations", []):
            priority_class = rec.get("priority", "medium")
            recommendations_html += f"""
            <div class="recommendation">
                <strong>{priority_class.upper()}</strong> - {rec["category"]}
                <br>é—®é¢˜: {rec["issue"]}
                <br>å»ºè®®: {rec["recommendation"]}
                <br>é¢„ä¼°å·¥ä½œé‡: {rec.get("estimated_effort", "unknown")}
            </div>
            """

        return html_template.format(
            generated_at=quality_report["report_summary"]["generated_at"],
            quality_score=quality_report["report_summary"]["quality_score"],
            rating=quality_report["report_summary"]["overall_rating"],
            total_tests=quality_report["detailed_metrics"]["test_results"]["total"],
            passed_tests=quality_report["detailed_metrics"]["test_results"]["passed"],
            failed_tests=quality_report["detailed_metrics"]["test_results"]["failed"],
            skipped_tests=quality_report["detailed_metrics"]["test_results"]["skipped"],
            error_tests=quality_report["detailed_metrics"]["test_results"]["errors"],
            coverage_percentage=quality_report["detailed_metrics"]["coverage"]["percentage"],
            recommendations=recommendations_html,
        )

    def get_metric_dashboard_data(self) -> Dict[str, Any]:
        """è·å–è´¨é‡æŒ‡æ ‡ä»ªè¡¨ç›˜æ•°æ®"""
        if not self.quality_history:
            return {"status": "no_data"}

        latest_metrics = self.quality_history[-1]

        return {
            "current_metrics": {
                "quality_score": latest_metrics.quality_score,
                "pass_rate": latest_metrics.pass_rate,
                "coverage_percentage": latest_metrics.coverage_percentage,
                "performance_score": latest_metrics.performance_score,
                "average_duration": latest_metrics.average_duration,
                "timestamp": latest_metrics.timestamp.isoformat(),
            },
            "trend_data": {
                "dates": [m.timestamp.strftime("%Y-%m-%d %H:%M") for m in self.quality_history[-20:]],
                "quality_scores": [m.quality_score for m in self.quality_history[-20:]],
                "pass_rates": [m.pass_rate for m in self.quality_history[-20:]],
                "coverage_percentages": [m.coverage_percentage for m in self.quality_history[-20:]],
            },
            "alert_summary": {
                "critical_issues": len([m for m in self.quality_history if m.quality_score < 60]),
                "warning_issues": len([m for m in self.quality_history if 60 <= m.quality_score < 80]),
                "healthy_executions": len([m for m in self.quality_history if m.quality_score >= 85]),
            },
        }


# ä½¿ç”¨ç¤ºä¾‹
def demo_test_quality_metrics():
    """æ¼”ç¤ºæµ‹è¯•è´¨é‡æŒ‡æ ‡åŠŸèƒ½"""
    print("ğŸš€ æ¼”ç¤ºæµ‹è¯•è´¨é‡æŒ‡æ ‡åŠŸèƒ½")

    # åˆ›å»ºè´¨é‡æŒ‡æ ‡ç³»ç»Ÿ
    quality_system = TestQualityMetrics()

    # ç”Ÿæˆæ¨¡æ‹Ÿæµ‹è¯•ç»“æœ
    test_results = []
    for i in range(100):
        status = np.random.choice(["passed", "failed", "skipped", "error"], p=[0.92, 0.05, 0.02, 0.01])
        duration = np.random.uniform(0.1, 8.0) if status == "passed" else np.random.uniform(0.5, 3.0)

        test_result = TestResult(
            test_id=f"test_{i + 1:03d}",
            test_name=f"Test Case {i + 1}",
            status=status,
            duration=duration,
            timestamp=datetime.now() - timedelta(hours=np.random.randint(0, 24)),
            error_message=f"Error message {i}" if status in ["failed", "error"] else None,
            metadata={
                "error_type": np.random.choice(["assertion", "timeout", "network", "unknown"]),
                "failure_count": np.random.randint(1, 5) if status != "passed" else 0,
            },
        )
        test_results.append(test_result)

    # è®¡ç®—è´¨é‡æŒ‡æ ‡
    suite_metrics = quality_system.calculate_test_suite_metrics(test_results, ["src/main.py", "src/utils.py"])

    if suite_metrics:
        print("\nğŸ“Š æµ‹è¯•å¥—ä»¶è´¨é‡æŒ‡æ ‡:")
        print(f"   æ€»æµ‹è¯•æ•°: {suite_metrics.total_tests}")
        print(f"   é€šè¿‡ç‡: {suite_metrics.pass_rate:.1f}%")
        print(f"   è¦†ç›–ç‡: {suite_metrics.coverage_percentage:.1f}%")
        print(f"   å¯é æ€§å¾—åˆ†: {suite_metrics.reliability_score:.1f}")
        print(f"   æ€§èƒ½å¾—åˆ†: {suite_metrics.performance_score:.1f}")
        print(f"   ç»¼åˆè´¨é‡å¾—åˆ†: {suite_metrics.quality_score:.1f}")

        # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        quality_report = quality_system.generate_quality_report(suite_metrics)
        print(f"\nğŸ“ˆ è´¨é‡è¯„çº§: {quality_report['report_summary']['overall_rating']}")

        # å¯¼å‡ºè´¨é‡æŒ‡æ ‡
        json_file = quality_system.export_metrics(suite_metrics, "json", "/tmp/test_quality_report.json")
        print(f"ğŸ“„ JSONæŠ¥å‘Šå·²ä¿å­˜: {json_file}")

        # è·å–ä»ªè¡¨ç›˜æ•°æ®
        dashboard_data = quality_system.get_metric_dashboard_data()
        print("\nğŸ¯ ä»ªè¡¨ç›˜æ•°æ®æ›´æ–°æˆåŠŸ")

        # æ˜¾ç¤ºæ”¹è¿›å»ºè®®
        print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        for rec in quality_report.get("quality_recommendations", [])[:3]:
            print(f"   {rec['priority'].upper()}: {rec['recommendation']}")


if __name__ == "__main__":
    demo_test_quality_metrics()
