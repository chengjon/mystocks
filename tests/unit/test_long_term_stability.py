#!/usr/bin/env python3
"""
é•¿æœŸç¨³å®šæ€§æµ‹è¯•
Phase 6.4.4 - é•¿æœŸç¨³å®šæ€§æµ‹è¯•

æ‰§è¡Œé•¿æœŸç¨³å®šæ€§æµ‹è¯•ï¼ŒéªŒè¯å†…å­˜æ³„æ¼å’Œèµ„æºæ¸…ç†ï¼Œç¡®ä¿GPUåŠ é€Ÿå¼•æ“åœ¨é•¿æœŸè¿è¡Œä¸­çš„ç¨³å®šæ€§
"""

import asyncio
import numpy as np
import sys
import time
import gc
import psutil
from pathlib import Path
from typing import Dict, Any
import tracemalloc
import weakref

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


class LongTermStabilityTester:
    """é•¿æœŸç¨³å®šæ€§æµ‹è¯•å™¨"""

    def __init__(self):
        self.test_results = {}
        self.process = psutil.Process()
        self.initial_memory = None
        self.memory_samples = []

    async def run_comprehensive_stability_tests(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢çš„é•¿æœŸç¨³å®šæ€§æµ‹è¯•"""
        print("ğŸš€ GPUåŠ é€Ÿå¼•æ“é•¿æœŸç¨³å®šæ€§æµ‹è¯•...")

        # å¯åŠ¨å†…å­˜è·Ÿè¸ª
        tracemalloc.start()
        gc.collect()
        self.initial_memory = self.process.memory_info().rss

        stability_tests = [
            ("é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§æµ‹è¯•", self.test_long_running_stability),
            ("å†…å­˜æ³„æ¼æ£€æµ‹æµ‹è¯•", self.test_memory_leak_detection),
            ("èµ„æºæ¸…ç†éªŒè¯æµ‹è¯•", self.test_resource_cleanup),
            ("å¹¶å‘å‹åŠ›ç¨³å®šæ€§æµ‹è¯•", self.test_concurrent_stress_stability),
            ("å¼‚å¸¸æ¢å¤ç¨³å®šæ€§æµ‹è¯•", self.test_exception_recovery_stability),
            ("å†…å­˜æ± é•¿æœŸç¨³å®šæ€§æµ‹è¯•", self.test_memory_pool_stability),
        ]

        for test_name, test_func in stability_tests:
            print(f"   ğŸ§ª {test_name}...")
            try:
                start_time = time.time()
                result = await test_func()
                execution_time = time.time() - start_time

                self.test_results[test_name] = {
                    "success": result.get("success", False),
                    "execution_time": execution_time,
                    "details": result,
                    "memory_usage_mb": self.get_current_memory_usage(),
                }

                status = "âœ…" if result.get("success", False) else "âŒ"
                print(f"   {status} {test_name} ({execution_time:.2f}s)")

            except Exception as e:
                print(f"   âŒ {test_name}å¤±è´¥: {e}")
                self.test_results[test_name] = {
                    "success": False,
                    "error": str(e),
                    "execution_time": 0,
                    "memory_usage_mb": self.get_current_memory_usage(),
                }

        # åœæ­¢å†…å­˜è·Ÿè¸ªå¹¶ç”ŸæˆæŠ¥å‘Š
        tracemalloc.stop()
        return self.generate_stability_report()

    async def test_long_running_stability(self) -> Dict[str, Any]:
        """æµ‹è¯•é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§"""
        try:
            from src.gpu.core.kernels import MatrixKernelEngine, TransformKernelEngine

            # åˆå§‹åŒ–å†…æ ¸
            matrix_kernel = MatrixKernelEngine()
            transform_kernel = TransformKernelEngine()

            await matrix_kernel.initialize()
            await transform_kernel.initialize()

            # é•¿æœŸè¿è¡Œæµ‹è¯•å‚æ•°
            duration_minutes = 2  # 2åˆ†é’Ÿæµ‹è¯•ï¼ˆå®é™…ä¸­å¯èƒ½æ˜¯æ•°å°æ—¶ï¼‰
            operations_per_minute = 30
            total_operations = duration_minutes * operations_per_minute

            start_time = time.time()
            operation_results = []
            memory_samples = []

            print(f"      â±ï¸  è¿è¡Œ {duration_minutes} åˆ†é’Ÿï¼Œæ€»è®¡ {total_operations} æ¬¡æ“ä½œ...")

            for i in range(total_operations):
                # è®°å½•å†…å­˜ä½¿ç”¨
                memory_mb = self.get_current_memory_usage()
                memory_samples.append(memory_mb)

                # éšæœºé€‰æ‹©æ“ä½œç±»å‹
                if i % 2 == 0:
                    # çŸ©é˜µè¿ç®—
                    size = np.random.choice([128, 256, 512])
                    matrix_a = np.random.random((size, size)).astype(np.float32)
                    matrix_b = np.random.random((size, size)).astype(np.float32)

                    from src.gpu.core.kernels.standardized_interface import (
                        MatrixOperationType,
                        MatrixConfig,
                    )

                    config = MatrixConfig(operation_type=MatrixOperationType.MULTIPLY)

                    result = await matrix_kernel.execute_matrix_operation(matrix_a, matrix_b, config)
                    operation_results.append(("matrix", result.success))
                else:
                    # å˜æ¢æ“ä½œ
                    data_size = np.random.choice([1000, 5000, 10000])
                    test_data = np.random.random(data_size).astype(np.float32)

                    from src.gpu.core.kernels.standardized_interface import (
                        TransformOperationType,
                        TransformConfig,
                    )

                    config = TransformConfig(operation_type=TransformOperationType.NORMALIZE)

                    result = await transform_kernel.execute_transform_operation(test_data, config)
                    operation_results.append(("transform", result.success))

                # å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼ˆæ¨¡æ‹Ÿå®é™…ä½¿ç”¨ï¼‰
                if i % 10 == 0:
                    gc.collect()

                # æ§åˆ¶æ‰§è¡Œé¢‘ç‡
                elapsed = time.time() - start_time
                expected_elapsed = (i + 1) / operations_per_minute * 60
                if elapsed < expected_elapsed:
                    await asyncio.sleep(expected_elapsed - elapsed)

            # åˆ†æç»“æœ
            successful_operations = sum(1 for _, success in operation_results if success)
            total_operations = len(operation_results)

            # å†…å­˜å¢é•¿åˆ†æ
            if len(memory_samples) > 10:
                initial_memory = np.mean(memory_samples[:10])
                final_memory = np.mean(memory_samples[-10:])
                memory_growth_mb = final_memory - initial_memory
                memory_growth_rate = memory_growth_mb / (duration_minutes * 60)  # MB/second
            else:
                memory_growth_mb = 0
                memory_growth_rate = 0

            return {
                "success": successful_operations >= total_operations * 0.95,  # 95%æˆåŠŸç‡
                "total_operations": total_operations,
                "successful_operations": successful_operations,
                "success_rate": successful_operations / total_operations,
                "operation_breakdown": {
                    "matrix_operations": sum(1 for op, _ in operation_results if op == "matrix"),
                    "transform_operations": sum(1 for op, _ in operation_results if op == "transform"),
                },
                "memory_analysis": {
                    "initial_memory_mb": np.mean(memory_samples[:10]) if memory_samples else 0,
                    "final_memory_mb": np.mean(memory_samples[-10:]) if memory_samples else 0,
                    "memory_growth_mb": memory_growth_mb,
                    "memory_growth_rate_mb_per_sec": memory_growth_rate,
                    "max_memory_mb": max(memory_samples) if memory_samples else 0,
                    "min_memory_mb": min(memory_samples) if memory_samples else 0,
                },
                "stability_duration_minutes": duration_minutes,
                "operations_per_minute": total_operations / duration_minutes,
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    async def test_memory_leak_detection(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜æ³„æ¼æ£€æµ‹"""
        try:
            from src.gpu.core.kernels import MatrixKernelEngine
            from src.gpu.core.hardware_abstraction.memory_pool import get_memory_pool

            # åˆå§‹åŒ–ç»„ä»¶
            matrix_kernel = MatrixKernelEngine()
            memory_pool = get_memory_pool()

            await matrix_kernel.initialize()
            await memory_pool.initialize()

            # å†…å­˜æ³„æ¼æ£€æµ‹å‚æ•°
            iterations = 200
            matrix_size = 512

            # åŸºçº¿å†…å­˜ä½¿ç”¨
            gc.collect()
            baseline_memory = self.get_current_memory_usage()
            memory_samples = [baseline_memory]

            # åˆ›å»ºå¼±å¼•ç”¨è·Ÿè¸ª
            weak_references = []

            print(f"      ğŸ” æ‰§è¡Œ {iterations} æ¬¡è¿­ä»£æ£€æµ‹å†…å­˜æ³„æ¼...")

            for i in range(iterations):
                # åˆ›å»ºå¤§å‹å¯¹è±¡
                large_matrix = np.random.random((matrix_size, matrix_size)).astype(np.float32)
                large_data = np.random.random(50000).astype(np.float32)

                # æ·»åŠ å¼±å¼•ç”¨
                weak_ref_matrix = weakref.ref(large_matrix)
                weak_ref_data = weakref.ref(large_data)
                weak_references.append(weak_ref_matrix)
                weak_references.append(weak_ref_data)

                # æ‰§è¡ŒGPUæ“ä½œ
                from src.gpu.core.kernels.standardized_interface import (
                    MatrixOperationType,
                    MatrixConfig,
                )

                config = MatrixConfig(operation_type=MatrixOperationType.MULTIPLY)
                result = await matrix_kernel.execute_matrix_operation(large_matrix, large_matrix.T, config)

                # å†…å­˜æ± æ“ä½œ
                block_sizes = [1024, 4096, 16384]
                allocated_blocks = []
                for size in block_sizes:
                    block_id = await memory_pool.allocate(size)
                    if block_id:
                        allocated_blocks.append(block_id)

                # é‡Šæ”¾å†…å­˜æ± 
                for block_id in allocated_blocks:
                    await memory_pool.deallocate(block_id)

                # è®°å½•å†…å­˜ä½¿ç”¨
                current_memory = self.get_current_memory_usage()
                memory_samples.append(current_memory)

                # åˆ é™¤å¤§å‹å¯¹è±¡å¼•ç”¨
                del large_matrix, large_data

                # å®šæœŸåƒåœ¾å›æ”¶
                if i % 20 == 0:
                    gc.collect()

                # æ£€æŸ¥å¼±å¼•ç”¨
                if i % 50 == 0:
                    still_alive_matrix = sum(1 for ref in weak_references[::2] if ref() is not None)
                    still_alive_data = sum(1 for ref in weak_references[1::2] if ref() is not None)
                    print(f"         ç¬¬ {i} æ¬¡è¿­ä»£: å­˜æ´»å¯¹è±¡ - çŸ©é˜µ: {still_alive_matrix}, æ•°æ®: {still_alive_data}")

            # æœ€ç»ˆåƒåœ¾å›æ”¶
            gc.collect()
            final_memory = self.get_current_memory_usage()
            memory_samples.append(final_memory)

            # åˆ†æå†…å­˜ä½¿ç”¨æ¨¡å¼
            memory_growth = final_memory - baseline_memory
            memory_variance = np.var(memory_samples) if memory_samples else 0

            # æ£€æŸ¥å¯¹è±¡æ¸…ç†
            alive_matrix_objects = sum(1 for ref in weak_references[::2] if ref() is not None)
            alive_data_objects = sum(1 for ref in weak_references[1::2] if ref() is not None)
            total_created_objects = len(weak_references)
            cleanup_rate = (total_created_objects - alive_matrix_objects - alive_data_objects) / total_created_objects

            return {
                "success": memory_growth < 100 and cleanup_rate > 0.9,  # å†…å­˜å¢é•¿å°äº100MBä¸”æ¸…ç†ç‡>90%
                "iterations": iterations,
                "memory_analysis": {
                    "baseline_memory_mb": baseline_memory,
                    "final_memory_mb": final_memory,
                    "memory_growth_mb": memory_growth,
                    "memory_variance_mb2": memory_variance,
                    "max_memory_mb": max(memory_samples),
                    "memory_stability": memory_growth < 50,  # 50MBé˜ˆå€¼
                },
                "object_cleanup_analysis": {
                    "total_created_objects": total_created_objects,
                    "alive_matrix_objects": alive_matrix_objects,
                    "alive_data_objects": alive_data_objects,
                    "cleanup_rate": cleanup_rate,
                    "cleanup_successful": cleanup_rate > 0.9,
                },
                "memory_pool_stats": memory_pool.get_stats(),
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    async def test_resource_cleanup(self) -> Dict[str, Any]:
        """æµ‹è¯•èµ„æºæ¸…ç†"""
        try:
            from src.gpu.core.kernels import MatrixKernelEngine, TransformKernelEngine
            from src.gpu.core.hardware_abstraction.memory_pool import get_memory_pool

            cleanup_results = []

            # æµ‹è¯•1: å†…æ ¸å¼•æ“èµ„æºæ¸…ç†
            for i in range(10):
                kernel = MatrixKernelEngine()
                await kernel.initialize()

                # æ‰§è¡Œä¸€äº›æ“ä½œ
                matrix = np.random.random((256, 256)).astype(np.float32)
                from src.gpu.core.kernels.standardized_interface import (
                    MatrixOperationType,
                    MatrixConfig,
                )

                config = MatrixConfig(operation_type=MatrixOperationType.MULTIPLY)
                result = await kernel.execute_matrix_operation(matrix, matrix, config)

                # æ¸…ç†èµ„æº
                kernel_cleanup_success = hasattr(kernel, "cleanup") and callable(getattr(kernel, "cleanup"))
                if kernel_cleanup_success:
                    await kernel.cleanup()

                # å¼±å¼•ç”¨æ£€æŸ¥
                weak_ref = weakref.ref(kernel)
                del kernel
                gc.collect()

                cleanup_results.append(
                    {
                        "test": f"kernel_cleanup_{i}",
                        "operation_successful": result.success,
                        "cleanup_available": kernel_cleanup_success,
                        "object_collected": weak_ref() is None,
                    }
                )

            # æµ‹è¯•2: å†…å­˜æ± èµ„æºæ¸…ç†
            memory_pool = get_memory_pool()
            await memory_pool.initialize()

            # åˆ†é…å¤§é‡å†…å­˜å—
            allocated_blocks = []
            for i in range(100):
                block_id = await memory_pool.allocate(np.random.randint(1024, 65536))
                if block_id:
                    allocated_blocks.append(block_id)

            # é‡Šæ”¾éƒ¨åˆ†å†…å­˜å—
            for block_id in allocated_blocks[:50]:
                await memory_pool.deallocate(block_id)

            # è·å–æ¸…ç†å‰ç»Ÿè®¡
            before_stats = memory_pool.get_stats()

            # æ‰§è¡Œå®Œæ•´æ¸…ç†
            if hasattr(memory_pool, "cleanup") and callable(getattr(memory_pool, "cleanup")):
                await memory_pool.cleanup()
                cleanup_available = True
            else:
                cleanup_available = False

            # è·å–æ¸…ç†åç»Ÿè®¡
            after_stats = memory_pool.get_stats()

            cleanup_results.append(
                {
                    "test": "memory_pool_cleanup",
                    "blocks_allocated": len(allocated_blocks),
                    "blocks_deallocated": 50,
                    "cleanup_available": cleanup_available,
                    "before_stats": before_stats,
                    "after_stats": after_stats,
                }
            )

            # æµ‹è¯•3: å¼‚å¸¸æƒ…å†µä¸‹çš„èµ„æºæ¸…ç†
            for i in range(5):
                try:
                    kernel = TransformKernelEngine()
                    await kernel.initialize()

                    # æ•…æ„è§¦å‘é”™è¯¯
                    invalid_data = np.array([])
                    from src.gpu.core.kernels.standardized_interface import (
                        TransformOperationType,
                        TransformConfig,
                    )

                    config = TransformConfig(operation_type=TransformOperationType.NORMALIZE)
                    result = await kernel.execute_transform_operation(invalid_data, config)

                    # æ¸…ç†
                    if hasattr(kernel, "cleanup") and callable(getattr(kernel, "cleanup")):
                        await kernel.cleanup()

                    cleanup_results.append(
                        {
                            "test": f"exception_cleanup_{i}",
                            "error_triggered": not result.success,
                            "cleanup_successful": True,
                        }
                    )

                except Exception as e:
                    cleanup_results.append(
                        {
                            "test": f"exception_cleanup_{i}",
                            "error_triggered": True,
                            "cleanup_successful": True,  # å¼‚å¸¸åä»èƒ½æ¸…ç†
                            "exception_message": str(e),
                        }
                    )

            successful_cleanups = sum(1 for r in cleanup_results if r.get("cleanup_successful", False))
            total_cleanup_tests = len(cleanup_results)

            return {
                "success": successful_cleanups >= total_cleanup_tests * 0.8,
                "total_cleanup_tests": total_cleanup_tests,
                "successful_cleanups": successful_cleanups,
                "cleanup_success_rate": successful_cleanups / total_cleanup_tests,
                "detailed_results": cleanup_results,
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    async def test_concurrent_stress_stability(self) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶å‘å‹åŠ›ç¨³å®šæ€§"""
        try:
            from src.gpu.core.kernels import MatrixKernelEngine

            # å¹¶å‘å‹åŠ›æµ‹è¯•å‚æ•°
            concurrent_tasks = 20
            operations_per_task = 50
            matrix_size = 256

            async def concurrent_stress_worker(worker_id: int):
                """å¹¶å‘å·¥ä½œå™¨"""
                kernel = MatrixKernelEngine()
                await kernel.initialize()

                operation_results = []
                memory_usage_samples = []

                for op_id in range(operations_per_task):
                    # è®°å½•å†…å­˜ä½¿ç”¨
                    memory_mb = self.get_current_memory_usage()
                    memory_usage_samples.append(memory_mb)

                    # æ‰§è¡ŒçŸ©é˜µæ“ä½œ
                    matrix_a = np.random.random((matrix_size, matrix_size)).astype(np.float32)
                    matrix_b = np.random.random((matrix_size, matrix_size)).astype(np.float32)

                    from src.gpu.core.kernels.standardized_interface import (
                        MatrixOperationType,
                        MatrixConfig,
                    )

                    config = MatrixConfig(operation_type=MatrixOperationType.MULTIPLY)

                    try:
                        start_time = time.time()
                        result = await kernel.execute_matrix_operation(matrix_a, matrix_b, config)
                        execution_time = time.time() - start_time

                        operation_results.append(
                            {
                                "operation_id": op_id,
                                "success": result.success,
                                "execution_time": execution_time,
                                "memory_usage": memory_mb,
                            }
                        )
                    except Exception as e:
                        operation_results.append(
                            {
                                "operation_id": op_id,
                                "success": False,
                                "error": str(e),
                                "memory_usage": memory_mb,
                            }
                        )

                    # æ¸…ç†å¼•ç”¨
                    del matrix_a, matrix_b

                return {
                    "worker_id": worker_id,
                    "total_operations": operations_per_task,
                    "successful_operations": sum(1 for r in operation_results if r.get("success", False)),
                    "operation_results": operation_results,
                    "memory_samples": memory_usage_samples,
                    "max_memory_mb": max(memory_usage_samples) if memory_usage_samples else 0,
                    "min_memory_mb": min(memory_usage_samples) if memory_usage_samples else 0,
                }

            print(f"      ğŸ”„ å¯åŠ¨ {concurrent_tasks} ä¸ªå¹¶å‘ä»»åŠ¡...")

            # å¯åŠ¨å¹¶å‘ä»»åŠ¡
            start_time = time.time()
            concurrent_tasks_list = [concurrent_stress_worker(i) for i in range(concurrent_tasks)]

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            worker_results = await asyncio.gather(*concurrent_tasks_list, return_exceptions=True)
            total_execution_time = time.time() - start_time

            # åˆ†æç»“æœ
            valid_results = [r for r in worker_results if isinstance(r, dict)]
            total_operations = sum(r.get("total_operations", 0) for r in valid_results)
            successful_operations = sum(r.get("successful_operations", 0) for r in valid_results)

            # å†…å­˜ç¨³å®šæ€§åˆ†æ
            all_memory_samples = []
            for result in valid_results:
                all_memory_samples.extend(result.get("memory_samples", []))

            if all_memory_samples:
                memory_stability = {
                    "avg_memory_mb": np.mean(all_memory_samples),
                    "max_memory_mb": max(all_memory_samples),
                    "min_memory_mb": min(all_memory_samples),
                    "memory_variance": np.var(all_memory_samples),
                    "memory_stable": np.var(all_memory_samples) < 100,  # æ–¹å·®é˜ˆå€¼
                }
            else:
                memory_stability = {}

            return {
                "success": successful_operations >= total_operations * 0.9 and len(valid_results) == concurrent_tasks,
                "concurrent_workers": concurrent_tasks,
                "total_execution_time": total_execution_time,
                "total_operations": total_operations,
                "successful_operations": successful_operations,
                "success_rate": successful_operations / total_operations if total_operations > 0 else 0,
                "operations_per_second": total_operations / total_execution_time,
                "memory_stability": memory_stability,
                "worker_results_summary": {
                    "completed_workers": len(valid_results),
                    "failed_workers": len(worker_results) - len(valid_results),
                    "avg_operations_per_worker": total_operations / len(valid_results) if valid_results else 0,
                },
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    async def test_exception_recovery_stability(self) -> Dict[str, Any]:
        """æµ‹è¯•å¼‚å¸¸æ¢å¤ç¨³å®šæ€§"""
        try:
            from src.gpu.core.kernels import MatrixKernelEngine, TransformKernelEngine

            recovery_tests = []

            # æµ‹è¯•1: æ— æ•ˆè¾“å…¥æ¢å¤
            for i in range(10):
                kernel = MatrixKernelEngine()
                await kernel.initialize()

                # æµ‹è¯•å„ç§æ— æ•ˆè¾“å…¥
                invalid_inputs = [
                    np.array([]),  # ç©ºæ•°ç»„
                    np.random.random((0, 100)),  # é›¶ç»´åº¦
                    None,  # Noneè¾“å…¥
                    "invalid_string",  # å­—ç¬¦ä¸²è¾“å…¥
                ]

                recovery_success = 0
                for invalid_input in invalid_inputs:
                    try:
                        from src.gpu.core.kernels.standardized_interface import (
                            MatrixOperationType,
                            MatrixConfig,
                        )

                        config = MatrixConfig(operation_type=MatrixOperationType.MULTIPLY)

                        if invalid_input is not None:
                            result = await kernel.execute_matrix_operation(invalid_input, invalid_input, config)
                            if not result.success:
                                recovery_success += 1
                        else:
                            recovery_success += 1  # æ­£ç¡®å¤„ç†None
                    except Exception:
                        recovery_success += 1  # å¼‚å¸¸è¢«æ­£ç¡®æ•è·

                recovery_tests.append(
                    {
                        "test": f"invalid_input_recovery_{i}",
                        "recovery_rate": recovery_success / len(invalid_inputs),
                        "successful_recoveries": recovery_success,
                        "total_tests": len(invalid_inputs),
                    }
                )

            # æµ‹è¯•2: å†…å­˜å‹åŠ›ä¸‹çš„å¼‚å¸¸æ¢å¤
            for i in range(5):
                kernel = TransformKernelEngine()
                await kernel.initialize()

                try:
                    # å°è¯•å¤„ç†è¶…å¤§æ•°ç»„
                    huge_array = np.random.random(1000000).astype(np.float32)  # 1Må…ƒç´ 

                    from src.gpu.core.kernels.standardized_interface import (
                        TransformOperationType,
                        TransformConfig,
                    )

                    config = TransformConfig(operation_type=TransformOperationType.NORMALIZE)

                    start_time = time.time()
                    result = await kernel.execute_transform_operation(huge_array, config)
                    execution_time = time.time() - start_time

                    recovery_tests.append(
                        {
                            "test": f"memory_pressure_recovery_{i}",
                            "handled_large_data": result.success,
                            "execution_time": execution_time,
                            "data_size": len(huge_array),
                        }
                    )

                except Exception as e:
                    recovery_tests.append(
                        {
                            "test": f"memory_pressure_recovery_{i}",
                            "handled_large_data": False,
                            "exception_handled": True,
                            "error_message": str(e)[:100],  # æˆªæ–­é”™è¯¯ä¿¡æ¯
                        }
                    )

            # æµ‹è¯•3: å¿«é€Ÿè¿ç»­æ“ä½œçš„ç¨³å®šæ€§
            kernel = MatrixKernelEngine()
            await kernel.initialize()

            rapid_operations = []
            for i in range(100):
                try:
                    matrix = np.random.random((128, 128)).astype(np.float32)
                    from src.gpu.core.kernels.standardized_interface import (
                        MatrixOperationType,
                        MatrixConfig,
                    )

                    config = MatrixConfig(operation_type=MatrixOperationType.MULTIPLY)

                    result = await kernel.execute_matrix_operation(matrix, matrix, config)
                    rapid_operations.append(result.success)
                except Exception:
                    rapid_operations.append(False)

            rapid_success_rate = sum(rapid_operations) / len(rapid_operations)
            recovery_tests.append(
                {
                    "test": "rapid_operations_stability",
                    "total_operations": len(rapid_operations),
                    "successful_operations": sum(rapid_operations),
                    "success_rate": rapid_success_rate,
                    "stable_performance": rapid_success_rate > 0.95,
                }
            )

            # åˆ†ææ¢å¤èƒ½åŠ›
            total_recovery_tests = len(recovery_tests)
            successful_recoveries = sum(
                1
                for test in recovery_tests
                if test.get("recovery_rate", 0) > 0.8
                or test.get("success_rate", 0) > 0.8
                or test.get("handled_large_data", False)
                or test.get("stable_performance", False)
            )

            return {
                "success": successful_recoveries >= total_recovery_tests * 0.8,
                "total_recovery_tests": total_recovery_tests,
                "successful_recoveries": successful_recoveries,
                "recovery_success_rate": successful_recoveries / total_recovery_tests,
                "detailed_results": recovery_tests,
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    async def test_memory_pool_stability(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜æ± é•¿æœŸç¨³å®šæ€§"""
        try:
            from src.gpu.core.hardware_abstraction.memory_pool import get_memory_pool

            memory_pool = get_memory_pool()
            await memory_pool.initialize()

            # é•¿æœŸå†…å­˜æ± æµ‹è¯•å‚æ•°
            duration_seconds = 60  # 1åˆ†é’Ÿ
            allocation_cycles = 1000
            allocation_sizes = [512, 1024, 2048, 4096, 8192, 16384, 32768]

            allocation_results = []
            deallocation_results = []
            pool_stats_history = []

            start_time = time.time()
            cycle = 0

            print(f"      ğŸ’¾ æ‰§è¡Œ {allocation_cycles} æ¬¡å†…å­˜åˆ†é…å¾ªç¯...")

            while cycle < allocation_cycles and (time.time() - start_time) < duration_seconds:
                # éšæœºåˆ†é…
                size = np.random.choice(allocation_sizes)
                alloc_start = time.time()
                block_id = await memory_pool.allocate(size)
                alloc_time = time.time() - alloc_start

                if block_id:
                    allocation_results.append(
                        {
                            "cycle": cycle,
                            "size": size,
                            "allocation_time": alloc_time,
                            "success": True,
                            "block_id": block_id,
                        }
                    )

                    # éšæœºå†³å®šç«‹å³é‡Šæ”¾è¿˜æ˜¯å»¶è¿Ÿé‡Šæ”¾
                    if np.random.random() > 0.5:
                        # ç«‹å³é‡Šæ”¾
                        dealloc_start = time.time()
                        success = await memory_pool.deallocate(block_id)
                        dealloc_time = time.time() - dealloc_start

                        deallocation_results.append(
                            {
                                "cycle": cycle,
                                "block_id": block_id,
                                "deallocation_time": dealloc_time,
                                "success": success,
                            }
                        )
                else:
                    allocation_results.append(
                        {
                            "cycle": cycle,
                            "size": size,
                            "allocation_time": alloc_time,
                            "success": False,
                            "block_id": None,
                        }
                    )

                # å®šæœŸè®°å½•ç»Ÿè®¡ä¿¡æ¯
                if cycle % 100 == 0:
                    stats = memory_pool.get_stats()
                    pool_stats_history.append(
                        {
                            "cycle": cycle,
                            "timestamp": time.time() - start_time,
                            "stats": stats.copy(),
                        }
                    )
                    print(f"         ç¬¬ {cycle} æ¬¡å¾ªç¯: æ± æ•ˆç‡={stats.get('pool_efficiency', 0):.1%}")

                cycle += 1

            # æœ€ç»ˆæ¸…ç† - é‡Šæ”¾æ‰€æœ‰å‰©ä½™å—
            final_stats = memory_pool.get_stats()
            if hasattr(memory_pool, "cleanup") and callable(getattr(memory_pool, "cleanup")):
                await memory_pool.cleanup()

            # åˆ†æç»“æœ
            successful_allocations = sum(1 for r in allocation_results if r["success"])
            successful_deallocations = sum(1 for r in deallocation_results if r["success"])

            allocation_times = [r["allocation_time"] for r in allocation_results if r["success"]]
            deallocation_times = [r["deallocation_time"] for r in deallocation_results if r["success"]]

            # å†…å­˜æ± ç¨³å®šæ€§åˆ†æ
            if pool_stats_history:
                pool_efficiency_history = [s["stats"].get("pool_efficiency", 0) for s in pool_stats_history]
                pool_stability = {
                    "avg_efficiency": np.mean(pool_efficiency_history),
                    "min_efficiency": min(pool_efficiency_history),
                    "max_efficiency": max(pool_efficiency_history),
                    "efficiency_stable": np.std(pool_efficiency_history) < 0.1,  # 10%æ ‡å‡†å·®é˜ˆå€¼
                }
            else:
                pool_stability = {}

            return {
                "success": successful_allocations >= allocation_cycles * 0.95
                and successful_deallocations >= len(deallocation_results) * 0.95,
                "allocation_cycles": cycle,
                "allocation_analysis": {
                    "total_allocations": len(allocation_results),
                    "successful_allocations": successful_allocations,
                    "allocation_success_rate": successful_allocations / len(allocation_results),
                    "avg_allocation_time": np.mean(allocation_times) if allocation_times else 0,
                    "min_allocation_time": min(allocation_times) if allocation_times else 0,
                    "max_allocation_time": max(allocation_times) if allocation_times else 0,
                },
                "deallocation_analysis": {
                    "total_deallocations": len(deallocation_results),
                    "successful_deallocations": successful_deallocations,
                    "deallocation_success_rate": (
                        successful_deallocations / len(deallocation_results) if deallocation_results else 1
                    ),
                    "avg_deallocation_time": np.mean(deallocation_times) if deallocation_times else 0,
                    "min_deallocation_time": min(deallocation_times) if deallocation_times else 0,
                    "max_deallocation_time": max(deallocation_times) if deallocation_times else 0,
                },
                "pool_stability": pool_stability,
                "final_pool_stats": final_stats,
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def get_current_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        try:
            return self.process.memory_info().rss / (1024 * 1024)
        except:
            return 0

    def generate_stability_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç¨³å®šæ€§æµ‹è¯•æŠ¥å‘Š"""
        total_suites = len(self.test_results)
        successful_suites = sum(1 for r in self.test_results.values() if r.get("success", False))

        # è®¡ç®—æ€»ä½“å†…å­˜ä½¿ç”¨
        memory_usages = [r.get("memory_usage_mb", 0) for r in self.test_results.values()]
        max_memory_usage = max(memory_usages) if memory_usages else 0
        total_memory_growth = max_memory_usage - (self.initial_memory / (1024 * 1024)) if self.initial_memory else 0

        return {
            "stability_test_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "stability_phase": "Phase 6.4.4",
            "total_test_suites": total_suites,
            "successful_test_suites": successful_suites,
            "failed_test_suites": total_suites - successful_suites,
            "stability_success_rate": (successful_suites / total_suites * 100) if total_suites > 0 else 0,
            "detailed_results": self.test_results,
            "memory_analysis": {
                "initial_memory_mb": self.initial_memory / (1024 * 1024) if self.initial_memory else 0,
                "max_memory_usage_mb": max_memory_usage,
                "total_memory_growth_mb": total_memory_growth,
                "memory_leak_detected": total_memory_growth > 200,  # 200MBé˜ˆå€¼
            },
            "summary": {
                "long_running_stable": self.test_results.get("é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§æµ‹è¯•", {}).get("success", False),
                "memory_leak_free": not self.test_results.get("å†…å­˜æ³„æ¼æ£€æµ‹æµ‹è¯•", {})
                .get("details", {})
                .get("memory_analysis", {})
                .get("memory_stability", True),
                "resource_cleanup_working": self.test_results.get("èµ„æºæ¸…ç†éªŒè¯æµ‹è¯•", {}).get("success", False),
                "concurrent_stress_stable": self.test_results.get("å¹¶å‘å‹åŠ›ç¨³å®šæ€§æµ‹è¯•", {}).get("success", False),
                "exception_recovery_working": self.test_results.get("å¼‚å¸¸æ¢å¤ç¨³å®šæ€§æµ‹è¯•", {}).get("success", False),
                "memory_pool_stable": self.test_results.get("å†…å­˜æ± é•¿æœŸç¨³å®šæ€§æµ‹è¯•", {}).get("success", False),
                "overall_stability_achieved": successful_suites >= total_suites * 0.8,
            },
        }

    def print_summary(self, report: Dict[str, Any]):
        """æ‰“å°ç¨³å®šæ€§æµ‹è¯•æ‘˜è¦"""
        print("\n" + "=" * 80)
        print("ğŸ“Š GPUåŠ é€Ÿå¼•æ“é•¿æœŸç¨³å®šæ€§æµ‹è¯•æŠ¥å‘Š")
        print("=" * 80)

        summary = report["summary"]
        memory_analysis = report["memory_analysis"]

        print(
            f"ğŸ“ˆ ç¨³å®šæ€§æµ‹è¯•æˆåŠŸç‡: {report['stability_success_rate']:.1f}% ({report['successful_test_suites']}/{report['total_test_suites']})"
        )
        print(f"ğŸ•’ æµ‹è¯•æ—¶é—´: {report['stability_test_timestamp']}")
        print(f"ğŸ’¾ å†…å­˜å¢é•¿: {memory_analysis['total_memory_growth_mb']:.1f}MB")

        print("\nğŸ”§ ç¨³å®šæ€§æŒ‡æ ‡:")
        print(f"   âœ… é•¿æœŸè¿è¡Œç¨³å®š: {'æ˜¯' if summary['long_running_stable'] else 'å¦'}")
        print(f"   âœ… æ— å†…å­˜æ³„æ¼: {'æ˜¯' if summary['memory_leak_free'] else 'å¦'}")
        print(f"   âœ… èµ„æºæ¸…ç†æ­£å¸¸: {'æ˜¯' if summary['resource_cleanup_working'] else 'å¦'}")
        print(f"   âœ… å¹¶å‘å‹åŠ›ç¨³å®š: {'æ˜¯' if summary['concurrent_stress_stable'] else 'å¦'}")
        print(f"   âœ… å¼‚å¸¸æ¢å¤æ­£å¸¸: {'æ˜¯' if summary['exception_recovery_working'] else 'å¦'}")
        print(f"   âœ… å†…å­˜æ± ç¨³å®š: {'æ˜¯' if summary['memory_pool_stable'] else 'å¦'}")
        print(f"ğŸš€ æ•´ä½“ç¨³å®šæ€§è¾¾æˆ: {'æ˜¯' if summary['overall_stability_achieved'] else 'å¦'}")

        print("\nğŸ“‹ è¯¦ç»†ç»“æœ:")
        for suite_name, result in report["detailed_results"].items():
            status = "âœ…" if result.get("success", False) else "âŒ"
            execution_time = result.get("execution_time", 0)
            memory_usage = result.get("memory_usage_mb", 0)
            print(f"   {status} {suite_name} ({execution_time:.2f}s, {memory_usage:.1f}MB)")

        # æ˜¾ç¤ºå…³é”®ç¨³å®šæ€§æ•°æ®
        if "é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§æµ‹è¯•" in report["detailed_results"]:
            long_run_result = report["detailed_results"]["é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§æµ‹è¯•"]["details"]
            if long_run_result.get("success", False):
                print("\nâ° é•¿æœŸè¿è¡Œæ‘˜è¦:")
                print(f"   â€¢ è¿è¡Œæ—¶é•¿: {long_run_result.get('stability_duration_minutes', 0)} åˆ†é’Ÿ")
                print(f"   â€¢ æ“ä½œæˆåŠŸç‡: {long_run_result.get('success_rate', 0) * 100:.1f}%")
                print(
                    f"   â€¢ å†…å­˜å¢é•¿ç‡: {long_run_result.get('memory_analysis', {}).get('memory_growth_rate_mb_per_sec', 0):.3f} MB/s"
                )

        if "å†…å­˜æ³„æ¼æ£€æµ‹æµ‹è¯•" in report["detailed_results"]:
            leak_result = report["detailed_results"]["å†…å­˜æ³„æ¼æ£€æµ‹æµ‹è¯•"]["details"]
            if leak_result.get("success", False):
                print("\nğŸ” å†…å­˜æ³„æ¼æ‘˜è¦:")
                print(f"   â€¢ å†…å­˜å¢é•¿: {leak_result.get('memory_analysis', {}).get('memory_growth_mb', 0):.1f}MB")
                print(
                    f"   â€¢ å¯¹è±¡æ¸…ç†ç‡: {leak_result.get('object_cleanup_analysis', {}).get('cleanup_rate', 0) * 100:.1f}%"
                )

        if "å¹¶å‘å‹åŠ›ç¨³å®šæ€§æµ‹è¯•" in report["detailed_results"]:
            concurrent_result = report["detailed_results"]["å¹¶å‘å‹åŠ›ç¨³å®šæ€§æµ‹è¯•"]["details"]
            if concurrent_result.get("success", False):
                print("\nğŸ”„ å¹¶å‘å‹åŠ›æ‘˜è¦:")
                print(f"   â€¢ å¹¶å‘ä»»åŠ¡æ•°: {concurrent_result.get('concurrent_workers', 0)}")
                print(f"   â€¢ æˆåŠŸç‡: {concurrent_result.get('success_rate', 0) * 100:.1f}%")
                print(f"   â€¢ æ“ä½œååé‡: {concurrent_result.get('operations_per_second', 0):.1f} ops/s")

        print("\n" + "=" * 80)


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Phase 6.4.4 GPUåŠ é€Ÿå¼•æ“é•¿æœŸç¨³å®šæ€§æµ‹è¯•")
    print("=" * 80)

    tester = LongTermStabilityTester()

    # è¿è¡Œç¨³å®šæ€§æµ‹è¯•
    report = await tester.run_comprehensive_stability_tests()

    # æ‰“å°æ‘˜è¦
    tester.print_summary(report)

    return report


if __name__ == "__main__":
    report = asyncio.run(main())
