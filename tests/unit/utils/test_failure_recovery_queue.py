"""
Failure Recovery Queue Test Suite
æ•…éšœæ¢å¤é˜Ÿåˆ—æµ‹è¯•å¥—ä»¶

åˆ›å»ºæ—¥æœŸ: 2025-12-20
ç‰ˆæœ¬: 1.0.0
æµ‹è¯•æ¨¡å—: src.utils.failure_recovery_queue (109è¡Œ)
"""

import json
import os
import sqlite3
import tempfile
from unittest.mock import patch

import pytest

from src.utils.failure_recovery_queue import FailureRecoveryQueue


class TestFailureRecoveryQueue:
    """å¤±è´¥æ¢å¤é˜Ÿåˆ—æµ‹è¯•"""

    @pytest.fixture
    def temp_db_path(self):
        """åˆ›å»ºä¸´æ—¶æ•°æ®åº“è·¯å¾„"""
        temp_dir = tempfile.mkdtemp()
        db_path = os.path.join(temp_dir, "test_queue.db")
        yield db_path
        # æ¸…ç†
        if os.path.exists(db_path):
            os.remove(db_path)
        os.rmdir(temp_dir)

    @pytest.fixture
    def queue(self, temp_db_path):
        """åˆ›å»ºé˜Ÿåˆ—å®ä¾‹"""
        return FailureRecoveryQueue(db_path=temp_db_path)

    def test_init_default_path(self):
        """æµ‹è¯•é»˜è®¤è·¯å¾„åˆå§‹åŒ–"""
        with patch("os.makedirs") as mock_makedirs:
            queue = FailureRecoveryQueue()
            expected_path = "/tmp/mystocks_recovery_queue.db"
            assert queue.db_path == expected_path
            mock_makedirs.assert_called_once_with("/tmp", exist_ok=True)

    def test_init_custom_path(self, temp_db_path):
        """æµ‹è¯•è‡ªå®šä¹‰è·¯å¾„åˆå§‹åŒ–"""
        with patch("os.makedirs") as mock_makedirs:
            queue = FailureRecoveryQueue(db_path=temp_db_path)
            assert queue.db_path == temp_db_path
            mock_makedirs.assert_called_once_with(os.path.dirname(temp_db_path), exist_ok=True)

    def test_init_db_table_creation(self, temp_db_path):
        """æµ‹è¯•æ•°æ®åº“è¡¨åˆå§‹åŒ–"""
        queue = FailureRecoveryQueue(db_path=temp_db_path)

        # éªŒè¯è¡¨æ˜¯å¦åˆ›å»º
        conn = sqlite3.connect(temp_db_path)
        cursor = conn.cursor()

        cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = cursor.fetchall()
        table_names = [table[0] for table in tables]

        # åº”è¯¥åŒ…å«outbox_queueè¡¨ï¼Œå¯èƒ½è¿˜æœ‰sqlite_sequenceè¡¨ï¼ˆSQLiteç³»ç»Ÿè¡¨ï¼‰
        assert "outbox_queue" in table_names

        # éªŒè¯è¡¨ç»“æ„
        cursor.execute("PRAGMA table_info(outbox_queue)")
        columns = cursor.fetchall()
        column_names = [col[1] for col in columns]

        expected_columns = [
            "id",
            "classification",
            "target_database",
            "data_json",
            "created_at",
            "retry_count",
            "status",
        ]

        for col in expected_columns:
            assert col in column_names

        conn.close()

    def test_enqueue_basic(self, queue):
        """æµ‹è¯•åŸºæœ¬å…¥é˜Ÿæ“ä½œ"""
        test_data = {"symbol": "600000", "price": 10.5, "volume": 1000}

        queue.enqueue("market_data", "postgresql", test_data)

        # éªŒè¯æ•°æ®æ˜¯å¦æ­£ç¡®æ’å…¥
        conn = sqlite3.connect(queue.db_path)
        cursor = conn.cursor()

        cursor.execute("SELECT classification, target_database, data_json FROM outbox_queue")
        result = cursor.fetchone()

        assert result is not None
        assert result[0] == "market_data"
        assert result[1] == "postgresql"

        # éªŒè¯JSONæ•°æ®
        stored_data = json.loads(result[2])
        assert stored_data == test_data

        conn.close()

    def test_enqueue_multiple_items(self, queue):
        """æµ‹è¯•å¤šä¸ªé¡¹ç›®å…¥é˜Ÿ"""
        items = [
            ("market_data", "postgresql", {"symbol": "600000", "price": 10.5}),
            ("reference_data", "tdengine", {"exchange": "SH", "name": "å¹³å®‰é“¶è¡Œ"}),
            ("derived_data", "postgresql", {"ma5": 10.2, "ma20": 11.3}),
        ]

        for classification, target_db, data in items:
            queue.enqueue(classification, target_db, data)

        # éªŒè¯æ‰€æœ‰æ•°æ®éƒ½æ’å…¥æˆåŠŸ
        conn = sqlite3.connect(queue.db_path)
        cursor = conn.cursor()

        cursor.execute("SELECT COUNT(*) FROM outbox_queue")
        count = cursor.fetchone()[0]

        assert count == 3

        # éªŒè¯æ•°æ®é¡ºåºå’Œå†…å®¹
        cursor.execute("SELECT classification, target_database, data_json FROM outbox_queue ORDER BY created_at")
        results = cursor.fetchall()

        for i, (classification, target_db, data) in enumerate(items):
            assert results[i][0] == classification
            assert results[i][1] == target_db
            stored_data = json.loads(results[i][2])
            assert stored_data == data

        conn.close()

    def test_get_pending_items_empty(self, queue):
        """æµ‹è¯•è·å–ç©ºçš„å¾…å¤„ç†é˜Ÿåˆ—"""
        items = queue.get_pending_items()
        assert items == []

    def test_get_pending_items_with_data(self, queue):
        """æµ‹è¯•è·å–æœ‰æ•°æ®çš„å¾…å¤„ç†é˜Ÿåˆ—"""
        # å…ˆæ·»åŠ ä¸€äº›æ•°æ®
        test_items = [
            ("market_data", "postgresql", {"symbol": "600000"}),
            ("reference_data", "tdengine", {"exchange": "SH"}),
            ("derived_data", "postgresql", {"indicator": "MA"}),
        ]

        for classification, target_db, data in test_items:
            queue.enqueue(classification, target_db, data)

        # è·å–å¾…å¤„ç†é¡¹ç›®
        items = queue.get_pending_items()

        assert len(items) == 3

        # éªŒè¯è¿”å›çš„æ•°æ®æ ¼å¼
        for i, (id, classification, target_db, data_json) in enumerate(items):
            assert isinstance(id, int)
            assert classification == test_items[i][0]
            assert target_db == test_items[i][1]

            # éªŒè¯JSONå¯ä»¥æ­£ç¡®è§£æ
            data = json.loads(data_json)
            assert data == test_items[i][2]

    def test_get_pending_items_with_limit(self, queue):
        """æµ‹è¯•å¸¦é™åˆ¶çš„è·å–å¾…å¤„ç†é¡¹ç›®"""
        # æ·»åŠ 5ä¸ªé¡¹ç›®
        for i in range(5):
            queue.enqueue("market_data", "postgresql", {"symbol": f"60000{i}"})

        # é™åˆ¶è·å–3ä¸ª
        items = queue.get_pending_items(limit=3)
        assert len(items) == 3

        # è·å–æ‰€æœ‰é¡¹ç›®
        all_items = queue.get_pending_items(limit=100)
        assert len(all_items) == 5

    def test_get_pending_items_ordering(self, queue):
        """æµ‹è¯•å¾…å¤„ç†é¡¹ç›®çš„æ’åºï¼ˆæŒ‰åˆ›å»ºæ—¶é—´ï¼‰"""
        # æŒ‰ç‰¹å®šé¡ºåºæ·»åŠ é¡¹ç›®
        items = [
            (
                "market_data",
                "postgresql",
                {"symbol": "600000", "timestamp": "2025-01-01T09:00:00"},
            ),
            ("reference_data", "tdengine", {"exchange": "SH", "name": "å¹³å®‰é“¶è¡Œ"}),
            ("derived_data", "postgresql", {"indicator": "MA", "value": 10.5}),
        ]

        for classification, target_db, data in items:
            queue.enqueue(classification, target_db, data)

        # è·å–é¡¹ç›®å¹¶éªŒè¯é¡ºåº
        retrieved_items = queue.get_pending_items()

        assert len(retrieved_items) == 3

        # éªŒè¯é¡ºåºï¼ˆåº”è¯¥æŒ‰ç…§æ’å…¥é¡ºåºï¼Œå› ä¸ºåˆ›å»ºæ—¶é—´æ˜¯é€’å¢çš„ï¼‰
        for i, item in enumerate(items):
            assert retrieved_items[i][1] == item[0]  # classification
            assert retrieved_items[i][2] == item[1]  # target_database

    def test_enqueue_json_serialization(self, queue):
        """æµ‹è¯•JSONåºåˆ—åŒ–"""
        complex_data = {
            "symbol": "600000",
            "data": {
                "price": [10.1, 10.2, 10.3],
                "volume": {"total": 1000000, "buy": 500000, "sell": 500000},
            },
            "metadata": {
                "source": "akshare",
                "timestamp": "2025-12-20T10:00:00Z",
                "flags": ["real_time", "verified"],
            },
        }

        queue.enqueue("market_data", "postgresql", complex_data)

        # éªŒè¯æ•°æ®å¯ä»¥æ­£ç¡®æ£€ç´¢å’Œååºåˆ—åŒ–
        items = queue.get_pending_items()
        assert len(items) == 1

        retrieved_data = json.loads(items[0][3])
        assert retrieved_data == complex_data

    def test_enqueue_with_none_data(self, queue):
        """æµ‹è¯•åŒ…å«Noneå€¼çš„JSONåºåˆ—åŒ–"""
        data_with_none = {
            "symbol": "600000",
            "price": None,
            "volume": 1000,
            "metadata": None,
        }

        queue.enqueue("market_data", "postgresql", data_with_none)

        items = queue.get_pending_items()
        retrieved_data = json.loads(items[0][3])

        assert retrieved_data["symbol"] == "600000"
        assert retrieved_data["price"] is None
        assert retrieved_data["volume"] == 1000
        assert retrieved_data["metadata"] is None

    def test_database_connection_error(self):
        """æµ‹è¯•æ•°æ®åº“è¿æ¥é”™è¯¯"""
        # ä½¿ç”¨æ— æ•ˆè·¯å¾„
        invalid_path = "/invalid/path/that/does/not/exist/test.db"

        with patch("os.makedirs"):
            with pytest.raises(Exception):
                FailureRecoveryQueue(db_path=invalid_path)

    def test_large_data_serialization(self, queue):
        """æµ‹è¯•å¤§æ•°æ®çš„JSONåºåˆ—åŒ–"""
        # åˆ›å»ºä¸€ä¸ªè¾ƒå¤§çš„æ•°æ®å¯¹è±¡
        large_data = {
            "market_data": [{"symbol": f"60000{i}", "price": i * 0.1, "volume": i * 1000} for i in range(1000)]
        }

        # è¿™åº”è¯¥èƒ½å¤Ÿæ­£å¸¸åºåˆ—åŒ–
        queue.enqueue("market_data", "postgresql", large_data)

        items = queue.get_pending_items()
        assert len(items) == 1

        retrieved_data = json.loads(items[0][3])
        assert len(retrieved_data["market_data"]) == 1000

    def test_concurrent_enqueue(self, queue):
        """æµ‹è¯•å¹¶å‘å…¥é˜Ÿæ“ä½œ"""
        import threading
        import time

        results = []
        errors = []

        def enqueue_worker(worker_id):
            try:
                for i in range(5):  # å‡å°‘å¹¶å‘å‹åŠ›
                    data = {"worker_id": worker_id, "item": i, "timestamp": time.time()}
                    queue.enqueue("test_data", "postgresql", data)
                    results.append((worker_id, i))
                    time.sleep(0.001)  # æ·»åŠ å°å»¶è¿Ÿé¿å…å†²çª
            except Exception as e:
                errors.append((worker_id, str(e)))

        # åˆ›å»ºå¤šä¸ªçº¿ç¨‹åŒæ—¶å…¥é˜Ÿ
        threads = []
        for i in range(2):  # å‡å°‘çº¿ç¨‹æ•°
            thread = threading.Thread(target=enqueue_worker, args=(i,))
            threads.append(thread)
            thread.start()

        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()

        # å…è®¸ä¸€äº›å¹¶å‘å†²çªï¼ˆSQLiteé”é—®é¢˜ï¼‰
        print(f"Concurrent enqueue results: {len(results)} successful, {len(errors)} errors")

        # éªŒè¯è‡³å°‘æœ‰ä¸€äº›æ•°æ®æˆåŠŸå…¥é˜Ÿ
        assert len(results) >= 5, f"Too few successful operations: {len(results)}"
        assert len(errors) < 10, f"Too many errors: {errors}"

        # éªŒè¯æˆåŠŸå…¥é˜Ÿçš„æ•°æ®
        items = queue.get_pending_items(limit=100)
        assert len(items) >= 5

    def test_database_file_persistence(self, temp_db_path):
        """æµ‹è¯•æ•°æ®åº“æ–‡ä»¶æŒä¹…åŒ–"""
        # åˆ›å»ºé˜Ÿåˆ—å¹¶æ·»åŠ æ•°æ®
        queue1 = FailureRecoveryQueue(db_path=temp_db_path)
        test_data = {"symbol": "600000", "price": 10.5}
        queue1.enqueue("market_data", "postgresql", test_data)

        # åˆ›å»ºæ–°çš„é˜Ÿåˆ—å®ä¾‹ï¼ŒéªŒè¯æ•°æ®ä»ç„¶å­˜åœ¨
        queue2 = FailureRecoveryQueue(db_path=temp_db_path)
        items = queue2.get_pending_items()

        assert len(items) == 1
        retrieved_data = json.loads(items[0][3])
        assert retrieved_data == test_data


class TestFailureRecoveryQueueEdgeCases:
    """å¤±è´¥æ¢å¤é˜Ÿåˆ—è¾¹ç•Œæƒ…å†µæµ‹è¯•"""

    @pytest.fixture
    def temp_db_path(self):
        """åˆ›å»ºä¸´æ—¶æ•°æ®åº“è·¯å¾„"""
        temp_dir = tempfile.mkdtemp()
        db_path = os.path.join(temp_dir, "test_edge_cases.db")
        yield db_path
        # æ¸…ç†
        if os.path.exists(db_path):
            os.remove(db_path)
        os.rmdir(temp_dir)

    @pytest.fixture
    def queue(self, temp_db_path):
        """åˆ›å»ºé˜Ÿåˆ—å®ä¾‹"""
        return FailureRecoveryQueue(db_path=temp_db_path)

    def test_enqueue_empty_data(self, queue):
        """æµ‹è¯•ç©ºæ•°æ®å…¥é˜Ÿ"""
        queue.enqueue("test", "postgresql", {})

        items = queue.get_pending_items()
        assert len(items) == 1

        retrieved_data = json.loads(items[0][3])
        assert retrieved_data == {}

    def test_enqueue_large_strings(self, queue):
        """æµ‹è¯•å¤§å­—ç¬¦ä¸²æ•°æ®"""
        large_string = "x" * 10000  # 10KBå­—ç¬¦ä¸²
        data = {"large_field": large_string}

        queue.enqueue("test", "postgresql", data)

        items = queue.get_pending_items()
        retrieved_data = json.loads(items[0][3])
        assert len(retrieved_data["large_field"]) == 10000

    def test_special_characters_in_data(self, queue):
        """æµ‹è¯•åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„æ•°æ®"""
        special_data = {
            "unicode_text": "æµ‹è¯•ä¸­æ–‡ğŸš€ğŸ“ˆ",
            "special_chars": "!@#$%^&*()_+-=[]{}|;':\",./<>?",
            "newlines": "Line 1\nLine 2\nLine 3",
            "tabs": "Column1\tColumn2\tColumn3",
        }

        queue.enqueue("test", "postgresql", special_data)

        items = queue.get_pending_items()
        retrieved_data = json.loads(items[0][3])
        assert retrieved_data == special_data

    def test_numeric_data_types(self, queue):
        """æµ‹è¯•å„ç§æ•°å€¼ç±»å‹"""
        numeric_data = {
            "integer": 42,
            "float": 3.14159,
            "negative_int": -100,
            "zero": 0,
            "large_int": 999999999999,
            "scientific": 1.23e-10,
            "infinity": float("inf"),
        }

        queue.enqueue("test", "postgresql", numeric_data)

        items = queue.get_pending_items()
        retrieved_data = json.loads(items[0][3])

        # éªŒè¯æ•°å€¼ç±»å‹ï¼ˆJSONå¯èƒ½å°†æŸäº›ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼‰
        assert retrieved_data["integer"] == 42
        assert abs(retrieved_data["float"] - 3.14159) < 0.00001
        assert retrieved_data["negative_int"] == -100
        assert retrieved_data["zero"] == 0


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    pytest.main([__file__, "-v"])
