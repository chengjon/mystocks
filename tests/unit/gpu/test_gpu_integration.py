#!/usr/bin/env python3
"""
GPUåŠ é€Ÿå¼•æ“é›†æˆæµ‹è¯•å¥—ä»¶
Phase 6.4.2 - GPUåŠ é€Ÿå¼•æ“é›†æˆæµ‹è¯•

éªŒè¯æ‰€æœ‰GPUç»„ä»¶çš„ååŒå·¥ä½œï¼ŒåŒ…æ‹¬HALã€å†…æ ¸å±‚ã€å†…å­˜æ± å’Œé«˜çº§åŠŸèƒ½çš„å®Œæ•´é›†æˆ
"""

import asyncio
import sys
import time
from pathlib import Path
from typing import Any, Dict

import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


class GPUEngineIntegrationTester:
    """GPUåŠ é€Ÿå¼•æ“é›†æˆæµ‹è¯•å™¨"""

    def __init__(self):
        self.test_results = {}
        self.performance_metrics = {}

    async def run_comprehensive_integration_tests(self) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢çš„é›†æˆæµ‹è¯•"""
        print("ğŸš€ GPUåŠ é€Ÿå¼•æ“é›†æˆæµ‹è¯•...")

        test_suites = [
            ("HALå±‚é›†æˆæµ‹è¯•", self.test_hal_integration),
            ("å†…æ ¸å±‚ååŒæµ‹è¯•", self.test_kernel_coordination),
            ("å†…å­˜æ± é›†æˆæµ‹è¯•", self.test_memory_pool_integration),
            ("ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•", self.test_end_to_end_workflow),
            ("æ€§èƒ½å‹åŠ›æµ‹è¯•", self.test_performance_stress),
            ("é”™è¯¯æ¢å¤æµ‹è¯•", self.test_error_recovery),
            ("å¹¶å‘æ“ä½œæµ‹è¯•", self.test_concurrent_operations),
        ]

        for suite_name, test_func in test_suites:
            print(f"   ğŸ§ª {suite_name}...")
            try:
                start_time = time.time()
                result = await test_func()
                execution_time = time.time() - start_time

                self.test_results[suite_name] = {
                    "success": result.get("success", False),
                    "execution_time": execution_time,
                    "details": result,
                }

                status = "âœ…" if result.get("success", False) else "âŒ"
                print(f"   {status} {suite_name} ({execution_time:.2f}s)")

            except Exception as e:
                print(f"   âŒ {suite_name}å¤±è´¥: {e}")
                self.test_results[suite_name] = {
                    "success": False,
                    "error": str(e),
                    "execution_time": 0,
                }

        return self.generate_integration_report()

    async def test_hal_integration(self) -> Dict[str, Any]:
        """æµ‹è¯•HALå±‚é›†æˆ"""
        try:
            from src.gpu.core.hardware_abstraction import (
                get_gpu_resource_manager,
                get_memory_pool,
            )
            from src.gpu.core.kernels import get_kernel_executor

            # åˆå§‹åŒ–HALç»„ä»¶
            gpu_manager = get_gpu_resource_manager()
            memory_pool = get_memory_pool()
            kernel_executor = get_kernel_executor()

            # æµ‹è¯•åˆå§‹åŒ– (KernelExecutoræ²¡æœ‰initializeæ–¹æ³•)
            await gpu_manager.initialize()
            await memory_pool.initialize()
            # kernel_executor åœ¨å®ä¾‹åŒ–æ—¶å·²ç»åˆå§‹åŒ–å®Œæˆ

            # æµ‹è¯•è®¾å¤‡æ£€æµ‹
            devices = gpu_manager.get_available_devices()
            if not devices:
                return {"success": True, "simulated_mode": True, "devices": 0}

            # æµ‹è¯•èµ„æºåˆ†é…
            from src.gpu.core.hardware_abstraction.interfaces import (
                AllocationRequest,
                PerformanceProfile,
                StrategyPriority,
            )

            request = AllocationRequest(
                strategy_id="integration_test",
                required_memory=1024,  # 1GB
                priority=StrategyPriority.MEDIUM,
                performance_profile=PerformanceProfile(),
            )

            context = await gpu_manager.allocate_context(request)

            return {
                "success": True,
                "devices_detected": len(devices),
                "context_allocated": context is not None,
                "hal_components_initialized": 3,
                "simulated_mode": len(devices) > 0 and "Simulated" in devices[0].name,
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    async def test_kernel_coordination(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…æ ¸å±‚ååŒå·¥ä½œ"""
        try:
            from src.gpu.core.kernels import MatrixKernelEngine, TransformKernelEngine
            from src.gpu.core.kernels.standardized_interface import (
                MatrixConfig,
                MatrixOperationType,
                TransformConfig,
                TransformOperationType,
            )

            # åˆå§‹åŒ–å†…æ ¸
            matrix_kernel = MatrixKernelEngine()
            transform_kernel = TransformKernelEngine()

            await matrix_kernel.initialize()
            await transform_kernel.initialize()

            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_matrix = np.random.random((256, 256)).astype(np.float32)
            test_data = np.random.random(1000).astype(np.float32)

            # æµ‹è¯•çŸ©é˜µè¿ç®—
            matrix_config = MatrixConfig(operation_type=MatrixOperationType.MULTIPLY)
            matrix_result = await matrix_kernel.execute_matrix_operation(test_matrix, test_matrix.T, matrix_config)

            # æµ‹è¯•å˜æ¢è¿ç®—
            transform_config = TransformConfig(operation_type=TransformOperationType.NORMALIZE)
            transform_result = await transform_kernel.execute_transform_operation(test_data, transform_config)

            # æµ‹è¯•é“¾å¼æ“ä½œï¼ˆå…ˆå˜æ¢åçŸ©é˜µè¿ç®—ï¼‰
            chained_data = np.random.random((512, 512)).astype(np.float32)
            normalize_config = TransformConfig(operation_type=TransformOperationType.NORMALIZE)
            normalized_result = await transform_kernel.execute_transform_operation(chained_data, normalize_config)

            if normalized_result.success:
                # å°†æ ‡å‡†åŒ–åçš„æ•°æ®è½¬æ¢ä¸ºçŸ©é˜µè¿›è¡Œä¹˜æ³•
                matrix_config = MatrixConfig(operation_type=MatrixOperationType.MULTIPLY)
                final_result = await matrix_kernel.execute_matrix_operation(
                    normalized_result.result_data,
                    normalized_result.result_data.T,
                    matrix_config,
                )
            else:
                final_result = None

            return {
                "success": matrix_result.success and transform_result.success,
                "matrix_operations": matrix_result.success,
                "transform_operations": transform_result.success,
                "chained_operations": final_result.success if final_result else False,
                "matrix_execution_time": matrix_result.execution_time_ms,
                "transform_execution_time": transform_result.execution_time_ms,
                "kernels_coordination": True,
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    async def test_memory_pool_integration(self) -> Dict[str, Any]:
        """æµ‹è¯•å†…å­˜æ± é›†æˆ"""
        try:
            from src.gpu.core.hardware_abstraction.memory_pool import get_memory_pool

            memory_pool = get_memory_pool()
            await memory_pool.initialize()

            # æµ‹è¯•å¤šæ¬¡å†…å­˜åˆ†é…å’Œé‡Šæ”¾
            allocation_sizes = [1024, 4096, 16384, 65536, 262144]  # 1KBåˆ°256KB
            allocated_blocks = []

            # åˆ†é…é˜¶æ®µ
            for size in allocation_sizes:
                block_id = await memory_pool.allocate(size)
                if block_id:
                    allocated_blocks.append((block_id, size))

            # æµ‹è¯•å¹¶å‘åˆ†é…
            concurrent_tasks = []
            for i in range(10):
                task = memory_pool.allocate(8192)  # 8KB
                concurrent_tasks.append(task)

            concurrent_results = await asyncio.gather(*concurrent_tasks)
            concurrent_allocated = sum(1 for r in concurrent_results if r)

            # é‡Šæ”¾é˜¶æ®µ
            deallocation_success = 0
            for block_id, size in allocated_blocks:
                success = await memory_pool.deallocate(block_id)
                if success:
                    deallocation_success += 1

            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = memory_pool.get_stats()

            return {
                "success": len(allocated_blocks) == len(allocation_sizes),
                "allocations_successful": len(allocated_blocks),
                "total_allocation_size": sum(allocation_sizes),
                "concurrent_allocations": concurrent_allocated,
                "concurrent_success_rate": concurrent_allocated / 10,
                "deallocations_successful": deallocation_success,
                "pool_efficiency": stats.get("pool_efficiency", 0),
                "peak_memory_usage": stats.get("peak_memory_usage", 0),
                "memory_integration": True,
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    async def test_end_to_end_workflow(self) -> Dict[str, Any]:
        """æµ‹è¯•ç«¯åˆ°ç«¯å·¥ä½œæµ"""
        try:
            # æ¨¡æ‹Ÿå®Œæ•´çš„é‡åŒ–äº¤æ˜“å·¥ä½œæµ
            from src.gpu.core.hardware_abstraction import get_memory_pool
            from src.gpu.core.kernels import MatrixKernelEngine, TransformKernelEngine

            # åˆå§‹åŒ–ç»„ä»¶
            matrix_kernel = MatrixKernelEngine()
            transform_kernel = TransformKernelEngine()
            memory_pool = get_memory_pool()

            await matrix_kernel.initialize()
            await transform_kernel.initialize()
            await memory_pool.initialize()

            # æ­¥éª¤1: æ¨¡æ‹Ÿå¸‚åœºæ•°æ®ï¼ˆä»·æ ¼åºåˆ—ï¼‰
            price_data = np.random.random(10000).astype(np.float32) * 100 + 50

            # æ­¥éª¤2: è®¡ç®—æ”¶ç›Šç‡ï¼ˆå˜æ¢æ“ä½œï¼‰
            from src.gpu.core.kernels.standardized_interface import (
                TransformConfig,
                TransformOperationType,
            )

            return_config = TransformConfig(operation_type=TransformOperationType.RETURN)
            return_result = await transform_kernel.execute_transform_operation(price_data, return_config)

            # æ­¥éª¤3: è®¡ç®—æ³¢åŠ¨ç‡ï¼ˆå˜æ¢æ“ä½œï¼‰
            volatility_config = TransformConfig(operation_type=TransformOperationType.VOLATILITY)
            volatility_result = await transform_kernel.execute_transform_operation(price_data, volatility_config)

            # æ­¥éª¤4: åˆ›å»ºç›¸å…³çŸ©é˜µï¼ˆçŸ©é˜µæ“ä½œï¼‰
            # å°†ä»·æ ¼åºåˆ—è½¬æ¢ä¸ºå¤šèµ„äº§ä»·æ ¼çŸ©é˜µ
            price_matrix = np.random.random((100, 50)).astype(np.float32)  # 100ä¸ªæ—¶é—´ç‚¹ï¼Œ50ä¸ªèµ„äº§

            # è®¡ç®—æ”¶ç›Šç‡çŸ©é˜µ
            returns_matrix = np.diff(price_matrix, axis=0)

            # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
            returns_matrix_normalized = (returns_matrix - np.mean(returns_matrix, axis=0)) / np.std(
                returns_matrix, axis=0
            )
            correlation_matrix = np.corrcoef(returns_matrix_normalized.T)

            # æ­¥éª¤5: é£é™©è®¡ç®—ï¼ˆçŸ©é˜µè¿ç®—ï¼‰
            from src.gpu.core.kernels.standardized_interface import (
                MatrixConfig,
                MatrixOperationType,
            )

            risk_config = MatrixConfig(operation_type=MatrixOperationType.MULTIPLY)

            # æ¨¡æ‹Ÿé£é™©æƒé‡çŸ©é˜µ
            risk_weights = np.random.random((50, 50)).astype(np.float32)
            risk_result = await matrix_kernel.execute_matrix_operation(correlation_matrix, risk_weights, risk_config)

            workflow_success = return_result.success and volatility_result.success and risk_result.success

            return {
                "success": workflow_success,
                "price_data_points": len(price_data),
                "return_calculation": return_result.success,
                "volatility_calculation": volatility_result.success,
                "correlation_matrix_size": correlation_matrix.shape,
                "risk_calculation": risk_result.success,
                "workflow_stages_completed": sum(
                    [
                        return_result.success,
                        volatility_result.success,
                        risk_result.success,
                    ]
                ),
                "total_workflow_stages": 3,
                "workflow_completion_rate": sum(
                    [
                        return_result.success,
                        volatility_result.success,
                        risk_result.success,
                    ]
                )
                / 3,
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    async def test_performance_stress(self) -> Dict[str, Any]:
        """æµ‹è¯•æ€§èƒ½å‹åŠ›"""
        try:
            from src.gpu.core.kernels import MatrixKernelEngine

            matrix_kernel = MatrixKernelEngine()
            await matrix_kernel.initialize()

            # å‹åŠ›æµ‹è¯•ï¼šå¤§çŸ©é˜µè¿ç®—
            large_sizes = [512, 1024, 2048]
            stress_results = []

            for size in large_sizes:
                # åˆ›å»ºå¤§çŸ©é˜µ
                matrix_a = np.random.random((size, size)).astype(np.float32)
                matrix_b = np.random.random((size, size)).astype(np.float32)

                # æ‰§è¡ŒçŸ©é˜µä¹˜æ³•
                from src.gpu.core.kernels.standardized_interface import (
                    MatrixConfig,
                    MatrixOperationType,
                )

                config = MatrixConfig(operation_type=MatrixOperationType.MULTIPLY)

                start_time = time.time()
                result = await matrix_kernel.execute_matrix_operation(matrix_a, matrix_b, config)
                execution_time = time.time() - start_time

                if result.success:
                    gflops = (2 * size**3) / execution_time / 1e9
                    stress_results.append(
                        {
                            "matrix_size": size,
                            "execution_time": execution_time,
                            "performance_gflops": gflops,
                            "memory_usage_mb": result.memory_used_bytes / (1024 * 1024),
                        }
                    )

            # æµ‹è¯•å¤šæ¬¡è¿­ä»£ç¨³å®šæ€§
            iterations = 10
            test_matrix = np.random.random((512, 512)).astype(np.float32)
            iteration_times = []

            for i in range(iterations):
                config = MatrixConfig(operation_type=MatrixOperationType.MULTIPLY)
                start_time = time.time()
                result = await matrix_kernel.execute_matrix_operation(test_matrix, test_matrix, config)
                iteration_time = time.time() - start_time

                if result.success:
                    iteration_times.append(iteration_time)

            return {
                "success": len(stress_results) > 0 and len(iteration_times) > 0,
                "stress_tests_passed": len(stress_results),
                "max_matrix_size_tested": max(s["matrix_size"] for s in stress_results) if stress_results else 0,
                "peak_performance_gflops": (
                    max(s["performance_gflops"] for s in stress_results) if stress_results else 0
                ),
                "average_performance_gflops": (
                    sum(s["performance_gflops"] for s in stress_results) / len(stress_results) if stress_results else 0
                ),
                "iteration_stability": {
                    "iterations": len(iteration_times),
                    "avg_time": sum(iteration_times) / len(iteration_times) if iteration_times else 0,
                    "std_time": np.std(iteration_times) if iteration_times else 0,
                    "performance_cv": (
                        np.std(iteration_times) / np.mean(iteration_times)
                        if iteration_times and np.mean(iteration_times) > 0
                        else 0
                    ),
                },
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    async def test_error_recovery(self) -> Dict[str, Any]:
        """æµ‹è¯•é”™è¯¯æ¢å¤"""
        try:
            from src.gpu.core.kernels import MatrixKernelEngine, TransformKernelEngine

            matrix_kernel = MatrixKernelEngine()
            transform_kernel = TransformKernelEngine()

            await matrix_kernel.initialize()
            await transform_kernel.initialize()

            error_recovery_tests = []

            # æµ‹è¯•1: ä¸å…¼å®¹çš„çŸ©é˜µç»´åº¦
            try:
                from src.gpu.core.kernels.standardized_interface import (
                    MatrixConfig,
                    MatrixOperationType,
                )

                matrix_a = np.random.random((100, 200)).astype(np.float32)
                matrix_b = np.random.random((300, 400)).astype(np.float32)  # ä¸å…¼å®¹
                config = MatrixConfig(operation_type=MatrixOperationType.MULTIPLY)
                result = await matrix_kernel.execute_matrix_operation(matrix_a, matrix_b, config)
                error_recovery_tests.append(
                    {
                        "test": "incompatible_matrices",
                        "handled_gracefully": not result.success,
                        "error_message": result.error_message if not result.success else None,
                    }
                )
            except Exception as e:
                error_recovery_tests.append(
                    {
                        "test": "incompatible_matrices",
                        "handled_gracefully": False,
                        "error_message": str(e),
                    }
                )

            # æµ‹è¯•2: æ— æ•ˆçš„å˜æ¢æ“ä½œ
            try:
                from src.gpu.core.kernels.standardized_interface import (
                    TransformConfig,
                    TransformOperationType,
                )

                invalid_data = np.array([])  # ç©ºæ•°ç»„
                config = TransformConfig(operation_type=TransformOperationType.NORMALIZE)
                result = await transform_kernel.execute_transform_operation(invalid_data, config)
                error_recovery_tests.append(
                    {
                        "test": "invalid_transform_data",
                        "handled_gracefully": not result.success,
                        "error_message": result.error_message if not result.success else None,
                    }
                )
            except Exception as e:
                error_recovery_tests.append(
                    {
                        "test": "invalid_transform_data",
                        "handled_gracefully": False,
                        "error_message": str(e),
                    }
                )

            # æµ‹è¯•3: å†…å­˜åˆ†é…æ¢å¤
            try:
                from src.gpu.core.hardware_abstraction.memory_pool import (
                    get_memory_pool,
                )

                memory_pool = get_memory_pool()
                await memory_pool.initialize()

                # å°è¯•åˆ†é…è¶…å¤§å†…å­˜å—
                huge_block = await memory_pool.allocate(1024 * 1024 * 1024)  # 1GB
                deallocation_success = await memory_pool.deallocate(huge_block) if huge_block else True

                error_recovery_tests.append(
                    {
                        "test": "huge_memory_allocation",
                        "handled_gracefully": True,  # CPUå›é€€åº”è¯¥èƒ½å¤„ç†
                        "allocation_successful": huge_block is not None,
                        "deallocation_successful": deallocation_success,
                    }
                )
            except Exception as e:
                error_recovery_tests.append(
                    {
                        "test": "huge_memory_allocation",
                        "handled_gracefully": False,
                        "error_message": str(e),
                    }
                )

            successful_recoveries = sum(1 for test in error_recovery_tests if test.get("handled_gracefully", False))

            return {
                "success": successful_recoveries >= 2,  # è‡³å°‘2/3æµ‹è¯•é€šè¿‡
                "total_tests": len(error_recovery_tests),
                "successful_recoveries": successful_recoveries,
                "recovery_rate": successful_recoveries / len(error_recovery_tests),
                "detailed_results": error_recovery_tests,
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    async def test_concurrent_operations(self) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶å‘æ“ä½œ"""
        try:
            from src.gpu.core.kernels import MatrixKernelEngine

            matrix_kernel = MatrixKernelEngine()
            await matrix_kernel.initialize()

            # åˆ›å»ºå¹¶å‘ä»»åŠ¡
            async def concurrent_matrix_operation(matrix_size: int, operation_id: int):
                try:
                    matrix_a = np.random.random((matrix_size, matrix_size)).astype(np.float32)
                    matrix_b = np.random.random((matrix_size, matrix_size)).astype(np.float32)

                    from src.gpu.core.kernels.standardized_interface import (
                        MatrixConfig,
                        MatrixOperationType,
                    )

                    config = MatrixConfig(operation_type=MatrixOperationType.MULTIPLY)

                    start_time = time.time()
                    result = await matrix_kernel.execute_matrix_operation(matrix_a, matrix_b, config)
                    execution_time = time.time() - start_time

                    return {
                        "operation_id": operation_id,
                        "success": result.success,
                        "execution_time": execution_time,
                        "matrix_size": matrix_size,
                        "performance_gflops": (2 * matrix_size**3) / execution_time / 1e9 if result.success else 0,
                    }
                except Exception as e:
                    return {
                        "operation_id": operation_id,
                        "success": False,
                        "error": str(e),
                        "matrix_size": matrix_size,
                    }

            # å¯åŠ¨å¤šä¸ªå¹¶å‘æ“ä½œ
            concurrent_tasks = []
            matrix_sizes = [256, 512, 256, 512, 256]  # ä¸åŒå¤§å°çš„çŸ©é˜µ

            for i, size in enumerate(matrix_sizes):
                task = concurrent_matrix_operation(size, i)
                concurrent_tasks.append(task)

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            results = await asyncio.gather(*concurrent_tasks, return_exceptions=True)

            # åˆ†æç»“æœ
            successful_operations = sum(1 for r in results if isinstance(r, dict) and r.get("success", False))
            total_operations = len(results)

            performance_stats = []
            for r in results:
                if isinstance(r, dict) and r.get("success", False):
                    performance_stats.append(r.get("performance_gflops", 0))

            return {
                "success": successful_operations >= total_operations * 0.8,  # 80%æˆåŠŸç‡
                "total_concurrent_operations": total_operations,
                "successful_operations": successful_operations,
                "concurrency_success_rate": successful_operations / total_operations,
                "average_performance_gflops": (
                    sum(performance_stats) / len(performance_stats) if performance_stats else 0
                ),
                "performance_variance": np.var(performance_stats) if performance_stats else 0,
                "detailed_results": [r for r in results if isinstance(r, dict)],
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def generate_integration_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆé›†æˆæµ‹è¯•æŠ¥å‘Š"""
        total_suites = len(self.test_results)
        successful_suites = sum(1 for r in self.test_results.values() if r.get("success", False))

        # è®¡ç®—æ€»ä½“æ‰§è¡Œæ—¶é—´
        total_execution_time = sum(r.get("execution_time", 0) for r in self.test_results.values())

        return {
            "test_timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "integration_phase": "Phase 6.4.2",
            "total_test_suites": total_suites,
            "successful_test_suites": successful_suites,
            "failed_test_suites": total_suites - successful_suites,
            "success_rate": (successful_suites / total_suites * 100) if total_suites > 0 else 0,
            "total_execution_time": total_execution_time,
            "detailed_results": self.test_results,
            "summary": {
                "hal_integration_working": self.test_results.get("HALå±‚é›†æˆæµ‹è¯•", {}).get("success", False),
                "kernel_coordination_working": self.test_results.get("å†…æ ¸å±‚ååŒæµ‹è¯•", {}).get("success", False),
                "memory_pool_integration_working": self.test_results.get("å†…å­˜æ± é›†æˆæµ‹è¯•", {}).get("success", False),
                "end_to_end_workflow_working": self.test_results.get("ç«¯åˆ°ç«¯å·¥ä½œæµæµ‹è¯•", {}).get("success", False),
                "performance_stress_passed": self.test_results.get("æ€§èƒ½å‹åŠ›æµ‹è¯•", {}).get("success", False),
                "error_recovery_working": self.test_results.get("é”™è¯¯æ¢å¤æµ‹è¯•", {}).get("success", False),
                "concurrent_operations_working": self.test_results.get("å¹¶å‘æ“ä½œæµ‹è¯•", {}).get("success", False),
                "overall_integration_successful": successful_suites >= total_suites * 0.8,
            },
        }

    def print_summary(self, report: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "=" * 70)
        print("ğŸ“Š GPUåŠ é€Ÿå¼•æ“é›†æˆæµ‹è¯•æŠ¥å‘Š")
        print("=" * 70)

        summary = report["summary"]
        print(
            f"ğŸ“ˆ é›†æˆæµ‹è¯•æˆåŠŸç‡: {report['success_rate']:.1f}% ({report['successful_test_suites']}/{report['total_test_suites']})"
        )
        print(f"ğŸ•’ æµ‹è¯•æ—¶é—´: {report['test_timestamp']}")
        print(f"â±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {report['total_execution_time']:.2f}ç§’")

        print("\nğŸ”§ ç»„ä»¶çŠ¶æ€:")
        print(f"   âœ… HALå±‚é›†æˆ: {'æ­£å¸¸' if summary['hal_integration_working'] else 'å¼‚å¸¸'}")
        print(f"   âœ… å†…æ ¸ååŒ: {'æ­£å¸¸' if summary['kernel_coordination_working'] else 'å¼‚å¸¸'}")
        print(f"   âœ… å†…å­˜æ± é›†æˆ: {'æ­£å¸¸' if summary['memory_pool_integration_working'] else 'å¼‚å¸¸'}")
        print(f"   âœ… ç«¯åˆ°ç«¯å·¥ä½œæµ: {'æ­£å¸¸' if summary['end_to_end_workflow_working'] else 'å¼‚å¸¸'}")
        print(f"   âœ… æ€§èƒ½å‹åŠ›æµ‹è¯•: {'é€šè¿‡' if summary['performance_stress_passed'] else 'å¤±è´¥'}")
        print(f"   âœ… é”™è¯¯æ¢å¤: {'æ­£å¸¸' if summary['error_recovery_working'] else 'å¼‚å¸¸'}")
        print(f"   âœ… å¹¶å‘æ“ä½œ: {'æ­£å¸¸' if summary['concurrent_operations_working'] else 'å¼‚å¸¸'}")
        print(f"ğŸš€ æ•´ä½“é›†æˆæˆåŠŸ: {'æ˜¯' if summary['overall_integration_successful'] else 'å¦'}")

        print("\nğŸ“‹ è¯¦ç»†ç»“æœ:")
        for suite_name, result in report["detailed_results"].items():
            status = "âœ…" if result.get("success", False) else "âŒ"
            execution_time = result.get("execution_time", 0)
            print(f"   {status} {suite_name} ({execution_time:.2f}s)")

        # æ˜¾ç¤ºæ€§èƒ½æ•°æ®
        if "æ€§èƒ½å‹åŠ›æµ‹è¯•" in report["detailed_results"]:
            perf_result = report["detailed_results"]["æ€§èƒ½å‹åŠ›æµ‹è¯•"]["details"]
            if perf_result.get("success", False):
                print("\nâš¡ æ€§èƒ½æ‘˜è¦:")
                print(
                    f"   â€¢ æœ€å¤§çŸ©é˜µè§„æ¨¡: {perf_result['max_matrix_size_tested']}x{perf_result['max_matrix_size_tested']}"
                )
                print(f"   â€¢ å³°å€¼æ€§èƒ½: {perf_result['peak_performance_gflops']:.2f} GFLOPS")
                print(f"   â€¢ å¹³å‡æ€§èƒ½: {perf_result['average_performance_gflops']:.2f} GFLOPS")

                if "iteration_stability" in perf_result:
                    stability = perf_result["iteration_stability"]
                    print(f"   â€¢ è¿­ä»£ç¨³å®šæ€§: CV={stability['performance_cv']:.3f} (è¶Šä½è¶Šå¥½)")

        # æ˜¾ç¤ºå¹¶å‘æ•°æ®
        if "å¹¶å‘æ“ä½œæµ‹è¯•" in report["detailed_results"]:
            concurrent_result = report["detailed_results"]["å¹¶å‘æ“ä½œæµ‹è¯•"]["details"]
            if concurrent_result.get("success", False):
                print("\nğŸ”„ å¹¶å‘æ“ä½œæ‘˜è¦:")
                print(f"   â€¢ å¹¶å‘æˆåŠŸç‡: {concurrent_result['concurrency_success_rate'] * 100:.1f}%")
                print(f"   â€¢ å¹³å‡æ€§èƒ½: {concurrent_result['average_performance_gflops']:.2f} GFLOPS")

        print("\n" + "=" * 70)


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Phase 6.4.2 GPUåŠ é€Ÿå¼•æ“é›†æˆæµ‹è¯•")
    print("=" * 70)

    tester = GPUEngineIntegrationTester()

    # è¿è¡Œé›†æˆæµ‹è¯•
    report = await tester.run_comprehensive_integration_tests()

    # æ‰“å°æ‘˜è¦
    tester.print_summary(report)

    return report


if __name__ == "__main__":
    report = asyncio.run(main())
