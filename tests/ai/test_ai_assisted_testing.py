#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MyStocks AIè¾…åŠ©æµ‹è¯•å·¥å…·
æä¾›æ™ºèƒ½æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆã€ä¼˜åŒ–å’Œæ•…éšœè¯Šæ–­
é›†æˆASTåˆ†æå’Œé¡¹ç›®ä¸Šä¸‹æ–‡æ„ŸçŸ¥
"""

import ast
import asyncio
import hashlib
import os
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Set

import pytest

# Import specific test configuration
try:
    from tests.config.test_config import ai_config, test_data
except ImportError:
    # Fallback if test_config doesn't exist
    ai_config = {}
    test_data = {}


class TestPriority(Enum):
    """æµ‹è¯•ä¼˜å…ˆçº§"""

    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


class TestCategory(Enum):
    """æµ‹è¯•ç±»åˆ«"""

    UNIT = "unit"
    INTEGRATION = "integration"
    PERFORMANCE = "performance"
    SECURITY = "security"
    COMPATIBILITY = "compatibility"


@dataclass
class TestCase:
    """å¢å¼ºçš„æµ‹è¯•ç”¨ä¾‹æ•°æ®ç»“æ„"""

    name: str
    description: str
    code: str
    method_name: str
    coverage: List[str]
    complexity_score: float
    priority: TestPriority
    category: TestCategory
    dependencies: List[str] = field(default_factory=list)
    tags: Set[str] = field(default_factory=set)
    execution_time_estimate: float = 0.0
    flakiness_score: float = 0.0
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class AnalysisResult:
    """å¢å¼ºçš„ä»£ç åˆ†æç»“æœ"""

    method_name: str
    complexity: int
    length: int
    cyclomatic_complexity: int
    cognitive_complexity: int
    coupling_score: float
    cohesion_score: float
    test_coverage: List[str]
    dependencies: List[str]
    risk_level: str  # low, medium, high
    security_issues: List[str]
    performance_issues: List[str]
    maintainability_score: float


class ProjectContextAnalyzer:
    """é¡¹ç›®ä¸Šä¸‹æ–‡åˆ†æå™¨"""

    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root) if project_root else Path(__file__).parent.parent.parent
        self.context_cache = {}

    def get_project_structure(self) -> Dict[str, Any]:
        """è·å–é¡¹ç›®ç»“æ„"""
        if "structure" in self.context_cache:
            return self.context_cache["structure"]

        structure = {
            "modules": [],
            "config_files": [],
            "test_files": [],
            "api_endpoints": [],
            "business_entities": [],
        }

        # æ‰«æé¡¹ç›®ç›®å½•
        for root, dirs, files in os.walk(self.project_root):
            root_path = Path(root)

            # æ’é™¤ç‰¹å®šç›®å½•
            dirs[:] = [d for d in dirs if not d.startswith(".") and d not in ["__pycache__", "node_modules"]]

            # åˆ†ææ¨¡å—
            if "__init__.py" in files:
                module_path = root_path.relative_to(self.project_root)
                structure["modules"].append(str(module_path))

            # æ”¶é›†é…ç½®æ–‡ä»¶
            for file in files:
                if file.endswith((".yaml", ".yml", ".json", ".toml", ".ini")):
                    structure["config_files"].append(str(root_path / file))

            # æ”¶é›†æµ‹è¯•æ–‡ä»¶
            if "test_" in file or file.endswith("_test.py"):
                structure["test_files"].append(str(root_path / file))

        self.context_cache["structure"] = structure
        return structure

    def detect_patterns(self, source_code: str) -> Dict[str, Any]:
        """æ£€æµ‹ä»£ç æ¨¡å¼"""
        patterns = {
            "data_models": [],
            "api_handlers": [],
            "business_logic": [],
            "utility_functions": [],
            "external_calls": [],
        }

        tree = ast.parse(source_code)

        for node in ast.walk(tree):
            # æ£€æµ‹æ•°æ®æ¨¡å‹ï¼ˆPydanticç±»ï¼‰
            if isinstance(node, ast.ClassDef) and any(
                isinstance(base, ast.Attribute) and base.attr == "BaseModel"
                for base in node.bases
                if isinstance(base, ast.Attribute)
            ):
                patterns["data_models"].append(node.name)

            # æ£€æµ‹APIå¤„ç†å™¨
            elif isinstance(node, ast.FunctionDef) and (
                "api" in node.name.lower() or "endpoint" in node.name.lower() or "route" in node.name.lower()
            ):
                patterns["api_handlers"].append(node.name)

            # æ£€æµ‹ä¸šåŠ¡é€»è¾‘
            elif isinstance(node, ast.FunctionDef) and any(
                keyword in node.name.lower()
                for keyword in [
                    "calculate",
                    "get",
                    "set",
                    "update",
                    "process",
                    "analyze",
                ]
            ):
                patterns["business_logic"].append(node.name)

            # æ£€æµ‹å¤–éƒ¨è°ƒç”¨
            elif isinstance(node, ast.Call):
                if isinstance(node.func, ast.Attribute) and node.func.attr == "requests":
                    patterns["external_calls"].append("HTTP requests detected")
                elif isinstance(node.func, ast.Attribute) and node.func.attr == "fetch":
                    patterns["external_calls"].append("Data fetch detected")

        return patterns


class AITestGenerator:
    """å¢å¼ºçš„AIæµ‹è¯•ç”Ÿæˆå™¨"""

    def __init__(self, project_context: ProjectContextAnalyzer = None):
        self.test_cache_dir = Path(__file__).parent / "cache"
        self.test_cache_dir.mkdir(exist_ok=True)
        self.context_analyzer = project_context or ProjectContextAnalyzer()
        self.pattern_library = self._load_pattern_library()

    def _load_pattern_library(self) -> Dict[str, Any]:
        """åŠ è½½æµ‹è¯•æ¨¡å¼åº“"""
        return {
            "financial_functions": {
                "patterns": [
                    "get_stock_daily",
                    "get_index_daily",
                    "calculate_profit_loss",
                ],
                "test_cases": [
                    "normal_input",
                    "boundary_conditions",
                    "invalid_symbols",
                    "date_range_validation",
                ],
            },
            "data_processing": {
                "patterns": ["fetch_kline_data", "process_market_data"],
                "test_cases": [
                    "data_format_validation",
                    "missing_data_handling",
                    "data_quality_checks",
                ],
            },
            "api_endpoints": {
                "patterns": ["get_", "post_", "put_", "delete_"],
                "test_cases": [
                    "status_code_verification",
                    "response_schema_validation",
                    "authentication",
                    "rate_limiting",
                ],
            },
        }

    async def generate_test_cases_from_source(self, source_code: str, method_name: str) -> List[TestCase]:
        """ä»æºä»£ç ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ - å¢å¼ºç‰ˆ"""
        print(f"ğŸ¤– AIæ­£åœ¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹: {method_name}")

        try:
            # è§£ææºä»£ç 
            tree = ast.parse(source_code)

            # æŸ¥æ‰¾ç›®æ ‡æ–¹æ³•
            target_method = None
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef) and node.name == method_name:
                    target_method = node
                    break

            if not target_method:
                raise ValueError(f"æ–¹æ³• {method_name} æœªæ‰¾åˆ°")

            # é¡¹ç›®ä¸Šä¸‹æ–‡åˆ†æ
            project_structure = self.context_analyzer.get_project_structure()
            detected_patterns = self.context_analyzer.detect_patterns(source_code)

            # æ·±åº¦åˆ†ææ–¹æ³•ç»“æ„
            analysis = await self._analyze_method_structure_enhanced(target_method)

            # åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
            test_cases = await self._generate_test_cases_for_method_enhanced(
                method_name,
                target_method,
                analysis,
                detected_patterns,
                project_structure,
            )

            # ä¼˜åŒ–æµ‹è¯•ç”¨ä¾‹
            optimized_cases = await self._optimize_test_cases(test_cases)

            return optimized_cases

        except Exception as e:
            print(f"âŒ æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå¤±è´¥: {str(e)}")
            return []

    async def _analyze_method_structure_enhanced(self, method_node: ast.FunctionDef) -> AnalysisResult:
        """å¢å¼ºçš„æ–¹æ³•ç»“æ„åˆ†æ"""
        # åŸºç¡€åˆ†æ
        args = []
        parameters = []
        return_types = []
        exceptions = []

        # æå–å‚æ•°ä¿¡æ¯
        for arg in method_node.args.args:
            args.append(arg.arg)
            parameters.append(
                {
                    "name": arg.arg,
                    "type": self._get_annotation_type(arg.annotation),
                    "has_default": arg.default is not None,
                }
            )

        # åˆ†æå¼‚å¸¸å¤„ç†
        exceptions = self._extract_exception_types(method_node)

        # è®¡ç®—å„ç§å¤æ‚åº¦
        cyclomatic_complexity = self._calculate_cyclomatic_complexity(method_node)
        cognitive_complexity = self._calculate_cognitive_complexity(method_node)
        coupling_score = self._calculate_coupling_score(method_node)
        cohesion_score = self._calculate_cohesion_score(method_node)

        # åˆ†æå®‰å…¨é—®é¢˜å’Œæ€§èƒ½é—®é¢˜
        security_issues = self._analyze_security_issues(method_node)
        performance_issues = self._analyze_performance_issues(method_node)

        # è®¡ç®—å¯ç»´æŠ¤æ€§åˆ†æ•°
        maintainability_score = self._calculate_maintainability_score(
            cyclomatic_complexity, cognitive_complexity, coupling_score, cohesion_score
        )

        return AnalysisResult(
            method_name=method_node.name,
            complexity=len(method_node.body),
            length=len(method_node.body),
            cyclomatic_complexity=cyclomatic_complexity,
            cognitive_complexity=cognitive_complexity,
            coupling_score=coupling_score,
            cohesion_score=cohesion_score,
            test_coverage=[],
            dependencies=self._extract_dependencies(method_node),
            risk_level=self._assess_risk_level(cyclomatic_complexity, coupling_score),
            security_issues=security_issues,
            performance_issues=performance_issues,
            maintainability_score=maintainability_score,
        )

    def _analyze_method_structure(self, method_node: ast.FunctionDef) -> Dict[str, Any]:
        """åˆ†ææ–¹æ³•ç»“æ„ - å…¼å®¹æ€§æ–¹æ³•"""
        analysis = {
            "arguments": [],
            "parameters": [],
            "return_types": [],
            "exceptions": [],
            "complexity": self._calculate_complexity(method_node),
            "length": len(method_node.body),
            "control_flow": [],
        }

        # æå–å‚æ•°ä¿¡æ¯
        for arg in method_node.args.args:
            analysis["arguments"].append(arg.arg)
            analysis["parameters"].append({"name": arg.arg, "type": self._get_annotation_type(arg.annotation)})

        # åˆ†ææ§åˆ¶æµ
        for node in method_node.body:
            if isinstance(node, (ast.If, ast.For, ast.While, ast.Try)):
                analysis["control_flow"].append(type(node).__name__)

        return analysis

    # æ–°å¢çš„å¢å¼ºåˆ†ææ–¹æ³•
    def _extract_exception_types(self, method_node: ast.FunctionDef) -> List[str]:
        """æå–å¼‚å¸¸ç±»å‹"""
        exceptions = []
        for node in ast.walk(method_node):
            if isinstance(node, ast.ExceptHandler):
                if node.type:
                    if isinstance(node.type, ast.Name):
                        exceptions.append(node.type.id)
                    elif isinstance(node.type, ast.Attribute):
                        exceptions.append(node.type.attr)
        return list(set(exceptions))

    def _calculate_cyclomatic_complexity(self, node: ast.FunctionDef) -> int:
        """è®¡ç®—åœˆå¤æ‚åº¦ - å¢å¼ºç‰ˆ"""
        complexity = 1  # åŸºç¡€å¤æ‚åº¦

        for child in ast.walk(node):
            if isinstance(
                child,
                (
                    ast.If,
                    ast.For,
                    ast.While,
                    ast.ExceptHandler,
                    ast.With,
                    ast.AsyncWith,
                    ast.comprehension,
                    ast.DictComp,
                    ast.ListComp,
                    ast.SetComp,
                    ast.GeneratorExp,
                ),
            ):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1
            elif isinstance(child, ast.Compare):
                complexity += len(child.ops) - 1

        return complexity

    def _calculate_cognitive_complexity(self, node: ast.FunctionDef) -> int:
        """è®¡ç®—è®¤çŸ¥å¤æ‚åº¦"""
        complexity = 0
        nesting_level = 0

        def _calculate_complexity_recursive(n, level):
            nonlocal complexity
            complexity += level

            for child in ast.iter_child_nodes(n):
                if isinstance(child, (ast.If, ast.For, ast.While, ast.Try)):
                    _calculate_complexity_recursive(child, level + 1)
                else:
                    _calculate_complexity_recursive(child, level)

        _calculate_complexity_recursive(node, 0)
        return complexity

    def _calculate_coupling_score(self, node: ast.FunctionDef) -> float:
        """è®¡ç®—è€¦åˆåº¦åˆ†æ•°"""
        external_calls = 0
        internal_calls = 0

        for child in ast.walk(node):
            if isinstance(child, ast.Call):
                if isinstance(child.func, ast.Name):
                    internal_calls += 1
                elif isinstance(child.func, ast.Attribute):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å¤–éƒ¨è°ƒç”¨
                    if child.func.value.id not in ["self", "cls", "pytest"]:
                        external_calls += 1

        total_calls = internal_calls + external_calls
        if total_calls == 0:
            return 0.0

        return external_calls / total_calls

    def _calculate_cohesion_score(self, node: ast.FunctionDef) -> float:
        """è®¡ç®—å†…èšåº¦åˆ†æ•°"""
        variables = set()
        variable_usages = {}

        # æ”¶é›†å˜é‡å®šä¹‰å’Œä½¿ç”¨
        for child in ast.walk(node):
            if isinstance(child, ast.Name) and isinstance(child.ctx, ast.Store):
                variables.add(child.id)
                variable_usages[child.id] = 0
            elif isinstance(child, ast.Name) and isinstance(child.ctx, ast.Load):
                if child.id in variable_usages:
                    variable_usages[child.id] += 1

        if len(variables) == 0:
            return 1.0

        # è®¡ç®—å†…èšåº¦
        total_usages = sum(variable_usages.values())
        if total_usages == 0:
            return 0.5

        return min(total_usages / (len(variables) * 2), 1.0)

    def _extract_dependencies(self, node: ast.FunctionDef) -> List[str]:
        """æå–ä¾èµ–é¡¹"""
        dependencies = []

        for child in ast.walk(node):
            if isinstance(child, ast.Import):
                for alias in child.names:
                    dependencies.append(alias.name)
            elif isinstance(child, ast.ImportFrom):
                if child.module:
                    dependencies.append(child.module)

        return list(set(dependencies))

    def _analyze_security_issues(self, node: ast.FunctionDef) -> List[str]:
        """åˆ†æå®‰å…¨é—®é¢˜"""
        security_issues = []

        # æ£€æŸ¥SQLæ³¨å…¥é£é™©
        for child in ast.walk(node):
            if isinstance(child, ast.BinOp) and isinstance(child.op, ast.Mod):
                if isinstance(child.left, ast.Constant) and "SELECT" in str(child.left.value):
                    security_issues.append("æ½œåœ¨çš„SQLæ³¨å…¥é£é™©")

        # æ£€æŸ¥å‘½ä»¤æ³¨å…¥é£é™©
        for child in ast.walk(node):
            if isinstance(child, ast.Call):
                if isinstance(child.func, ast.Attribute) and child.func.attr in [
                    "exec",
                    "eval",
                ]:
                    security_issues.append("å‘½ä»¤æ³¨å…¥é£é™©")

        # æ£€æŸ¥æ•æ„Ÿæ•°æ®æ—¥å¿—
        for child in ast.walk(node):
            if isinstance(child, ast.Call):
                if isinstance(child.func, ast.Attribute) and child.func.attr in [
                    "print",
                    "logger",
                    "log",
                ]:
                    if isinstance(child.args[0], ast.Constant) and "password" in str(child.args[0]).lower():
                        security_issues.append("æ•æ„Ÿæ•°æ®å¯èƒ½è¢«è®°å½•")

        return security_issues

    def _analyze_performance_issues(self, node: ast.FunctionDef) -> List[str]:
        """åˆ†ææ€§èƒ½é—®é¢˜"""
        performance_issues = []

        # æ£€æŸ¥å¾ªç¯ä¸­å¯èƒ½çš„æ€§èƒ½é—®é¢˜
        loops = []
        for child in ast.walk(node):
            if isinstance(child, (ast.For, ast.While)):
                loops.append(child)

        # æ£€æŸ¥åµŒå¥—å¾ªç¯
        for i, loop1 in enumerate(loops):
            for loop2 in loops[i + 1 :]:
                if self._is_nested_loop(loop1, loop2):
                    performance_issues.append("æ·±å±‚åµŒå¥—å¾ªç¯å¯èƒ½å½±å“æ€§èƒ½")
                    break

        # æ£€æŸ¥é‡å¤è®¡ç®—
        calculations = []
        for child in ast.walk(node):
            if isinstance(child, ast.BinOp) and isinstance(child.op, (ast.Add, ast.Sub, ast.Mult, ast.Div, ast.Pow)):
                calculations.append(child)

        # ç®€å•çš„é‡å¤æ£€æµ‹
        calc_strs = [ast.dump(calc) for calc in calculations]
        if len(calc_strs) != len(set(calc_strs)):
            performance_issues.append("å¯èƒ½å­˜åœ¨é‡å¤è®¡ç®—")

        return performance_issues

    def _is_nested_loop(self, outer: ast.AST, inner: ast.AST) -> bool:
        """æ£€æŸ¥å¾ªç¯æ˜¯å¦åµŒå¥—"""
        outer_line = outer.lineno
        outer_end = getattr(outer, "end_lineno", outer_line + 1)
        inner_line = inner.lineno
        inner_end = getattr(inner, "end_lineno", inner_line + 1)

        return inner_line > outer_line and inner_end <= outer_end

    def _calculate_maintainability_score(
        self, cyclomatic: int, cognitive: int, coupling: float, cohesion: float
    ) -> float:
        """è®¡ç®—å¯ç»´æŠ¤æ€§åˆ†æ•°"""
        # åŸºäºå¤šä¸ªå› ç´ è®¡ç®—ç»¼åˆåˆ†æ•°
        complexity_score = max(0, (20 - cyclomatic - cognitive) / 20)
        coupling_score = max(0, (1 - coupling))
        cohesion_score = cohesion

        # åŠ æƒå¹³å‡
        overall_score = complexity_score * 0.4 + coupling_score * 0.3 + cohesion_score * 0.3

        return round(overall_score, 2)

    def _assess_risk_level(self, cyclomatic_complexity: int, coupling_score: float) -> str:
        """è¯„ä¼°é£é™©ç­‰çº§"""
        if cyclomatic_complexity > 20 or coupling_score > 0.8:
            return "high"
        elif cyclomatic_complexity > 10 or coupling_score > 0.6:
            return "medium"
        else:
            return "low"

    async def _generate_test_cases_for_method_enhanced(
        self,
        method_name: str,
        method_node: ast.FunctionDef,
        analysis: AnalysisResult,
        patterns: Dict[str, Any],
        project_structure: Dict[str, Any],
    ) -> List[TestCase]:
        """ä¸ºæ–¹æ³•ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ - å¢å¼ºç‰ˆ"""
        test_cases = []

        # åŸºäºé¡¹ç›®ç»“æ„ç¡®å®šæµ‹è¯•ç±»åˆ«
        category = self._determine_test_category(method_name, patterns, project_structure)

        # åŸºäºåˆ†æç»“æœç¡®å®šä¼˜å…ˆçº§
        priority = self._determine_test_priority(analysis)

        # 1. åŸºç¡€åŠŸèƒ½æµ‹è¯•
        normal_cases = self._create_normal_cases_enhanced(method_name, analysis, category, priority)
        test_cases.extend(normal_cases)

        # 2. è¾¹ç•Œæ¡ä»¶æµ‹è¯•
        boundary_cases = self._create_boundary_cases_enhanced(method_name, analysis, category, priority)
        test_cases.extend(boundary_cases)

        # 3. å¼‚å¸¸å¤„ç†æµ‹è¯•
        exception_cases = self._create_exception_cases_enhanced(method_name, analysis, category, priority)
        test_cases.extend(exception_cases)

        # 4. å‚æ•°éªŒè¯æµ‹è¯•
        validation_cases = self._create_validation_cases_enhanced(method_name, analysis, category, priority)
        test_cases.extend(validation_cases)

        # 5. å®‰å…¨æµ‹è¯•
        security_cases = self._create_security_test_cases(method_name, analysis, category, priority)
        test_cases.extend(security_cases)

        # 6. æ€§èƒ½æµ‹è¯•
        performance_cases = self._create_performance_test_cases(method_name, analysis, category, priority)
        test_cases.extend(performance_cases)

        # 7. æ¨¡å¼ç‰¹å®šçš„æµ‹è¯•ç”¨ä¾‹
        pattern_cases = self._create_pattern_specific_tests(method_name, patterns, category, priority)
        test_cases.extend(pattern_cases)

        return test_cases

    def _determine_test_category(
        self,
        method_name: str,
        patterns: Dict[str, Any],
        project_structure: Dict[str, Any],
    ) -> TestCategory:
        """ç¡®å®šæµ‹è¯•ç±»åˆ«"""
        method_lower = method_name.lower()

        # APIå¤„ç†å‡½æ•°
        if any(pattern in method_lower for pattern in ["get_", "post_", "put_", "delete_", "api_", "endpoint"]):
            return TestCategory.INTEGRATION

        # æ•°æ®å¤„ç†å‡½æ•°
        elif any(pattern in method_lower for pattern in ["calculate_", "process_", "analyze_", "transform_"]):
            return TestCategory.PERFORMANCE

        # ä¸šåŠ¡é€»è¾‘å‡½æ•°
        elif any(pattern in method_lower for pattern in ["get_", "set_", "update_", "save_", "delete_"]):
            return TestCategory.UNIT

        # å®‰å…¨ç›¸å…³å‡½æ•°
        elif any(pattern in method_lower for pattern in ["auth_", "validate_", "encrypt_", "decrypt_"]):
            return TestCategory.SECURITY

        else:
            return TestCategory.UNIT

    def _determine_test_priority(self, analysis: AnalysisResult) -> TestPriority:
        """ç¡®å®šæµ‹è¯•ä¼˜å…ˆçº§"""
        # åŸºäºé£é™©ç­‰çº§ã€å¤æ‚åº¦å’Œå®‰å…¨/æ€§èƒ½é—®é¢˜
        priority_score = 0

        # é£é™©ç­‰çº§æƒé‡
        if analysis.risk_level == "high":
            priority_score += 3
        elif analysis.risk_level == "medium":
            priority_score += 2

        # å¤æ‚åº¦æƒé‡
        if analysis.cyclomatic_complexity > 15:
            priority_score += 3
        elif analysis.cyclomatic_complexity > 8:
            priority_score += 2

        # å®‰å…¨é—®é¢˜æƒé‡
        priority_score += len(analysis.security_issues) * 2

        # æ€§èƒ½é—®é¢˜æƒé‡
        priority_score += len(analysis.performance_issues) * 1

        # å¯ç»´æŠ¤æ€§åˆ†æ•°æƒé‡
        if analysis.maintainability_score < 0.5:
            priority_score += 2

        # ç¡®å®šä¼˜å…ˆçº§
        if priority_score >= 8:
            return TestPriority.CRITICAL
        elif priority_score >= 5:
            return TestPriority.HIGH
        elif priority_score >= 3:
            return TestPriority.MEDIUM
        else:
            return TestPriority.LOW

    async def _create_normal_cases_enhanced(
        self,
        method_name: str,
        analysis: AnalysisResult,
        category: TestCategory,
        priority: TestPriority,
    ) -> List[TestCase]:
        """åˆ›å»ºæ­£å¸¸æµ‹è¯•ç”¨ä¾‹"""
        test_cases = []

        # åŸºç¡€åŠŸèƒ½æµ‹è¯•
        basic_case = TestCase(
            name=f"test_{method_name}_basic",
            description=f"åŸºæœ¬åŠŸèƒ½æµ‹è¯•: {method_name}",
            code=self._generate_basic_test_case(analysis),
            category=category,
            priority=priority,
            method_name=method_name,
            coverage=["normal_input"],
            complexity_score=analysis.cyclomatic_complexity,
            metadata={"complexity": analysis.complexity, "type": "basic"},
        )
        test_cases.append(basic_case)

        # å‚æ•°éªŒè¯æµ‹è¯•
        if analysis.parameters:
            param_validation_case = TestCase(
                name=f"test_{method_name}_parameter_validation",
                description=f"å‚æ•°éªŒè¯æµ‹è¯•: {method_name}",
                code=self._generate_parameter_validation_test(analysis),
                category=TestCategory.UNIT,
                priority=self._adjust_priority(priority, 1),
                method_name=method_name,
                coverage=["parameter_validation"],
                complexity_score=analysis.cyclomatic_complexity,
                metadata={
                    "complexity": analysis.complexity,
                    "type": "parameter_validation",
                },
            )
            test_cases.append(param_validation_case)

        # è¿”å›å€¼æµ‹è¯•
        return_case = TestCase(
            name=f"test_{method_name}_return_validation",
            description=f"è¿”å›å€¼éªŒè¯æµ‹è¯•: {method_name}",
            code=self._generate_return_validation_test(analysis),
            category=TestCategory.UNIT,
            priority=self._adjust_priority(priority, 1),
            method_name=method_name,
            coverage=["return_validation"],
            complexity_score=analysis.cyclomatic_complexity,
            metadata={"complexity": analysis.complexity, "type": "return_validation"},
        )
        test_cases.append(return_case)

        return test_cases

    async def _create_boundary_cases_enhanced(
        self,
        method_name: str,
        analysis: AnalysisResult,
        category: TestCategory,
        priority: TestPriority,
    ) -> List[TestCase]:
        """åˆ›å»ºè¾¹ç•Œæµ‹è¯•ç”¨ä¾‹"""
        test_cases = []

        # è¾¹ç•Œæ¡ä»¶æµ‹è¯•
        if analysis.complexity > 3:
            boundary_case = TestCase(
                name=f"test_{method_name}_boundary",
                description=f"è¾¹ç•Œæ¡ä»¶æµ‹è¯•: {method_name}",
                code=self._generate_boundary_test_case(analysis),
                category=TestCategory.INTEGRATION,
                priority=self._adjust_priority(priority, 1),
                method_name=method_name,
                coverage=["boundary_conditions"],
                complexity_score=analysis.cyclomatic_complexity,
                metadata={"complexity": analysis.complexity, "type": "boundary"},
            )
            test_cases.append(boundary_case)

        # æå€¼æµ‹è¯•
        extreme_case = TestCase(
            name=f"test_{method_name}_extreme_values",
            description=f"æå€¼æµ‹è¯•: {method_name}",
            code=self._generate_extreme_values_test(analysis),
            category=TestCategory.PERFORMANCE,
            priority=self._adjust_priority(priority, 2),
            method_name=method_name,
            coverage=["extreme_values"],
            complexity_score=analysis.cyclomatic_complexity,
            metadata={"complexity": analysis.complexity, "type": "extreme_values"},
        )
        test_cases.append(extreme_case)

        # ç©ºå€¼æµ‹è¯•
        null_case = TestCase(
            name=f"test_{method_name}_null_values",
            description=f"ç©ºå€¼æµ‹è¯•: {method_name}",
            code=self._generate_null_values_test(analysis),
            category=TestCategory.UNIT,
            priority=self._adjust_priority(priority, 1),
            method_name=method_name,
            coverage=["null_values"],
            complexity_score=analysis.cyclomatic_complexity,
            metadata={"complexity": analysis.complexity, "type": "null_values"},
        )
        test_cases.append(null_case)

        return test_cases

    async def _create_exception_cases_enhanced(
        self,
        method_name: str,
        analysis: AnalysisResult,
        category: TestCategory,
        priority: TestPriority,
    ) -> List[TestCase]:
        """åˆ›å»ºå¼‚å¸¸æµ‹è¯•ç”¨ä¾‹"""
        test_cases = []

        # å¼‚å¸¸å¤„ç†æµ‹è¯•
        exception_case = TestCase(
            name=f"test_{method_name}_exceptions",
            description=f"å¼‚å¸¸å¤„ç†æµ‹è¯•: {method_name}",
            code=self._generate_exception_test_case(analysis),
            category=TestCategory.SECURITY,
            priority=self._adjust_priority(priority, 2),
            method_name=method_name,
            coverage=["exception_handling"],
            complexity_score=analysis.cyclomatic_complexity,
            metadata={
                "complexity": analysis.complexity,
                "security_issues": analysis.security_issues,
                "performance_issues": analysis.performance_issues,
            },
        )
        test_cases.append(exception_case)

        # é”™è¯¯ä¼ æ’­æµ‹è¯•
        error_propagation_case = TestCase(
            name=f"test_{method_name}_error_propagation",
            description=f"é”™è¯¯ä¼ æ’­æµ‹è¯•: {method_name}",
            code=self._generate_error_propagation_test(analysis),
            category=TestCategory.INTEGRATION,
            priority=self._adjust_priority(priority, 2),
            method_name=method_name,
            coverage=["error_propagation"],
            complexity_score=analysis.cyclomatic_complexity,
            metadata={"complexity": analysis.complexity, "type": "error_propagation"},
        )
        test_cases.append(error_propagation_case)

        # èµ„æºæ¸…ç†æµ‹è¯•
        if analysis.performance_issues and any("resource" in issue for issue in analysis.performance_issues):
            cleanup_case = TestCase(
                name=f"test_{method_name}_resource_cleanup",
                description=f"èµ„æºæ¸…ç†æµ‹è¯•: {method_name}",
                code=self._generate_resource_cleanup_test(analysis),
                category=TestCategory.PERFORMANCE,
                priority=self._adjust_priority(priority, 1),
                method_name=method_name,
                coverage=["resource_cleanup"],
                complexity_score=analysis.cyclomatic_complexity,
                metadata={
                    "complexity": analysis.complexity,
                    "type": "resource_cleanup",
                },
            )
            test_cases.append(cleanup_case)

        return test_cases

    async def _create_validation_cases_enhanced(
        self,
        method_name: str,
        analysis: AnalysisResult,
        category: TestCategory,
        priority: TestPriority,
    ) -> List[TestCase]:
        """åˆ›å»ºéªŒè¯æµ‹è¯•ç”¨ä¾‹"""
        test_cases = []

        # è¾“å…¥éªŒè¯æµ‹è¯•
        input_validation_case = TestCase(
            name=f"test_{method_name}_input_validation",
            description=f"è¾“å…¥éªŒè¯æµ‹è¯•: {method_name}",
            code=self._generate_input_validation_test(analysis),
            category=TestCategory.SECURITY,
            priority=self._adjust_priority(priority, 1),
            method_name=method_name,
            coverage=["input_validation"],
            complexity_score=analysis.cyclomatic_complexity,
            metadata={"complexity": analysis.complexity, "type": "input_validation"},
        )
        test_cases.append(input_validation_case)

        # ç±»å‹éªŒè¯æµ‹è¯•
        type_validation_case = TestCase(
            name=f"test_{method_name}_type_validation",
            description=f"ç±»å‹éªŒè¯æµ‹è¯•: {method_name}",
            code=self._generate_type_validation_test(analysis),
            category=TestCategory.UNIT,
            priority=self._adjust_priority(priority, 1),
            method_name=method_name,
            coverage=["type_validation"],
            complexity_score=analysis.cyclomatic_complexity,
            metadata={"complexity": analysis.complexity, "type": "type_validation"},
        )
        test_cases.append(type_validation_case)

        # æ ¼å¼éªŒè¯æµ‹è¯•
        format_validation_case = TestCase(
            name=f"test_{method_name}_format_validation",
            description=f"æ ¼å¼éªŒè¯æµ‹è¯•: {method_name}",
            code=self._generate_format_validation_test(analysis),
            category=TestCategory.UNIT,
            priority=self._adjust_priority(priority, 2),
            method_name=method_name,
            coverage=["format_validation"],
            complexity_score=analysis.cyclomatic_complexity,
            metadata={"complexity": analysis.complexity, "type": "format_validation"},
        )
        test_cases.append(format_validation_case)

        return test_cases

    async def _create_security_test_cases(
        self,
        method_name: str,
        analysis: AnalysisResult,
        category: TestCategory,
        priority: TestPriority,
    ) -> List[TestCase]:
        """åˆ›å»ºå®‰å…¨æµ‹è¯•ç”¨ä¾‹"""
        test_cases = []

        # SQLæ³¨å…¥æµ‹è¯•
        if analysis.security_issues and any("sql" in issue.lower() for issue in analysis.security_issues):
            sql_injection_case = TestCase(
                name=f"test_{method_name}_sql_injection",
                description=f"SQLæ³¨å…¥é˜²æŠ¤æµ‹è¯•: {method_name}",
                code=self._generate_sql_injection_test(analysis),
                category=TestCategory.SECURITY,
                priority=TestPriority.CRITICAL,
                method_name=method_name,
                coverage=["sql_injection"],
                complexity_score=analysis.cyclomatic_complexity,
                metadata={"complexity": analysis.complexity, "type": "sql_injection"},
            )
            test_cases.append(sql_injection_case)

        # XSSæµ‹è¯•
        xss_case = TestCase(
            name=f"test_{method_name}_xss_protection",
            description=f"XSSé˜²æŠ¤æµ‹è¯•: {method_name}",
            code=self._generate_xss_test(analysis),
            category=TestCategory.SECURITY,
            priority=TestPriority.CRITICAL,
            method_name=method_name,
            coverage=["xss_protection"],
            complexity_score=analysis.cyclomatic_complexity,
            metadata={"complexity": analysis.complexity, "type": "xss_protection"},
        )
        test_cases.append(xss_case)

        # CSRFæµ‹è¯•
        csrf_case = TestCase(
            name=f"test_{method_name}_csrf_protection",
            description=f"CSRFé˜²æŠ¤æµ‹è¯•: {method_name}",
            code=self._generate_csrf_test(analysis),
            category=TestCategory.SECURITY,
            priority=TestPriority.CRITICAL,
            method_name=method_name,
            coverage=["csrf_protection"],
            complexity_score=analysis.cyclomatic_complexity,
            metadata={"complexity": analysis.complexity, "type": "csrf_protection"},
        )
        test_cases.append(csrf_case)

        # æƒé™éªŒè¯æµ‹è¯•
        auth_case = TestCase(
            name=f"test_{method_name}_authorization",
            description=f"æƒé™éªŒè¯æµ‹è¯•: {method_name}",
            code=self._generate_authorization_test(analysis),
            category=TestCategory.SECURITY,
            priority=TestPriority.HIGH,
            method_name=method_name,
            coverage=["authorization"],
            complexity_score=analysis.cyclomatic_complexity,
            metadata={"complexity": analysis.complexity, "type": "authorization"},
        )
        test_cases.append(auth_case)

        return test_cases

    async def _create_performance_test_cases(
        self,
        method_name: str,
        analysis: AnalysisResult,
        category: TestCategory,
        priority: TestPriority,
    ) -> List[TestCase]:
        """åˆ›å»ºæ€§èƒ½æµ‹è¯•ç”¨ä¾‹"""
        test_cases = []

        # æ€§èƒ½åŸºå‡†æµ‹è¯•
        performance_case = TestCase(
            name=f"test_{method_name}_performance",
            description=f"æ€§èƒ½åŸºå‡†æµ‹è¯•: {method_name}",
            code=self._generate_performance_test(analysis),
            category=TestCategory.PERFORMANCE,
            priority=self._adjust_priority(priority, 1),
            method_name=method_name,
            coverage=["performance_benchmark"],
            complexity_score=analysis.cyclomatic_complexity,
            metadata={
                "complexity": analysis.complexity,
                "type": "performance_benchmark",
            },
        )
        test_cases.append(performance_case)

        # å†…å­˜ä½¿ç”¨æµ‹è¯•
        memory_case = TestCase(
            name=f"test_{method_name}_memory_usage",
            description=f"å†…å­˜ä½¿ç”¨æµ‹è¯•: {method_name}",
            code=self._generate_memory_usage_test(analysis),
            category=TestCategory.PERFORMANCE,
            priority=self._adjust_priority(priority, 2),
            method_name=method_name,
            coverage=["memory_usage"],
            complexity_score=analysis.cyclomatic_complexity,
            metadata={"complexity": analysis.complexity, "type": "memory_usage"},
        )
        test_cases.append(memory_case)

        # å¹¶å‘æµ‹è¯•
        if analysis.cyclomatic_complexity > 5:
            concurrency_case = TestCase(
                name=f"test_{method_name}_concurrency",
                description=f"å¹¶å‘æµ‹è¯•: {method_name}",
                code=self._generate_concurrency_test(analysis),
                category=TestCategory.PERFORMANCE,
                priority=self._adjust_priority(priority, 1),
                method_name=method_name,
                coverage=["concurrency"],
                complexity_score=analysis.cyclomatic_complexity,
                metadata={"complexity": analysis.complexity, "type": "concurrency"},
            )
            test_cases.append(concurrency_case)

        # è¶…æ—¶æµ‹è¯•
        timeout_case = TestCase(
            name=f"test_{method_name}_timeout",
            description=f"è¶…æ—¶æµ‹è¯•: {method_name}",
            code=self._generate_timeout_test(analysis),
            category=TestCategory.PERFORMANCE,
            priority=self._adjust_priority(priority, 2),
            method_name=method_name,
            coverage=["timeout"],
            complexity_score=analysis.cyclomatic_complexity,
            metadata={"complexity": analysis.complexity, "type": "timeout"},
        )
        test_cases.append(timeout_case)

        return test_cases

    async def _create_pattern_specific_tests(
        self,
        method_name: str,
        patterns: Dict[str, Any],
        category: TestCategory,
        priority: TestPriority,
    ) -> List[TestCase]:
        """åˆ›å»ºç‰¹å®šæ¨¡å¼æµ‹è¯•"""
        test_cases = []

        # é€’å½’å‡½æ•°æµ‹è¯•
        if self._is_recursive_function(method_name):
            recursive_case = TestCase(
                name=f"test_{method_name}_recursive",
                description=f"é€’å½’å‡½æ•°æµ‹è¯•: {method_name}",
                code=self._generate_recursive_test(method_name),
                category=TestCategory.PERFORMANCE,
                priority=self._adjust_priority(priority, 1),
                method_name=method_name,
                coverage=["recursive"],
                complexity_score=10.0,  # é€’å½’å‡½æ•°å¤æ‚åº¦è¾ƒé«˜
                metadata={"type": "recursive"},
            )
            test_cases.append(recursive_case)

        # å›è°ƒå‡½æ•°æµ‹è¯•
        if self._has_callback_function(method_name):
            callback_case = TestCase(
                name=f"test_{method_name}_callback",
                description=f"å›è°ƒå‡½æ•°æµ‹è¯•: {method_name}",
                code=self._generate_callback_test(method_name),
                category=TestCategory.INTEGRATION,
                priority=self._adjust_priority(priority, 1),
                method_name=method_name,
                coverage=["callback"],
                complexity_score=8.0,
                metadata={"type": "callback"},
            )
            test_cases.append(callback_case)

        # å¼‚æ­¥å‡½æ•°æµ‹è¯•
        if self._is_async_function(method_name):
            async_case = TestCase(
                name=f"test_{method_name}_async",
                description=f"å¼‚æ­¥å‡½æ•°æµ‹è¯•: {method_name}",
                code=self._generate_async_test(method_name),
                category=TestCategory.INTEGRATION,
                priority=self._adjust_priority(priority, 1),
                method_name=method_name,
                coverage=["async"],
                complexity_score=8.0,
                metadata={"type": "async"},
            )
            test_cases.append(async_case)

        # çŠ¶æ€ç®¡ç†æµ‹è¯•
        if self._has_state_management(method_name):
            state_case = TestCase(
                name=f"test_{method_name}_state_management",
                description=f"çŠ¶æ€ç®¡ç†æµ‹è¯•: {method_name}",
                code=self._generate_state_management_test(method_name),
                category=TestCategory.INTEGRATION,
                priority=self._adjust_priority(priority, 2),
                method_name=method_name,
                coverage=["state_management"],
                complexity_score=9.0,
                metadata={"type": "state_management"},
            )
            test_cases.append(state_case)

        # äº‹åŠ¡æµ‹è¯•
        if self._has_transaction_logic(method_name):
            transaction_case = TestCase(
                name=f"test_{method_name}_transaction",
                description=f"äº‹åŠ¡æµ‹è¯•: {method_name}",
                code=self._generate_transaction_test(method_name),
                category=TestCategory.INTEGRATION,
                priority=self._adjust_priority(priority, 1),
                method_name=method_name,
                coverage=["transaction"],
                complexity_score=8.0,
                metadata={"type": "transaction"},
            )
            test_cases.append(transaction_case)

        return test_cases

    # Helper methods for test case generation
    def _adjust_priority(self, priority: TestPriority, adjustment: int) -> TestPriority:
        """è°ƒæ•´ä¼˜å…ˆçº§"""
        priority_order = [
            TestPriority.LOW,
            TestPriority.MEDIUM,
            TestPriority.HIGH,
            TestPriority.CRITICAL,
        ]
        current_index = priority_order.index(priority)
        new_index = max(0, min(3, current_index - adjustment))
        return priority_order[new_index]

    def _generate_basic_test_case(self, analysis: AnalysisResult) -> str:
        """ç”ŸæˆåŸºç¡€æµ‹è¯•ç”¨ä¾‹"""
        return f"""
def test_{analysis.method_name}_basic():
    # åŸºæœ¬åŠŸèƒ½æµ‹è¯•
    # TODO: å®ç°å…·ä½“çš„æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_parameter_validation_test(self, analysis: AnalysisResult) -> str:
        """ç”Ÿæˆå‚æ•°éªŒè¯æµ‹è¯•"""
        return f"""
def test_{analysis.method_name}_parameter_validation():
    # å‚æ•°éªŒè¯æµ‹è¯•
    # TODO: å®ç°å‚æ•°éªŒè¯é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_return_validation_test(self, analysis: AnalysisResult) -> str:
        """ç”Ÿæˆè¿”å›å€¼éªŒè¯æµ‹è¯•"""
        return f"""
def test_{analysis.method_name}_return_validation():
    # è¿”å›å€¼éªŒè¯æµ‹è¯•
    # TODO: å®ç°è¿”å›å€¼éªŒè¯é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_boundary_test_case(self, analysis: AnalysisResult) -> str:
        """ç”Ÿæˆè¾¹ç•Œæµ‹è¯•ç”¨ä¾‹"""
        return f"""
def test_{analysis.method_name}_boundary():
    # è¾¹ç•Œæ¡ä»¶æµ‹è¯•
    # TODO: å®ç°è¾¹ç•Œæ¡ä»¶æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_extreme_values_test(self, analysis: AnalysisResult) -> str:
        """ç”Ÿæˆæå€¼æµ‹è¯•"""
        return f"""
def test_{analysis.method_name}_extreme_values():
    # æå€¼æµ‹è¯•
    # TODO: å®ç°æå€¼æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_null_values_test(self, analysis: AnalysisResult) -> str:
        """ç”Ÿæˆç©ºå€¼æµ‹è¯•"""
        return f"""
def test_{analysis.method_name}_null_values():
    # ç©ºå€¼æµ‹è¯•
    # TODO: å®ç°ç©ºå€¼æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_exception_test_case(self, analysis: AnalysisResult) -> str:
        """ç”Ÿæˆå¼‚å¸¸æµ‹è¯•ç”¨ä¾‹"""
        return f"""
def test_{analysis.method_name}_exceptions():
    # å¼‚å¸¸å¤„ç†æµ‹è¯•
    # TODO: å®ç°å¼‚å¸¸å¤„ç†æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_error_propagation_test(self, analysis: AnalysisResult) -> str:
        """ç”Ÿæˆé”™è¯¯ä¼ æ’­æµ‹è¯•"""
        return f"""
def test_{analysis.method_name}_error_propagation():
    # é”™è¯¯ä¼ æ’­æµ‹è¯•
    # TODO: å®ç°é”™è¯¯ä¼ æ’­æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_resource_cleanup_test(self, analysis: AnalysisResult) -> str:
        """ç”Ÿæˆèµ„æºæ¸…ç†æµ‹è¯•"""
        return f"""
def test_{analysis.method_name}_resource_cleanup():
    # èµ„æºæ¸…ç†æµ‹è¯•
    # TODO: å®ç°èµ„æºæ¸…ç†æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_input_validation_test(self, analysis: AnalysisResult) -> str:
        """ç”Ÿæˆè¾“å…¥éªŒè¯æµ‹è¯•"""
        return f"""
def test_{analysis.method_name}_input_validation():
    # è¾“å…¥éªŒè¯æµ‹è¯•
    # TODO: å®ç°è¾“å…¥éªŒè¯æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_type_validation_test(self, analysis: AnalysisResult) -> str:
        """ç”Ÿæˆç±»å‹éªŒè¯æµ‹è¯•"""
        return f"""
def test_{analysis.method_name}_type_validation():
    # ç±»å‹éªŒè¯æµ‹è¯•
    # TODO: å®ç°ç±»å‹éªŒè¯æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_format_validation_test(self, analysis: AnalysisResult) -> str:
        """ç”Ÿæˆæ ¼å¼éªŒè¯æµ‹è¯•"""
        return f"""
def test_{analysis.method_name}_format_validation():
    # æ ¼å¼éªŒè¯æµ‹è¯•
    # TODO: å®ç°æ ¼å¼éªŒè¯æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_sql_injection_test(self, analysis: AnalysisResult) -> str:
        """ç”ŸæˆSQLæ³¨å…¥æµ‹è¯•"""
        return f"""
def test_{analysis.method_name}_sql_injection():
    # SQLæ³¨å…¥é˜²æŠ¤æµ‹è¯•
    # TODO: å®ç°SQLæ³¨å…¥é˜²æŠ¤æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_xss_test(self, analysis: AnalysisResult) -> str:
        """ç”ŸæˆXSSæµ‹è¯•"""
        return f"""
def test_{analysis.method_name}_xss_protection():
    # XSSé˜²æŠ¤æµ‹è¯•
    # TODO: å®ç°XSSé˜²æŠ¤æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_csrf_test(self, analysis: AnalysisResult) -> str:
        """ç”ŸæˆCSRFæµ‹è¯•"""
        return f"""
def test_{analysis.method_name}_csrf_protection():
    # CSRFé˜²æŠ¤æµ‹è¯•
    # TODO: å®ç°CSRFé˜²æŠ¤æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_authorization_test(self, analysis: AnalysisResult) -> str:
        """ç”Ÿæˆæƒé™éªŒè¯æµ‹è¯•"""
        return f"""
def test_{analysis.method_name}_authorization():
    # æƒé™éªŒè¯æµ‹è¯•
    # TODO: å®ç°æƒé™éªŒè¯æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_performance_test(self, analysis: AnalysisResult) -> str:
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•"""
        return f"""
def test_{analysis.method_name}_performance():
    # æ€§èƒ½åŸºå‡†æµ‹è¯•
    # TODO: å®ç°æ€§èƒ½åŸºå‡†æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_memory_usage_test(self, analysis: AnalysisResult) -> str:
        """ç”Ÿæˆå†…å­˜ä½¿ç”¨æµ‹è¯•"""
        return f"""
def test_{analysis.method_name}_memory_usage():
    # å†…å­˜ä½¿ç”¨æµ‹è¯•
    # TODO: å®ç°å†…å­˜ä½¿ç”¨æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_concurrency_test(self, analysis: AnalysisResult) -> str:
        """ç”Ÿæˆå¹¶å‘æµ‹è¯•"""
        return f"""
def test_{analysis.method_name}_concurrency():
    # å¹¶å‘æµ‹è¯•
    # TODO: å®ç°å¹¶å‘æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_timeout_test(self, analysis: AnalysisResult) -> str:
        """ç”Ÿæˆè¶…æ—¶æµ‹è¯•"""
        return f"""
def test_{analysis.method_name}_timeout():
    # è¶…æ—¶æµ‹è¯•
    # TODO: å®ç°è¶…æ—¶æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_recursive_test(self, method_name: str) -> str:
        """ç”Ÿæˆé€’å½’æµ‹è¯•"""
        return f"""
def test_{method_name}_recursive():
    # é€’å½’å‡½æ•°æµ‹è¯•
    # TODO: å®ç°é€’å½’æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_callback_test(self, method_name: str) -> str:
        """ç”Ÿæˆå›è°ƒæµ‹è¯•"""
        return f"""
def test_{method_name}_callback():
    # å›è°ƒå‡½æ•°æµ‹è¯•
    # TODO: å®ç°å›è°ƒæµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_async_test(self, method_name: str) -> str:
        """ç”Ÿæˆå¼‚æ­¥æµ‹è¯•"""
        return f"""
def test_{method_name}_async():
    # å¼‚æ­¥å‡½æ•°æµ‹è¯•
    # TODO: å®ç°å¼‚æ­¥æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_state_management_test(self, method_name: str) -> str:
        """ç”ŸæˆçŠ¶æ€ç®¡ç†æµ‹è¯•"""
        return f"""
def test_{method_name}_state_management():
    # çŠ¶æ€ç®¡ç†æµ‹è¯•
    # TODO: å®ç°çŠ¶æ€ç®¡ç†æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    def _generate_transaction_test(self, method_name: str) -> str:
        """ç”Ÿæˆäº‹åŠ¡æµ‹è¯•"""
        return f"""
def test_{method_name}_transaction():
    # äº‹åŠ¡æµ‹è¯•
    # TODO: å®ç°äº‹åŠ¡æµ‹è¯•é€»è¾‘
    assert True  # å ä½ç¬¦ï¼Œéœ€è¦å®ç°å…·ä½“æµ‹è¯•
"""

    # Pattern detection methods
    def _is_recursive_function(self, method_name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºé€’å½’å‡½æ•°"""
        # ç®€åŒ–çš„é€’å½’æ£€æµ‹é€»è¾‘
        recursive_patterns = [
            "recursive",
            "fibonacci",
            "factorial",
            "tree_",
            "traverse",
        ]
        return any(pattern in method_name.lower() for pattern in recursive_patterns)

    def _has_callback_function(self, method_name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å›è°ƒå‡½æ•°"""
        callback_patterns = ["callback", "handler", "listener", "observer"]
        return any(pattern in method_name.lower() for pattern in callback_patterns)

    def _is_async_function(self, method_name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºå¼‚æ­¥å‡½æ•°"""
        return method_name.startswith("async_") or "async" in method_name.lower()

    def _has_state_management(self, method_name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«çŠ¶æ€ç®¡ç†"""
        state_patterns = ["state", "cache", "session", "context", "manager"]
        return any(pattern in method_name.lower() for pattern in state_patterns)

    def _has_transaction_logic(self, method_name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«äº‹åŠ¡é€»è¾‘"""
        transaction_patterns = ["transaction", "commit", "rollback", "save", "update"]
        return any(pattern in method_name.lower() for pattern in transaction_patterns)

    async def _optimize_test_cases(self, test_cases: List[TestCase]) -> List[TestCase]:
        """ä¼˜åŒ–æµ‹è¯•ç”¨ä¾‹"""
        optimized = []

        for test_case in test_cases:
            # ä¼°ç®—æ‰§è¡Œæ—¶é—´
            test_case.execution_time_estimate = self._estimate_execution_time(test_case)

            # è®¡ç®—ä¸ç¨³å®šåˆ†æ•°
            test_case.flakiness_score = self._calculate_flakiness_score(test_case)

            # æ ¹æ®ä¼˜å…ˆçº§å’Œå¤æ‚åº¦è¿‡æ»¤
            if test_case.priority != TestPriority.LOW or test_case.complexity_score < 2.0:
                optimized.append(test_case)

        # å»é‡
        unique_cases = {}
        for case in optimized:
            case_hash = self._generate_test_hash(case)
            if case_hash not in unique_cases:
                unique_cases[case_hash] = case

        return list(unique_cases.values())

    def _generate_test_hash(self, test_case: TestCase) -> str:
        """ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å“ˆå¸Œ"""
        content = f"{test_case.name}{test_case.description}{test_case.code}"
        return hashlib.md5(content.encode()).hexdigest()

    def _estimate_execution_time(self, test_case: TestCase) -> float:
        """ä¼°ç®—æµ‹è¯•æ‰§è¡Œæ—¶é—´"""
        # åŸºäºæµ‹è¯•ç±»å‹å’Œå¤æ‚åº¦ä¼°ç®—
        base_time = 0.1  # åŸºç¡€æ—¶é—´

        if test_case.category == TestCategory.PERFORMANCE:
            base_time = 1.0
        elif test_case.category == TestCategory.INTEGRATION:
            base_time = 0.5
        elif test_case.category == TestCategory.SECURITY:
            base_time = 0.3

        # å¤æ‚åº¦è°ƒæ•´
        complexity_factor = test_case.complexity_score * 0.1

        return base_time + complexity_factor

    def _calculate_flakiness_score(self, test_case: TestCase) -> float:
        """è®¡ç®—æµ‹è¯•ä¸ç¨³å®šåˆ†æ•°"""
        # åŸºäºå„ç§å› ç´ è®¡ç®—
        factors = []

        # ç½‘ç»œç›¸å…³æµ‹è¯•
        if "api" in test_case.name.lower() or "http" in test_case.name.lower():
            factors.append(0.3)

        # å¼‚æ­¥æµ‹è¯•
        if "async" in test_case.code.lower():
            factors.append(0.2)

        # æ—¶é—´ç›¸å…³æµ‹è¯•
        if "time" in test_case.name.lower() or "date" in test_case.name.lower():
            factors.append(0.2)

        # å¤–éƒ¨ä¾èµ–
        if "requests" in test_case.code.lower() or "fetch" in test_case.code.lower():
            factors.append(0.3)

        return min(sum(factors), 1.0)

    def _calculate_complexity(self, node: ast.FunctionDef) -> int:
        """è®¡ç®—åœˆå¤æ‚åº¦"""
        complexity = 1  # åŸºç¡€å¤æ‚åº¦

        # è®¡ç®—æ¡ä»¶è¯­å¥
        for child in ast.walk(node):
            if isinstance(
                child,
                (
                    ast.If,
                    ast.For,
                    ast.While,
                    ast.ExceptHandler,
                    ast.With,
                    ast.AsyncWith,
                ),
            ):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1

        return complexity

    def _get_annotation_type(self, annotation: Optional[ast.AST]) -> str:
        """è·å–å‚æ•°ç±»å‹æ³¨è§£"""
        if annotation is None:
            return "Any"

        if isinstance(annotation, ast.Name):
            return annotation.id
        elif isinstance(annotation, ast.Subscript):
            return f"{annotation.value.id}[...]"
        elif isinstance(annotation, ast.Constant):
            return str(annotation.value)
        else:
            return "Unknown"

    def _generate_test_cases_for_method(
        self, method_name: str, method_node: ast.FunctionDef, analysis: Dict[str, Any]
    ) -> List[TestCase]:
        """ä¸ºæ–¹æ³•ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"""
        test_cases = []

        # 1. æ­£å¸¸æƒ…å†µæµ‹è¯•ç”¨ä¾‹
        normal_case = self._create_normal_case(method_name, analysis)
        test_cases.append(normal_case)

        # 2. è¾¹ç•Œæ¡ä»¶æµ‹è¯•ç”¨ä¾‹
        boundary_cases = self._create_boundary_cases(method_name, analysis)
        test_cases.extend(boundary_cases)

        # 3. å¼‚å¸¸æƒ…å†µæµ‹è¯•ç”¨ä¾‹
        exception_cases = self._create_exception_cases(method_name, analysis)
        test_cases.extend(exception_cases)

        # 4. å‚æ•°éªŒè¯æµ‹è¯•ç”¨ä¾‹
        validation_cases = self._create_validation_cases(method_name, analysis)
        test_cases.extend(validation_cases)

        return test_cases

    def _create_normal_case(self, method_name: str, analysis: Dict[str, Any]) -> TestCase:
        """åˆ›å»ºæ­£å¸¸æƒ…å†µæµ‹è¯•ç”¨ä¾‹"""
        test_code = f"""
def test_{method_name}_normal_case():
    # æ­£å¸¸æƒ…å†µæµ‹è¯•
    args = self._generate_normal_args({analysis["arguments"]})
    result = {method_name}(*args)
    assert result is not None
    assert isinstance(result, {self._get_expected_return_type(analysis)})
"""

        return TestCase(
            name=f"test_{method_name}_normal_case",
            description="éªŒè¯æ–¹æ³•åœ¨æ­£å¸¸è¾“å…¥ä¸‹çš„è¡Œä¸º",
            code=test_code.strip(),
            method_name=method_name,
            coverage=["normal_input"],
            complexity_score=1.0,
            created_at=datetime.now(),
            metadata={"test_type": "normal", "priority": "high"},
        )

    def _create_boundary_cases(self, method_name: str, analysis: Dict[str, Any]) -> List[TestCase]:
        """åˆ›å»ºè¾¹ç•Œæ¡ä»¶æµ‹è¯•ç”¨ä¾‹"""
        cases = []

        # ç©ºå€¼è¾¹ç•Œæµ‹è¯•
        boundary_code = f"""
def test_{method_name}_boundary_cases():
    # è¾¹ç•Œæ¡ä»¶æµ‹è¯•
    # 1. ç©ºå€¼æµ‹è¯•
    with pytest.raises(ValueError):
        {method_name}(None, None)

    # 2. ç©ºå­—ç¬¦ä¸²æµ‹è¯•
    empty_args = [""] * len({analysis["arguments"]})
    result = {method_name}(*empty_args)
    assert result is not None
"""

        cases.append(
            TestCase(
                name=f"test_{method_name}_boundary_cases",
                description="éªŒè¯æ–¹æ³•åœ¨è¾¹ç•Œæ¡ä»¶ä¸‹çš„è¡Œä¸º",
                code=boundary_code.strip(),
                method_name=method_name,
                coverage=["boundary_conditions"],
                complexity_score=1.2,
                created_at=datetime.now(),
                metadata={"test_type": "boundary", "priority": "medium"},
            )
        )

        return cases

    def _create_exception_cases(self, method_name: str, analysis: Dict[str, Any]) -> List[TestCase]:
        """åˆ›å»ºå¼‚å¸¸æƒ…å†µæµ‹è¯•ç”¨ä¾‹"""
        exception_code = f"""
def test_{method_name}_exception_cases():
    # å¼‚å¸¸æƒ…å†µæµ‹è¯•
    # 1. æ— æ•ˆå‚æ•°ç±»å‹
    invalid_args = ["invalid"] * len({analysis["arguments"]})
    with pytest.raises(TypeError):
        {method_name}(*invalid_args)

    # 2. è¶…å‡ºèŒƒå›´å‚æ•°
    out_of_range_args = [999999] * len({analysis["arguments"]})
    result = {method_name}(*out_of_range_args)
    # éªŒè¯å¼‚å¸¸å¤„ç†æˆ–é»˜è®¤è¿”å›å€¼
"""

        return [
            TestCase(
                name=f"test_{method_name}_exception_cases",
                description="éªŒè¯æ–¹æ³•åœ¨å¼‚å¸¸è¾“å…¥ä¸‹çš„è¡Œä¸º",
                code=exception_code.strip(),
                method_name=method_name,
                coverage=["exception_handling"],
                complexity_score=1.5,
                created_at=datetime.now(),
                metadata={"test_type": "exception", "priority": "medium"},
            )
        ]

    def _create_validation_cases(self, method_name: str, analysis: Dict[str, Any]) -> List[TestCase]:
        """åˆ›å»ºå‚æ•°éªŒè¯æµ‹è¯•ç”¨ä¾‹"""
        validation_code = f"""
def test_{method_name}_parameter_validation():
    # å‚æ•°éªŒè¯æµ‹è¯•
    # 1. å‚æ•°ç±»å‹éªŒè¯
    for param in {analysis["parameters"]}:
        invalid_value = self._generate_invalid_value(param['type'])
        with pytest.raises((TypeError, ValueError)):
            {method_name}({invalid_value})

    # 2. å‚æ•°èŒƒå›´éªŒè¯
    if any(param['type'] in ['int', 'float'] for param in {analysis["parameters"]}):
        negative_args = [-1] * len({analysis["arguments"]})
        result = {method_name}(*negative_args)
        # éªŒè¯è´Ÿæ•°å¤„ç†
"""

        return [
            TestCase(
                name=f"test_{method_name}_parameter_validation",
                description="éªŒè¯æ–¹æ³•å‚æ•°éªŒè¯é€»è¾‘",
                code=validation_code.strip(),
                method_name=method_name,
                coverage=["parameter_validation"],
                complexity_score=1.3,
                created_at=datetime.now(),
                metadata={"test_type": "validation", "priority": "low"},
            )
        ]

    def _get_expected_return_type(self, analysis: Dict[str, Any]) -> str:
        """è·å–é¢„æœŸè¿”å›ç±»å‹"""
        # åŸºäºæ–¹æ³•åç§°å’Œå‚æ•°æ¨æ–­è¿”å›ç±»å‹
        if "get" in analysis.get("method_name", ""):
            return "dict"
        elif "is" in analysis.get("method_name", ""):
            return "bool"
        elif "calculate" in analysis.get("method_name", ""):
            return "float"
        else:
            return "Any"

    async def optimize_test_suite(self, test_files: List[str]) -> Dict[str, Any]:
        """ä¼˜åŒ–æµ‹è¯•å¥—ä»¶"""
        print("ğŸ¤– AIæ­£åœ¨ä¼˜åŒ–æµ‹è¯•å¥—ä»¶...")

        optimization_results = {}

        for test_file in test_files:
            try:
                with open(test_file, "r", encoding="utf-8") as f:
                    source_code = f.read()

                # åˆ†æç°æœ‰æµ‹è¯•
                analysis = self._analyze_test_file(source_code, test_file)

                # ç”Ÿæˆä¼˜åŒ–å»ºè®®
                suggestions = await self._generate_optimization_suggestions(analysis)

                optimization_results[test_file] = {
                    "analysis": analysis,
                    "suggestions": suggestions,
                    "improvement_score": self._calculate_improvement_score(suggestions),
                }

            except Exception as e:
                print(f"âŒ æµ‹è¯•æ–‡ä»¶ {test_file} åˆ†æå¤±è´¥: {str(e)}")
                optimization_results[test_file] = {"error": str(e)}

        return optimization_results

    def _analyze_test_file(self, source_code: str, file_path: str) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•æ–‡ä»¶"""
        try:
            tree = ast.parse(source_code)

            test_methods = []
            total_lines = len(source_code.split("\n"))

            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef) and node.name.startswith("test_"):
                    test_methods.append(
                        {
                            "name": node.name,
                            "lines": node.end_lineno - node.lineno + 1 if node.end_lineno else 0,
                            "complexity": self._calculate_complexity(node),
                        }
                    )

            return {
                "file_path": file_path,
                "total_lines": total_lines,
                "test_count": len(test_methods),
                "test_methods": test_methods,
                "avg_complexity": sum(m["complexity"] for m in test_methods) / len(test_methods) if test_methods else 0,
                "max_complexity": max(m["complexity"] for m in test_methods) if test_methods else 0,
            }

        except Exception as e:
            return {"error": str(e)}

    async def _generate_optimization_suggestions(self, analysis: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []

        if "avg_complexity" in analysis and analysis["avg_complexity"] > 10:
            suggestions.append("æµ‹è¯•æ–¹æ³•å¤æ‚åº¦è¿‡é«˜ï¼Œå»ºè®®æ‹†åˆ†ä¸ºå¤šä¸ªç®€å•çš„æµ‹è¯•")

        if "max_complexity" in analysis and analysis["max_complexity"] > 20:
            suggestions.append("å­˜åœ¨éå¸¸å¤æ‚çš„æµ‹è¯•æ–¹æ³•ï¼Œè€ƒè™‘ä½¿ç”¨å‚æ•°åŒ–æµ‹è¯•æˆ–é‡æ„")

        if "test_count" in analysis and analysis["test_count"] < 5:
            suggestions.append("æµ‹è¯•è¦†ç›–ç‡è¾ƒä½ï¼Œå»ºè®®å¢åŠ æ›´å¤šæµ‹è¯•ç”¨ä¾‹")

        # æ·»åŠ AIä¼˜åŒ–å»ºè®®
        if analysis.get("test_count", 0) > 0:
            suggestions.extend(
                [
                    "å»ºè®®æ·»åŠ æ•°æ®é©±åŠ¨æµ‹è¯•ä»¥æé«˜è¦†ç›–ç‡",
                    "è€ƒè™‘ä½¿ç”¨pytest.mark.parametrizeè¿›è¡Œå‚æ•°åŒ–æµ‹è¯•",
                    "å»ºè®®æ·»åŠ æ€§èƒ½åŸºå‡†æµ‹è¯•",
                    "è€ƒè™‘æ·»åŠ å¥‘çº¦æµ‹è¯•éªŒè¯APIè§„èŒƒ",
                ]
            )

        return suggestions

    def _calculate_improvement_score(self, suggestions: List[str]) -> float:
        """è®¡ç®—æ”¹è¿›åˆ†æ•°"""
        base_score = 0
        for suggestion in suggestions:
            if "å¤æ‚åº¦" in suggestion:
                base_score += 30
            elif "è¦†ç›–ç‡" in suggestion:
                base_score += 25
            elif "å‚æ•°åŒ–" in suggestion:
                base_score += 20
            elif "æ€§èƒ½" in suggestion:
                base_score += 15
            else:
                base_score += 10

        return min(base_score, 100) / 100.0


class IntelligentTestOptimizer:
    """æ™ºèƒ½æµ‹è¯•ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.ai_generator = AITestGenerator()

    async def optimize_test_coverage(self, module_path: str) -> Dict[str, Any]:
        """ä¼˜åŒ–æµ‹è¯•è¦†ç›–ç‡"""
        print(f"ğŸ¤– AIæ­£åœ¨ä¼˜åŒ– {module_path} çš„æµ‹è¯•è¦†ç›–ç‡...")

        # åˆ†ææºä»£ç 
        source_code = self._read_module_source(module_path)

        # æå–æ‰€æœ‰å¯æµ‹è¯•æ–¹æ³•
        testable_methods = self._extract_testable_methods(source_code)

        # ç”Ÿæˆç¼ºå¤±çš„æµ‹è¯•ç”¨ä¾‹
        generated_tests = []
        for method in testable_methods:
            if not self._has_test_case(method["name"], module_path):
                test_cases = self.ai_generator.generate_test_cases_from_source(source_code, method["name"])
                generated_tests.extend(test_cases)

        # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
        report = {
            "module_path": module_path,
            "total_methods": len(testable_methods),
            "tested_methods": len([m for m in testable_methods if self._has_test_case(m["name"], module_path)]),
            "coverage_percentage": self._calculate_coverage(testable_methods, module_path),
            "generated_tests": len(generated_tests),
            "suggestions": self._generate_coverage_suggestions(testable_methods, module_path),
        }

        return report

    def _read_module_source(self, module_path: str) -> str:
        """è¯»å–æ¨¡å—æºä»£ç """
        try:
            with open(module_path, "r", encoding="utf-8") as f:
                return f.read()
        except Exception as e:
            print(f"âŒ æ— æ³•è¯»å–æ¨¡å—æ–‡ä»¶ {module_path}: {str(e)}")
            return ""

    def _extract_testable_methods(self, source_code: str) -> List[Dict[str, Any]]:
        """æå–å¯æµ‹è¯•æ–¹æ³•"""
        try:
            tree = ast.parse(source_code)
            methods = []

            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    # è·³è¿‡æµ‹è¯•æ–¹æ³•å’Œç§æœ‰æ–¹æ³•
                    if not node.name.startswith("_") and not node.name.startswith("test_"):
                        methods.append(
                            {
                                "name": node.name,
                                "line": node.lineno,
                                "complexity": self.ai_generator._calculate_complexity(node),
                                "args": [arg.arg for arg in node.args.args],
                                "has_return": len(node.body) > 0 and isinstance(node.body[-1], ast.Return),
                            }
                        )

            return methods

        except Exception as e:
            print(f"âŒ æºä»£ç è§£æå¤±è´¥: {str(e)}")
            return []

    def _has_test_case(self, method_name: str, module_path: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²æœ‰æµ‹è¯•ç”¨ä¾‹"""
        test_file = module_path.replace(".py", "_test.py")
        test_dir = module_path.replace(".py", "/tests/test_")

        test_paths = [test_file, test_dir]

        for path in test_paths:
            if Path(path).exists():
                try:
                    with open(path, "r", encoding="utf-8") as f:
                        content = f.read()
                        if f"test_{method_name}" in content:
                            return True
                except Exception:
                    pass

        return False

    def _calculate_coverage(self, methods: List[Dict[str, Any]], module_path: str) -> float:
        """è®¡ç®—æµ‹è¯•è¦†ç›–ç‡"""
        tested_count = len([m for m in methods if self._has_test_case(m["name"], module_path)])
        total_count = len(methods)
        return (tested_count / total_count * 100) if total_count > 0 else 0

    def _generate_coverage_suggestions(self, methods: List[Dict[str, Any]], module_path: str) -> List[str]:
        """ç”Ÿæˆè¦†ç›–ç‡ä¼˜åŒ–å»ºè®®"""
        suggestions = []

        # åˆ†ææœªæµ‹è¯•çš„æ–¹æ³•
        untested_methods = [m for m in methods if not self._has_test_case(m["name"], module_path)]

        if len(untested_methods) > 0:
            # æŒ‰å¤æ‚åº¦æ’åº
            untested_methods.sort(key=lambda x: x["complexity"], reverse=True)

            # ä¸ºé«˜å¤æ‚åº¦æ–¹æ³•ç”Ÿæˆä¼˜å…ˆçº§å»ºè®®
            high_complexity_untested = [m for m in untested_methods if m["complexity"] > 10]
            if high_complexity_untested:
                suggestions.append(f"ä¼˜å…ˆæµ‹è¯•é«˜å¤æ‚åº¦æ–¹æ³•: {[m['name'] for m in high_complexity_untested[:3]]}")

            # æµ‹è¯•è¦†ç›–ç‡ä½çš„å»ºè®®
            coverage = self._calculate_coverage(methods, module_path)
            if coverage < 50:
                suggestions.append("å½“å‰æµ‹è¯•è¦†ç›–ç‡ä½äº50%ï¼Œå»ºè®®å¢åŠ åŸºç¡€åŠŸèƒ½æµ‹è¯•")

            # ä¸šåŠ¡å…³é”®åŠŸèƒ½å»ºè®®
            critical_methods = [
                m for m in untested_methods if "get" in m["name"].lower() or "calculate" in m["name"].lower()
            ]
            if critical_methods:
                suggestions.append(f"å»ºè®®ä¸ºä¸šåŠ¡æ ¸å¿ƒæ–¹æ³•æ·»åŠ æµ‹è¯•: {[m['name'] for m in critical_methods[:3]]}")

        return suggestions


# AIè¾…åŠ©æµ‹è¯•å·¥å…·
class AITestAssistant:
    """AIæµ‹è¯•åŠ©æ‰‹"""

    def __init__(self):
        self.generator = AITestGenerator()
        self.optimizer = IntelligentTestOptimizer()

    async def generate_comprehensive_test_suite(self, target_module: str) -> Dict[str, Any]:
        """ç”Ÿæˆå…¨é¢çš„æµ‹è¯•å¥—ä»¶"""
        print(f"ğŸ¤– AIæ­£åœ¨ä¸º {target_module} ç”Ÿæˆå…¨é¢æµ‹è¯•å¥—ä»¶...")

        # åˆ†æç›®æ ‡æ¨¡å—
        source_code = self.generator._read_module_source(target_module)
        methods = self.generator._extract_testable_methods(source_code)

        comprehensive_tests = []

        # ä¸ºæ¯ä¸ªæ–¹æ³•ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        for method in methods:
            test_cases = self.generator.generate_test_cases_from_source(source_code, method["name"])
            comprehensive_tests.extend(test_cases)

        # ç”Ÿæˆæµ‹è¯•å¥—ä»¶æ–‡ä»¶
        test_suite_file = self._generate_test_suite_file(target_module, comprehensive_tests)

        return {
            "target_module": target_module,
            "generated_tests": len(comprehensive_tests),
            "test_file": test_suite_file,
            "coverage_analysis": await self.optimizer.optimize_test_coverage(target_module),
            "ai_recommendations": self._generate_ai_recommendations(methods, comprehensive_tests),
        }

    def _generate_test_suite_file(self, target_module: str, test_cases: List[TestCase]) -> str:
        """ç”Ÿæˆæµ‹è¯•å¥—ä»¶æ–‡ä»¶"""
        module_name = Path(target_module).stem
        test_file = f"tests/{module_name}_comprehensive_test.py"

        with open(test_file, "w", encoding="utf-8") as f:
            f.write(
                f"""#!/usr/bin/env python3
# -*- coding: utf-8 -*-
\"\"\"
{module_name} ç»¼åˆæµ‹è¯•å¥—ä»¶
AIç”Ÿæˆçš„å…¨é¢æµ‹è¯•ç”¨ä¾‹
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
\"\"\"

import pytest
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Import specific functions from the target module
try:
    from {module_name} import calculate_profit_loss, get_stock_price
except ImportError:
    # Fallback imports if the specific functions don't exist
    try:
        import {module_name}
    except ImportError:
        pass

"""
            )

            for test_case in test_cases:
                f.write(
                    f"""
{test_case.code}

"""
                )

                # æ·»åŠ è¾…åŠ©æ–¹æ³•
                if "test_type" in test_case.metadata and test_case.metadata["test_type"] == "boundary":
                    f.write(
                        """
    def _generate_normal_args(self, args):
        \"\"\"ç”Ÿæˆæ­£å¸¸å‚æ•°å€¼\"\"\"
        return ["normal_value"] * len(args)

    def _generate_invalid_value(self, param_type):
        \"\"\"ç”Ÿæˆæ— æ•ˆå‚æ•°å€¼\"\"\"
        invalid_values = {
            "str": 123,
            "int": "invalid",
            "float": "not_float",
            "bool": "not_bool"
        }
        return invalid_values.get(param_type, None)

"""
                    )

                f.write("\n")

        return test_file

    def _generate_ai_recommendations(self, methods: List[Dict[str, Any]], test_cases: List[TestCase]) -> List[str]:
        """ç”ŸæˆAIå»ºè®®"""
        recommendations = []

        # å¤æ‚åº¦åˆ†æå»ºè®®
        high_complexity = [m for m in methods if m["complexity"] > 10]
        if high_complexity:
            recommendations.append(f"æ£€æµ‹åˆ° {len(high_complexity)} ä¸ªé«˜å¤æ‚åº¦æ–¹æ³•ï¼Œå»ºè®®é‡æ„æˆ–æ‹†åˆ†")

        # è¦†ç›–ç‡å»ºè®®
        covered_methods = len([tc for tc in test_cases if tc.complexity_score < 1.5])
        total_generated = len(test_cases)
        if total_generated > 0:
            coverage_ratio = covered_methods / total_generated
            if coverage_ratio < 0.8:
                recommendations.append(f"å»ºè®®å¢åŠ è¾¹ç•Œå’Œå¼‚å¸¸æµ‹è¯•ç”¨ä¾‹ï¼Œå½“å‰åŸºç¡€ç”¨ä¾‹å æ¯”: {coverage_ratio:.1%}")

        # æ€§èƒ½å»ºè®®
        if len(test_cases) > 20:
            recommendations.append("æµ‹è¯•ç”¨ä¾‹æ•°é‡è¾ƒå¤šï¼Œå»ºè®®è€ƒè™‘ä½¿ç”¨æµ‹è¯•åˆ†ç»„æˆ–å¹¶è¡Œæ‰§è¡Œ")

        # ç»´æŠ¤æ€§å»ºè®®
        avg_test_complexity = sum(tc.complexity_score for tc in test_cases) / len(test_cases)
        if avg_test_complexity > 1.3:
            recommendations.append("æµ‹è¯•ç”¨ä¾‹å¤æ‚åº¦è¾ƒé«˜ï¼Œå»ºè®®ä¿æŒæµ‹è¯•ç®€å•æ˜äº†")

        return recommendations


# Pytestæµ‹è¯•ç”¨ä¾‹
@pytest.mark.ai_assisted
async def test_ai_test_generation():
    """AIæµ‹è¯•ç”Ÿæˆæµ‹è¯•"""
    ai_assistant = AITestAssistant()

    # ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
    test_cases = ai_assistant.generator.generate_test_cases_from_source(
        """
def calculate_profit_loss(symbol, start_date, end_date):
    \"\"\"è®¡ç®—ç›ˆäº\"\"\"
    if not symbol or not start_date or not end_date:
        raise ValueError("å‚æ•°ä¸èƒ½ä¸ºç©º")

    # è·å–å†å²æ•°æ®
    data = fetch_kline_data(symbol, start_date, end_date)

    # è®¡ç®—ç›ˆäº
    profit_loss = 0.0
    for i in range(1, len(data)):
        change = data[i]['close'] - data[i-1]['close']
        profit_loss += change

    return round(profit_loss, 2)
        """,
        "calculate_profit_loss",
    )

    assert len(test_cases) >= 4  # åº”è¯¥ç”Ÿæˆè‡³å°‘4ä¸ªæµ‹è¯•ç”¨ä¾‹
    assert any("normal_case" in tc.name for tc in test_cases)
    assert any("boundary_cases" in tc.name for tc in test_cases)
    assert any("exception_cases" in tc.name for tc in test_cases)


@pytest.mark.ai_assisted
async def test_test_suite_optimization():
    """æµ‹è¯•å¥—ä»¶ä¼˜åŒ–æµ‹è¯•"""
    ai_assistant = AITestAssistant()

    # ä¼˜åŒ–æµ‹è¯•å¥—ä»¶
    test_files = ["src/adapters/financial_adapter.py", "src/data_access.py"]

    optimization_results = await ai_assistant.optimizer.optimize_test_suite(test_files)

    assert len(optimization_results) >= 1
    assert all("analysis" in result for result in optimization_results.values())


@pytest.mark.ai_assisted
async def test_comprehensive_test_generation():
    """å…¨é¢æµ‹è¯•å¥—ä»¶ç”Ÿæˆæµ‹è¯•"""
    ai_assistant = AITestAssistant()

    # ä¸ºfinancial_adapterç”Ÿæˆç»¼åˆæµ‹è¯•å¥—ä»¶
    result = await ai_assistant.generate_comprehensive_test_suite("src/adapters/financial_adapter.py")

    assert "target_module" in result
    assert "generated_tests" in result
    assert result["generated_tests"] > 0
    assert "coverage_analysis" in result
    assert "ai_recommendations" in result


if __name__ == "__main__":
    # è¿è¡ŒAIè¾…åŠ©æµ‹è¯•
    import asyncio

    async def main():
        print("ğŸ¤– å¯åŠ¨AIè¾…åŠ©æµ‹è¯•å·¥å…·...")

        # æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆ
        ai_assistant = AITestAssistant()

        # ç”Ÿæˆç¤ºä¾‹æµ‹è¯•ç”¨ä¾‹
        test_cases = ai_assistant.generator.generate_test_cases_from_source(
            """
def get_stock_price(symbol):
    \"\"\"è·å–è‚¡ç¥¨ä»·æ ¼\"\"\"
    if symbol == "600519":
        return {"symbol": "600519", "price": 1800.0, "change": 2.5}
    elif symbol == "600036":
        return {"symbol": "600036", "price": 45.6, "change": -1.2}
    else:
        return None
            """,
            "get_stock_price",
        )

        print(f"âœ… ç”Ÿæˆäº† {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")

        # ç”Ÿæˆç»¼åˆæµ‹è¯•å¥—ä»¶
        result = await ai_assistant.generate_comprehensive_test_suite("src/adapters/financial_adapter.py")

        print("ğŸ“Š æµ‹è¯•å¥—ä»¶ç”Ÿæˆç»“æœ:")
        print(f"  - ç›®æ ‡æ¨¡å—: {result['target_module']}")
        print(f"  - ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹æ•°: {result['generated_tests']}")
        print(f"  - è¦†ç›–ç‡åˆ†æ: {result['coverage_analysis']['coverage_percentage']:.1f}%")
        print(f"  - æµ‹è¯•æ–‡ä»¶: {result['test_file']}")
        print(f"  - AIå»ºè®®: {len(result['ai_recommendations'])} æ¡")

        for rec in result["ai_recommendations"]:
            print(f"    â€¢ {rec}")

    asyncio.run(main())
