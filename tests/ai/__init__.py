"""
AIæµ‹è¯•å·¥å…·åŒ…

æä¾›å…¨é¢çš„AIè¾…åŠ©æµ‹è¯•åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æ™ºèƒ½æµ‹è¯•ç”Ÿæˆ
- æ•°æ®åˆ†æ
- æ•°æ®ç®¡ç†
- æµ‹è¯•é›†æˆ
"""

from .test_ai_assisted_testing import (
    AITestGenerator,
    IntelligentTestOptimizer,
    AITestAssistant,
)
from .test_data_analyzer import AITestDataAnalyzer, AnomalyDetection, TrendPrediction
from .test_data_manager import AITestDataManager, TestDataProfile, DataGenerationRequest
from .test_integration_system import (
    AITestIntegrationSystem,
    TestOrchestrationConfig,
    TestExecutionPlan,
    TestExecutionResult,
    TestPhase,
    IntelligentTestPlanner,
    SmartTestExecutor,
)

# å¯¼å‡ºæ‰€æœ‰ä¸»è¦ç±»å’Œå‡½æ•°
__all__ = [
    # æ ¸å¿ƒAIæµ‹è¯•ç»„ä»¶
    "AITestGenerator",
    "IntelligentTestOptimizer",
    "AITestAssistant",
    "AITestDataAnalyzer",
    "AITestDataManager",
    "AITestIntegrationSystem",
    # æ•°æ®æ¨¡å‹
    "TestDataProfile",
    "DataGenerationRequest",
    "TestOrchestrationConfig",
    "TestExecutionPlan",
    "TestExecutionResult",
    "AnomalyDetection",
    "TrendPrediction",
    # æšä¸¾
    "TestPhase",
    # ä¾¿æ·å‡½æ•°
    "create_ai_testing_session",
    "run_ai_test_suite",
]


def create_ai_testing_session(config: dict = None) -> AITestIntegrationSystem:
    """åˆ›å»ºAIæµ‹è¯•ä¼šè¯"""
    if config is None:
        config = {
            "max_concurrent_tests": 5,
            "enable_ai_enhancement": True,
            "auto_optimize": True,
            "report_format": "comprehensive",
        }

    orchestration_config = TestOrchestrationConfig(**config)
    return AITestIntegrationSystem(orchestration_config)


async def run_ai_test_suite(project_context: dict, test_executors: dict = None, config: dict = None) -> dict:
    """è¿è¡ŒAIæµ‹è¯•å¥—ä»¶

    Args:
        project_context: é¡¹ç›®ä¸Šä¸‹æ–‡ä¿¡æ¯
        test_executors: æµ‹è¯•æ‰§è¡Œå™¨å­—å…¸
        config: é…ç½®å‚æ•°

    Returns:
        æµ‹è¯•ç»“æœå­—å…¸
    """
    if test_executors is None:
        test_executors = {}

    # åˆ›å»ºæµ‹è¯•ä¼šè¯
    session = create_ai_testing_session(config)

    # è¿è¡Œæ™ºèƒ½æµ‹è¯•
    results = await session.run_intelligent_testing(project_context, test_executors)

    return results


def quick_test_analysis(test_results: list) -> dict:
    """å¿«é€Ÿæµ‹è¯•åˆ†æ

    Args:
        test_results: æµ‹è¯•ç»“æœåˆ—è¡¨

    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    analyzer = AITestDataAnalyzer()

    # æ£€æµ‹å¼‚å¸¸
    anomalies = analyzer.detect_test_anomalies(test_results)

    # é¢„æµ‹è¶‹åŠ¿
    trends = analyzer.predict_test_trends(test_results)

    # ç”Ÿæˆæ´å¯Ÿ
    insights = analyzer.generate_test_insights(test_results)

    return {
        "anomalies": [a.dict() for a in anomalies],
        "trends": [t.dict() for t in trends],
        "insights": insights,
    }


def generate_test_data(profile_name: str, data_schema: dict, request_params: dict = None) -> dict:
    """ç”Ÿæˆæµ‹è¯•æ•°æ®

    Args:
        profile_name: æ•°æ®æ¡£æ¡ˆåç§°
        data_schema: æ•°æ®æ¨¡å¼
        request_params: è¯·æ±‚å‚æ•°

    Returns:
        ç”Ÿæˆçš„æµ‹è¯•æ•°æ®
    """
    data_manager = AITestDataManager()

    try:
        return data_manager.generate_test_data(profile_name, data_schema, request_params)
    except ValueError as e:
        print(f"é”™è¯¯: {e}")
        return {}


def optimize_test_plan(plan_id: str = None) -> dict:
    """ä¼˜åŒ–æµ‹è¯•è®¡åˆ’

    Args:
        plan_id: è®¡åˆ’IDï¼ˆå¯é€‰ï¼‰

    Returns:
        ä¼˜åŒ–ç»“æœ
    """
    # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„ä¼˜åŒ–é€»è¾‘
    return {
        "plan_id": plan_id,
        "optimizations_applied": [],
        "estimated_improvements": {},
    }


# ä¾¿æ·é…ç½®æ¨¡æ¿
DEFAULT_CONFIG = {
    "max_concurrent_tests": 10,
    "enable_ai_enhancement": True,
    "auto_optimize": True,
    "enable_performance_monitoring": True,
    "report_format": "comprehensive",
    "data_retention_days": 30,
}

PERFORMANCE_CONFIG = {
    "max_concurrent_tests": 5,
    "enable_ai_enhancement": True,
    "auto_optimize": False,
    "enable_performance_monitoring": True,
    "report_format": "detailed",
    "data_retention_days": 7,
}

QUICK_CONFIG = {
    "max_concurrent_tests": 3,
    "enable_ai_enhancement": False,
    "auto_optimize": False,
    "enable_performance_monitoring": False,
    "report_format": "basic",
    "data_retention_days": 1,
}


def get_config_template(template_name: str = "default") -> dict:
    """è·å–é…ç½®æ¨¡æ¿

    Args:
        template_name: æ¨¡æ¿åç§° (default, performance, quick)

    Returns:
        é…ç½®å­—å…¸
    """
    templates = {
        "default": DEFAULT_CONFIG,
        "performance": PERFORMANCE_CONFIG,
        "quick": QUICK_CONFIG,
    }

    return templates.get(template_name, DEFAULT_CONFIG)


# é¡¹ç›®ç¤ºä¾‹ç”¨æ³•
def create_my_stocks_test_context():
    """åˆ›å»ºMyStocksé¡¹ç›®æµ‹è¯•ä¸Šä¸‹æ–‡"""
    return {
        "project_name": "MyStocks",
        "project_type": "web_application",
        "modules_count": 15,
        "modules": [
            "authentication",
            "database",
            "api",
            "trading",
            "market_data",
            "user_management",
            "monitoring",
            "reporting",
        ],
        "features": ["api", "database", "ui", "realtime_data", "security"],
        "complexity_level": "medium",
        "critical_components": ["authentication", "database", "api"],
        "testing_requirements": {
            "coverage": 80,
            "performance_threshold": 2.0,
            "security_tests": True,
        },
    }


def create_my_stocks_test_executors():
    """åˆ›å»ºMyStocksæµ‹è¯•æ‰§è¡Œå™¨"""
    return {
        "unit_tests": run_unit_tests,
        "integration_tests": run_integration_tests,
        "api_contract_tests": run_api_contract_tests,
        "e2e_tests": run_e2e_tests,
        "performance_tests": run_performance_tests,
    }


async def run_unit_tests():
    """è¿è¡Œå•å…ƒæµ‹è¯•"""
    print("ğŸ§ª è¿è¡Œå•å…ƒæµ‹è¯•...")
    await asyncio.sleep(2)  # æ¨¡æ‹Ÿæ‰§è¡Œ
    return {"passed": 25, "failed": 1, "skipped": 0}


async def run_integration_tests():
    """è¿è¡Œé›†æˆæµ‹è¯•"""
    print("ğŸ”— è¿è¡Œé›†æˆæµ‹è¯•...")
    await asyncio.sleep(5)
    return {"passed": 18, "failed": 2, "skipped": 0}


async def run_api_contract_tests():
    """è¿è¡ŒAPIå¥‘çº¦æµ‹è¯•"""
    print("ğŸ“‹ è¿è¡ŒAPIå¥‘çº¦æµ‹è¯•...")
    await asyncio.sleep(3)
    return {"passed": 12, "failed": 0, "skipped": 0}


async def run_e2e_tests():
    """è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•"""
    print("ğŸŒ è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•...")
    await asyncio.sleep(10)
    return {"passed": 8, "failed": 1, "skipped": 1}


async def run_performance_tests():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    print("âš¡ è¿è¡Œæ€§èƒ½æµ‹è¯•...")
    await asyncio.sleep(15)
    return {"passed": 5, "failed": 0, "skipped": 0}


# ä½¿ç”¨ç¤ºä¾‹
async def demo_ai_testing():
    """æ¼”ç¤ºAIæµ‹è¯•åŠŸèƒ½"""
    print("ğŸ¤– AIæµ‹è¯•æ¼”ç¤º")

    # 1. åˆ›å»ºæµ‹è¯•ä¸Šä¸‹æ–‡
    context = create_my_stocks_test_context()

    # 2. åˆ›å»ºæµ‹è¯•æ‰§è¡Œå™¨
    executors = create_my_stocks_test_executors()

    # 3. è¿è¡ŒAIæµ‹è¯•
    results = await run_ai_test_suite(context, executors)

    # 4. æ˜¾ç¤ºç»“æœ
    print("\n=== æµ‹è¯•ç»“æœ ===")
    print(f"è®¡åˆ’åç§°: {results['metadata']['plan_name']}")
    print(f"æ€»æ‰§è¡Œæ—¶é—´: {results['execution_summary']['total_duration']}s")
    print(f"é€šè¿‡ç‡: {results['execution_summary']['success_rate']}%")
    print(f"å»ºè®®æ•°é‡: {len(results['recommendations'])}")

    for i, rec in enumerate(results["recommendations"], 1):
        print(f"{i}. {rec}")


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(demo_ai_testing())
