"""
é¢„æµ‹åˆ†æå™¨

æä¾›åŸºäºæœºå™¨å­¦ä¹ çš„æµ‹è¯•ç»“æœé¢„æµ‹ã€è¶‹åŠ¿åˆ†æå’Œé£é™©è¯„ä¼°åŠŸèƒ½ã€‚
"""

import json
import logging
import math
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor
from collections import defaultdict, deque
import uuid

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import mlflow
import mlflow.sklearn
import mlflow.keras

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PredictionModel(Enum):
    """é¢„æµ‹æ¨¡å‹æšä¸¾"""

    LINEAR_REGRESSION = "linear_regression"
    RIDGE_REGRESSION = "ridge_regression"
    LASSO_REGRESSION = "lasso_regression"
    RANDOM_FOREST = "random_forest"
    GRADIENT_BOOSTING = "gradient_boosting"
    XGBOOST = "xgboost"
    SVR = "svr"
    MLP = "mlp"
    LSTM = "lstm"
    ENSEMBLE = "ensemble"


class PredictionTask(Enum):
    """é¢„æµ‹ä»»åŠ¡æšä¸¾"""

    TEST_DURATION = "test_duration"
    PASS_RATE = "pass_rate"
    FAILURE_RATE = "failure_rate"
    RESOURCE_USAGE = "resource_usage"
    COVERAGE_SCORE = "coverage_score"
    MAINTENANCE_BURDEN = "maintenance_burden"
    FLAKINESS_PREDICTION = "flakiness_prediction"


class PredictionConfidence(Enum):
    """é¢„æµ‹ç½®ä¿¡åº¦æšä¸¾"""

    VERY_LOW = "very_low"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    VERY_HIGH = "very_high"


@dataclass
class PredictionRequest:
    """é¢„æµ‹è¯·æ±‚"""

    task: PredictionTask
    model_type: PredictionModel
    historical_data: List[Dict[str, Any]]
    future_horizon: int  # é¢„æµ‹æ—¶é—´æ­¥æ•°
    confidence_threshold: float = 0.8
    features: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class PredictionResult:
    """é¢„æµ‹ç»“æœ"""

    request: PredictionRequest
    predictions: List[float]
    confidence: PredictionConfidence
    prediction_interval: Tuple[float, float]
    feature_importance: Dict[str, float]
    model_performance: Dict[str, float]
    timestamp: datetime = field(default_factory=datetime.now)
    model_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    error_message: str = ""


class FeatureExtractor:
    """ç‰¹å¾æå–å™¨"""

    def __init__(self):
        self.feature_names = []
        self.scaler = StandardScaler()

    def extract_features(self, historical_data: List[Dict[str, Any]]) -> np.ndarray:
        """æå–ç‰¹å¾"""
        if not historical_data:
            return np.array([])

        # å‡†å¤‡ç‰¹å¾çŸ©é˜µ
        feature_matrix = []

        for data_point in historical_data:
            features = []

            # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
            if "duration" in data_point:
                features.extend(
                    [
                        data_point["duration"],
                        data_point["duration"] ** 2,  # å¹³æ–¹ç‰¹å¾
                        math.sqrt(data_point["duration"]),  # å¹³æ–¹æ ¹ç‰¹å¾
                        math.log(data_point["duration"] + 1),  # å¯¹æ•°ç‰¹å¾
                    ]
                )

            if "memory_usage" in data_point:
                features.extend([data_point["memory_usage"], data_point["memory_usage"] ** 2])

            if "cpu_usage" in data_point:
                features.extend([data_point["cpu_usage"], data_point["cpu_usage"] ** 2])

            # æ—¶é—´ç‰¹å¾
            if "timestamp" in data_point:
                ts = pd.to_datetime(data_point["timestamp"])
                features.extend(
                    [
                        ts.hour,  # å°æ—¶
                        ts.dayofweek,  # æ˜ŸæœŸå‡ 
                        ts.day,  # æ—¥æœŸ
                        ts.month,  # æœˆä»½
                        math.sin(2 * math.pi * ts.hour / 24),  # å‘¨æœŸæ€§ç‰¹å¾
                        math.cos(2 * math.pi * ts.hour / 24),
                        math.sin(2 * math.pi * ts.dayofweek / 7),
                        math.cos(2 * math.pi * ts.dayofweek / 7),
                    ]
                )

            # ç§»åŠ¨å¹³å‡ç‰¹å¾
            if len(feature_matrix) > 0:
                prev_features = feature_matrix[-1]
                features.extend(
                    [
                        np.mean(prev_features[-4:]),  # å‰4ä¸ªç‰¹å¾çš„å‡å€¼
                        np.std(prev_features[-4:]),  # å‰4ä¸ªç‰¹å¾çš„æ ‡å‡†å·®
                        prev_features[0],  # ä¸Šä¸€æ¬¡çš„duration
                    ]
                )

            feature_matrix.append(features)

        if feature_matrix:
            # æ ‡å‡†åŒ–ç‰¹å¾
            feature_array = np.array(feature_matrix)
            if len(self.feature_names) == 0:
                self.feature_names = [f"feature_{i}" for i in range(feature_array.shape[1])]
                self.scaler.fit(feature_array)

            return self.scaler.transform(feature_array)

        return np.array([])

    def get_feature_names(self) -> List[str]:
        """è·å–ç‰¹å¾åç§°"""
        return self.feature_names


class TimeSeriesPredictor:
    """æ—¶é—´åºåˆ—é¢„æµ‹å™¨"""

    def __init__(self, model_type: PredictionModel):
        self.model_type = model_type
        self.model = None
        self.scaler = MinMaxScaler()
        self.is_trained = False
        self.training_history = []

    def train(self, time_series: np.ndarray) -> Dict[str, float]:
        """è®­ç»ƒæ¨¡å‹"""
        if len(time_series) < 10:
            raise ValueError("æ—¶é—´åºåˆ—æ•°æ®ä¸è¶³ï¼Œè‡³å°‘éœ€è¦10ä¸ªæ•°æ®ç‚¹")

        # æ•°æ®å‡†å¤‡
        X, y = self._prepare_time_series_data(time_series)

        # æ¨¡å‹é€‰æ‹©å’Œè®­ç»ƒ
        if self.model_type == PredictionModel.LINEAR_REGRESSION:
            self.model = LinearRegression()
        elif self.model_type == PredictionModel.RIDGE_REGRESSION:
            self.model = Ridge(alpha=1.0)
        elif self.model_type == PredictionModel.LASSO_REGRESSION:
            self.model = Lasso(alpha=1.0)
        elif self.model_type == PredictionModel.RANDOM_FOREST:
            self.model = RandomForestRegressor(n_estimators=100, random_state=42)
        elif self.model_type == PredictionModel.GRADIENT_BOOSTING:
            self.model = GradientBoostingRegressor(n_estimators=100, random_state=42)
        elif self.model_type == PredictionModel.XGBOOST:
            self.model = xgb.XGBRegressor(n_estimators=100, random_state=42)
        elif self.model_type == PredictionModel.SVR:
            self.model = SVR(kernel="rbf", C=100, gamma=0.1, epsilon=0.1)
        elif self.model_type == PredictionModel.MLP:
            self.model = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42)
        elif self.model_type == PredictionModel.LSTM:
            self.model = self._create_lstm_model(X.shape[1])
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")

        # è®­ç»ƒæ¨¡å‹
        if self.model_type == PredictionModel.LSTM:
            # LSTMéœ€è¦ç‰¹æ®Šå¤„ç†
            history = self.model.fit(
                X,
                y,
                epochs=100,
                batch_size=32,
                verbose=0,
                callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],
            )
            training_loss = history.history["loss"][-1] if history.history["loss"] else float("inf")
        else:
            self.model.fit(X, y)
            training_loss = 0

        # è¯„ä¼°æ¨¡å‹
        predictions = self.model.predict(X)
        mse = mean_squared_error(y, predictions)
        mae = mean_absolute_error(y, predictions)
        r2 = r2_score(y, predictions)

        self.is_trained = True
        self.training_history.append(
            {
                "timestamp": datetime.now(),
                "model_type": self.model_type.value,
                "mse": mse,
                "mae": mae,
                "r2": r2,
                "training_loss": training_loss,
            }
        )

        return {"mse": mse, "mae": mae, "r2": r2, "training_loss": training_loss}

    def _prepare_time_series_data(self, time_series: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """å‡†å¤‡æ—¶é—´åºåˆ—æ•°æ®"""
        # åˆ›å»ºæ»‘åŠ¨çª—å£
        window_size = 5
        X, y = [], []

        for i in range(len(time_series) - window_size):
            X.append(time_series[i : i + window_size])
            y.append(time_series[i + window_size])

        return np.array(X), np.array(y)

    def _create_lstm_model(self, input_shape: int):
        """åˆ›å»ºLSTMæ¨¡å‹"""
        model = Sequential(
            [
                LSTM(50, return_sequences=True, input_shape=(input_shape, 1)),
                Dropout(0.2),
                LSTM(50, return_sequences=False),
                Dropout(0.2),
                Dense(25),
                Dense(1),
            ]
        )

        model.compile(optimizer=Adam(learning_rate=0.001), loss="mse")
        return model

    def predict(self, steps: int) -> List[float]:
        """é¢„æµ‹æœªæ¥å€¼"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")

        predictions = []
        current_sequence = self._get_initial_sequence()

        for _ in range(steps):
            if self.model_type == PredictionModel.LSTM:
                # LSTMéœ€è¦3Dè¾“å…¥
                input_data = current_sequence.reshape(1, -1, 1)
                pred = self.model.predict(input_data, verbose=0)[0, 0]
                # æ›´æ–°åºåˆ—
                current_sequence = np.roll(current_sequence, -1)
                current_sequence[-1] = pred
            else:
                pred = self.model.predict(current_sequence.reshape(1, -1))[0]
                # æ›´æ–°åºåˆ—
                current_sequence = np.roll(current_sequence, -1)
                current_sequence[-1] = pred

            predictions.append(pred)

        return predictions

    def _get_initial_sequence(self) -> np.ndarray:
        """è·å–åˆå§‹åºåˆ—"""
        # è¿™é‡Œåº”è¯¥ä»è®­ç»ƒæ•°æ®ä¸­è·å–ï¼Œç®€åŒ–å¤„ç†
        return np.random.randn(5)


class RiskAssessor:
    """é£é™©è¯„ä¼°å™¨"""

    def __init__(self):
        self.risk_thresholds = {"critical": 0.8, "high": 0.6, "medium": 0.4, "low": 0.2}
        self.historical_risks = []

    def assess_test_risk(self, test_data: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°æµ‹è¯•é£é™©"""
        risk_score = 0
        risk_factors = []

        # åŸºäºå†å²çš„å¤±è´¥ç‡
        if "failure_history" in test_data:
            failure_rate = test_data["failure_history"].get("failure_rate", 0)
            risk_score += failure_rate * 0.3
            risk_factors.append(f"å†å²å¤±è´¥ç‡: {failure_rate:.2%}")

        # åŸºäºæ‰§è¡Œæ—¶é—´
        if "duration" in test_data:
            duration = test_data["duration"]
            if duration > 10:  # é•¿æ—¶é—´æµ‹è¯•
                risk_score += 0.2
                risk_factors.append(f"æ‰§è¡Œæ—¶é—´è¿‡é•¿: {duration:.2f}s")
            elif duration > 5:
                risk_score += 0.1
                risk_factors.append(f"æ‰§è¡Œæ—¶é—´è¾ƒé•¿: {duration:.2f}s")

        # åŸºäºèµ„æºä½¿ç”¨
        if "memory_usage" in test_data and "cpu_usage" in test_data:
            memory_score = min(test_data["memory_usage"] / 1000, 1.0)  # å½’ä¸€åŒ–åˆ°1GB
            cpu_score = min(test_data["cpu_usage"] / 100, 1.0)  # å½’ä¸€åŒ–åˆ°100%
            resource_risk = (memory_score + cpu_score) / 2 * 0.2
            risk_score += resource_risk
            risk_factors.append(f"èµ„æºä½¿ç”¨é£é™©: {resource_risk:.2f}")

        # åŸºäºä»£ç å¤æ‚åº¦
        if "complexity_metrics" in test_data:
            complexity = test_data["complexity_metrics"]
            if complexity.get("cyclomatic_complexity", 0) > 20:
                risk_score += 0.15
                risk_factors.append(f"åœˆå¤æ‚åº¦è¿‡é«˜: {complexity['cyclomatic_complexity']}")
            if complexity.get("cognitive_complexity", 0) > 15:
                risk_score += 0.15
                risk_factors.append(f"è®¤çŸ¥å¤æ‚åº¦è¿‡é«˜: {complexity['cognitive_complexity']}")

        # åŸºäºä¾èµ–å…³ç³»
        if "dependencies" in test_data:
            deps = test_data["dependencies"]
            unstable_deps = sum(1 for d in deps if d.get("stability", 1.0) < 0.7)
            if unstable_deps > 0:
                risk_score += unstable_deps * 0.1
                risk_factors.append(f"ä¸ç¨³å®šä¾èµ–: {unstable_deps}")

        # ç¡®å®šé£é™©ç­‰çº§
        risk_level = "low"
        for level, threshold in self.risk_thresholds.items():
            if risk_score >= threshold:
                risk_level = level
                break

        return {
            "risk_score": min(risk_score, 1.0),
            "risk_level": risk_level,
            "risk_factors": risk_factors,
            "timestamp": datetime.now(),
        }

    def predict_future_risks(self, historical_risks: List[Dict]) -> List[Dict[str, Any]]:
        """é¢„æµ‹æœªæ¥é£é™©"""
        predictions = []

        if len(historical_risks) < 5:
            return predictions

        # æå–é£é™©åˆ†æ•°åºåˆ—
        risk_scores = [r["risk_score"] for r in historical_risks]

        # ç®€å•çš„è¶‹åŠ¿é¢„æµ‹
        if len(risk_scores) >= 3:
            trend = np.polyfit(range(len(risk_scores)), risk_scores, 1)[0]

            # é¢„æµ‹æœªæ¥3ä¸ªæ—¶é—´ç‚¹çš„é£é™©
            future_steps = 3
            for i in range(1, future_steps + 1):
                future_index = len(risk_scores) + i
                # åŸºäºè¶‹åŠ¿å’Œéšæœºæ€§é¢„æµ‹
                predicted_score = max(0, min(1, risk_scores[-1] + trend * i + np.random.normal(0, 0.1)))

                # ç¡®å®šé£é™©ç­‰çº§
                risk_level = "low"
                for level, threshold in self.risk_thresholds.items():
                    if predicted_score >= threshold:
                        risk_level = level
                        break

                predictions.append(
                    {
                        "step": i,
                        "predicted_risk_score": predicted_score,
                        "risk_level": risk_level,
                        "confidence": "medium" if abs(trend) < 0.1 else ("low" if abs(trend) > 0.2 else "high"),
                        "timestamp": datetime.now() + timedelta(days=i),
                    }
                )

        return predictions

    def generate_risk_mitigation_plan(self, risk_assessment: Dict) -> List[str]:
        """ç”Ÿæˆé£é™©ç¼“è§£è®¡åˆ’"""
        mitigation_plan = []

        risk_level = risk_assessment["risk_level"]
        risk_factors = risk_assessment["risk_factors"]

        if risk_level == "critical":
            mitigation_plan.extend(["ç«‹å³æš‚åœç›¸å…³æµ‹è¯•", "è¿›è¡Œå…¨é¢ä»£ç å®¡æŸ¥", "å¢åŠ ç›‘æ§é¢‘ç‡", "å‡†å¤‡å›æ»šè®¡åˆ’"])

        for factor in risk_factors:
            if "å¤±è´¥ç‡" in factor:
                mitigation_plan.append("åˆ†æå¤±è´¥åŸå› å¹¶ä¿®å¤")
            elif "æ‰§è¡Œæ—¶é—´" in factor:
                mitigation_plan.append("ä¼˜åŒ–æµ‹è¯•æ€§èƒ½æˆ–æ‹†åˆ†æµ‹è¯•")
            elif "èµ„æºä½¿ç”¨" in factor:
                mitigation_plan.append("ä¼˜åŒ–èµ„æºä½¿ç”¨æˆ–å¢åŠ èµ„æº")
            elif "å¤æ‚åº¦" in factor:
                mitigation_plan.append("é‡æ„ä»£ç é™ä½å¤æ‚åº¦")
            elif "ä¾èµ–" in factor:
                mitigation_plan.append("æ›´æ–°æˆ–æ›¿æ¢ä¸ç¨³å®šä¾èµ–")

        # é€šç”¨ç¼“è§£æªæ–½
        mitigation_plan.extend(["å¢åŠ æµ‹è¯•è¦†ç›–ç‡", "æ·»åŠ è‡ªåŠ¨åŒ–ç›‘æ§", "å»ºç«‹é¢„è­¦æœºåˆ¶", "å®šæœŸé£é™©è¯„å®¡"])

        return list(set(mitigation_plan))  # å»é‡


class PredictionAnalyzer:
    """é¢„æµ‹åˆ†æå™¨ä¸»ç±»"""

    def __init__(self):
        self.predictors = {}
        self.feature_extractor = FeatureExtractor()
        self.risk_assessor = RiskAssessor()
        self.model_registry = {}
        self.prediction_history = deque(maxlen=1000)

        # MLflowè·Ÿè¸ª
        try:
            mlflow.set_tracking_uri("http://localhost:5000")
            mlflow.set_experiment("test_prediction_analyzer")
        except Exception as e:
            logger.warning(f"MLflowè¿æ¥å¤±è´¥: {e}")

    def register_model(self, model_id: str, model: Any, model_type: PredictionModel):
        """æ³¨å†Œæ¨¡å‹"""
        self.model_registry[model_id] = {
            "model": model,
            "type": model_type,
            "registered_at": datetime.now(),
            "usage_count": 0,
        }

    def make_prediction(self, request: PredictionRequest) -> PredictionResult:
        """æ‰§è¡Œé¢„æµ‹"""
        logger.info(f"å¼€å§‹é¢„æµ‹ä»»åŠ¡: {request.task.value} ä½¿ç”¨æ¨¡å‹: {request.model_type.value}")

        try:
            # éªŒè¯è¾“å…¥æ•°æ®
            if not request.historical_data:
                raise ValueError("å†å²æ•°æ®ä¸èƒ½ä¸ºç©º")

            # æå–ç‰¹å¾
            features = self.feature_extractor.extract_features(request.historical_data)
            if len(features) == 0:
                raise ValueError("æ— æ³•æå–æœ‰æ•ˆç‰¹å¾")

            # é€‰æ‹©é¢„æµ‹å™¨
            predictor_key = f"{request.task.value}_{request.model_type.value}"
            if predictor_key not in self.predictors:
                self.predictors[predictor_key] = TimeSeriesPredictor(request.model_type)

            predictor = self.predictors[predictor_key]

            # å‡†å¤‡ç›®æ ‡å˜é‡
            target_values = self._extract_target_values(request.historical_data, request.task)
            if len(target_values) < 10:
                raise ValueError(f"ç”¨äº{request.task.value}çš„æ•°æ®ä¸è¶³")

            # è®­ç»ƒæ¨¡å‹
            training_metrics = predictor.train(np.array(target_values))

            # æ‰§è¡Œé¢„æµ‹
            predictions = predictor.predict(request.future_horizon)

            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_confidence(predictions, training_metrics)

            # è®¡ç®—é¢„æµ‹åŒºé—´
            prediction_interval = self._calculate_prediction_interval(predictions, confidence)

            # ç‰¹å¾é‡è¦æ€§åˆ†æ
            feature_importance = self._analyze_feature_importance(features, target_values)

            # è¯„ä¼°æ¨¡å‹æ€§èƒ½
            model_performance = {
                "mse": training_metrics["mse"],
                "mae": training_metrics["mae"],
                "r2": training_metrics["r2"],
                "training_loss": training_metrics["training_loss"],
            }

            # åˆ›å»ºé¢„æµ‹ç»“æœ
            result = PredictionResult(
                request=request,
                predictions=predictions,
                confidence=confidence,
                prediction_interval=prediction_interval,
                feature_importance=feature_importance,
                model_performance=model_performance,
            )

            # è®°å½•å†å²
            self.prediction_history.append(
                {
                    "timestamp": datetime.now(),
                    "result": result,
                    "model_type": request.model_type.value,
                    "task": request.task.value,
                }
            )

            # è®°å½•åˆ°MLflow
            self._log_to_mlflow(result)

            logger.info(f"é¢„æµ‹å®Œæˆï¼Œç½®ä¿¡åº¦: {confidence.value}")
            return result

        except Exception as e:
            logger.error(f"é¢„æµ‹å¤±è´¥: {e}")
            return PredictionResult(
                request=request,
                predictions=[],
                confidence=PredictionConfidence.VERY_LOW,
                prediction_interval=(0, 0),
                feature_importance={},
                model_performance={},
                error_message=str(e),
            )

    def _extract_target_values(self, historical_data: List[Dict[str, Any]], task: PredictionTask) -> List[float]:
        """æå–ç›®æ ‡å€¼"""
        target_values = []

        for data_point in historical_data:
            if task == PredictionTask.TEST_DURATION:
                value = data_point.get("duration", 0)
            elif task == PredictionTask.PASS_RATE:
                value = data_point.get("pass_rate", 1.0)
            elif task == PredictionTask.FAILURE_RATE:
                value = data_point.get("failure_rate", 0.0)
            elif task == PredictionTask.RESOURCE_USAGE:
                value = data_point.get("memory_usage", 0) + data_point.get("cpu_usage", 0)
            elif task == PredictionTask.COVERAGE_SCORE:
                value = data_point.get("coverage_score", 0.0)
            elif task == PredictionTask.MAINTENANCE_BURDEN:
                # ç»´æŠ¤è´Ÿæ‹…çš„ç®€åŒ–è®¡ç®—
                complexity = data_point.get("complexity_metrics", {}).get("cyclomatic_complexity", 1)
                dependencies = len(data_point.get("dependencies", []))
                value = complexity * 0.5 + dependencies * 0.3
            elif task == PredictionTask.FLAKINESS_PREDICTION:
                value = data_point.get("flakiness_score", 0.0)
            else:
                value = 0

            target_values.append(value)

        return target_values

    def _calculate_confidence(self, predictions: List[float], training_metrics: Dict) -> PredictionConfidence:
        """è®¡ç®—é¢„æµ‹ç½®ä¿¡åº¦"""
        # åŸºäºå¤šä¸ªå› ç´ è®¡ç®—ç½®ä¿¡åº¦
        confidence_score = 0.5  # åŸºç¡€åˆ†æ•°

        # åŸºäºRÂ²åˆ†æ•°
        if training_metrics["r2"] > 0.8:
            confidence_score += 0.2
        elif training_metrics["r2"] > 0.5:
            confidence_score += 0.1

        # åŸºäºMAE
        if training_metrics["mae"] < 0.1:
            confidence_score += 0.1
        elif training_metrics["mae"] < 0.3:
            confidence_score += 0.05

        # åŸºäºè®­ç»ƒæŸå¤±
        if training_metrics["training_loss"] < 0.1:
            confidence_score += 0.1
        elif training_metrics["training_loss"] < 0.5:
            confidence_score += 0.05

        # ç¡®å®šç½®ä¿¡åº¦ç­‰çº§
        if confidence_score >= 0.8:
            return PredictionConfidence.VERY_HIGH
        elif confidence_score >= 0.65:
            return PredictionConfidence.HIGH
        elif confidence_score >= 0.4:
            return PredictionConfidence.MEDIUM
        elif confidence_score >= 0.2:
            return PredictionConfidence.LOW
        else:
            return PredictionConfidence.VERY_LOW

    def _calculate_prediction_interval(
        self, predictions: List[float], confidence: PredictionConfidence
    ) -> Tuple[float, float]:
        """è®¡ç®—é¢„æµ‹åŒºé—´"""
        if not predictions:
            return (0, 0)

        mean_pred = np.mean(predictions)
        std_pred = np.std(predictions)

        # æ ¹æ®ç½®ä¿¡åº¦è°ƒæ•´åŒºé—´å®½åº¦
        if confidence == PredictionConfidence.VERY_HIGH:
            interval_multiplier = 1.0
        elif confidence == PredictionConfidence.HIGH:
            interval_multiplier = 1.5
        elif confidence == PredictionConfidence.MEDIUM:
            interval_multiplier = 2.0
        elif confidence == PredictionConfidence.LOW:
            interval_multiplier = 3.0
        else:
            interval_multiplier = 4.0

        lower_bound = mean_pred - (std_pred * interval_multiplier)
        upper_bound = mean_pred + (std_pred * interval_multiplier)

        return (max(0, lower_bound), upper_bound)

    def _analyze_feature_importance(self, features: np.ndarray, target_values: List[float]) -> Dict[str, float]:
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        if len(features) < 10:
            return {}

        try:
            # ä½¿ç”¨éšæœºæ£®æ—åˆ†æç‰¹å¾é‡è¦æ€§
            rf = RandomForestRegressor(n_estimators=100, random_state=42)
            rf.fit(features, target_values)

            feature_names = self.feature_extractor.get_feature_names()
            if feature_names:
                importance_dict = dict(zip(feature_names, rf.feature_importances_))
                return importance_dict
            else:
                # å¦‚æœæ²¡æœ‰ç‰¹å¾åç§°ï¼Œè¿”å›é‡è¦æ€§æŒ‡æ•°
                return {f"feature_{i}": importance for i, importance in enumerate(rf.feature_importances_)}
        except Exception as e:
            logger.error(f"ç‰¹å¾é‡è¦æ€§åˆ†æå¤±è´¥: {e}")
            return {}

    def _log_to_mlflow(self, result: PredictionResult):
        """è®°å½•åˆ°MLflow"""
        try:
            with mlflow.start_run():
                # è®°å½•å‚æ•°
                mlflow.log_param("task", result.request.task.value)
                mlflow.log_param("model_type", result.request.model_type.value)
                mlflow.log_param("future_horizon", result.request.future_horizon)
                mlflow.log_param("confidence", result.confidence.value)

                # è®°å½•æŒ‡æ ‡
                mlflow.log_metric("mse", result.model_performance.get("mse", 0))
                mlflow.log_metric("mae", result.model_performance.get("mae", 0))
                mlflow.log_metric("r2", result.model_performance.get("r2", 0))

                # è®°å½•é¢„æµ‹ç»“æœ
                for i, pred in enumerate(result.predictions):
                    mlflow.log_metric(f"prediction_{i}", pred)

                # è®°å½•ç‰¹å¾é‡è¦æ€§
                for feature, importance in result.feature_importance.items():
                    mlflow.log_metric(f"feature_importance_{feature}", importance)

                # ä¿å­˜æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if result.request.model_type in [
                    PredictionModel.RANDOM_FOREST,
                    PredictionModel.GRADIENT_BOOSTING,
                ]:
                    mlflow.sklearn.log_model(
                        self.predictors.get(f"{result.request.task.value}_{result.request.model_type.value}").model,
                        "prediction_model",
                    )

        except Exception as e:
            logger.warning(f"MLflowè®°å½•å¤±è´¥: {e}")

    def batch_predict(self, requests: List[PredictionRequest]) -> List[PredictionResult]:
        """æ‰¹é‡é¢„æµ‹"""
        results = []

        # æ ¹æ®æ¨¡å‹ç±»å‹åˆ†ç»„
        model_groups = defaultdict(list)
        for req in requests:
            model_key = req.model_type.value
            model_groups[model_key].append(req)

        # å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor() as executor:
            futures = []

            for model_type, req_list in model_groups.items():
                # å¯¹åŒä¸€æ¨¡å‹ç±»å‹çš„è¯·æ±‚ä½¿ç”¨ç›¸åŒçš„é¢„æµ‹å™¨
                for req in req_list:
                    future = executor.submit(self.make_prediction, req)
                    futures.append(future)

            # æ”¶é›†ç»“æœ
            for future in as_completed(futures):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    logger.error(f"æ‰¹é‡é¢„æµ‹å¤±è´¥: {e}")

        return results

    def generate_prediction_report(self, results: List[PredictionResult]) -> Dict[str, Any]:
        """ç”Ÿæˆé¢„æµ‹æŠ¥å‘Š"""
        if not results:
            return {}

        # æŒ‰ä»»åŠ¡åˆ†ç»„
        task_groups = defaultdict(list)
        for result in results:
            task_groups[result.request.task].append(result)

        report = {
            "generated_at": datetime.now().isoformat(),
            "total_predictions": len(results),
            "by_task": {},
            "summary": {},
            "recommendations": [],
        }

        # åˆ†ææ¯ä¸ªä»»åŠ¡
        for task, task_results in task_groups.items():
            task_summary = {
                "predictions_count": len(task_results),
                "average_confidence": self._calculate_average_confidence(task_results),
                "success_rate": sum(1 for r in task_results if not r.error_message) / len(task_results),
                "average_prediction": np.mean([r.predictions[0] for r in task_results if r.predictions]),
                "trend": self._analyze_prediction_trend(task_results),
            }

            report["by_task"][task.value] = task_summary

        # æ€»ä½“æ€»ç»“
        all_confidences = [r.confidence for r in results]
        confidence_distribution = {
            "very_high": sum(1 for c in all_confidences if c == PredictionConfidence.VERY_HIGH),
            "high": sum(1 for c in all_confidences if c == PredictionConfidence.HIGH),
            "medium": sum(1 for c in all_confidences if c == PredictionConfidence.MEDIUM),
            "low": sum(1 for c in all_confidences if c == PredictionConfidence.LOW),
            "very_low": sum(1 for c in all_confidences if c == PredictionConfidence.VERY_LOW),
        }

        report["summary"]["confidence_distribution"] = confidence_distribution
        report["summary"]["overall_success_rate"] = sum(1 for r in results if not r.error_message) / len(results)

        # ç”Ÿæˆå»ºè®®
        report["recommendations"] = self._generate_prediction_recommendations(results)

        return report

    def _calculate_average_confidence(self, results: List[PredictionResult]) -> float:
        """è®¡ç®—å¹³å‡ç½®ä¿¡åº¦"""
        confidence_scores = {
            PredictionConfidence.VERY_HIGH: 1.0,
            PredictionConfidence.HIGH: 0.8,
            PredictionConfidence.MEDIUM: 0.6,
            PredictionConfidence.LOW: 0.4,
            PredictionConfidence.VERY_LOW: 0.2,
        }

        scores = [confidence_scores[r.confidence] for r in results]
        return sum(scores) / len(scores) if scores else 0

    def _analyze_prediction_trend(self, results: List[PredictionResult]) -> str:
        """åˆ†æé¢„æµ‹è¶‹åŠ¿"""
        if len(results) < 2:
            return "insufficient_data"

        # è·å–ç¬¬ä¸€ä¸ªé¢„æµ‹å€¼
        first_pred = results[0].predictions[0] if results[0].predictions else 0
        last_pred = results[-1].predictions[0] if results[-1].predictions else 0

        change = (last_pred - first_pred) / first_pred if first_pred != 0 else 0

        if change > 0.1:
            return "increasing"
        elif change < -0.1:
            return "decreasing"
        else:
            return "stable"

    def _generate_prediction_recommendations(self, results: List[PredictionResult]) -> List[str]:
        """ç”Ÿæˆé¢„æµ‹å»ºè®®"""
        recommendations = []

        # åˆ†æç½®ä¿¡åº¦
        low_confidence = [
            r for r in results if r.confidence in [PredictionConfidence.LOW, PredictionConfidence.VERY_LOW]
        ]
        if low_confidence:
            recommendations.append(f"æœ‰ {len(low_confidence)} ä¸ªé¢„æµ‹ç½®ä¿¡åº¦è¾ƒä½ï¼Œå»ºè®®å¢åŠ å†å²æ•°æ®æˆ–é€‰æ‹©æ›´åˆé€‚çš„æ¨¡å‹")

        # åˆ†æé”™è¯¯
        error_predictions = [r for r in results if r.error_message]
        if error_predictions:
            recommendations.append(f"æœ‰ {len(error_predictions)} ä¸ªé¢„æµ‹å¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ•°æ®")

        # æ¨¡å‹å»ºè®®
        model_usage = defaultdict(int)
        for r in results:
            model_usage[r.request.model_type.value] += 1

        best_model = max(model_usage.items(), key=lambda x: x[1])[0]
        recommendations.append(f"æ¨èæ¨¡å‹ '{best_model}' åœ¨ç±»ä¼¼ä»»åŠ¡ä¸­ä½¿ç”¨")

        return recommendations

    def export_predictions(self, results: List[PredictionResult], output_path: str, format: str = "json"):
        """å¯¼å‡ºé¢„æµ‹ç»“æœ"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        if format == "json":
            data = {"export_timestamp": datetime.now().isoformat(), "predictions": []}

            for result in results:
                pred_data = {
                    "model_id": result.model_id,
                    "task": result.request.task.value,
                    "model_type": result.request.model_type.value,
                    "predictions": result.predictions,
                    "confidence": result.confidence.value,
                    "prediction_interval": result.prediction_interval,
                    "timestamp": result.timestamp.isoformat(),
                    "error_message": result.error_message,
                }

                if result.error_message:
                    pred_data["error"] = result.error_message

                data["predictions"].append(pred_data)

            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

        elif format == "csv":
            # åˆ›å»ºCSVæ–‡ä»¶
            df_data = []
            for result in results:
                for i, pred in enumerate(result.predictions):
                    df_data.append(
                        {
                            "model_id": result.model_id,
                            "task": result.request.task.value,
                            "model_type": result.request.model_type.value,
                            "step": i + 1,
                            "prediction": pred,
                            "confidence": result.confidence.value,
                            "lower_bound": result.prediction_interval[0],
                            "upper_bound": result.prediction_interval[1],
                            "timestamp": result.timestamp.isoformat(),
                            "error": result.error_message or "",
                        }
                    )

            df = pd.DataFrame(df_data)
            df.to_csv(output_path, index=False)

        logger.info(f"é¢„æµ‹ç»“æœå·²å¯¼å‡ºåˆ°: {output_path}")


# ä½¿ç”¨ç¤ºä¾‹
def demo_prediction_analyzer():
    """æ¼”ç¤ºé¢„æµ‹åˆ†æå™¨åŠŸèƒ½"""
    print("ğŸš€ æ¼”ç¤ºé¢„æµ‹åˆ†æå™¨åŠŸèƒ½")

    # åˆ›å»ºé¢„æµ‹åˆ†æå™¨
    analyzer = PredictionAnalyzer()

    # å‡†å¤‡å†å²æ•°æ®
    historical_data = []
    base_time = datetime.now() - timedelta(days=30)

    for i in range(30):
        timestamp = base_time + timedelta(days=i)

        # æµ‹è¯•æŒç»­æ—¶é—´æ•°æ®
        historical_data.append(
            {
                "timestamp": timestamp.isoformat(),
                "duration": 5.0 + random.gauss(0, 1) + i * 0.1,  # è½»å¾®é€’å¢è¶‹åŠ¿
                "memory_usage": 50 + random.gauss(0, 10),
                "cpu_usage": 30 + random.gauss(0, 5),
                "pass_rate": 0.9 + random.gauss(0, 0.05),
                "coverage_score": 0.85 + random.gauss(0, 0.02),
            }
        )

    # åˆ›å»ºé¢„æµ‹è¯·æ±‚
    requests = [
        PredictionRequest(
            task=PredictionTask.TEST_DURATION,
            model_type=PredictionModel.RANDOM_FOREST,
            historical_data=historical_data,
            future_horizon=7,
            confidence_threshold=0.8,
        ),
        PredictionRequest(
            task=PredictionTask.PASS_RATE,
            model_type=PredictionModel.LSTM,
            historical_data=historical_data,
            future_horizon=5,
            confidence_threshold=0.7,
        ),
        PredictionRequest(
            task=PredictionTask.RESOURCE_USAGE,
            model_type=PredictionModel.XGBOOST,
            historical_data=historical_data,
            future_horizon=10,
            confidence_threshold=0.6,
        ),
    ]

    # æ‰§è¡Œæ‰¹é‡é¢„æµ‹
    results = analyzer.batch_predict(requests)

    # æ˜¾ç¤ºç»“æœ
    print("\nğŸ“Š é¢„æµ‹ç»“æœ:")
    for i, result in enumerate(results):
        print(f"\né¢„æµ‹ {i + 1}:")
        print(f"  ä»»åŠ¡: {result.request.task.value}")
        print(f"  æ¨¡å‹: {result.request.model_type.value}")
        print(f"  ç½®ä¿¡åº¦: {result.confidence.value}")
        print(f"  é¢„æµ‹åŒºé—´: {result.prediction_interval[0]:.2f} - {result.prediction_interval[1]:.2f}")
        print(f"  æœªæ¥7å¤©é¢„æµ‹: {result.predictions[:7]}")

        if result.error_message:
            print(f"  é”™è¯¯: {result.error_message}")

    # ç”ŸæˆæŠ¥å‘Š
    report = analyzer.generate_prediction_report(results)
    print("\nğŸ“ˆ é¢„æµ‹æŠ¥å‘Š:")
    print(f"  æ€»é¢„æµ‹æ•°: {report['summary']['total_predictions']}")
    print(f"  æˆåŠŸç‡: {report['summary']['overall_success_rate']:.2%}")

    if "recommendations" in report:
        print("\nğŸ’¡ å»ºè®®:")
        for rec in report["recommendations"]:
            print(f"  - {rec}")

    # å¯¼å‡ºç»“æœ
    analyzer.export_predictions(results, "prediction_results.json", "json")
    analyzer.export_predictions(results, "prediction_results.csv", "csv")


if __name__ == "__main__":
    demo_prediction_analyzer()
