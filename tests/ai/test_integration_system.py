#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæµ‹è¯•é›†æˆç³»ç»Ÿ
æä¾›æ™ºèƒ½çš„æµ‹è¯•ç¼–æ’ã€æ‰§è¡Œå’Œåè°ƒåŠŸèƒ½
"""

import asyncio
import json
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from enum import Enum

from .test_ai_assisted_testing import AITestGenerator
from .test_data_analyzer import AITestDataAnalyzer
from .test_data_manager import AITestDataManager
from ..contract.contract_engine import ContractTestEngine

logger = logging.getLogger(__name__)


class TestPhase(Enum):
    """æµ‹è¯•é˜¶æ®µæšä¸¾"""

    PLANNING = "planning"
    GENERATION = "generation"
    EXECUTION = "execution"
    ANALYSIS = "analysis"
    OPTIMIZATION = "optimization"
    REPORTING = "reporting"


@dataclass
class TestOrchestrationConfig:
    """æµ‹è¯•ç¼–æ’é…ç½®"""

    max_concurrent_tests: int = 10
    enable_ai_enhancement: bool = True
    auto_optimize: bool = True
    enable_smart_retry: bool = True
    enable_performance_monitoring: bool = True
    report_format: str = "comprehensive"  # basic, detailed, comprehensive
    storage_path: str = "test_results/ai_integrated"
    data_retention_days: int = 30


@dataclass
class TestExecutionPlan:
    """æµ‹è¯•æ‰§è¡Œè®¡åˆ’"""

    id: str
    name: str
    description: str
    phases: List[TestPhase]
    test_suites: List[str]
    data_profiles: List[str]
    execution_order: List[str]
    dependencies: Dict[str, List[str]] = field(default_factory=dict)
    estimated_duration: float = 0.0
    priority: int = 1
    tags: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class TestExecutionResult:
    """æµ‹è¯•æ‰§è¡Œç»“æœ"""

    plan_id: str
    phase: TestPhase
    status: str  # pending, running, completed, failed, cancelled
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    duration: float = 0.0
    test_cases_executed: int = 0
    test_cases_passed: int = 0
    test_cases_failed: int = 0
    test_cases_skipped: int = 0
    error_message: Optional[str] = None
    ai_insights: Dict[str, Any] = field(default_factory=dict)
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    data_analysis: Dict[str, Any] = field(default_factory=dict)


class IntelligentTestPlanner:
    """æ™ºèƒ½æµ‹è¯•è§„åˆ’å™¨"""

    def __init__(self, ai_generator: AITestGenerator, data_manager: AITestDataManager):
        self.ai_generator = ai_generator
        self.data_manager = data_manager

    def create_test_plan(self, project_context: Dict[str, Any]) -> TestExecutionPlan:
        """åˆ›å»ºæ™ºèƒ½æµ‹è¯•è®¡åˆ’"""
        print("ğŸ¤– AIæ­£åœ¨åˆ›å»ºæµ‹è¯•è®¡åˆ’...")

        plan_id = f"plan_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        plan_name = (
            f"AIä¼˜åŒ–æµ‹è¯•è®¡åˆ’ - {project_context.get('project_name', 'MyStocks')}"
        )

        # åˆ†æé¡¹ç›®ä¸Šä¸‹æ–‡
        analysis = self._analyze_project_context(project_context)

        # ç¡®å®šæµ‹è¯•é˜¶æ®µ
        phases = self._determine_test_phases(analysis)

        # é€‰æ‹©æµ‹è¯•å¥—ä»¶
        test_suites = self._select_test_suites(analysis)

        # é€‰æ‹©æ•°æ®æ¡£æ¡ˆ
        data_profiles = self._select_data_profiles(analysis)

        # ç¡®å®šæ‰§è¡Œé¡ºåº
        execution_order = self._determine_execution_order(test_suites, data_profiles)

        # è®¡ç®—é¢„ä¼°æ—¶é—´
        estimated_duration = self._estimate_duration(test_suites, data_profiles)

        # åˆ›å»ºæ‰§è¡Œè®¡åˆ’
        plan = TestExecutionPlan(
            id=plan_id,
            name=plan_name,
            description=f"åŸºäºAIåˆ†æçš„å…¨é¢æµ‹è¯•è®¡åˆ’ï¼Œè¦†ç›– {len(test_suites)} ä¸ªæµ‹è¯•å¥—ä»¶",
            phases=phases,
            test_suites=test_suites,
            data_profiles=data_profiles,
            execution_order=execution_order,
            estimated_duration=estimated_duration,
            priority=analysis.get("priority", 1),
            tags=analysis.get("tags", []),
            metadata={
                "project_context": project_context,
                "ai_analysis": analysis,
                "created_at": datetime.now().isoformat(),
            },
        )

        logger.info(f"åˆ›å»ºæµ‹è¯•è®¡åˆ’: {plan_name} (é¢„ä¼°æ—¶é—´: {estimated_duration:.2f}s)")
        return plan

    def _analyze_project_context(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æé¡¹ç›®ä¸Šä¸‹æ–‡"""
        analysis = {
            "project_type": context.get("project_type", "web_application"),
            "complexity": "medium",
            "critical_components": [],
            "test_coverage_areas": [],
            "risk_factors": [],
            "priority": 1,
            "tags": [],
        }

        # åˆ†æå¤æ‚åº¦
        if context.get("modules_count", 0) > 20:
            analysis["complexity"] = "high"
        elif context.get("modules_count", 0) < 10:
            analysis["complexity"] = "low"

        # è¯†åˆ«å…³é”®ç»„ä»¶
        critical_modules = ["authentication", "database", "api", "payment", "security"]
        for module in context.get("modules", []):
            if any(crit in module.lower() for crit in critical_modules):
                analysis["critical_components"].append(module)

        # ç¡®å®šæµ‹è¯•è¦†ç›–é¢†åŸŸ
        if "api" in context.get("features", []):
            analysis["test_coverage_areas"].append("api_contract")
        if "database" in context.get("features", []):
            analysis["test_coverage_areas"].append("database")
        if "ui" in context.get("features", []):
            analysis["test_coverage_areas"].append("ui_e2e")

        # è¯„ä¼°é£é™©å› ç´ 
        if analysis["complexity"] == "high":
            analysis["risk_factors"].append("high_complexity")
        if len(analysis["critical_components"]) > 3:
            analysis["risk_factors"].append("many_critical_components")

        return analysis

    def _determine_test_phases(self, analysis: Dict[str, Any]) -> List[TestPhase]:
        """ç¡®å®šæµ‹è¯•é˜¶æ®µ"""
        phases = [TestPhase.PLANNING]

        # åŸºäºå¤æ‚åº¦å†³å®šæ˜¯å¦éœ€è¦æ•°æ®ç”Ÿæˆé˜¶æ®µ
        if analysis["complexity"] in ["high", "medium"]:
            phases.append(TestPhase.GENERATION)

        phases.extend([TestPhase.EXECUTION, TestPhase.ANALYSIS])

        # å¦‚æœæœ‰ä¼˜åŒ–éœ€æ±‚ï¼Œæ·»åŠ ä¼˜åŒ–é˜¶æ®µ
        if analysis.get("risk_factors"):
            phases.append(TestPhase.OPTIMIZATION)

        phases.append(TestPhase.REPORTING)
        return phases

    def _select_test_suites(self, analysis: Dict[str, Any]) -> List[str]:
        """é€‰æ‹©æµ‹è¯•å¥—ä»¶"""
        suites = ["unit_tests", "integration_tests"]

        # åŸºäºæµ‹è¯•è¦†ç›–é¢†åŸŸé€‰æ‹©
        if "api_contract" in analysis["test_coverage_areas"]:
            suites.append("api_contract_tests")
        if "database" in analysis["test_coverage_areas"]:
            suites.append("database_tests")
        if "ui_e2e" in analysis["test_coverage_areas"]:
            suites.append("e2e_tests")

        # å¦‚æœå¤æ‚åº¦é«˜ï¼Œæ·»åŠ æ€§èƒ½æµ‹è¯•
        if analysis["complexity"] == "high":
            suites.append("performance_tests")

        return suites

    def _select_data_profiles(self, analysis: Dict[str, Any]) -> List[str]:
        """é€‰æ‹©æ•°æ®æ¡£æ¡ˆ"""
        profiles = ["unit_test_data", "integration_test_data"]

        # åŸºäºæµ‹è¯•è¦†ç›–é¢†åŸŸé€‰æ‹©
        if "api_contract" in analysis["test_coverage_areas"]:
            profiles.append("integration_test_data")
        if "database" in analysis["test_coverage_areas"]:
            profiles.append("integration_test_data")
        if "e2e_tests" in analysis.get("test_suites", []):
            profiles.append("e2e_test_data")

        return list(set(profiles))  # å»é‡

    def _determine_execution_order(
        self, test_suites: List[str], data_profiles: List[str]
    ) -> List[str]:
        """ç¡®å®šæ‰§è¡Œé¡ºåº"""
        order = []

        # ä¼˜å…ˆæ‰§è¡Œå•å…ƒæµ‹è¯•
        if "unit_tests" in test_suites:
            order.append("unit_tests")

        # ç„¶åæ˜¯æ•°æ®å‡†å¤‡
        for profile in data_profiles:
            order.append(f"prepare_{profile}")

        # æ¥ç€æ˜¯å…¶ä»–æµ‹è¯•
        for suite in test_suites:
            if suite not in order:
                order.append(suite)

        # æœ€åæ˜¯æ¸…ç†
        order.append("cleanup")

        return order

    def _estimate_duration(
        self, test_suites: List[str], data_profiles: List[str]
    ) -> float:
        """é¢„ä¼°æ‰§è¡Œæ—¶é—´"""
        base_times = {
            "unit_tests": 30,
            "integration_tests": 60,
            "api_contract_tests": 45,
            "database_tests": 40,
            "e2e_tests": 120,
            "performance_tests": 180,
            "prepare_unit_test_data": 5,
            "prepare_integration_test_data": 10,
            "prepare_e2e_test_data": 15,
            "cleanup": 10,
        }

        total_time = 0
        for item in self._determine_execution_order(test_suites, data_profiles):
            total_time += base_times.get(item, 30)

        # æ·»åŠ ç¼“å†²æ—¶é—´
        total_time *= 1.2

        return total_time


class SmartTestExecutor:
    """æ™ºèƒ½æµ‹è¯•æ‰§è¡Œå™¨"""

    def __init__(self, config: TestOrchestrationConfig):
        self.config = config
        self.max_workers = config.max_concurrent_tests
        self.semaphore = asyncio.Semaphore(self.max_workers)
        self.execution_results: Dict[str, TestExecutionResult] = {}

    async def execute_test_plan(
        self, plan: TestExecutionPlan, test_executors: Dict[str, Callable]
    ) -> Dict[str, TestExecutionResult]:
        """æ‰§è¡Œæµ‹è¯•è®¡åˆ’"""
        print(f"ğŸ¤– AIæ­£åœ¨æ‰§è¡Œæµ‹è¯•è®¡åˆ’: {plan.name}")

        self.execution_results = {}
        start_time = datetime.now()

        # æŒ‰é˜¶æ®µæ‰§è¡Œ
        for phase in plan.phases:
            if phase == TestPhase.PLANNING:
                await self._execute_planning_phase(plan)
            elif phase == TestPhase.GENERATION:
                await self._execute_generation_phase(plan)
            elif phase == TestPhase.EXECUTION:
                await self._execute_execution_phase(plan, test_executors)
            elif phase == TestPhase.ANALYSIS:
                await self._execute_analysis_phase(plan)
            elif phase == TestPhase.OPTIMIZATION:
                await self._execute_optimization_phase(plan)
            elif phase == TestPhase.REPORTING:
                await self._execute_reporting_phase(plan)

        # è®¡ç®—æ€»æ‰§è¡Œæ—¶é—´
        total_duration = (datetime.now() - start_time).total_seconds()

        logger.info(f"æµ‹è¯•è®¡åˆ’æ‰§è¡Œå®Œæˆ: {plan.name} (è€—æ—¶: {total_duration:.2f}s)")
        return self.execution_results

    async def _execute_planning_phase(self, plan: TestExecutionPlan):
        """æ‰§è¡Œè§„åˆ’é˜¶æ®µ"""
        result = TestExecutionResult(
            plan_id=plan.id, phase=TestPhase.PLANNING, status="completed"
        )
        result.start_time = datetime.now()
        result.end_time = datetime.now()
        result.duration = 0.0
        result.ai_insights = {
            "plan_created": True,
            "phases_count": len(plan.phases),
            "test_suites_count": len(plan.test_suites),
            "estimated_duration": plan.estimated_duration,
        }

        self.execution_results[TestPhase.PLANNING.value] = result

    async def _execute_generation_phase(self, plan: TestExecutionPlan):
        """æ‰§è¡Œæ•°æ®ç”Ÿæˆé˜¶æ®µ"""
        print("ğŸ¤– AIæ­£åœ¨ç”Ÿæˆæµ‹è¯•æ•°æ®...")

        result = TestExecutionResult(
            plan_id=plan.id, phase=TestPhase.GENERATION, status="running"
        )
        result.start_time = datetime.now()

        try:
            # å¹¶è¡Œç”Ÿæˆæ•°æ®
            generation_tasks = []
            for profile_name in plan.data_profiles:
                task = asyncio.create_task(
                    self._generate_data_for_profile(profile_name),
                    name=f"generate_{profile_name}",
                )
                generation_tasks.append(task)

            # ç­‰å¾…æ‰€æœ‰æ•°æ®ç”Ÿæˆå®Œæˆ
            await asyncio.gather(*generation_tasks)

            result.status = "completed"
            result.test_cases_executed = len(plan.data_profiles)
            result.test_cases_passed = len(plan.data_profiles)  # å‡è®¾éƒ½æˆåŠŸ

        except Exception as e:
            result.status = "failed"
            result.error_message = str(e)

        finally:
            result.end_time = datetime.now()
            result.duration = (result.end_time - result.start_time).total_seconds()

        self.execution_results[TestPhase.GENERATION.value] = result

    async def _generate_data_for_profile(self, profile_name: str):
        """ä¸ºæŒ‡å®šæ¡£æ¡ˆç”Ÿæˆæ•°æ®"""
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„AIæ•°æ®ç”Ÿæˆå™¨
            await asyncio.sleep(1)  # æ¨¡æ‹Ÿç”Ÿæˆè¿‡ç¨‹
            print(f"âœ“ æ•°æ®æ¡£æ¡ˆç”Ÿæˆå®Œæˆ: {profile_name}")
        except Exception as e:
            logger.error(f"æ•°æ®æ¡£æ¡ˆ {profile_name} ç”Ÿæˆå¤±è´¥: {e}")

    async def _execute_execution_phase(
        self, plan: TestExecutionPlan, test_executors: Dict[str, Callable]
    ):
        """æ‰§è¡Œæµ‹è¯•é˜¶æ®µ"""
        print("ğŸ¤– AIæ­£åœ¨æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹...")

        result = TestExecutionResult(
            plan_id=plan.id, phase=TestPhase.EXECUTION, status="running"
        )
        result.start_time = datetime.now()

        try:
            # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
            execution_tasks = []
            for test_suite in plan.test_suites:
                if test_suite in test_executors:
                    task = asyncio.create_task(
                        self._execute_test_suite_with_semaphore(
                            test_suite, test_executors[test_suite]
                        ),
                        name=f"execute_{test_suite}",
                    )
                    execution_tasks.append(task)

            # ç­‰å¾…æ‰€æœ‰æµ‹è¯•æ‰§è¡Œå®Œæˆ
            suite_results = await asyncio.gather(
                *execution_tasks, return_exceptions=True
            )

            # æ±‡æ€»ç»“æœ
            total_passed = 0
            total_failed = 0
            total_skipped = 0

            for i, suite_result in enumerate(suite_results):
                if isinstance(suite_result, Exception):
                    total_failed += 1
                elif isinstance(suite_result, dict):
                    total_passed += suite_result.get("passed", 0)
                    total_failed += suite_result.get("failed", 0)
                    total_skipped += suite_result.get("skipped", 0)
                else:
                    total_passed += 1

            result.test_cases_executed = len(plan.test_suites)
            result.test_cases_passed = total_passed
            result.test_cases_failed = total_failed
            result.test_cases_skipped = total_skipped

            result.status = (
                "completed" if total_failed == 0 else "completed_with_failures"
            )

        except Exception as e:
            result.status = "failed"
            result.error_message = str(e)

        finally:
            result.end_time = datetime.now()
            result.duration = (result.end_time - result.start_time).total_seconds()

        self.execution_results[TestPhase.EXECUTION.value] = result

    async def _execute_test_suite_with_semaphore(
        self, test_suite: str, executor: Callable
    ):
        """ä½¿ç”¨ä¿¡å·é‡æ‰§è¡Œæµ‹è¯•å¥—ä»¶"""
        async with self.semaphore:
            return await executor()

    async def _execute_analysis_phase(self, plan: TestExecutionPlan):
        """æ‰§è¡Œåˆ†æé˜¶æ®µ"""
        print("ğŸ¤– AIæ­£åœ¨åˆ†ææµ‹è¯•ç»“æœ...")

        result = TestExecutionResult(
            plan_id=plan.id, phase=TestPhase.ANALYSIS, status="running"
        )
        result.start_time = datetime.now()

        try:
            # æ”¶é›†æ‰§è¡Œç»“æœ
            execution_result = self.execution_results.get(TestPhase.EXECUTION.value)
            if not execution_result:
                raise ValueError("æ‰§è¡Œé˜¶æ®µç»“æœä¸å­˜åœ¨")

            # æ‰§è¡ŒAIåˆ†æ
            analysis = self._perform_ai_analysis(execution_result)

            result.status = "completed"
            result.test_cases_executed = 1  # åˆ†æä½œä¸ºä¸€ä¸ªæ•´ä½“ä»»åŠ¡
            result.test_cases_passed = 1
            result.data_analysis = analysis

        except Exception as e:
            result.status = "failed"
            result.error_message = str(e)

        finally:
            result.end_time = datetime.now()
            result.duration = (result.end_time - result.start_time).total_seconds()

        self.execution_results[TestPhase.ANALYSIS.value] = result

    def _perform_ai_analysis(
        self, execution_result: TestExecutionResult
    ) -> Dict[str, Any]:
        """æ‰§è¡ŒAIåˆ†æ"""
        analysis = {
            "test_quality_score": 0.0,
            "performance_insights": [],
            "optimization_suggestions": [],
            "risk_assessment": "low",
        }

        # è®¡ç®—æµ‹è¯•è´¨é‡åˆ†æ•°
        if execution_result.test_cases_executed > 0:
            pass_rate = (
                execution_result.test_cases_passed
                / execution_result.test_cases_executed
            )
            analysis["test_quality_score"] = round(pass_rate * 100, 2)

        # æ€§èƒ½æ´å¯Ÿ
        if execution_result.duration > 60:  # è¶…è¿‡1åˆ†é’Ÿ
            analysis["performance_insights"].append(
                "æµ‹è¯•æ‰§è¡Œæ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–æµ‹è¯•ç”¨ä¾‹"
            )

        # ä¼˜åŒ–å»ºè®®
        if execution_result.test_cases_failed > 0:
            analysis["optimization_suggestions"].append(
                f"ä¿®å¤ {execution_result.test_cases_failed} ä¸ªå¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹"
            )

        # é£é™©è¯„ä¼°
        if analysis["test_quality_score"] < 80:
            analysis["risk_assessment"] = "high"
        elif analysis["test_quality_score"] < 90:
            analysis["risk_assessment"] = "medium"

        return analysis

    async def _execute_optimization_phase(self, plan: TestExecutionPlan):
        """æ‰§è¡Œä¼˜åŒ–é˜¶æ®µ"""
        print("ğŸ¤– AIæ­£åœ¨ä¼˜åŒ–æµ‹è¯•é…ç½®...")

        result = TestExecutionResult(
            plan_id=plan.id, phase=TestPhase.OPTIMIZATION, status="running"
        )
        result.start_time = datetime.now()

        try:
            # æ‰§è¡ŒAIä¼˜åŒ–
            optimization_result = self._perform_ai_optimization(plan)

            result.status = "completed"
            result.test_cases_executed = 1
            result.test_cases_passed = 1
            result.ai_insights = optimization_result

        except Exception as e:
            result.status = "failed"
            result.error_message = str(e)

        finally:
            result.end_time = datetime.now()
            result.duration = (result.end_time - result.start_time).total_seconds()

        self.execution_results[TestPhase.OPTIMIZATION.value] = result

    def _perform_ai_optimization(self, plan: TestExecutionPlan) -> Dict[str, Any]:
        """æ‰§è¡ŒAIä¼˜åŒ–"""
        optimizations = {
            "suggested_changes": [],
            "performance_improvements": [],
            "quality_enhancements": [],
        }

        # åŸºäºæ‰§è¡Œç»“æœç”Ÿæˆä¼˜åŒ–å»ºè®®
        execution_result = self.execution_results.get(TestPhase.EXECUTION.value)
        if execution_result and execution_result.test_cases_failed > 0:
            optimizations["suggested_changes"].append(
                {
                    "type": "test_fix",
                    "description": f"ä¿®å¤ {execution_result.test_cases_failed} ä¸ªå¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹",
                    "priority": "high",
                }
            )

        analysis_result = self.execution_results.get(TestPhase.ANALYSIS.value)
        if analysis_result and analysis_result.data_analysis:
            quality_score = analysis_result.data_analysis.get("test_quality_score", 0)
            if quality_score < 90:
                optimizations["quality_enhancements"].append(
                    {
                        "type": "test_coverage",
                        "description": f"æå‡æµ‹è¯•è¦†ç›–ç‡è‡³ 90% (å½“å‰: {quality_score}%)",
                        "priority": "medium",
                    }
                )

        return optimizations

    async def _execute_reporting_phase(self, plan: TestExecutionPlan):
        """æ‰§è¡ŒæŠ¥å‘Šé˜¶æ®µ"""
        print("ğŸ¤– AIæ­£åœ¨ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")

        result = TestExecutionResult(
            plan_id=plan.id, phase=TestPhase.REPORTING, status="running"
        )
        result.start_time = datetime.now()

        try:
            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            report = self._generate_comprehensive_report(plan)

            result.status = "completed"
            result.test_cases_executed = 1
            result.test_cases_passed = 1
            result.ai_insights = {
                "report_generated": True,
                "report_summary": report["summary"],
            }

        except Exception as e:
            result.status = "failed"
            result.error_message = str(e)

        finally:
            result.end_time = datetime.now()
            result.duration = (result.end_time - result.start_time).total_seconds()

        self.execution_results[TestPhase.REPORTING.value] = result

    def _generate_comprehensive_report(self, plan: TestExecutionPlan) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        report = {
            "summary": {
                "plan_name": plan.name,
                "total_duration": 0,
                "total_test_cases": 0,
                "total_passed": 0,
                "total_failed": 0,
                "overall_success_rate": 0,
            },
            "phase_results": {},
            "ai_insights": {},
            "recommendations": [],
        }

        # æ±‡æ€»å„é˜¶æ®µç»“æœ
        total_duration = 0
        total_test_cases = 0
        total_passed = 0
        total_failed = 0

        for phase_result in self.execution_results.values():
            total_duration += phase_result.duration
            total_test_cases += phase_result.test_cases_executed
            total_passed += phase_result.test_cases_passed
            total_failed += phase_result.test_cases_failed

            report["phase_results"][phase_result.phase.value] = {
                "status": phase_result.status,
                "duration": phase_result.duration,
                "test_cases": {
                    "executed": phase_result.test_cases_executed,
                    "passed": phase_result.test_cases_passed,
                    "failed": phase_result.test_cases_failed,
                    "skipped": phase_result.test_cases_skipped,
                },
            }

            # æ”¶é›†AIæ´å¯Ÿ
            if phase_result.ai_insights:
                report["ai_insights"][phase_result.phase.value] = (
                    phase_result.ai_insights
                )

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        report["summary"]["total_duration"] = round(total_duration, 2)
        report["summary"]["total_test_cases"] = total_test_cases
        report["summary"]["total_passed"] = total_passed
        report["summary"]["total_failed"] = total_failed
        report["summary"]["overall_success_rate"] = round(
            (total_passed / total_test_cases * 100) if total_test_cases > 0 else 0, 2
        )

        # ç”Ÿæˆå»ºè®®
        if total_failed > 0:
            report["recommendations"].append(f"ä¿®å¤ {total_failed} ä¸ªå¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹")
        if total_duration > 300:  # è¶…è¿‡5åˆ†é’Ÿ
            report["recommendations"].append("ä¼˜åŒ–æµ‹è¯•æ‰§è¡Œé€Ÿåº¦")

        return report


class AITestIntegrationSystem:
    """AIæµ‹è¯•é›†æˆç³»ç»Ÿ - ä¸»æ§åˆ¶å™¨"""

    def __init__(self, config: TestOrchestrationConfig):
        self.config = config
        self.ai_generator = AITestGenerator()
        self.data_analyzer = AITestDataAnalyzer()
        self.data_manager = AITestDataManager()
        self.test_planner = IntelligentTestPlanner(self.ai_generator, self.data_manager)
        self.test_executor = SmartTestExecutor(config)
        self.test_engine = ContractTestEngine()

        # åˆ›å»ºå­˜å‚¨ç›®å½•
        Path(config.storage_path).mkdir(parents=True, exist_ok=True)

    async def run_intelligent_testing(
        self, project_context: Dict[str, Any], test_executors: Dict[str, Callable]
    ) -> Dict[str, Any]:
        """è¿è¡Œæ™ºèƒ½æµ‹è¯•"""
        print("ğŸš€ å¯åŠ¨AIæ™ºèƒ½æµ‹è¯•ç³»ç»Ÿ...")

        try:
            # 1. åˆ›å»ºæµ‹è¯•è®¡åˆ’
            test_plan = self.test_planner.create_test_plan(project_context)

            # 2. æ‰§è¡Œæµ‹è¯•è®¡åˆ’
            execution_results = await self.test_executor.execute_test_plan(
                test_plan, test_executors
            )

            # 3. åˆ†ææµ‹è¯•ç»“æœ
            analysis_result = self.analyze_test_results(execution_results)

            # 4. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            final_report = self.generate_final_report(
                test_plan, execution_results, analysis_result
            )

            # 5. ä¿å­˜ç»“æœ
            self.save_test_results(test_plan, execution_results, final_report)

            # 6. è‡ªåŠ¨ä¼˜åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.config.auto_optimize:
                await self.auto_optimize_testing(execution_results)

            print("âœ… AIæ™ºèƒ½æµ‹è¯•å®Œæˆ!")
            return final_report

        except Exception as e:
            logger.error(f"æ™ºèƒ½æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            return {"error": str(e), "status": "failed"}

    def analyze_test_results(
        self, execution_results: Dict[str, TestExecutionResult]
    ) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœ"""
        print("ğŸ¤– AIæ­£åœ¨åˆ†ææµ‹è¯•ç»“æœ...")

        analysis = {
            "overall_summary": {},
            "phase_analysis": {},
            "ai_insights": {},
            "trends": {},
            "recommendations": [],
        }

        # æ€»ä½“åˆ†æ
        total_duration = 0
        total_tests = 0
        total_passed = 0
        total_failed = 0

        for result in execution_results.values():
            total_duration += result.duration
            total_tests += result.test_cases_executed
            total_passed += result.test_cases_passed
            total_failed += result.test_cases_failed

            # é˜¶æ®µåˆ†æ
            phase_key = result.phase.value
            analysis["phase_analysis"][phase_key] = {
                "status": result.status,
                "duration": result.duration,
                "efficiency": result.test_cases_passed / result.test_cases_executed
                if result.test_cases_executed > 0
                else 0,
            }

            # AIæ´å¯Ÿ
            if result.ai_insights:
                analysis["ai_insights"][phase_key] = result.ai_insights

        # æ€»ä½“ç»Ÿè®¡
        analysis["overall_summary"] = {
            "total_duration": round(total_duration, 2),
            "total_tests": total_tests,
            "total_passed": total_passed,
            "total_failed": total_failed,
            "success_rate": round(total_passed / total_tests * 100, 2)
            if total_tests > 0
            else 0,
        }

        # è¶‹åŠ¿åˆ†æ
        analysis["trends"] = self.data_analyzer.analyze_test_trends(execution_results)

        # ç”Ÿæˆå»ºè®®
        if total_failed > 0:
            analysis["recommendations"].append(f"ä¼˜å…ˆä¿®å¤ {total_failed} ä¸ªå¤±è´¥çš„æµ‹è¯•")
        if total_duration > 300:
            analysis["recommendations"].append("ä¼˜åŒ–æµ‹è¯•æ‰§è¡Œæ•ˆç‡")

        return analysis

    def generate_final_report(
        self,
        test_plan: TestExecutionPlan,
        execution_results: Dict[str, TestExecutionResult],
        analysis: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        print("ğŸ¤– AIæ­£åœ¨ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")

        report = {
            "metadata": {
                "plan_id": test_plan.id,
                "plan_name": test_plan.name,
                "generated_at": datetime.now().isoformat(),
                "system_version": "1.0.0",
            },
            "execution_summary": analysis["overall_summary"],
            "plan_details": {
                "phases": [phase.value for phase in test_plan.phases],
                "test_suites": test_plan.test_suites,
                "data_profiles": test_plan.data_profiles,
                "estimated_duration": test_plan.estimated_duration,
                "actual_duration": analysis["overall_summary"]["total_duration"],
            },
            "execution_results": {},
            "ai_analysis": analysis["ai_insights"],
            "trends_analysis": analysis["trends"],
            "recommendations": analysis["recommendations"],
            "data_management_insights": self.data_manager.get_data_insights(),
        }

        # æ·»åŠ å„é˜¶æ®µè¯¦ç»†ç»“æœ
        for phase, result in execution_results.items():
            report["execution_results"][phase] = {
                "status": result.status,
                "duration": result.duration,
                "test_cases": {
                    "executed": result.test_cases_executed,
                    "passed": result.test_cases_passed,
                    "failed": result.test_cases_failed,
                    "skipped": result.test_cases_skipped,
                },
                "performance_metrics": result.performance_metrics,
                "data_analysis": result.data_analysis,
            }

        return report

    def save_test_results(
        self,
        test_plan: TestExecutionPlan,
        execution_results: Dict[str, TestExecutionResult],
        report: Dict[str, Any],
    ):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"test_results_{test_plan.id}_{timestamp}.json"
        filepath = Path(self.config.storage_path) / filename

        try:
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)

            # åˆ›å»ºç¬¦å·é“¾æ¥
            latest_link = Path(self.config.storage_path) / "latest_results.json"
            if latest_link.exists():
                latest_link.unlink()
            latest_link.symlink_to(filename)

            logger.info(f"æµ‹è¯•ç»“æœå·²ä¿å­˜: {filepath}")

        except Exception as e:
            logger.error(f"ä¿å­˜æµ‹è¯•ç»“æœå¤±è´¥: {e}")

    async def auto_optimize_testing(
        self, execution_results: Dict[str, TestExecutionResult]
    ):
        """è‡ªåŠ¨ä¼˜åŒ–æµ‹è¯•"""
        print("ğŸ¤– AIæ­£åœ¨è‡ªåŠ¨ä¼˜åŒ–æµ‹è¯•...")

        try:
            # ä¼˜åŒ–æ•°æ®ç®¡ç†
            test_data_for_optimization = [
                {
                    "profile_name": "unit_test_data",
                    "status": "success" if result.status == "completed" else "failed",
                    "execution_time": result.duration,
                    "timestamp": datetime.now().isoformat(),
                }
                for result in execution_results.values()
                if result.test_cases_executed > 0
            ]

            self.data_manager.optimize_data_management(test_data_for_optimization)

            logger.info("æµ‹è¯•è‡ªåŠ¨ä¼˜åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"è‡ªåŠ¨ä¼˜åŒ–å¤±è´¥: {e}")

    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        return {
            "config": {
                "max_concurrent_tests": self.config.max_concurrent_tests,
                "enable_ai_enhancement": self.config.enable_ai_enhancement,
                "auto_optimize": self.config.auto_optimize,
                "storage_path": self.config.storage_path,
            },
            "system_components": {
                "ai_generator": "active",
                "data_analyzer": "active",
                "data_manager": "active",
                "test_planner": "active",
                "test_executor": "active",
                "test_engine": "active",
            },
            "data_storage": self.data_manager.get_data_insights(),
            "recent_executions": len(self.test_executor.execution_results),
        }
