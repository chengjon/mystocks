#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIæµ‹è¯•æ•°æ®ç®¡ç†å™¨
æä¾›æ™ºèƒ½çš„æµ‹è¯•æ•°æ®ç”Ÿæˆã€ç®¡ç†å’Œä¼˜åŒ–åŠŸèƒ½
"""

import json
import logging
import random
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
import numpy as np
from dataclasses import dataclass, field
import hashlib

from .test_ai_assisted_testing import AITestGenerator, IntelligentTestOptimizer
from .test_data_analyzer import AITestDataAnalyzer

logger = logging.getLogger(__name__)


@dataclass
class TestDataProfile:
    """æµ‹è¯•æ•°æ®é…ç½®æ¡£æ¡ˆ"""

    name: str
    description: str
    data_type: str  # 'unit', 'integration', 'e2e', 'performance'
    size: int
    constraints: Dict[str, Any] = field(default_factory=dict)
    freshness: float = 1.0  # 0-1 æ•°æ®æ–°é²œåº¦
    quality_score: float = 1.0  # 0-1 æ•°æ®è´¨é‡åˆ†æ•°
    usage_count: int = 0
    last_used: datetime = field(default_factory=datetime.now)
    generation_source: str = "ai_generated"
    cost_metrics: Dict[str, float] = field(default_factory=dict)


@dataclass
class DataGenerationRequest:
    """æ•°æ®ç”Ÿæˆè¯·æ±‚"""

    profile_name: str
    target_size: int
    data_schema: Dict[str, Any]
    use_ai_enhancement: bool = True
    simulate_real_data: bool = True
    seed: Optional[int] = None
    constraints: Dict[str, Any] = field(default_factory=dict)


class IntelligentDataGenerator:
    """æ™ºèƒ½æ•°æ®ç”Ÿæˆå™¨"""

    def __init__(self):
        self.data_cache: Dict[str, Any] = {}
        self.generation_history: List[Dict[str, Any]] = []
        self.generator = AITestGenerator()

    def generate_test_data(self, request: DataGenerationRequest) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        print(f"ğŸ¤– AIæ­£åœ¨ç”Ÿæˆæµ‹è¯•æ•°æ®: {request.profile_name}")

        start_time = time.time()

        try:
            # ç”ŸæˆåŸºç¡€æ•°æ®
            if request.use_ai_enhancement:
                data = self._generate_ai_enhanced_data(request)
            else:
                data = self._generate_basic_data(request)

            # åº”ç”¨çº¦æŸ
            constrained_data = self._apply_constraints(data, request.constraints)

            # æ¨¡æ‹ŸçœŸå®æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if request.simulate_real_data:
                constrained_data = self._simulate_real_patterns(constrained_data)

            # ç¼“å­˜ç»“æœ
            data_key = self._generate_data_key(request)
            self.data_cache[data_key] = constrained_data

            # è®°å½•ç”Ÿæˆå†å²
            self._record_generation(request, constrained_data, start_time)

            return constrained_data

        except Exception as e:
            logger.error(f"æ•°æ®ç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_fallback_data(request)

    def _generate_ai_enhanced_data(
        self, request: DataGenerationRequest
    ) -> Dict[str, Any]:
        """AIå¢å¼ºçš„æ•°æ®ç”Ÿæˆ"""
        # ä½¿ç”¨AIåˆ†ææ•°æ®æ¨¡å¼
        if request.seed:
            np.random.seed(request.seed)
            random.seed(request.seed)

        data = {}

        # ç”Ÿæˆæ•´æ•°å‹æ•°æ®
        for field_name, field_schema in request.data_schema.get(
            "properties", {}
        ).items():
            field_type = field_schema.get("type")

            if field_type == "integer":
                data[field_name] = self._generate_integer_data(field_schema)
            elif field_type == "number":
                data[field_name] = self._generate_number_data(field_schema)
            elif field_type == "string":
                data[field_name] = self._generate_string_data(field_schema)
            elif field_type == "boolean":
                data[field_name] = random.choice([True, False])
            elif field_type == "array":
                data[field_name] = self._generate_array_data(field_schema)
            elif field_type == "object":
                data[field_name] = self._generate_object_data(field_schema)

        return data

    def _generate_integer_data(self, schema: Dict[str, Any]) -> int:
        """ç”Ÿæˆæ•´æ•°æ•°æ®"""
        minimum = schema.get("minimum", 0)
        maximum = schema.get("maximum", 1000)

        # åº”ç”¨èŒƒå›´çº¦æŸ
        if minimum is not None and maximum is not None:
            return random.randint(minimum, maximum)
        elif minimum is not None:
            return random.randint(minimum, minimum + 1000)
        else:
            return random.randint(0, 1000)

    def _generate_number_data(self, schema: Dict[str, Any]) -> float:
        """ç”Ÿæˆæµ®ç‚¹æ•°æ•°æ®"""
        minimum = schema.get("minimum", 0.0)
        maximum = schema.get("maximum", 1000.0)

        # ç”Ÿæˆå¸¦è¶‹åŠ¿çš„æ•°å€¼ï¼ˆæ¨¡æ‹ŸçœŸå®å¸‚åœºæ•°æ®ï¼‰
        trend_factor = random.uniform(0.95, 1.05)
        value = random.uniform(minimum, maximum) * trend_factor

        return round(value, 2)

    def _generate_string_data(self, schema: Dict[str, Any]) -> str:
        """ç”Ÿæˆå­—ç¬¦ä¸²æ•°æ®"""
        format_type = schema.get("format", "")

        if format_type == "email":
            return f"test_{random.randint(1000, 9999)}@example.com"
        elif format_type == "date-time":
            return datetime.now().isoformat()
        elif format_type == "date":
            return datetime.now().date().isoformat()
        else:
            # ç”Ÿæˆæœ‰æ„ä¹‰çš„æµ‹è¯•å­—ç¬¦ä¸²
            patterns = [
                f"æµ‹è¯•_{random.choice(['ç”¨æˆ·', 'è‚¡ç¥¨', 'ç­–ç•¥', 'æŒ‡æ ‡'])}_{random.randint(1, 100)}",
                f"data_{random.choice(['market', 'trade', 'analysis', 'report'])}_{random.randint(100, 999)}",
                f"{random.choice(['æˆåŠŸ', 'å¤±è´¥', 'å¤„ç†ä¸­', 'å·²å®Œæˆ'])}_{random.randint(1000, 9999)}",
            ]
            return random.choice(patterns)

    def _generate_array_data(self, schema: Dict[str, Any]) -> List[Any]:
        """ç”Ÿæˆæ•°ç»„æ•°æ®"""
        min_items = schema.get("minItems", 1)
        max_items = schema.get("maxItems", 5)
        item_count = random.randint(min_items, max_items)

        items_schema = schema.get("items", {})
        item_type = items_schema.get("type", "string")

        items = []
        for i in range(item_count):
            if item_type == "string":
                items.append(f"item_{i + 1}")
            elif item_type == "integer":
                items.append(random.randint(1, 100))
            elif item_type == "number":
                items.append(round(random.uniform(0.1, 100.0), 2))

        return items

    def _generate_object_data(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå¯¹è±¡æ•°æ®"""
        properties = schema.get("properties", {})
        obj = {}

        for prop_name, prop_schema in properties.items():
            obj[prop_name] = self._generate_string_data(prop_schema)

        return obj

    def _apply_constraints(
        self, data: Dict[str, Any], constraints: Dict[str, Any]
    ) -> Dict[str, Any]:
        """åº”ç”¨çº¦æŸæ¡ä»¶"""
        constraint_type = constraints.get("type")

        if constraint_type == "range":
            return self._apply_range_constraints(data, constraints)
        elif constraint_type == "pattern":
            return self._apply_pattern_constraints(data, constraints)
        elif constraint_type == "uniqueness":
            return self._apply_uniqueness_constraints(data, constraints)
        else:
            return data

    def _apply_range_constraints(
        self, data: Dict[str, Any], constraints: Dict[str, Any]
    ) -> Dict[str, Any]:
        """åº”ç”¨èŒƒå›´çº¦æŸ"""
        field_ranges = constraints.get("ranges", {})

        for field_name, range_info in field_ranges.items():
            if field_name in data and isinstance(data[field_name], (int, float)):
                min_val = range_info.get("min")
                max_val = range_info.get("max")

                if min_val is not None and data[field_name] < min_val:
                    data[field_name] = min_val
                if max_val is not None and data[field_name] > max_val:
                    data[field_name] = max_val

        return data

    def _apply_pattern_constraints(
        self, data: Dict[str, Any], constraints: Dict[str, Any]
    ) -> Dict[str, Any]:
        """åº”ç”¨æ¨¡å¼çº¦æŸ"""
        # å¯ä»¥æ·»åŠ æ›´å¤æ‚çš„æ¨¡å¼åŒ¹é…é€»è¾‘
        return data

    def _apply_uniqueness_constraints(
        self, data: Dict[str, Any], constraints: Dict[str, Any]
    ) -> Dict[str, Any]:
        """åº”ç”¨å”¯ä¸€æ€§çº¦æŸ"""
        # ç¡®ä¿ç”Ÿæˆçš„æ•°æ®å…·æœ‰å”¯ä¸€æ€§
        return data

    def _simulate_real_patterns(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ¨¡æ‹ŸçœŸå®æ•°æ®æ¨¡å¼"""
        # æ·»åŠ æ—¶é—´åºåˆ—è¶‹åŠ¿
        if "price" in data:
            # ä»·æ ¼é€šå¸¸å‘ˆç°è¶‹åŠ¿æ€§
            base_price = data["price"]
            if isinstance(base_price, (int, float)):
                data["price"] = base_price * random.uniform(0.95, 1.05)

        # æ·»åŠ ç›¸å…³æ€§
        if "volume" in data and "price" in data:
            # äº¤æ˜“é‡å’Œä»·æ ¼æœ‰ç›¸å…³æ€§
            volume_multiplier = data.get("price", 1) / 100
            data["volume"] = int(data.get("volume", 1000) * volume_multiplier)

        return data

    def _generate_basic_data(self, request: DataGenerationRequest) -> Dict[str, Any]:
        """ç”ŸæˆåŸºç¡€æ•°æ®"""
        # ç®€å•çš„æ•°æ®ç”Ÿæˆé€»è¾‘
        return {
            "id": f"test_{random.randint(1000, 9999)}",
            "name": f"æµ‹è¯•æ•°æ®_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "value": random.randint(1, 100),
            "timestamp": datetime.now().isoformat(),
        }

    def _generate_fallback_data(self, request: DataGenerationRequest) -> Dict[str, Any]:
        """ç”Ÿæˆå¤‡ç”¨æ•°æ®"""
        return {
            "id": f"fallback_{hashlib.md5(str(time.time()).encode()).hexdigest()[:8]}",
            "name": "å¤‡ç”¨æµ‹è¯•æ•°æ®",
            "value": 0,
            "timestamp": datetime.now().isoformat(),
            "error": "æ•°æ®ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ•°æ®",
        }

    def _generate_data_key(self, request: DataGenerationRequest) -> str:
        """ç”Ÿæˆæ•°æ®ç¼“å­˜é”®"""
        key_data = f"{request.profile_name}_{request.target_size}_{json.dumps(request.data_schema, sort_keys=True)}"
        return hashlib.md5(key_data.encode()).hexdigest()

    def _record_generation(
        self, request: DataGenerationRequest, data: Dict[str, Any], start_time: float
    ):
        """è®°å½•ç”Ÿæˆå†å²"""
        history_entry = {
            "timestamp": datetime.now(),
            "profile_name": request.profile_name,
            "target_size": request.target_size,
            "use_ai_enhancement": request.use_ai_enhancement,
            "generation_time": time.time() - start_time,
            "data_size": len(json.dumps(data)),
            "cache_hit": False,
        }
        self.generation_history.append(history_entry)

        # ä¿ç•™æœ€è¿‘1000æ¡è®°å½•
        if len(self.generation_history) > 1000:
            self.generation_history = self.generation_history[-1000:]


class IntelligentDataOptimizer:
    """æ™ºèƒ½æ•°æ®ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.optimizer = IntelligentTestOptimizer()

    def optimize_data_profiles(
        self, profiles: List[TestDataProfile], test_results: List[Dict[str, Any]]
    ) -> List[TestDataProfile]:
        """ä¼˜åŒ–æ•°æ®æ¡£æ¡ˆ"""
        print("ğŸ¤– AIæ­£åœ¨ä¼˜åŒ–æ•°æ®æ¡£æ¡ˆ...")

        optimized_profiles = []

        for profile in profiles:
            # åˆ†ææµ‹è¯•ç»“æœä¸­çš„æ€§èƒ½æŒ‡æ ‡
            performance_metrics = self._analyze_performance_metrics(
                profile, test_results
            )

            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            optimization_suggestions = self._generate_optimization_suggestions(
                profile, performance_metrics
            )

            # åº”ç”¨ä¼˜åŒ–
            optimized_profile = self._apply_optimizations(
                profile, optimization_suggestions
            )

            optimized_profiles.append(optimized_profile)

        return optimized_profiles

    def _analyze_performance_metrics(
        self, profile: TestDataProfile, test_results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½æŒ‡æ ‡"""
        metrics = {
            "avg_execution_time": 0,
            "success_rate": 0,
            "memory_usage": 0,
            "cpu_usage": 0,
            "data_freshness_needed": True,
        }

        # è¿‡æ»¤ç›¸å…³çš„æµ‹è¯•ç»“æœ
        relevant_results = [
            r for r in test_results if r.get("data_profile") == profile.name
        ]

        if relevant_results:
            execution_times = [r.get("execution_time", 0) for r in relevant_results]
            success_count = sum(
                1 for r in relevant_results if r.get("status") == "success"
            )

            metrics["avg_execution_time"] = sum(execution_times) / len(execution_times)
            metrics["success_rate"] = success_count / len(relevant_results)

            # æ£€æŸ¥æ•°æ®æ–°é²œåº¦
            last_used = profile.last_used
            time_diff = datetime.now() - last_used

            metrics["data_freshness_needed"] = time_diff.total_seconds() > 3600  # 1å°æ—¶

        return metrics

    def _generate_optimization_suggestions(
        self, profile: TestDataProfile, metrics: Dict[str, Any]
    ) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []

        if metrics["avg_execution_time"] > 1.0:  # æ‰§è¡Œæ—¶é—´è¿‡é•¿
            suggestions.append(
                f"å‡å°‘æ•°æ®å¤§å°ä»¥é™ä½æ‰§è¡Œæ—¶é—´ (å½“å‰: {metrics['avg_execution_time']:.2f}s)"
            )

        if metrics["success_rate"] < 0.9:  # æˆåŠŸç‡ä½
            suggestions.append(
                f"æ”¹å–„æ•°æ®è´¨é‡ä»¥æé«˜æˆåŠŸç‡ (å½“å‰: {metrics['success_rate']:.2%})"
            )

        if metrics["data_freshness_needed"]:
            suggestions.append("æ›´æ–°æ•°æ®ä»¥ä¿æŒæ–°é²œåº¦")

        if profile.quality_score < 0.8:
            suggestions.append("æå‡æ•°æ®è´¨é‡åˆ†æ•° (å½“å‰: {profile.quality_score:.2f})")

        return suggestions

    def _apply_optimizations(
        self, profile: TestDataProfile, suggestions: List[str]
    ) -> TestDataProfile:
        """åº”ç”¨ä¼˜åŒ–"""
        optimized_profile = TestDataProfile(
            name=profile.name,
            description=profile.description,
            data_type=profile.data_type,
            size=max(profile.size // 2, 100),  # å‡å°‘æ•°æ®å¤§å°
            constraints=profile.constraints,
            freshness=min(profile.freshness + 0.1, 1.0),  # æé«˜æ–°é²œåº¦
            quality_score=min(profile.quality_score + 0.1, 1.0),  # æé«˜è´¨é‡åˆ†æ•°
            usage_count=profile.usage_count,
            last_used=datetime.now(),
            generation_source=f"ai_optimized_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            cost_metrics=profile.cost_metrics,
        )

        # è®°å½•ä¼˜åŒ–å†å²
        logger.info(f"æ•°æ®æ¡£æ¡ˆ {profile.name} å·²ä¼˜åŒ–: {suggestions}")

        return optimized_profile


class AITestDataManager:
    """AIæµ‹è¯•æ•°æ®ç®¡ç†å™¨ - ä¸»æ§åˆ¶å™¨"""

    def __init__(self):
        self.profiles: Dict[str, TestDataProfile] = {}
        self.generator = IntelligentDataGenerator()
        self.optimizer = IntelligentDataOptimizer()
        self.analyzer = AITestDataAnalyzer()
        self.data_storage_path = Path("test_data/ai_managed")
        self.data_storage_path.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–é»˜è®¤æ¡£æ¡ˆ
        self._initialize_default_profiles()

    def _initialize_default_profiles(self):
        """åˆå§‹åŒ–é»˜è®¤æ•°æ®æ¡£æ¡ˆ"""
        default_profiles = [
            TestDataProfile(
                name="unit_test_data",
                description="å•å…ƒæµ‹è¯•æ•°æ®",
                data_type="unit",
                size=100,
                constraints={"type": "unit", "coverage": 80},
                generation_source="ai_generated",
            ),
            TestDataProfile(
                name="integration_test_data",
                description="é›†æˆæµ‹è¯•æ•°æ®",
                data_type="integration",
                size=500,
                constraints={"type": "integration", "complexity": "medium"},
                generation_source="ai_generated",
            ),
            TestDataProfile(
                name="e2e_test_data",
                description="ç«¯åˆ°ç«¯æµ‹è¯•æ•°æ®",
                data_type="e2e",
                size=1000,
                constraints={"type": "e2e", "realism": "high"},
                generation_source="ai_generated",
            ),
            TestDataProfile(
                name="performance_test_data",
                description="æ€§èƒ½æµ‹è¯•æ•°æ®",
                data_type="performance",
                size=10000,
                constraints={"type": "performance", "size": "large"},
                generation_source="ai_generated",
            ),
        ]

        for profile in default_profiles:
            self.profiles[profile.name] = profile

    def create_data_profile(
        self,
        name: str,
        description: str,
        data_type: str,
        size: int,
        constraints: Dict[str, Any] = None,
    ) -> TestDataProfile:
        """åˆ›å»ºæ•°æ®æ¡£æ¡ˆ"""
        profile = TestDataProfile(
            name=name,
            description=description,
            data_type=data_type,
            size=size,
            constraints=constraints or {},
            generation_source="user_created",
        )

        self.profiles[name] = profile
        logger.info(f"åˆ›å»ºæ•°æ®æ¡£æ¡ˆ: {name}")
        return profile

    def generate_test_data(
        self,
        profile_name: str,
        data_schema: Dict[str, Any],
        request_params: Dict[str, Any] = None,
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        if profile_name not in self.profiles:
            raise ValueError(f"æ•°æ®æ¡£æ¡ˆ {profile_name} ä¸å­˜åœ¨")

        profile = self.profiles[profile_name]

        request = DataGenerationRequest(
            profile_name=profile_name,
            target_size=profile.size,
            data_schema=data_schema,
            use_ai_enhancement=request_params.get("use_ai_enhancement", True)
            if request_params
            else True,
            simulate_real_data=request_params.get("simulate_real_data", True)
            if request_params
            else True,
            seed=request_params.get("seed") if request_params else None,
            constraints=profile.constraints,
        )

        # ç”Ÿæˆæ•°æ®
        data = self.generator.generate_test_data(request)

        # æ›´æ–°æ¡£æ¡ˆä½¿ç”¨ç»Ÿè®¡
        profile.usage_count += 1
        profile.last_used = datetime.now()

        # ä¿å­˜æ•°æ®
        self._save_generated_data(profile_name, data)

        return data

    def _save_generated_data(self, profile_name: str, data: Dict[str, Any]):
        """ä¿å­˜ç”Ÿæˆçš„æ•°æ®"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{profile_name}_{timestamp}.json"
        filepath = self.data_storage_path / filename

        try:
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2, default=str)

            # åˆ›å»ºç¬¦å·é“¾æ¥æŒ‡å‘æœ€æ–°æ•°æ®
            latest_link = self.data_storage_path / f"{profile_name}_latest.json"
            if latest_link.exists():
                latest_link.unlink()
            latest_link.symlink_to(filename)

            logger.info(f"æµ‹è¯•æ•°æ®å·²ä¿å­˜: {filepath}")

        except Exception as e:
            logger.error(f"ä¿å­˜æµ‹è¯•æ•°æ®å¤±è´¥: {e}")

    def load_test_data(
        self, profile_name: str, timestamp: str = None
    ) -> Dict[str, Any]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        if timestamp:
            filename = f"{profile_name}_{timestamp}.json"
        else:
            filename = f"{profile_name}_latest.json"

        filepath = self.data_storage_path / filename

        if not filepath.exists():
            raise FileNotFoundError(f"æµ‹è¯•æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")

        try:
            with open(filepath, "r", encoding="utf-8") as f:
                data = json.load(f)

            logger.info(f"åŠ è½½æµ‹è¯•æ•°æ®: {filepath}")
            return data

        except Exception as e:
            logger.error(f"åŠ è½½æµ‹è¯•æ•°æ®å¤±è´¥: {e}")
            raise

    def analyze_data_quality(self, profile_name: str) -> Dict[str, Any]:
        """åˆ†ææ•°æ®è´¨é‡"""
        if profile_name not in self.profiles:
            raise ValueError(f"æ•°æ®æ¡£æ¡ˆ {profile_name} ä¸å­˜åœ¨")

        profile = self.profiles[profile_name]

        # è·å–æœ€è¿‘çš„æµ‹è¯•ç»“æœ
        recent_tests = [
            r
            for r in self.generator.generation_history
            if r["profile_name"] == profile_name
        ][-10:]  # æœ€è¿‘10æ¡

        # åˆ†æè´¨é‡æŒ‡æ ‡
        quality_analysis = {
            "profile_name": profile_name,
            "freshness": profile.freshness,
            "quality_score": profile.quality_score,
            "usage_count": profile.usage_count,
            "recent_test_count": len(recent_tests),
            "average_generation_time": 0,
            "success_rate": 0,
            "size_distribution": {},
        }

        if recent_tests:
            generation_times = [t["generation_time"] for t in recent_tests]
            quality_analysis["average_generation_time"] = sum(generation_times) / len(
                generation_times
            )

            # ä¼°ç®—æˆåŠŸç‡ï¼ˆåŸºäºæ•°æ®è´¨é‡ï¼‰
            quality_analysis["success_rate"] = min(profile.quality_score, 0.95)

        return quality_analysis

    def optimize_data_management(self, test_results: List[Dict[str, Any]] = None):
        """ä¼˜åŒ–æ•°æ®ç®¡ç†"""
        print("ğŸ¤– AIæ­£åœ¨ä¼˜åŒ–æ•°æ®ç®¡ç†...")

        if test_results is None:
            test_results = []

        # ä¼˜åŒ–æ•°æ®æ¡£æ¡ˆ
        profiles = list(self.profiles.values())
        optimized_profiles = self.optimizer.optimize_data_profiles(
            profiles, test_results
        )

        # æ›´æ–°æ¡£æ¡ˆ
        for profile in optimized_profiles:
            self.profiles[profile.name] = profile

        # æ¸…ç†è¿‡æœŸæ•°æ®
        self._cleanup_expired_data()

        # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
        self._generate_optimization_report()

        logger.info("æ•°æ®ç®¡ç†ä¼˜åŒ–å®Œæˆ")

    def _cleanup_expired_data(self):
        """æ¸…ç†è¿‡æœŸæ•°æ®"""
        cutoff_time = datetime.now() - timedelta(days=7)  # 7å¤©å‰çš„æ•°æ®

        for filepath in self.data_storage_path.glob("*.json"):
            if filepath.name.endswith("_latest.json"):
                continue

            try:
                # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                if datetime.fromtimestamp(filepath.stat().st_mtime) < cutoff_time:
                    filepath.unlink()
                    logger.info(f"åˆ é™¤è¿‡æœŸæ•°æ®: {filepath.name}")

            except Exception as e:
                logger.error(f"æ¸…ç†è¿‡æœŸæ•°æ®å¤±è´¥: {e}")

    def _generate_optimization_report(self):
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        report_path = self.data_storage_path / "optimization_report.json"

        report = {
            "timestamp": datetime.now().isoformat(),
            "profiles_count": len(self.profiles),
            "data_files_count": len(list(self.data_storage_path.glob("*.json")))
            - 1,  # æ’é™¤latest
            "total_storage_mb": self._calculate_storage_usage(),
            "profiles_summary": {},
        }

        for name, profile in self.profiles.items():
            report["profiles_summary"][name] = {
                "type": profile.data_type,
                "size": profile.size,
                "usage_count": profile.usage_count,
                "quality_score": profile.quality_score,
                "freshness": profile.freshness,
            }

        try:
            with open(report_path, "w", encoding="utf-8") as f:
                json.dump(report, f, ensure_ascii=False, indent=2)

            logger.info(f"ä¼˜åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

        except Exception as e:
            logger.error(f"ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Šå¤±è´¥: {e}")

    def _calculate_storage_usage(self) -> float:
        """è®¡ç®—å­˜å‚¨ä½¿ç”¨é‡"""
        total_size = 0

        for filepath in self.data_storage_path.glob("*.json"):
            if not filepath.name.endswith("_latest.json"):
                try:
                    total_size += filepath.stat().st_size
                except:
                    pass

        return round(total_size / (1024 * 1024), 2)  # MB

    def get_data_insights(self) -> Dict[str, Any]:
        """è·å–æ•°æ®æ´å¯Ÿ"""
        insights = {
            "profiles_overview": {},
            "generation_stats": self._get_generation_statistics(),
            "quality_trends": self._get_quality_trends(),
            "optimization_recommendations": self._get_optimization_recommendations(),
        }

        # æ·»åŠ æ¡£æ¡ˆæ¦‚è§ˆ
        for name, profile in self.profiles.items():
            insights["profiles_overview"][name] = {
                "type": profile.data_type,
                "size": profile.size,
                "usage_count": profile.usage_count,
                "quality_score": profile.quality_score,
                "freshness": profile.freshness,
                "last_used": profile.last_used.isoformat(),
            }

        return insights

    def _get_generation_statistics(self) -> Dict[str, Any]:
        """è·å–ç”Ÿæˆç»Ÿè®¡"""
        if not self.generator.generation_history:
            return {}

        recent_history = self.generator.generation_history[-100:]  # æœ€è¿‘100æ¡

        stats = {
            "total_generations": len(recent_history),
            "avg_generation_time": sum(t["generation_time"] for t in recent_history)
            / len(recent_history),
            "ai_enhanced_usage": sum(
                1 for t in recent_history if t["use_ai_enhancement"]
            ),
            "cache_hits": sum(1 for t in recent_history if t["cache_hit"]),
            "most_used_profiles": {},
        }

        # ç»Ÿè®¡æœ€å¸¸ç”¨çš„æ¡£æ¡ˆ
        profile_counts = {}
        for entry in recent_history:
            profile = entry["profile_name"]
            profile_counts[profile] = profile_counts.get(profile, 0) + 1

        stats["most_used_profiles"] = dict(
            sorted(profile_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        )

        return stats

    def _get_quality_trends(self) -> Dict[str, Any]:
        """è·å–è´¨é‡è¶‹åŠ¿"""
        trends = {}

        for name, profile in self.profiles.items():
            # æ¨¡æ‹Ÿè´¨é‡è¶‹åŠ¿åˆ†æ
            trends[name] = {
                "quality_score": profile.quality_score,
                "freshness_trend": "increasing"
                if profile.freshness > 0.8
                else "stable",
                "usage_trend": "high" if profile.usage_count > 10 else "medium",
            }

        return trends

    def _get_optimization_recommendations(self) -> List[str]:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # æ£€æŸ¥æ•°æ®æ–°é²œåº¦
        for name, profile in self.profiles.items():
            if profile.freshness < 0.5:
                recommendations.append(
                    f"æ¡£æ¡ˆ {name} éœ€è¦æ›´æ–°æ•°æ® (æ–°é²œåº¦: {profile.freshness:.2f})"
                )

        # æ£€æŸ¥ä½¿ç”¨é¢‘ç‡
        low_usage_profiles = [
            name for name, profile in self.profiles.items() if profile.usage_count == 0
        ]
        if low_usage_profiles:
            recommendations.append(
                f"ä»¥ä¸‹æ¡£æ¡ˆä»æœªè¢«ä½¿ç”¨: {', '.join(low_usage_profiles)}"
            )

        # æ£€æŸ¥å­˜å‚¨ä½¿ç”¨
        storage_usage = self._calculate_storage_usage()
        if storage_usage > 100:  # è¶…è¿‡100MB
            recommendations.append(f"å­˜å‚¨ä½¿ç”¨é‡è¾ƒé«˜ ({storage_usage:.2f}MB)ï¼Œå»ºè®®æ¸…ç†")

        return recommendations
