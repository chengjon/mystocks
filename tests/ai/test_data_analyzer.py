#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MyStocks AIæµ‹è¯•æ•°æ®åˆ†æå™¨
æä¾›æ™ºèƒ½æµ‹è¯•æ•°æ®åˆ†æã€æ¨¡å¼è¯†åˆ«å’Œé¢„æµ‹
"""

import asyncio
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from collections import defaultdict, Counter
import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
import logging


@dataclass
class TestPattern:
    """æµ‹è¯•æ¨¡å¼"""

    pattern_name: str
    frequency: int
    success_rate: float
    avg_duration: float
    confidence: float
    related_functions: List[str]


@dataclass
class TestTrend:
    """æµ‹è¯•è¶‹åŠ¿"""

    trend_name: str
    direction: str  # increasing, decreasing, stable
    change_rate: float
    impact_level: str  # high, medium, low
    predicted_value: float
    time_frame: str


@dataclass
class AnomalyDetection:
    """å¼‚å¸¸æ£€æµ‹ç»“æœ"""

    anomaly_id: str
    severity: str
    type: str
    description: str
    affected_tests: List[str]
    confidence_score: float
    recommended_action: str


class AITestDataAnalyzer:
    """AIæµ‹è¯•æ•°æ®åˆ†æå™¨"""

    def __init__(self, data_dir: str = "test_data"):
        self.data_dir = Path(data_dir)
        self.logger = logging.getLogger(__name__)
        self.cache_dir = Path(__file__).parent / "cache"
        self.cache_dir.mkdir(exist_ok=True)

    def analyze_test_patterns(self, test_results: List[Dict[str, Any]]) -> List[TestPattern]:
        """åˆ†ææµ‹è¯•æ¨¡å¼"""
        print("ğŸ¤– AIæ­£åœ¨åˆ†ææµ‹è¯•æ¨¡å¼...")

        patterns = []
        function_results = defaultdict(list)

        # æŒ‰å‡½æ•°åˆ†ç»„ç»“æœ
        for result in test_results:
            func_name = result.get("function_name", "unknown")
            function_results[func_name].append(result)

        # åˆ†ææ¯ä¸ªå‡½æ•°çš„æµ‹è¯•æ¨¡å¼
        for func_name, results in function_results.items():
            if len(results) >= 3:  # è‡³å°‘3æ¬¡ç»“æœæ‰èƒ½å½¢æˆæ¨¡å¼
                pattern = self._extract_function_pattern(func_name, results)
                if pattern:
                    patterns.append(pattern)

        # æŒ‰é¢‘ç‡æ’åº
        patterns.sort(key=lambda p: p.frequency, reverse=True)
        return patterns

    def _extract_function_pattern(self, func_name: str, results: List[Dict[str, Any]]) -> Optional[TestPattern]:
        """æå–å‡½æ•°æµ‹è¯•æ¨¡å¼"""
        try:
            # è®¡ç®—æˆåŠŸç‡
            success_count = sum(1 for r in results if r.get("status") == "passed")
            success_rate = success_count / len(results)

            # è®¡ç®—å¹³å‡æ‰§è¡Œæ—¶é—´
            durations = [r.get("duration", 0) for r in results if r.get("duration")]
            avg_duration = np.mean(durations) if durations else 0

            # è¯†åˆ«æ¨¡å¼ç±»å‹
            pattern_type = self._classify_pattern_type(func_name, results)

            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = min(len(results) / 10, 1.0)  # åŸºäºæ ·æœ¬é‡

            return TestPattern(
                pattern_name=pattern_type,
                frequency=len(results),
                success_rate=success_rate,
                avg_duration=avg_duration,
                confidence=confidence,
                related_functions=[func_name],
            )

        except Exception as e:
            self.logger.error(f"æ¨¡å¼æå–å¤±è´¥ {func_name}: {e}")
            return None

    def _classify_pattern_type(self, func_name: str, results: List[Dict[str, Any]]) -> str:
        """åˆ†ç±»æ¨¡å¼ç±»å‹"""
        func_lower = func_name.lower()

        # åŸºäºå‡½æ•°åç§°åˆ†ç±»
        if any(keyword in func_lower for keyword in ["get", "fetch", "retrieve"]):
            return "data_retrieval"
        elif any(keyword in func_lower for keyword in ["calculate", "compute", "analyze"]):
            return "calculation"
        elif any(keyword in func_lower for keyword in ["validate", "check", "verify"]):
            return "validation"
        elif any(keyword in func_lower for keyword in ["save", "store", "update"]):
            return "data_modification"
        else:
            return "general"

    def detect_test_anomalies(self, test_results: List[Dict[str, Any]]) -> List[AnomalyDetection]:
        """æ£€æµ‹æµ‹è¯•å¼‚å¸¸"""
        print("ğŸ¤– AIæ­£åœ¨æ£€æµ‹æµ‹è¯•å¼‚å¸¸...")

        anomalies = []

        # 1. æ£€æµ‹æ‰§è¡Œæ—¶é—´å¼‚å¸¸
        time_anomalies = self._detect_time_anomalies(test_results)
        anomalies.extend(time_anomalies)

        # 2. æ£€æµ‹å¤±è´¥ç‡å¼‚å¸¸
        failure_anomalies = self._detect_failure_anomalies(test_results)
        anomalies.extend(failure_anomalies)

        # 3. æ£€æµ‹æ¨¡å¼å˜åŒ–å¼‚å¸¸
        pattern_anomalies = self._detect_pattern_anomalies(test_results)
        anomalies.extend(pattern_anomalies)

        # 4. æ£€æµ‹èµ„æºä½¿ç”¨å¼‚å¸¸
        resource_anomalies = self._detect_resource_anomalies(test_results)
        anomalies.extend(resource_anomalies)

        return sorted(anomalies, key=lambda a: a.confidence_score, reverse=True)

    def _detect_time_anomalies(self, test_results: List[Dict[str, Any]]) -> List[AnomalyDetection]:
        """æ£€æµ‹æ—¶é—´å¼‚å¸¸"""
        anomalies = []
        time_data = defaultdict(list)

        for result in test_results:
            func_name = result.get("function_name", "unknown")
            duration = result.get("duration", 0)
            time_data[func_name].append(duration)

        for func_name, durations in time_data.items():
            if len(durations) >= 5:  # è‡³å°‘5æ¬¡æ•°æ®
                mean_time = np.mean(durations)
                std_time = np.std(durations)

                # æ£€æµ‹æœ€è¿‘å‡ æ¬¡æ˜¯å¦æœ‰å¼‚å¸¸
                recent_durations = durations[-5:]
                for duration in recent_durations:
                    if duration > mean_time + 2 * std_time:  # è¶…å‡º2ä¸ªæ ‡å‡†å·®
                        anomaly = AnomalyDetection(
                            anomaly_id=f"time_anomaly_{func_name}_{datetime.now().timestamp()}",
                            severity="high" if duration > mean_time * 3 else "medium",
                            type="execution_time_spike",
                            description=f"å‡½æ•° {func_name} æ‰§è¡Œæ—¶é—´å¼‚å¸¸: {duration:.2f}ms (å¹³å‡: {mean_time:.2f}ms)",
                            affected_tests=[func_name],
                            confidence_score=min((duration - mean_time) / (std_time + 1), 1.0),
                            recommended_action="æ£€æŸ¥å‡½æ•°æ˜¯å¦æœ‰æ€§èƒ½ç“¶é¢ˆæˆ–èµ„æºç«äº‰",
                        )
                        anomalies.append(anomaly)

        return anomalies

    def _detect_failure_anomalies(self, test_results: List[Dict[str, Any]]) -> List[AnomalyDetection]:
        """æ£€æµ‹å¤±è´¥ç‡å¼‚å¸¸"""
        anomalies = []
        failure_data = defaultdict(list)

        for result in test_results:
            func_name = result.get("function_name", "unknown")
            status = result.get("status", "unknown")
            failure_data[func_name].append(status)

        for func_name, statuses in failure_data.items():
            if len(statuses) >= 10:  # è‡³å°‘10æ¬¡æ•°æ®
                recent_failures = statuses[-5:]
                failure_rate = sum(1 for s in recent_failures if s != "passed") / len(recent_failures)

                # å¦‚æœæœ€è¿‘5æ¬¡å¤±è´¥ç‡è¶…è¿‡50%
                if failure_rate > 0.5:
                    anomaly = AnomalyDetection(
                        anomaly_id=f"failure_anomaly_{func_name}_{datetime.now().timestamp()}",
                        severity="high",
                        type="high_failure_rate",
                        description=f"å‡½æ•° {func_name} æœ€è¿‘å¤±è´¥ç‡è¿‡é«˜: {failure_rate:.1%}",
                        affected_tests=[func_name],
                        confidence_score=failure_rate,
                        recommended_action="ç«‹å³æ£€æŸ¥å‡½æ•°å®ç°å’Œä¾èµ–é¡¹",
                    )
                    anomalies.append(anomaly)

        return anomalies

    def _detect_pattern_anomalies(self, test_results: List[Dict[str, Any]]) -> List[AnomalyDetection]:
        """æ£€æµ‹æ¨¡å¼å˜åŒ–å¼‚å¸¸"""
        anomalies = []

        # æŒ‰æ—¶é—´æ’åº
        sorted_results = sorted(test_results, key=lambda x: x.get("timestamp", ""))

        # æ»‘åŠ¨çª—å£åˆ†æ
        window_size = 10
        for i in range(len(sorted_results) - window_size + 1):
            window = sorted_results[i : i + window_size]

            # åˆ†æçª—å£å†…çš„æ¨¡å¼
            pattern_score = self._calculate_window_pattern_score(window)

            # ä¸ä¹‹å‰çš„çª—å£å¯¹æ¯”
            if i > 0:
                prev_window = sorted_results[i - 1 : i + window_size - 1]
                prev_score = self._calculate_window_pattern_score(prev_window)

                # æ¨¡å¼åˆ†æ•°å˜åŒ–è¶…è¿‡30%
                if abs(pattern_score - prev_score) / prev_score > 0.3:
                    anomaly = AnomalyDetection(
                        anomaly_id=f"pattern_anomaly_{i}_{datetime.now().timestamp()}",
                        severity="medium",
                        type="pattern_change",
                        description=f"æµ‹è¯•æ¨¡å¼åœ¨ç¬¬ {i} æ¬¡æ‰§è¡Œå‘ç”Ÿæ˜¾è‘—å˜åŒ–",
                        affected_tests=list(set(r.get("function_name", "") for r in window)),
                        confidence_score=min(abs(pattern_score - prev_score) / prev_score, 1.0),
                        recommended_action="æ£€æŸ¥æ˜¯å¦æœ‰ä»£ç å˜æ›´æˆ–ç¯å¢ƒå˜åŒ–",
                    )
                    anomalies.append(anomaly)

        return anomalies

    def _calculate_window_pattern_score(self, window: List[Dict[str, Any]]) -> float:
        """è®¡ç®—çª—å£æ¨¡å¼åˆ†æ•°"""
        if not window:
            return 0.0

        # ç»¼åˆæˆåŠŸç‡ã€å¹³å‡æ—¶é—´ã€å‡½æ•°åˆ†å¸ƒç­‰å› ç´ 
        success_rate = sum(1 for r in window if r.get("status") == "passed") / len(window)

        durations = [r.get("duration", 0) for r in window if r.get("duration")]
        avg_duration = np.mean(durations) if durations else 0

        # å½’ä¸€åŒ–åˆ†æ•°
        score = success_rate * 0.7 + (1 / (1 + avg_duration / 1000)) * 0.3
        return score

    def _detect_resource_anomalies(self, test_results: List[Dict[str, Any]]) -> List[AnomalyDetection]:
        """æ£€æµ‹èµ„æºä½¿ç”¨å¼‚å¸¸"""
        anomalies = []

        # æ£€æµ‹å†…å­˜ä½¿ç”¨å¼‚å¸¸
        memory_data = defaultdict(list)
        for result in test_results:
            if "memory_usage" in result:
                func_name = result.get("function_name", "unknown")
                memory_data[func_name].append(result["memory_usage"])

        for func_name, usages in memory_data.items():
            if len(usages) >= 5:
                mean_usage = np.mean(usages)
                recent_usage = usages[-1]

                # å¦‚æœæœ€è¿‘ä½¿ç”¨é‡è¶…è¿‡å¹³å‡å€¼çš„3å€
                if recent_usage > mean_usage * 3:
                    anomaly = AnomalyDetection(
                        anomaly_id=f"memory_anomaly_{func_name}_{datetime.now().timestamp()}",
                        severity="high",
                        type="memory_spike",
                        description=f"å‡½æ•° {func_name} å†…å­˜ä½¿ç”¨å¼‚å¸¸: {recent_usage:.2f}MB (å¹³å‡: {mean_usage:.2f}MB)",
                        affected_tests=[func_name],
                        confidence_score=min((recent_usage - mean_usage) / (mean_usage + 1), 1.0),
                        recommended_action="æ£€æŸ¥å†…å­˜æ³„æ¼æˆ–å¤§æ•°æ®å¤„ç†é€»è¾‘",
                    )
                    anomalies.append(anomaly)

        return anomalies

    def predict_test_trends(self, test_results: List[Dict[str, Any]]) -> List[TestTrend]:
        """é¢„æµ‹æµ‹è¯•è¶‹åŠ¿"""
        print("ğŸ¤– AIæ­£åœ¨é¢„æµ‹æµ‹è¯•è¶‹åŠ¿...")

        trends = []

        # 1. è¦†ç›–ç‡è¶‹åŠ¿é¢„æµ‹
        coverage_trend = self._predict_coverage_trend(test_results)
        trends.append(coverage_trend)

        # 2. æ€§èƒ½è¶‹åŠ¿é¢„æµ‹
        performance_trend = self._predict_performance_trend(test_results)
        trends.append(performance_trend)

        # 3. å¤±è´¥ç‡è¶‹åŠ¿é¢„æµ‹
        failure_trend = self._predict_failure_trend(test_results)
        trends.append(failure_trend)

        # 4. æ‰§è¡Œæ—¶é—´è¶‹åŠ¿é¢„æµ‹
        duration_trend = self._predict_duration_trend(test_results)
        trends.append(duration_trend)

        return trends

    def _predict_coverage_trend(self, test_results: List[Dict[str, Any]]) -> TestTrend:
        """é¢„æµ‹è¦†ç›–ç‡è¶‹åŠ¿"""
        # æŒ‰æ—¶é—´åˆ†ç»„è®¡ç®—è¦†ç›–ç‡
        time_groups = defaultdict(lambda: {"total": 0, "covered": 0})

        for result in test_results:
            timestamp = result.get("timestamp", datetime.now().isoformat())
            func_name = result.get("function_name", "unknown")

            # ç®€å•æŒ‰å¤©åˆ†ç»„
            date_key = timestamp.split("T")[0]
            time_groups[date_key]["total"] += 1
            if result.get("status") == "passed":
                time_groups[date_key]["covered"] += 1

        # è®¡ç®—è¦†ç›–ç‡å˜åŒ–
        coverage_values = []
        for date in sorted(time_groups.keys()):
            coverage = time_groups[date]["covered"] / time_groups[date]["total"]
            coverage_values.append(coverage)

        if len(coverage_values) >= 3:
            # ç®€å•çº¿æ€§é¢„æµ‹
            recent_coverage = np.mean(coverage_values[-3:])
            previous_coverage = np.mean(coverage_values[-6:-3]) if len(coverage_values) >= 6 else recent_coverage

            change_rate = (recent_coverage - previous_coverage) / previous_coverage if previous_coverage > 0 else 0

            if change_rate > 0.05:
                direction = "increasing"
            elif change_rate < -0.05:
                direction = "decreasing"
            else:
                direction = "stable"

            return TestTrend(
                trend_name="test_coverage",
                direction=direction,
                change_rate=change_rate,
                impact_level="medium",
                predicted_value=recent_coverage * (1 + change_rate),
                time_frame="next_week",
            )

        return TestTrend(
            trend_name="test_coverage",
            direction="stable",
            change_rate=0.0,
            impact_level="low",
            predicted_value=0.8,
            time_frame="next_week",
        )

    def _predict_performance_trend(self, test_results: List[Dict[str, Any]]) -> TestTrend:
        """é¢„æµ‹æ€§èƒ½è¶‹åŠ¿"""
        durations = [r.get("duration", 0) for r in test_results if r.get("duration")]

        if len(durations) >= 10:
            recent_avg = np.mean(durations[-5:])
            previous_avg = np.mean(durations[-10:-5])

            change_rate = (recent_avg - previous_avg) / previous_avg if previous_avg > 0 else 0

            if change_rate > 0.1:
                direction = "increasing"  # æ€§èƒ½ä¸‹é™
            elif change_rate < -0.1:
                direction = "decreasing"  # æ€§èƒ½æå‡
            else:
                direction = "stable"

            return TestTrend(
                trend_name="performance",
                direction=direction,
                change_rate=change_rate,
                impact_level="high",
                predicted_value=recent_avg * (1 + change_rate),
                time_frame="next_week",
            )

        return TestTrend(
            trend_name="performance",
            direction="stable",
            change_rate=0.0,
            impact_level="medium",
            predicted_value=100.0,
            time_frame="next_week",
        )

    def _predict_failure_trend(self, test_results: List[Dict[str, Any]]) -> TestTrend:
        """é¢„æµ‹å¤±è´¥ç‡è¶‹åŠ¿"""
        failure_rates = []

        # æŒ‰æ—¶é—´åˆ†ç»„è®¡ç®—å¤±è´¥ç‡
        for i in range(0, len(test_results), 10):
            batch = test_results[i : i + 10]
            failures = sum(1 for r in batch if r.get("status") != "passed")
            failure_rate = failures / len(batch) if batch else 0
            failure_rates.append(failure_rate)

        if len(failure_rates) >= 3:
            recent_rate = np.mean(failure_rates[-3:])
            previous_rate = np.mean(failure_rates[:-3]) if len(failure_rates) > 3 else recent_rate

            change_rate = (recent_rate - previous_rate) / previous_rate if previous_rate > 0 else 0

            if change_rate > 0.2:
                direction = "increasing"
            elif change_rate < -0.2:
                direction = "decreasing"
            else:
                direction = "stable"

            return TestTrend(
                trend_name="failure_rate",
                direction=direction,
                change_rate=change_rate,
                impact_level="high",
                predicted_value=recent_rate * (1 + change_rate),
                time_frame="next_week",
            )

        return TestTrend(
            trend_name="failure_rate",
            direction="stable",
            change_rate=0.0,
            impact_level="medium",
            predicted_value=0.05,
            time_frame="next_week",
        )

    def _predict_duration_trend(self, test_results: List[Dict[str, Any]]) -> TestTrend:
        """é¢„æµ‹æ‰§è¡Œæ—¶é—´è¶‹åŠ¿"""
        durations = [r.get("duration", 0) for r in test_results if r.get("duration")]

        if len(durations) >= 20:
            # ä½¿ç”¨ç§»åŠ¨å¹³å‡è¿›è¡Œé¢„æµ‹
            window_size = 5
            moving_avgs = []
            for i in range(window_size, len(durations)):
                avg = np.mean(durations[i - window_size : i])
                moving_avgs.append(avg)

            if len(moving_avgs) >= 3:
                recent_avg = np.mean(moving_avgs[-3:])
                previous_avg = np.mean(moving_avgs[:-3]) if len(moving_avgs) > 3 else recent_avg

                change_rate = (recent_avg - previous_avg) / previous_avg if previous_avg > 0 else 0

                if change_rate > 0.15:
                    direction = "increasing"
                elif change_rate < -0.15:
                    direction = "decreasing"
                else:
                    direction = "stable"

                return TestTrend(
                    trend_name="execution_duration",
                    direction=direction,
                    change_rate=change_rate,
                    impact_level="medium",
                    predicted_value=recent_avg * (1 + change_rate),
                    time_frame="next_week",
                )

        return TestTrend(
            trend_name="execution_duration",
            direction="stable",
            change_rate=0.0,
            impact_level="low",
            predicted_value=50.0,
            time_frame="next_week",
        )

    def generate_intelligence_report(self, test_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæ™ºèƒ½åˆ†ææŠ¥å‘Š"""
        print("ğŸ¤– AIæ­£åœ¨ç”Ÿæˆæ™ºèƒ½åˆ†ææŠ¥å‘Š...")

        report = {
            "report_id": f"ai_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "generated_at": datetime.now().isoformat(),
            "analysis_period": self._get_analysis_period(test_results),
            "summary": {},
            "patterns": [],
            "anomalies": [],
            "trends": [],
            "recommendations": [],
        }

        # åˆ†ææµ‹è¯•æ¨¡å¼
        patterns = self.analyze_test_patterns(test_results)
        report["patterns"] = [self._pattern_to_dict(p) for p in patterns[:10]]  # å–å‰10ä¸ª

        # æ£€æµ‹å¼‚å¸¸
        anomalies = self.detect_test_anomalies(test_results)
        report["anomalies"] = [self._anomaly_to_dict(a) for a in anomalies[:10]]  # å–å‰10ä¸ª

        # é¢„æµ‹è¶‹åŠ¿
        trends = self.predict_test_trends(test_results)
        report["trends"] = [self._trend_to_dict(t) for t in trends]

        # ç”Ÿæˆæ‘˜è¦
        report["summary"] = self._generate_summary(patterns, anomalies, trends)

        # ç”Ÿæˆå»ºè®®
        report["recommendations"] = self._generate_recommendations(patterns, anomalies, trends)

        return report

    def _pattern_to_dict(self, pattern: TestPattern) -> Dict[str, Any]:
        """å°†æ¨¡å¼è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "pattern_name": pattern.pattern_name,
            "frequency": pattern.frequency,
            "success_rate": pattern.success_rate,
            "avg_duration": pattern.avg_duration,
            "confidence": pattern.confidence,
            "related_functions": pattern.related_functions,
        }

    def _anomaly_to_dict(self, anomaly: AnomalyDetection) -> Dict[str, Any]:
        """å°†å¼‚å¸¸è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "anomaly_id": anomaly.anomaly_id,
            "severity": anomaly.severity,
            "type": anomaly.type,
            "description": anomaly.description,
            "affected_tests": anomaly.affected_tests,
            "confidence_score": anomaly.confidence_score,
            "recommended_action": anomaly.recommended_action,
        }

    def _trend_to_dict(self, trend: TestTrend) -> Dict[str, Any]:
        """å°†è¶‹åŠ¿è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "trend_name": trend.trend_name,
            "direction": trend.direction,
            "change_rate": trend.change_rate,
            "impact_level": trend.impact_level,
            "predicted_value": trend.predicted_value,
            "time_frame": trend.time_frame,
        }

    def _get_analysis_period(self, test_results: List[Dict[str, Any]]) -> Dict[str, str]:
        """è·å–åˆ†ææ—¶é—´æ®µ"""
        if not test_results:
            return {"start": None, "end": None}

        timestamps = []
        for result in test_results:
            timestamp = result.get("timestamp", "")
            if timestamp:
                timestamps.append(timestamp)

        if timestamps:
            return {"start": min(timestamps), "end": max(timestamps)}
        return {"start": None, "end": None}

    def _generate_summary(
        self,
        patterns: List[TestPattern],
        anomalies: List[AnomalyDetection],
        trends: List[TestTrend],
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ‘˜è¦"""
        return {
            "total_patterns": len(patterns),
            "total_anomalies": len(anomalies),
            "anomaly_severity_distribution": Counter(a.severity for a in anomalies),
            "trend_directions": Counter(t.direction for t in trends),
            "most_common_pattern": max(patterns, key=lambda p: p.frequency).pattern_name if patterns else None,
            "highest_confidence_anomaly": max(anomalies, key=lambda a: a.confidence_score).type if anomalies else None,
        }

    def _generate_recommendations(
        self,
        patterns: List[TestPattern],
        anomalies: List[AnomalyDetection],
        trends: List[TestTrend],
    ) -> List[str]:
        """ç”Ÿæˆå»ºè®®"""
        recommendations = []

        # åŸºäºå¼‚å¸¸çš„å»ºè®®
        for anomaly in anomalies:
            if anomaly.severity == "high":
                recommendations.append(f"ğŸš¨ é«˜ä¼˜å…ˆçº§: {anomaly.description}")
                recommendations.append(f"  æ¨èæ“ä½œ: {anomaly.recommended_action}")

        # åŸºäºè¶‹åŠ¿çš„å»ºè®®
        for trend in trends:
            if trend.direction == "increasing" and trend.change_rate > 0.2:
                recommendations.append(f"âš ï¸  {trend.trend_name} æ­£åœ¨å¿«é€Ÿæ¶åŒ–ï¼Œéœ€è¦å…³æ³¨")

        # åŸºäºæ¨¡å¼çš„å»ºè®®
        if patterns:
            high_freq_patterns = [p for p in patterns if p.frequency > 20 and p.success_rate < 0.9]
            if high_freq_patterns:
                recommendations.append(f"ğŸ“Š {len(high_freq_patterns)} ä¸ªé«˜é¢‘æ¨¡å¼æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–")

        # é€šç”¨å»ºè®®
        recommendations.extend(
            [
                "ğŸ”§ å»ºè®®å®šæœŸè¿è¡ŒAIåˆ†æä»¥æŒç»­ç›‘æ§æµ‹è¯•è´¨é‡",
                "ğŸ“ˆ å…³æ³¨æµ‹è¯•è¦†ç›–ç‡å’Œæ€§èƒ½æŒ‡æ ‡çš„è¶‹åŠ¿å˜åŒ–",
                "ğŸ¯ ä¼˜å…ˆå¤„ç†é«˜ç½®ä¿¡åº¦å¼‚å¸¸ï¼Œé˜²æ­¢é—®é¢˜æ‰©å¤§",
            ]
        )

        return recommendations


# Enhanced AI Testing Framework Integration
# Complete integration with the comprehensive testing solution


class AnomalyDetector:
    """é«˜çº§å¼‚å¸¸æ£€æµ‹å™¨"""

    def __init__(self, contamination: float = 0.1, random_state: int = 42):
        self.contamination = contamination
        self.random_state = random_state
        self.model = IsolationForest(contamination=contamination, random_state=random_state, n_estimators=100)
        self.scaler = StandardScaler()
        self.is_fitted = False
        self.anomaly_history = []

    def fit(self, data: np.ndarray) -> "AnomalyDetector":
        """è®­ç»ƒå¼‚å¸¸æ£€æµ‹æ¨¡å‹"""
        scaled_data = self.scaler.fit_transform(data)
        self.model.fit(scaled_data)
        self.is_fitted = True
        return self

    def detect(self, data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """æ£€æµ‹å¼‚å¸¸ç‚¹"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fitæ–¹æ³•")

        scaled_data = self.scaler.transform(data)
        anomalies = self.model.predict(scaled_data)
        anomaly_scores = self.model.decision_function(scaled_data)

        # è®°å½•å¼‚å¸¸å†å²
        anomaly_count = np.sum(anomalies == -1)
        self.anomaly_history.append(
            {
                "timestamp": datetime.now(),
                "total_points": len(data),
                "anomaly_count": anomaly_count,
                "anomaly_rate": anomaly_count / len(data),
            }
        )

        return anomalies, anomaly_scores

    def get_anomaly_summary(self) -> Dict[str, Any]:
        """è·å–å¼‚å¸¸æ£€æµ‹æ‘˜è¦"""
        if not self.anomaly_history:
            return {"message": "æš‚æ— å¼‚å¸¸æ£€æµ‹å†å²"}

        history_df = pd.DataFrame(self.anomaly_history)
        return {
            "total_detections": len(self.anomaly_history),
            "total_anomalies": history_df["anomaly_count"].sum(),
            "average_anomaly_rate": history_df["anomaly_rate"].mean(),
            "max_anomaly_rate": history_df["anomaly_rate"].max(),
            "recent_anomalies": history_df.tail(5).to_dict("records"),
        }


class TrendAnalyzer:
    """è¶‹åŠ¿åˆ†æå™¨"""

    def __init__(self):
        self.trend_models = {}
        self.seasonal_decomposers = {}

    def analyze_trend(self, data: pd.Series, freq: str = "D") -> Dict[str, Any]:
        """åˆ†ææ—¶é—´åºåˆ—è¶‹åŠ¿"""
        try:
            # ç¡®ä¿æ•°æ®æ˜¯æ—¶é—´åºåˆ—
            if not isinstance(data.index, pd.DatetimeIndex):
                data.index = pd.to_datetime(data.index)

            # å¡«å……ç¼ºå¤±å€¼
            data = data.fillna(method="ffill").fillna(method="bfill")

            # å­£èŠ‚æ€§åˆ†è§£
            decomposition = seasonal_decompose(data, model="additive", period=min(freq, len(data) // 2))

            # è¶‹åŠ¿åˆ†æ
            trend = decomposition.trend.dropna()
            seasonal = decomposition.seasonal.dropna()
            residual = decomposition.resid.dropna()

            # è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡
            trend_slope = self._calculate_trend_slope(trend)
            seasonality_strength = self._calculate_seasonality_strength(seasonal, residual)

            return {
                "trend_direction": "upward" if trend_slope > 0 else "downward" if trend_slope < 0 else "stable",
                "trend_strength": abs(trend_slope),
                "seasonality_strength": seasonality_strength,
                "volatility": residual.std(),
                "decomposition": {
                    "trend": trend.to_dict(),
                    "seasonal": seasonal.to_dict(),
                    "residual": residual.to_dict(),
                },
            }
        except Exception as e:
            return {"error": f"è¶‹åŠ¿åˆ†æå¤±è´¥: {str(e)}"}

    def _calculate_trend_slope(self, series: pd.Series) -> float:
        """è®¡ç®—è¶‹åŠ¿æ–œç‡"""
        x = np.arange(len(series))
        y = series.values
        return np.polyfit(x, y, 1)[0]

    def _calculate_seasonality_strength(self, seasonal: pd.Series, residual: pd.Series) -> float:
        """è®¡ç®—å­£èŠ‚æ€§å¼ºåº¦"""
        var_seasonal = seasonal.var()
        var_residual = residual.var()
        return var_seasonal / (var_seasonal + var_residual) if (var_seasonal + var_residual) > 0 else 0


class PatternRecognizer:
    """æ¨¡å¼è¯†åˆ«å™¨"""

    def __init__(self):
        self.patterns = {
            "seasonal": {"min_strength": 0.3, "description": "å­£èŠ‚æ€§æ¨¡å¼"},
            "trend": {"min_strength": 0.1, "description": "è¶‹åŠ¿æ¨¡å¼"},
            "cyclical": {"min_strength": 0.2, "description": "å‘¨æœŸæ€§æ¨¡å¼"},
            "spike": {"threshold": 2.0, "description": "å°–å³°æ¨¡å¼"},
            "plateau": {"min_duration": 5, "description": "å¹³å°æ¨¡å¼"},
            "noise": {"max_strength": 0.1, "description": "å™ªå£°æ¨¡å¼"},
        }

    def recognize_patterns(self, data: pd.Series) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ•°æ®æ¨¡å¼"""
        patterns = []

        # 1. æ£€æµ‹å°–å³°æ¨¡å¼
        spike_patterns = self._detect_spike_patterns(data)
        patterns.extend(spike_patterns)

        # 2. æ£€æµ‹å¹³å°æ¨¡å¼
        plateau_patterns = self._detect_plateau_patterns(data)
        patterns.extend(plateau_patterns)

        # 3. ä½¿ç”¨DBSCANèšç±»æ£€æµ‹æ¨¡å¼
        cluster_patterns = self._detect_cluster_patterns(data)
        patterns.extend(cluster_patterns)

        # 4. æ£€æµ‹å‘¨æœŸæ€§æ¨¡å¼
        cyclical_patterns = self._detect_cyclical_patterns(data)
        patterns.extend(cyclical_patterns)

        return patterns

    def _detect_spike_patterns(self, data: pd.Series) -> List[Dict[str, Any]]:
        """æ£€æµ‹å°–å³°æ¨¡å¼"""
        patterns = []
        mean_val = data.mean()
        std_val = data.std()

        spike_threshold = mean_val + self.patterns["spike"]["threshold"] * std_val

        spike_indices = data[data > spike_threshold].index
        if len(spike_indices) > 0:
            patterns.append(
                {
                    "pattern_type": "spike",
                    "description": "æ£€æµ‹åˆ°å°–å³°æ¨¡å¼",
                    "indices": spike_indices.tolist(),
                    "count": len(spike_indices),
                    "strength": (data.max() - mean_val) / std_val,
                }
            )

        return patterns

    def _detect_plateau_patterns(self, data: pd.Series) -> List[Dict[str, Any]]:
        """æ£€æµ‹å¹³å°æ¨¡å¼"""
        patterns = []
        min_duration = self.patterns["plateau"]["min_duration"]

        # æ£€æµ‹è¿ç»­çš„ç›¸ä¼¼å€¼
        diff = data.diff().abs()
        plateaus = diff < 0.1 * data.std()

        # æ‰¾åˆ°è¿ç»­çš„å¹³å°æœŸ
        plateau_groups = (plateaus != plateaus.shift()).cumsum()
        plateau_stats = plateaus.groupby(plateau_groups).agg(["count", "all"])

        for group, stats in plateau_stats.iterrows():
            if stats["count"] >= min_duration and stats["all"]:
                plateau_indices = data[plateaus].index[plateaus.groupby(plateau_groups).cumsum() == group]
                patterns.append(
                    {
                        "pattern_type": "plateau",
                        "description": "æ£€æµ‹åˆ°å¹³å°æ¨¡å¼",
                        "indices": plateau_indices.tolist(),
                        "duration": stats["count"],
                        "value": data.loc[plateau_indices[0]],
                    }
                )

        return patterns

    def _detect_cluster_patterns(self, data: pd.Series) -> List[Dict[str, Any]]:
        """ä½¿ç”¨èšç±»æ£€æµ‹æ¨¡å¼"""
        patterns = []

        # å‡†å¤‡æ•°æ®
        values = data.values.reshape(-1, 1)

        # ä½¿ç”¨DBSCANèšç±»
        clustering = DBSCAN(eps=0.5, min_samples=5)
        labels = clustering.fit_predict(values)

        # åˆ†æèšç±»ç»“æœ
        unique_labels = set(labels)
        if len(unique_labels) > 1:  # æœ‰å¤šä¸ªèšç±»
            for label in unique_labels:
                if label != -1:  # ä¸æ˜¯å™ªå£°ç‚¹
                    cluster_indices = data.index[labels == label]
                    patterns.append(
                        {
                            "pattern_type": "cluster",
                            "description": f"èšç±» {label}",
                            "indices": cluster_indices.tolist(),
                            "size": len(cluster_indices),
                            "values": data.loc[cluster_indices].mean(),
                        }
                    )

        return patterns

    def _detect_cyclical_patterns(self, data: pd.Series) -> List[Dict[str, Any]]:
        """æ£€æµ‹å‘¨æœŸæ€§æ¨¡å¼"""
        patterns = []

        # ç®€å•çš„è‡ªç›¸å…³åˆ†æ
        if len(data) > 20:
            autocorr = data.autocorr(lag=10)
            if abs(autocorr) > 0.3:  # æœ‰æ˜¾è‘—çš„è‡ªç›¸å…³æ€§
                patterns.append(
                    {
                        "pattern_type": "cyclical",
                        "description": "æ£€æµ‹åˆ°å‘¨æœŸæ€§æ¨¡å¼",
                        "autocorrelation": autocorr,
                        "strength": abs(autocorr),
                    }
                )

        return patterns


class TestDataAnalyzer:
    """å¢å¼ºçš„æµ‹è¯•æ•°æ®åˆ†æå™¨"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.anomaly_detector = AnomalyDetector(contamination=self.config.get("contamination", 0.1))
        self.trend_analyzer = TrendAnalyzer()
        self.pattern_recognizer = PatternRecognizer()
        self.analysis_history = []

    def analyze_test_metrics(self, metrics_data: Dict[str, List[float]]) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•æŒ‡æ ‡æ•°æ®"""
        analysis_result = {
            "timestamp": datetime.now().isoformat(),
            "metrics_summary": {},
            "anomaly_detection": {},
            "trend_analysis": {},
            "pattern_recognition": {},
            "recommendations": [],
        }

        # 1. æŒ‡æ ‡æ‘˜è¦ç»Ÿè®¡
        for metric_name, values in metrics_data.items():
            if values:
                series = pd.Series(values)
                analysis_result["metrics_summary"][metric_name] = {
                    "count": len(values),
                    "mean": series.mean(),
                    "std": series.std(),
                    "min": series.min(),
                    "max": series.max(),
                    "median": series.median(),
                    "skewness": series.skew(),
                    "kurtosis": series.kurtosis(),
                    "coefficient_variation": series.std() / series.mean() if series.mean() != 0 else 0,
                }

        # 2. å¼‚å¸¸æ£€æµ‹
        for metric_name, values in metrics_data.items():
            if len(values) > 10:  # éœ€è¦è¶³å¤Ÿçš„æ•°æ®ç‚¹
                try:
                    data_array = np.array(values).reshape(-1, 1)
                    anomalies, scores = self.anomaly_detector.detect(data_array)

                    analysis_result["anomaly_detection"][metric_name] = {
                        "anomaly_count": int(np.sum(anomalies == -1)),
                        "anomaly_rate": float(np.mean(anomalies == -1)),
                        "anomaly_scores": scores.tolist(),
                        "summary": self.anomaly_detector.get_anomaly_summary(),
                    }
                except Exception as e:
                    analysis_result["anomaly_detection"][metric_name] = {"error": str(e)}

        # 3. è¶‹åŠ¿åˆ†æï¼ˆå¦‚æœæœ‰æ—¶é—´åºåˆ—æ•°æ®ï¼‰
        if "timestamp" in metrics_data and len(metrics_data["timestamp"]) > 10:
            try:
                # åˆ›å»ºæ—¶é—´åºåˆ—
                timestamps = pd.to_datetime(metrics_data["timestamp"])
                for metric_name in [k for k in metrics_data.keys() if k != "timestamp"]:
                    if len(metrics_data[metric_name]) == len(timestamps):
                        time_series = pd.Series(metrics_data[metric_name], index=timestamps)
                        trend_result = self.trend_analyzer.analyze_trend(time_series)
                        analysis_result["trend_analysis"][metric_name] = trend_result
            except Exception as e:
                analysis_result["trend_analysis"]["error"] = str(e)

        # 4. æ¨¡å¼è¯†åˆ«
        for metric_name, values in metrics_data.items():
            if len(values) > 20:
                try:
                    series = pd.Series(values)
                    patterns = self.pattern_recognizer.recognize_patterns(series)
                    analysis_result["pattern_recognition"][metric_name] = patterns
                except Exception as e:
                    analysis_result["pattern_recognition"][metric_name] = {"error": str(e)}

        # 5. ç”Ÿæˆå»ºè®®
        analysis_result["recommendations"] = self._generate_recommendations(analysis_result)

        # è®°å½•åˆ†æå†å²
        self.analysis_history.append(analysis_result)

        return analysis_result

    def _generate_recommendations(self, analysis_result: Dict[str, Any]) -> List[str]:
        """åŸºäºåˆ†æç»“æœç”Ÿæˆå»ºè®®"""
        recommendations = []

        # æ£€æŸ¥å¼‚å¸¸ç‡
        for metric_name, anomaly_data in analysis_result["anomaly_detection"].items():
            if isinstance(anomaly_data, dict) and "anomaly_rate" in anomaly_data:
                anomaly_rate = anomaly_data["anomaly_rate"]
                if anomaly_rate > 0.2:  # å¼‚å¸¸ç‡è¶…è¿‡20%
                    recommendations.append(f"{metric_name} å¼‚å¸¸ç‡è¾ƒé«˜ ({anomaly_rate:.2%})ï¼Œå»ºè®®æ£€æŸ¥æµ‹è¯•ç¯å¢ƒæˆ–æ•°æ®æº")

        # æ£€æŸ¥è¶‹åŠ¿
        for metric_name, trend_data in analysis_result["trend_analysis"].items():
            if isinstance(trend_data, dict) and "trend_direction" in trend_data:
                direction = trend_data["trend_direction"]
                strength = trend_data.get("trend_strength", 0)
                if direction == "upward" and strength > 0.5:
                    recommendations.append(f"{metric_name} å‘ˆä¸Šå‡è¶‹åŠ¿ (å¼ºåº¦: {strength:.2f})ï¼Œå¯èƒ½å­˜åœ¨æ€§èƒ½é€€åŒ–")
                elif direction == "downward" and strength > 0.5:
                    recommendations.append(f"{metric_name} å‘ˆä¸‹é™è¶‹åŠ¿ (å¼ºåº¦: {strength:.2f})ï¼Œæ€§èƒ½æ­£åœ¨æ”¹å–„")

        # æ£€æŸ¥æ³¢åŠ¨æ€§
        for metric_name, summary in analysis_result["metrics_summary"].items():
            cv = summary.get("coefficient_variation", 0)
            if cv > 0.5:  # å˜å¼‚ç³»æ•°è¶…è¿‡50%
                recommendations.append(f"{metric_name} æ³¢åŠ¨è¾ƒå¤§ (CV: {cv:.2f})ï¼Œå»ºè®®å¢åŠ ç¨³å®šæ€§æµ‹è¯•")

        return recommendations

    def generate_analysis_report(self, output_format: str = "html") -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if not self.analysis_history:
            return "æš‚æ— åˆ†æå†å²æ•°æ®"

        latest_analysis = self.analysis_history[-1]

        if output_format == "html":
            return self._generate_html_report(latest_analysis)
        elif output_format == "markdown":
            return self._generate_markdown_report(latest_analysis)
        else:
            return json.dumps(latest_analysis, indent=2, ensure_ascii=False)

    def _generate_html_report(self, analysis: Dict[str, Any]) -> str:
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        html_template = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>æµ‹è¯•æ•°æ®åˆ†ææŠ¥å‘Š</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .section {{ margin-bottom: 30px; }}
                .metric {{ background-color: #f5f5f5; padding: 10px; margin: 5px 0; border-radius: 5px; }}
                .anomaly {{ color: red; }}
                .trend-up {{ color: green; }}
                .trend-down {{ color: blue; }}
                .pattern {{ background-color: #e8f4f8; padding: 5px; margin: 5px 0; }}
                .recommendation {{ background-color: #fff3cd; padding: 10px; margin: 10px 0; border-left: 4px solid #ffc107; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <h1>æµ‹è¯•æ•°æ®åˆ†ææŠ¥å‘Š</h1>
            <p>ç”Ÿæˆæ—¶é—´: {analysis["timestamp"]}</p>

            <div class="section">
                <h2>æŒ‡æ ‡æ‘˜è¦</h2>
                {self._format_metrics_summary(analysis["metrics_summary"])}
            </div>

            <div class="section">
                <h2>å¼‚å¸¸æ£€æµ‹</h2>
                {self._format_anomaly_detection(analysis["anomaly_detection"])}
            </div>

            <div class="section">
                <h2>è¶‹åŠ¿åˆ†æ</h2>
                {self._format_trend_analysis(analysis["trend_analysis"])}
            </div>

            <div class="section">
                <h2>æ¨¡å¼è¯†åˆ«</h2>
                {self._format_pattern_recognition(analysis["pattern_recognition"])}
            </div>

            <div class="section">
                <h2>å»ºè®®</h2>
                {self._format_recommendations(analysis["recommendations"])}
            </div>
        </body>
        </html>
        """
        return html_template

    def _generate_markdown_report(self, analysis: Dict[str, Any]) -> str:
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        md_template = f"""# æµ‹è¯•æ•°æ®åˆ†ææŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {analysis["timestamp"]}

## æŒ‡æ ‡æ‘˜è¦

{self._format_metrics_summary_md(analysis["metrics_summary"])}

## å¼‚å¸¸æ£€æµ‹

{self._format_anomaly_detection_md(analysis["anomaly_detection"])}

## è¶‹åŠ¿åˆ†æ

{self._format_trend_analysis_md(analysis["trend_analysis"])}

## æ¨¡å¼è¯†åˆ«

{self._format_pattern_recognition_md(analysis["pattern_recognition"])}

## å»ºè®®

{self._format_recommendations_md(analysis["recommendations"])}
"""
        return md_template

    def _format_metrics_summary(self, summary: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æŒ‡æ ‡æ‘˜è¦ï¼ˆHTMLï¼‰"""
        html = ""
        for metric, stats in summary.items():
            html += f"""
            <div class="metric">
                <h3>{metric}</h3>
                <p>å¹³å‡å€¼: {stats["mean"]:.2f} | æ ‡å‡†å·®: {stats["std"]:.2f} |
                   æœ€å°å€¼: {stats["min"]:.2f} | æœ€å¤§å€¼: {stats["max"]:.2f}</p>
                <p>ååº¦: {stats["skewness"]:.2f} | å³°åº¦: {stats["kurtosis"]:.2f} |
                   å˜å¼‚ç³»æ•°: {stats["coefficient_variation"]:.2f}</p>
            </div>
            """
        return html

    def _format_anomaly_detection(self, anomaly_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–å¼‚å¸¸æ£€æµ‹ç»“æœï¼ˆHTMLï¼‰"""
        html = ""
        for metric, data in anomaly_data.items():
            if isinstance(data, dict) and "anomaly_rate" in data:
                anomaly_class = "anomaly" if data["anomaly_rate"] > 0.1 else ""
                html += f"""
                <div class="metric {anomaly_class}">
                    <h3>{metric}</h3>
                    <p>å¼‚å¸¸æ•°é‡: {data["anomaly_count"]} | å¼‚å¸¸ç‡: {data["anomaly_rate"]:.2%}</p>
                </div>
                """
            elif "error" in data:
                html += f"<p>é”™è¯¯: {data['error']}</p>"
        return html

    def _format_trend_analysis(self, trend_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–è¶‹åŠ¿åˆ†æç»“æœï¼ˆHTMLï¼‰"""
        html = ""
        for metric, data in trend_data.items():
            if isinstance(data, dict) and "trend_direction" in data:
                trend_class = f"trend-{data['trend_direction']}"
                html += f"""
                <div class="metric {trend_class}">
                    <h3>{metric}</h3>
                    <p>è¶‹åŠ¿æ–¹å‘: {data["trend_direction"]} | è¶‹åŠ¿å¼ºåº¦: {data["trend_strength"]:.2f}</p>
                    <p>å­£èŠ‚æ€§å¼ºåº¦: {data["seasonality_strength"]:.2f} | æ³¢åŠ¨æ€§: {data["volatility"]:.2f}</p>
                </div>
                """
        return html

    def _format_pattern_recognition(self, pattern_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æ¨¡å¼è¯†åˆ«ç»“æœï¼ˆHTMLï¼‰"""
        html = ""
        for metric, patterns in pattern_data.items():
            if isinstance(patterns, list):
                html += f"<h3>{metric}</h3>"
                for pattern in patterns:
                    html += f"""
                    <div class="pattern">
                        <strong>{pattern["pattern_type"]}</strong>: {pattern["description"]}
                        <ul>
                            <li>æ•°é‡: {pattern.get("count", pattern.get("size", "N/A"))}</li>
                            <li>å¼ºåº¦: {pattern.get("strength", pattern.get("autocorrelation", "N/A")):.2f}</li>
                        </ul>
                    </div>
                    """
        return html

    def _format_recommendations(self, recommendations: List[str]) -> str:
        """æ ¼å¼åŒ–å»ºè®®ï¼ˆHTMLï¼‰"""
        html = ""
        for rec in recommendations:
            html += f'<div class="recommendation">{rec}</div>'
        return html

    def _format_metrics_summary_md(self, summary: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æŒ‡æ ‡æ‘˜è¦ï¼ˆMarkdownï¼‰"""
        md = ""
        for metric, stats in summary.items():
            md += f"""
### {metric}

- **å¹³å‡å€¼**: {stats["mean"]:.2f}
- **æ ‡å‡†å·®**: {stats["std"]:.2f}
- **æœ€å°å€¼**: {stats["min"]:.2f}
- **æœ€å¤§å€¼**: {stats["max"]:.2f}
- **ä¸­ä½æ•°**: {stats["median"]:.2f}
- **ååº¦**: {stats["skewness"]:.2f}
- **å³°åº¦**: {stats["kurtosis"]:.2f}
- **å˜å¼‚ç³»æ•°**: {stats["coefficient_variation"]:.2f}

"""
        return md

    def _format_anomaly_detection_md(self, anomaly_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–å¼‚å¸¸æ£€æµ‹ç»“æœï¼ˆMarkdownï¼‰"""
        md = ""
        for metric, data in anomaly_data.items():
            if isinstance(data, dict) and "anomaly_rate" in data:
                md += f"""
#### {metric}

- **å¼‚å¸¸æ•°é‡**: {data["anomaly_count"]}
- **å¼‚å¸¸ç‡**: {data["anomaly_rate"]:.2%}

"""
            elif "error" in data:
                md += f"#### {metric}\n\né”™è¯¯: {data['error']}\n\n"
        return md

    def _format_trend_analysis_md(self, trend_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–è¶‹åŠ¿åˆ†æç»“æœï¼ˆMarkdownï¼‰"""
        md = ""
        for metric, data in trend_data.items():
            if isinstance(data, dict) and "trend_direction" in data:
                md += f"""
#### {metric}

- **è¶‹åŠ¿æ–¹å‘**: {data["trend_direction"]}
- **è¶‹åŠ¿å¼ºåº¦**: {data["trend_strength"]:.2f}
- **å­£èŠ‚æ€§å¼ºåº¦**: {data["seasonality_strength"]:.2f}
- **æ³¢åŠ¨æ€§**: {data["volatility"]:.2f}

"""
        return md

    def _format_pattern_recognition_md(self, pattern_data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æ¨¡å¼è¯†åˆ«ç»“æœï¼ˆMarkdownï¼‰"""
        md = ""
        for metric, patterns in pattern_data.items():
            if isinstance(patterns, list):
                md += f"#### {metric}\n\n"
                for pattern in patterns:
                    md += f"""
- **{pattern["pattern_type"]}**: {pattern["description"]}
  - æ•°é‡: {pattern.get("count", pattern.get("size", "N/A"))}
  - å¼ºåº¦: {pattern.get("strength", pattern.get("autocorrelation", "N/A")):.2f}

"""
        return md

    def _format_recommendations_md(self, recommendations: List[str]) -> str:
        """æ ¼å¼åŒ–å»ºè®®ï¼ˆMarkdownï¼‰"""
        md = ""
        for rec in recommendations:
            md += f"- {rec}\n"
        return md


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
async def demo_enhanced_data_analyzer():
    """æ¼”ç¤ºå¢å¼ºçš„æ•°æ®åˆ†æå™¨åŠŸèƒ½"""
    print("ğŸš€ æ¼”ç¤ºå¢å¼ºçš„æ•°æ®åˆ†æå™¨åŠŸèƒ½")

    # åˆ›å»ºåˆ†æå™¨
    analyzer = TestDataAnalyzer({"contamination": 0.05})  # 5%çš„å¼‚å¸¸ç‡

    # æ¨¡æ‹Ÿæµ‹è¯•æŒ‡æ ‡æ•°æ®
    test_metrics = {
        "response_time": [45, 42, 48, 43, 47, 150, 44, 46, 45, 43, 49, 151, 47, 45, 44],
        "throughput": [
            1200,
            1180,
            1220,
            1190,
            1210,
            800,
            1170,
            1230,
            1190,
            1210,
            1180,
            750,
            1200,
            1190,
            1220,
        ],
        "error_rate": [
            0.01,
            0.02,
            0.01,
            0.03,
            0.01,
            0.15,
            0.02,
            0.01,
            0.01,
            0.02,
            0.01,
            0.20,
            0.01,
            0.02,
            0.01,
        ],
        "timestamp": [
            "2024-01-01 09:00:00",
            "2024-01-01 09:01:00",
            "2024-01-01 09:02:00",
            "2024-01-01 09:03:00",
            "2024-01-01 09:04:00",
            "2024-01-01 09:05:00",
            "2024-01-01 09:06:00",
            "2024-01-01 09:07:00",
            "2024-01-01 09:08:00",
            "2024-01-01 09:09:00",
            "2024-01-01 09:10:00",
            "2024-01-01 09:11:00",
            "2024-01-01 09:12:00",
            "2024-01-01 09:13:00",
            "2024-01-01 09:14:00",
        ],
    }

    # æ‰§è¡Œåˆ†æ
    analysis_result = analyzer.analyze_test_metrics(test_metrics)

    # è¾“å‡ºç»“æœ
    print("\nğŸ“Š åˆ†æç»“æœæ‘˜è¦:")
    print(f"åˆ†ææ—¶é—´: {analysis_result['timestamp']}")
    print(f"å»ºè®®æ•°é‡: {len(analysis_result['recommendations'])}")

    print("\nğŸ” å¼‚å¸¸æ£€æµ‹ç»“æœ:")
    for metric, data in analysis_result["anomaly_detection"].items():
        if isinstance(data, dict) and "anomaly_rate" in data:
            print(f"  {metric}: {data['anomaly_count']} ä¸ªå¼‚å¸¸ ({data['anomaly_rate']:.2%})")

    print("\nğŸ“ˆ è¶‹åŠ¿åˆ†æç»“æœ:")
    for metric, data in analysis_result["trend_analysis"].items():
        if isinstance(data, dict) and "trend_direction" in data:
            print(f"  {metric}: {data['trend_direction']} (å¼ºåº¦: {data['trend_strength']:.2f})")

    print("\nğŸ”® è¯†åˆ«åˆ°çš„æ¨¡å¼:")
    for metric, patterns in analysis_result["pattern_recognition"].items():
        if isinstance(patterns, list):
            print(f"  {metric}: {len(patterns)} ä¸ªæ¨¡å¼")
            for pattern in patterns[:2]:  # æ˜¾ç¤ºå‰2ä¸ªæ¨¡å¼
                print(f"    - {pattern['pattern_type']}: {pattern['description']}")

    print("\nğŸ’¡ å»ºè®®:")
    for i, rec in enumerate(analysis_result["recommendations"], 1):
        print(f"  {i}. {rec}")

    # ç”ŸæˆHTMLæŠ¥å‘Š
    html_report = analyzer.generate_analysis_report("html")
    with open("analysis_report.html", "w", encoding="utf-8") as f:
        f.write(html_report)
    print("\nâœ… HTMLæŠ¥å‘Šå·²ç”Ÿæˆ: analysis_report.html")

    # ç”ŸæˆMarkdownæŠ¥å‘Š
    md_report = analyzer.generate_analysis_report("markdown")
    with open("analysis_report.md", "w", encoding="utf-8") as f:
        f.write(md_report)
    print("âœ… MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: analysis_report.md")


if __name__ == "__main__":
    # è¿è¡ŒåŸæœ‰æµ‹è¯•
    print("ğŸ¤– å¯åŠ¨AIæµ‹è¯•æ•°æ®åˆ†æå™¨...")
    test_anomaly_detection()
    print()
    test_pattern_analysis()
    print()
    test_trend_prediction()

    # è¿è¡Œå¢å¼ºåŠŸèƒ½æ¼”ç¤º
    print("\n" + "=" * 50)
    asyncio.run(demo_enhanced_data_analyzer())


# æµ‹è¯•å‡½æ•°
def test_anomaly_detection():
    """æµ‹è¯•å¼‚å¸¸æ£€æµ‹"""
    analyzer = AITestDataAnalyzer()

    # æ¨¡æ‹Ÿæµ‹è¯•ç»“æœ
    test_results = [
        {
            "function_name": "get_stock_price",
            "status": "passed",
            "duration": 100,
            "timestamp": "2024-12-12T10:00:00",
            "memory_usage": 50.5,
        },
        {
            "function_name": "get_stock_price",
            "status": "passed",
            "duration": 105,
            "timestamp": "2024-12-12T10:01:00",
            "memory_usage": 51.2,
        },
        {
            "function_name": "get_stock_price",
            "status": "failed",
            "duration": 5000,  # å¼‚å¸¸æ…¢
            "timestamp": "2024-12-12T10:02:00",
            "memory_usage": 200.0,  # å¼‚å¸¸é«˜å†…å­˜
        },
    ]

    # æ£€æµ‹å¼‚å¸¸
    anomalies = analyzer.detect_test_anomalies(test_results)

    print(f"æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸:")
    for anomaly in anomalies:
        print(f"  - {anomaly.description} (ç½®ä¿¡åº¦: {anomaly.confidence_score:.2f})")


def test_pattern_analysis():
    """æµ‹è¯•æ¨¡å¼åˆ†æ"""
    analyzer = AITestDataAnalyzer()

    # æ¨¡æ‹Ÿæµ‹è¯•ç»“æœ
    test_results = []
    for i in range(50):
        test_results.append(
            {
                "function_name": "get_stock_price",
                "status": "passed" if i % 10 != 0 else "failed",
                "duration": 100 + i % 20,
                "timestamp": f"2024-12-12T10:{i:02d}:00",
            }
        )

    # åˆ†ææ¨¡å¼
    patterns = analyzer.analyze_test_patterns(test_results)

    print(f"è¯†åˆ«åˆ° {len(patterns)} ä¸ªæ¨¡å¼:")
    for pattern in patterns[:5]:
        print(f"  - {pattern.pattern_name}: é¢‘ç‡={pattern.frequency}, æˆåŠŸç‡={pattern.success_rate:.2%}")


def test_trend_prediction():
    """æµ‹è¯•è¶‹åŠ¿é¢„æµ‹"""
    analyzer = AITestDataAnalyzer()

    # æ¨¡æ‹Ÿæµ‹è¯•ç»“æœï¼ˆæ¨¡æ‹Ÿæ€§èƒ½ä¸‹é™è¶‹åŠ¿ï¼‰
    test_results = []
    base_duration = 100
    for i in range(30):
        # æ¨¡æ‹Ÿæ€§èƒ½é€æ¸ä¸‹é™
        duration = base_duration + (i * 5)
        test_results.append(
            {
                "function_name": "calculate_indicators",
                "status": "passed",
                "duration": duration,
                "timestamp": f"2024-12-12T{i:02d}:00:00",
            }
        )

    # é¢„æµ‹è¶‹åŠ¿
    trends = analyzer.predict_test_trends(test_results)

    print("é¢„æµ‹çš„è¶‹åŠ¿:")
    for trend in trends:
        print(f"  - {trend.trend_name}: {trend.direction} (å˜åŒ–ç‡: {trend.change_rate:.2%})")


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    print("ğŸ¤– å¯åŠ¨AIæµ‹è¯•æ•°æ®åˆ†æå™¨...")

    test_anomaly_detection()
    print()
    test_pattern_analysis()
    print()
    test_trend_prediction()
