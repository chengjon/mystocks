#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•è‡ªåŠ¨åŒ–ç¼–æ’

æä¾›æ™ºèƒ½åŒ–çš„æµ‹è¯•æ‰§è¡Œç¼–æ’ã€ä¾èµ–ç®¡ç†ã€æ‰§è¡Œç­–ç•¥å’Œèµ„æºè°ƒåº¦åŠŸèƒ½ã€‚
"""

import asyncio
import json
import logging
import time
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)


class ExecutionStrategy(Enum):
    """æ‰§è¡Œç­–ç•¥æšä¸¾"""

    SEQUENTIAL = "sequential"  # é¡ºåºæ‰§è¡Œ
    PARALLEL = "parallel"  # å¹¶è¡Œæ‰§è¡Œ
    ADAPTIVE = "adaptive"  # è‡ªé€‚åº”æ‰§è¡Œ
    PIPELINE = "pipeline"  # ç®¡é“æ‰§è¡Œ
    DISTRIBUTED = "distributed"  # åˆ†å¸ƒå¼æ‰§è¡Œ


class TaskPriority(Enum):
    """ä»»åŠ¡ä¼˜å…ˆçº§æšä¸¾"""

    CRITICAL = 1  # å…³é”®ä»»åŠ¡
    HIGH = 2  # é«˜ä¼˜å…ˆçº§
    NORMAL = 3  # æ™®é€šä¼˜å…ˆçº§
    LOW = 4  # ä½ä¼˜å…ˆçº§
    BACKGROUND = 5  # åå°ä»»åŠ¡


@dataclass
class TestTask:
    """æµ‹è¯•ä»»åŠ¡å®šä¹‰"""

    id: str
    name: str
    description: str
    test_type: str  # unit, integration, e2e, performance, ai, security, chaos
    priority: TaskPriority
    dependencies: List[str] = field(default_factory=list)
    timeout: int = 300  # ç§’
    retry_count: int = 3
    retry_delay: float = 1.0
    tags: List[str] = field(default_factory=list)
    config: Dict[str, Any] = field(default_factory=dict)
    execution_mode: ExecutionStrategy = ExecutionStrategy.SEQUENTIAL

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "id": self.id,
            "name": self.name,
            "description": self.description,
            "test_type": self.test_type,
            "priority": self.priority.value,
            "dependencies": self.dependencies,
            "timeout": self.timeout,
            "retry_count": self.retry_count,
            "retry_delay": self.retry_delay,
            "tags": self.tags,
            "config": self.config,
            "execution_mode": self.execution_mode.value,
        }


@dataclass
class TestExecution:
    """æµ‹è¯•æ‰§è¡Œè®°å½•"""

    task_id: str
    task_name: str
    status: str  # pending, running, completed, failed, skipped
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    duration: float = 0.0
    result: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    retry_count: int = 0
    resources_used: Dict[str, Any] = field(default_factory=dict)
    metrics: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "task_id": self.task_id,
            "task_name": self.task_name,
            "status": self.status,
            "start_time": self.start_time.isoformat() if self.start_time else None,
            "end_time": self.end_time.isoformat() if self.end_time else None,
            "duration": self.duration,
            "result": self.result,
            "error_message": self.error_message,
            "retry_count": self.retry_count,
            "resources_used": self.resources_used,
            "metrics": self.metrics,
        }


@dataclass
class OrchestrationConfig:
    """ç¼–æ’é…ç½®"""

    max_concurrent_tasks: int = 5
    task_timeout: int = 600
    enable_retry: bool = True
    enable_parallel: bool = True
    enable_monitoring: bool = True
    enable_metrics: bool = True
    enable_resource_scaling: bool = False
    checkpoint_enabled: bool = True
    checkpoint_interval: int = 300  # 5åˆ†é’Ÿ
    log_level: str = "INFO"


class DependencyResolver:
    """ä¾èµ–å…³ç³»è§£æå™¨"""

    def __init__(self):
        self.tasks: Dict[str, TestTask] = {}
        self.execution_order: List[List[str]] = []

    def add_task(self, task: TestTask):
        """æ·»åŠ ä»»åŠ¡"""
        self.tasks[task.id] = task

    def resolve_dependencies(self) -> List[List[str]]:
        """è§£æä¾èµ–å…³ç³»å¹¶è¿”å›æ‰§è¡Œé¡ºåº"""
        self.execution_order = []

        # æ„å»ºä¾èµ–å›¾
        dependency_graph = {}
        all_tasks = set(self.tasks.keys())

        for task_id, task in self.tasks.items():
            dependencies = set(task.dependencies)
            dependent_tasks = set()

            # æŸ¥æ‰¾ä¾èµ–æ­¤ä»»åŠ¡çš„å…¶ä»–ä»»åŠ¡
            for other_id, other_task in self.tasks.items():
                if task_id in other_task.dependencies:
                    dependent_tasks.add(other_id)

            dependency_graph[task_id] = {
                "dependencies": dependencies,
                "dependents": dependent_tasks,
            }

        # æ‹“æ‰‘æ’åº
        in_degree = {
            task_id: len(dep_graph[task_id]["dependencies"]) for task_id, dep_graph in dependency_graph.items()
        }

        # æ‰¾å‡ºæ‰€æœ‰æ²¡æœ‰ä¾èµ–çš„ä»»åŠ¡
        queue = [task_id for task_id, degree in in_degree.items() if degree == 0]
        execution_levels = []

        while queue:
            # å½“å‰å±‚çš„ä»»åŠ¡
            current_level = []
            for _ in range(len(queue)):
                task_id = queue.pop(0)
                current_level.append(task_id)

                # æ›´æ–°ä¾èµ–æ­¤ä»»åŠ¡çš„ä»»åŠ¡çš„å…¥åº¦
                for dependent in dependency_graph[task_id]["dependents"]:
                    in_degree[dependent] -= 1
                    if in_degree[dependent] == 0:
                        queue.append(dependent)

            if current_level:
                execution_levels.append(current_level)

        self.execution_order = execution_levels
        return execution_levels

    def get_execution_plan(self) -> Dict[str, Any]:
        """è·å–æ‰§è¡Œè®¡åˆ’"""
        return {
            "total_tasks": len(self.tasks),
            "execution_levels": self.execution_order,
            "task_count_by_level": [len(level) for level in self.execution_order],
            "critical_path": self._find_critical_path(),
            "parallel_groups": len(self.execution_order),
            "estimated_duration": self._estimate_total_duration(),
        }

    def _find_critical_path(self) -> List[str]:
        """æŸ¥æ‰¾å…³é”®è·¯å¾„"""
        critical_path = []
        max_duration = 0

        for task in self.tasks.values():
            path_duration = 0
            current_task = task

            # å‘ä¸Šéå†ä¾èµ–é“¾
            while current_task:
                path_duration += current_task.timeout
                if current_task.dependencies:
                    # é€‰æ‹©æ‰§è¡Œæ—¶é—´æœ€é•¿çš„ä¾èµ–
                    dependency_durations = []
                    for dep_id in current_task.dependencies:
                        if dep_id in self.tasks:
                            dep_task = self.tasks[dep_id]
                            dependency_durations.append(dep_task.timeout + self._get_dependency_duration(dep_task))
                    if dependency_durations:
                        path_duration += max(dependency_durations)
                    break
                else:
                    break

                # æ›´æ–°å½“å‰ä»»åŠ¡
                if current_task.dependencies:
                    # é€‰æ‹©æ‰§è¡Œæ—¶é—´æœ€é•¿çš„ä¾èµ–
                    max_dep_duration = 0
                    max_dep_task = None
                    for dep_id in current_task.dependencies:
                        if dep_id in self.tasks:
                            dep_task = self.tasks[dep_id]
                            if dep_task.timeout > max_dep_duration:
                                max_dep_duration = dep_task.timeout
                                max_dep_task = dep_task
                    current_task = max_dep_task if max_dep_task else None
                else:
                    current_task = None

            if path_duration > max_duration:
                max_duration = path_duration
                critical_path = [task.id]

        return critical_path

    def _get_dependency_duration(self, task: TestTask) -> int:
        """è·å–ä»»åŠ¡ä¾èµ–çš„æ‰§è¡Œæ—¶é—´"""
        if not task.dependencies:
            return 0

        max_dep_duration = 0
        for dep_id in task.dependencies:
            if dep_id in self.tasks:
                dep_task = self.tasks[dep_id]
                dep_duration = dep_task.timeout + self._get_dependency_duration(dep_task)
                if dep_duration > max_dep_duration:
                    max_dep_duration = dep_duration

        return max_dep_duration

    def _estimate_total_duration(self) -> int:
        """ä¼°è®¡æ€»æ‰§è¡Œæ—¶é—´"""
        total_duration = 0

        for level in self.execution_order:
            # åŒä¸€å±‚çš„ä»»åŠ¡å¹¶è¡Œæ‰§è¡Œï¼Œå–æœ€é•¿æ—¶é—´
            level_max_duration = 0
            for task_id in level:
                if task_id in self.tasks:
                    task = self.tasks[task_id]
                    level_max_duration = max(level_max_duration, task.timeout)
            total_duration += level_max_duration

        return total_duration


class TaskExecutor:
    """ä»»åŠ¡æ‰§è¡Œå™¨"""

    def __init__(self, config: OrchestrationConfig):
        self.config = config
        self.active_executions: Dict[str, TestExecution] = {}
        self.completed_executions: Dict[str, TestExecution] = {}
        self.resource_monitor = ResourceMonitor()

    async def execute_task(self, task: TestTask) -> TestExecution:
        """æ‰§è¡Œå•ä¸ªä»»åŠ¡"""
        execution = TestExecution(task_id=task.id, task_name=task.name, status="running")

        self.active_executions[task.id] = execution

        try:
            # å¼€å§‹æ‰§è¡Œ
            execution.start_time = datetime.now()
            logger.info("å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task.name} ({task.id})")

            # ç›‘æ§èµ„æºä½¿ç”¨
            resource_task = asyncio.create_task(self.resource_monitor.monitor_resources(task.id, task.timeout))

            # æ‰§è¡Œä»»åŠ¡
            if task.execution_mode == ExecutionStrategy.SEQUENTIAL:
                result = await self._execute_sequential(task)
            elif task.execution_mode == ExecutionStrategy.PARALLEL:
                result = await self._execute_parallel(task)
            elif task.execution_mode == ExecutionStrategy.ADAPTIVE:
                result = await self._execute_adaptive(task)
            else:
                # é»˜è®¤æ‰§è¡Œ
                result = await self._execute_default(task)

            # åœæ­¢èµ„æºç›‘æ§
            resource_task.cancel()
            try:
                await resource_task
            except asyncio.CancelledError:
                pass

            # è®¾ç½®æ‰§è¡Œç»“æœ
            execution.end_time = datetime.now()
            execution.duration = (execution.end_time - execution.start_time).total_seconds()
            execution.status = "completed"
            execution.result = result
            execution.metrics = {
                "execution_time": execution.duration,
                "memory_usage": self.resource_monitor.get_memory_usage(task.id),
                "cpu_usage": self.resource_monitor.get_cpu_usage(task.id),
            }

            logger.info("ä»»åŠ¡æ‰§è¡Œå®Œæˆ: {task.name} ({execution.duration:.2f}ç§’)")

        except Exception as e:
            # å¤„ç†å¼‚å¸¸
            execution.end_time = datetime.now()
            execution.duration = (execution.end_time - execution.start_time).total_seconds()
            execution.status = "failed"
            execution.error_message = str(e)

            logger.error("ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {task.name} - {str(e)}")

        finally:
            # ä»æ´»åŠ¨æ‰§è¡Œä¸­ç§»é™¤
            self.active_executions.pop(task.id, None)
            self.completed_executions[task.id] = execution

        return execution

    async def _execute_sequential(self, task: TestTask) -> Dict[str, Any]:
        """é¡ºåºæ‰§è¡Œä»»åŠ¡"""
        # æ¨¡æ‹Ÿä»»åŠ¡æ‰§è¡Œ
        await asyncio.sleep(min(task.timeout, 10))  # æœ€å¤šç­‰å¾…10ç§’

        return {
            "status": "success",
            "task_id": task.id,
            "executed_at": datetime.now().isoformat(),
            "data": {"message": f"Sequential task {task.name} completed"},
        }

    async def _execute_parallel(self, task: TestTask) -> Dict[str, Any]:
        """å¹¶è¡Œæ‰§è¡Œä»»åŠ¡"""
        # åˆ›å»ºå­ä»»åŠ¡
        subtasks = []
        for i in range(min(3, task.config.get("subtask_count", 3))):
            subtask = asyncio.create_task(self._execute_subtask(f"{task.id}_{i}"))
            subtasks.append(subtask)

        # ç­‰å¾…æ‰€æœ‰å­ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*subtasks, return_exceptions=True)

        return {
            "status": "success",
            "task_id": task.id,
            "subtasks": len(subtasks),
            "results": results,
        }

    async def _execute_adaptive(self, task: TestTask) -> Dict[str, Any]:
        """è‡ªé€‚åº”æ‰§è¡Œä»»åŠ¡"""
        # æ ¹æ®ç³»ç»Ÿè´Ÿè½½åŠ¨æ€è°ƒæ•´æ‰§è¡Œç­–ç•¥
        system_load = self.resource_monitor.get_system_load()

        if system_load < 0.5:
            # è´Ÿè½½ä½ï¼Œä½¿ç”¨å¹¶è¡Œç­–ç•¥
            return await self._execute_parallel(task)
        else:
            # è´Ÿè½½é«˜ï¼Œä½¿ç”¨é¡ºåºç­–ç•¥
            return await self._execute_sequential(task)

    async def _execute_default(self, task: TestTask) -> Dict[str, Any]:
        """é»˜è®¤æ‰§è¡Œç­–ç•¥"""
        await asyncio.sleep(min(task.timeout, 5))
        return {
            "status": "success",
            "task_id": task.id,
            "message": f"Task {task.name} completed",
        }

    async def _execute_subtask(self, subtask_id: str) -> Dict[str, Any]:
        """æ‰§è¡Œå­ä»»åŠ¡"""
        await asyncio.sleep(random.uniform(1, 3))
        return {
            "subtask_id": subtask_id,
            "status": "completed",
            "result": f"Subtask {subtask_id} done",
        }

    async def execute_with_retry(self, task: TestTask) -> TestExecution:
        """å¸¦é‡è¯•çš„ä»»åŠ¡æ‰§è¡Œ"""
        last_error = None

        for attempt in range(task.retry_count + 1):
            if attempt > 0:
                await asyncio.sleep(task.retry_delay * attempt)

            try:
                execution = await self.execute_task(task)
                if execution.status == "completed":
                    return execution
                else:
                    last_error = execution.error_message
                    logger.warning("ä»»åŠ¡ {task.name} ç¬¬%(attempt)sæ¬¡æ‰§è¡Œå¤±è´¥")

            except Exception as e:
                last_error = str(e)
                logger.warning("ä»»åŠ¡ {task.name} ç¬¬%(attempt)sæ¬¡æ‰§è¡Œå¼‚å¸¸: {str(e)}")

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        execution = TestExecution(
            task_id=task.id,
            task_name=task.name,
            status="failed",
            error_message=f"æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥: {last_error}",
        )
        return execution


class ResourceMonitor:
    """èµ„æºç›‘æ§å™¨"""

    def __init__(self):
        self.task_resources: Dict[str, Dict[str, List[float]]] = {}
        self.system_resources = {"cpu": [], "memory": [], "disk": []}

    async def monitor_resources(self, task_id: str, duration: int):
        """ç›‘æ§ä»»åŠ¡èµ„æºä½¿ç”¨"""
        start_time = time.time()
        interval = 1.0  # 1ç§’é—´éš”

        while time.time() - start_time < duration:
            # æ”¶é›†ç³»ç»Ÿèµ„æº
            cpu_percent = psutil.cpu_percent()
            memory_percent = psutil.virtual_memory().percent
            disk_percent = psutil.disk_usage("/").percent

            # è®°å½•ç³»ç»Ÿèµ„æº
            self.system_resources["cpu"].append(cpu_percent)
            self.system_resources["memory"].append(memory_percent)
            self.system_resources["disk"].append(disk_percent)

            # å¦‚æœä»»åŠ¡æœ‰èµ„æºè®°å½•ï¼Œè®°å½•ä»»åŠ¡èµ„æº
            if task_id in self.task_resources:
                self.task_resources[task_id]["cpu"].append(cpu_percent)
                self.task_resources[task_id]["memory"].append(memory_percent)

            await asyncio.sleep(interval)

    def get_memory_usage(self, task_id: str) -> float:
        """è·å–ä»»åŠ¡å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if task_id in self.task_resources and "memory" in self.task_resources[task_id]:
            return max(self.task_resources[task_id]["memory"])
        return 0.0

    def get_cpu_usage(self, task_id: str) -> float:
        """è·å–ä»»åŠ¡CPUä½¿ç”¨æƒ…å†µ"""
        if task_id in self.task_resources and "cpu" in self.task_resources[task_id]:
            return max(self.task_resources[task_id]["cpu"])
        return 0.0

    def get_system_load(self) -> float:
        """è·å–ç³»ç»Ÿè´Ÿè½½"""
        if self.system_resources["cpu"]:
            return max(self.system_resources["cpu"][-10:])  # æœ€è¿‘10ä¸ªé‡‡æ ·
        return 0.0


class TestOrchestrator:
    """æµ‹è¯•ç¼–æ’å™¨ä¸»ç±»"""

    def __init__(self, config: OrchestrationConfig):
        self.config = config
        self.dependency_resolver = DependencyResolver()
        self.task_executor = TaskExecutor(config)
        self.task_queue = asyncio.Queue()
        self.execution_history: List[TestExecution] = []
        self.checkpoint_manager = CheckpointManager()

    def add_task(self, task: TestTask):
        """æ·»åŠ ä»»åŠ¡"""
        self.dependency_resolver.add_task(task)

    async def execute_all_tasks(self) -> Dict[str, Any]:
        """æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡"""
        logger.info("å¼€å§‹æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡")

        # è§£æä¾èµ–å…³ç³»
        execution_levels = self.dependency_resolver.resolve_dependencies()
        plan = self.dependency_resolver.get_execution_plan()

        logger.info("æ‰§è¡Œè®¡åˆ’: %(plan)s")

        # å¯åŠ¨ç›‘æ§ä»»åŠ¡
        monitor_task = asyncio.create_task(self._monitor_execution())

        # æŒ‰å±‚çº§æ‰§è¡Œ
        all_executions = []
        for level_idx, level in enumerate(execution_levels):
            logger.info("æ‰§è¡Œç¬¬ {level_idx + 1} å±‚ä»»åŠ¡: {len(level)} ä¸ªä»»åŠ¡")

            if level and self.config.enable_parallel:
                # å¹¶è¡Œæ‰§è¡ŒåŒä¸€å±‚çº§çš„ä»»åŠ¡
                level_executions = await self._execute_level_parallel(level)
            else:
                # é¡ºåºæ‰§è¡Œ
                level_executions = await self._execute_level_sequential(level)

            all_executions.extend(level_executions)

            # ä¿å­˜æ£€æŸ¥ç‚¹
            if self.config.checkpoint_enabled and level_idx % 2 == 0:
                await self.checkpoint_manager.save_checkpoint(all_executions)

        # åœæ­¢ç›‘æ§
        monitor_task.cancel()

        # ç”Ÿæˆæ‰§è¡ŒæŠ¥å‘Š
        report = self._generate_execution_report(all_executions)

        # ä¿å­˜æ‰§è¡Œå†å²
        self.execution_history.extend(all_executions)

        logger.info("æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼Œå…±æ‰§è¡Œ {len(all_executions)} ä¸ªä»»åŠ¡")
        return report

    async def _execute_level_parallel(self, task_ids: List[str]) -> List[TestExecution]:
        """å¹¶è¡Œæ‰§è¡Œå±‚çº§çš„ä»»åŠ¡"""
        semaphore = asyncio.Semaphore(self.config.max_concurrent_tasks)

        async def execute_with_semaphore(task_id: str) -> TestExecution:
            async with semaphore:
                if task_id in self.dependency_resolver.tasks:
                    task = self.dependency_resolver.tasks[task_id]
                    return await self.task_executor.execute_with_retry(task)
                else:
                    return TestExecution(
                        task_id=task_id,
                        task_name="Unknown Task",
                        status="failed",
                        error_message="Task not found",
                    )

        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = [execute_with_semaphore(task_id) for task_id in task_ids]
        executions = await asyncio.gather(*tasks, return_exceptions=True)

        # è¿‡æ»¤å¼‚å¸¸ç»“æœ
        valid_executions = []
        for execution in executions:
            if isinstance(execution, TestExecution):
                valid_executions.append(execution)
            else:
                logger.error("ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: %(execution)s")

        return valid_executions

    async def _execute_level_sequential(self, task_ids: List[str]) -> List[TestExecution]:
        """é¡ºåºæ‰§è¡Œå±‚çº§çš„ä»»åŠ¡"""
        executions = []

        for task_id in task_ids:
            if task_id in self.dependency_resolver.tasks:
                task = self.dependency_resolver.tasks[task_id]
                execution = await self.task_executor.execute_with_retry(task)
                executions.append(execution)
            else:
                logger.error("ä»»åŠ¡ä¸å­˜åœ¨: %(task_id)s")

        return executions

    async def _monitor_execution(self):
        """ç›‘æ§æ‰§è¡Œè¿‡ç¨‹"""
        while True:
            # è·å–æ‰§è¡Œç»Ÿè®¡
            active_count = len(self.task_executor.active_executions)
            completed_count = len(self.task_executor.completed_executions)

            logger.info("æ‰§è¡Œç»Ÿè®¡ - æ´»è·ƒä»»åŠ¡: %(active_count)s, å·²å®Œæˆ: %(completed_count)s")

            # æ£€æŸ¥æ˜¯å¦æœ‰é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡
            current_time = datetime.now()
            for execution in self.task_executor.active_executions.values():
                if execution.start_time:
                    duration = (current_time - execution.start_time).total_seconds()
                    if duration > execution.task.timeout * 1.5:
                        logger.warning("ä»»åŠ¡ {execution.task_name} æ‰§è¡Œè¶…æ—¶")

            await asyncio.sleep(self.config.task_timeout // 10)

    def _generate_execution_report(self, executions: List[TestExecution]) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰§è¡ŒæŠ¥å‘Š"""
        total_tasks = len(executions)
        completed_tasks = len([e for e in executions if e.status == "completed"])
        failed_tasks = len([e for e in executions if e.status == "failed"])

        # è®¡ç®—å¹³å‡æ‰§è¡Œæ—¶é—´
        avg_duration = sum(e.duration for e in executions) / total_tasks if total_tasks > 0 else 0

        # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
        by_type = {}
        for execution in executions:
            test_type = execution.config.get("test_type", "unknown")
            if test_type not in by_type:
                by_type[test_type] = {"total": 0, "completed": 0, "failed": 0}
            by_type[test_type]["total"] += 1
            if execution.status == "completed":
                by_type[test_type]["completed"] += 1
            elif execution.status == "failed":
                by_type[test_type]["failed"] += 1

        return {
            "execution_summary": {
                "total_tasks": total_tasks,
                "completed_tasks": completed_tasks,
                "failed_tasks": failed_tasks,
                "success_rate": completed_tasks / total_tasks if total_tasks > 0 else 0,
                "average_duration": avg_duration,
            },
            "task_type_statistics": by_type,
            "execution_details": [e.to_dict() for e in executions],
            "execution_plan": self.dependency_resolver.get_execution_plan(),
        }

    async def restore_from_checkpoint(self) -> Dict[str, Any]:
        """ä»æ£€æŸ¥ç‚¹æ¢å¤æ‰§è¡Œ"""
        try:
            checkpoint = await self.checkpoint_manager.load_latest_checkpoint()
            if checkpoint:
                logger.info("ä»æ£€æŸ¥ç‚¹æ¢å¤: {checkpoint['timestamp']}")
                self.execution_history.extend(checkpoint["executions"])
                return checkpoint
            else:
                logger.warning("æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹")
                return {"executions": [], "timestamp": None}
        except Exception as e:
            logger.error("æ£€æŸ¥ç‚¹æ¢å¤å¤±è´¥: {str(e)}")
            return {"executions": [], "timestamp": None}


class CheckpointManager:
    """æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""

    def __init__(self, checkpoint_dir: str = "checkpoints"):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)

    async def save_checkpoint(self, executions: List[TestExecution]):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        timestamp = datetime.now().isoformat()
        checkpoint = {
            "timestamp": timestamp,
            "executions": [e.to_dict() for e in executions],
        }

        filename = f"checkpoint_{timestamp.replace(':', '-')}.json"
        filepath = self.checkpoint_dir / filename

        try:
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(checkpoint, f, indent=2, ensure_ascii=False)
            logger.info("æ£€æŸ¥ç‚¹å·²ä¿å­˜: %(filepath)s")
        except Exception as e:
            logger.error("æ£€æŸ¥ç‚¹ä¿å­˜å¤±è´¥: {str(e)}")

    async def load_latest_checkpoint(self) -> Optional[Dict[str, Any]]:
        """åŠ è½½æœ€æ–°çš„æ£€æŸ¥ç‚¹"""
        checkpoint_files = list(self.checkpoint_dir.glob("checkpoint_*.json"))
        if not checkpoint_files:
            return None

        # æ‰¾åˆ°æœ€æ–°çš„æ–‡ä»¶
        latest_file = max(checkpoint_files, key=lambda f: f.stat().st_mtime)

        try:
            with open(latest_file, "r", encoding="utf-8") as f:
                checkpoint = json.load(f)
            logger.info("åŠ è½½æ£€æŸ¥ç‚¹: %(latest_file)s")
            return checkpoint
        except Exception as e:
            logger.error("æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {str(e)}")
            return None

    def list_checkpoints(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰æ£€æŸ¥ç‚¹"""
        checkpoints = []
        for checkpoint_file in self.checkpoint_dir.glob("checkpoint_*.json"):
            try:
                with open(checkpoint_file, "r", encoding="utf-8") as f:
                    checkpoint = json.load(f)
                checkpoints.append(
                    {
                        "filename": checkpoint_file.name,
                        "timestamp": checkpoint["timestamp"],
                        "execution_count": len(checkpoint.get("executions", [])),
                    }
                )
            except Exception as e:
                logger.error("è¯»å–æ£€æŸ¥ç‚¹å¤±è´¥ %(checkpoint_file)s: {str(e)}")

        return sorted(checkpoints, key=lambda x: x["timestamp"], reverse=True)


# ç®¡ç†å™¨å·¥å‚
class OrchestrationManager:
    """ç¼–æ’ç®¡ç†å™¨å·¥å‚"""

    @staticmethod
    def create_default_orchestrator() -> TestOrchestrator:
        """åˆ›å»ºé»˜è®¤ç¼–æ’å™¨"""
        config = OrchestrationConfig(
            max_concurrent_tasks=5,
            enable_parallel=True,
            enable_monitoring=True,
            checkpoint_enabled=True,
        )
        return TestOrchestrator(config)

    @staticmethod
    def create_performance_orchestrator() -> TestOrchestrator:
        """åˆ›å»ºæ€§èƒ½æµ‹è¯•ç¼–æ’å™¨"""
        config = OrchestrationConfig(
            max_concurrent_tasks=3,
            enable_parallel=False,
            task_timeout=1800,  # 30åˆ†é’Ÿ
            enable_retry=True,
        )
        return TestOrchestrator(config)

    @staticmethod
    def create_chaos_orchestrator() -> TestOrchestrator:
        """åˆ›å»ºæ··æ²Œå·¥ç¨‹ç¼–æ’å™¨"""
        config = OrchestrationConfig(
            max_concurrent_tasks=2,
            enable_parallel=False,
            task_timeout=600,
            fail_fast=False,
            log_level="WARNING",
        )
        return TestOrchestrator(config)


# ä½¿ç”¨ç¤ºä¾‹
async def demo_orchestration():
    """æ¼”ç¤ºç¼–æ’åŠŸèƒ½"""
    print("ğŸš€ æ¼”ç¤ºæµ‹è¯•ç¼–æ’åŠŸèƒ½")

    # åˆ›å»ºç¼–æ’å™¨
    orchestrator = OrchestrationManager.create_default_orchestrator()

    # æ·»åŠ æµ‹è¯•ä»»åŠ¡
    tasks = [
        TestTask(
            id="task_001",
            name="æ•°æ®åº“è¿æ¥æµ‹è¯•",
            description="æµ‹è¯•æ•°æ®åº“è¿æ¥",
            test_type="unit",
            priority=TaskPriority.HIGH,
            timeout=30,
        ),
        TestTask(
            id="task_002",
            name="APIé›†æˆæµ‹è¯•",
            description="æµ‹è¯•APIé›†æˆ",
            test_type="integration",
            priority=TaskPriority.HIGH,
            dependencies=["task_001"],
            timeout=60,
        ),
        TestTask(
            id="task_003",
            name="ç«¯åˆ°ç«¯æµ‹è¯•",
            description="æ‰§è¡Œç«¯åˆ°ç«¯æµ‹è¯•",
            test_type="e2e",
            priority=TaskPriority.NORMAL,
            dependencies=["task_002"],
            timeout=120,
        ),
        TestTask(
            id="task_004",
            name="æ€§èƒ½æµ‹è¯•",
            description="æ‰§è¡Œæ€§èƒ½æµ‹è¯•",
            test_type="performance",
            priority=TaskPriority.NORMAL,
            timeout=300,
        ),
        TestTask(
            id="task_005",
            name="å®‰å…¨æ‰«æ",
            description="æ‰§è¡Œå®‰å…¨æ‰«æ",
            test_type="security",
            priority=TaskPriority.LOW,
            timeout=180,
        ),
    ]

    for task in tasks:
        orchestrator.add_task(task)

    # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
    report = await orchestrator.execute_all_tasks()

    # æ˜¾ç¤ºæ‰§è¡Œç»“æœ
    print("\nğŸ“Š æ‰§è¡ŒæŠ¥å‘Š:")
    print(f"æ€»ä»»åŠ¡æ•°: {report['execution_summary']['total_tasks']}")
    print(f"æˆåŠŸä»»åŠ¡: {report['execution_summary']['completed_tasks']}")
    print(f"å¤±è´¥ä»»åŠ¡: {report['execution_summary']['failed_tasks']}")
    print(f"æˆåŠŸç‡: {report['execution_summary']['success_rate']:.2%}")
    print(f"å¹³å‡æ‰§è¡Œæ—¶é—´: {report['execution_summary']['average_duration']:.2f}ç§’")

    # æ˜¾ç¤ºä»»åŠ¡ç±»å‹ç»Ÿè®¡
    print("\nğŸ“ˆ ä»»åŠ¡ç±»å‹ç»Ÿè®¡:")
    for test_type, stats in report["task_type_statistics"].items():
        print(f"  {test_type}: {stats['completed']}/{stats['total']} æˆåŠŸ")

    # æ˜¾ç¤ºæ£€æŸ¥ç‚¹ä¿¡æ¯
    checkpoint_manager = CheckpointManager()
    checkpoints = checkpoint_manager.list_checkpoints()
    print(f"\nğŸ“ æ£€æŸ¥ç‚¹æ•°é‡: {len(checkpoints)}")


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    import random

    import psutil

    asyncio.run(demo_orchestration())
