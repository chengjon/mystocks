#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆæµ‹è¯•ç®¡ç†å™¨

æä¾›ä¸€ä¸ªç»Ÿä¸€çš„æµ‹è¯•ç®¡ç†æ¥å£ï¼Œé›†æˆæ‰€æœ‰æµ‹è¯•ç»„ä»¶å¹¶åè°ƒæµ‹è¯•æ‰§è¡Œæµç¨‹ã€‚
"""

import asyncio
import json
from datetime import datetime
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor
import logging
from abc import ABC, abstractmethod

from statistics import mean

# å¯¼å…¥æ‰€æœ‰æµ‹è¯•ç»„ä»¶
from .ai.test_intelligent_generator import TestDataGenerator as AITestDataGenerator
from .contract.test_contract_validator import ContractTestValidator
from .contract.test_contract_generator import APIContractGenerator
from .contract.test_contract_executor import ContractTestExecutor
from .performance.test_performance_suite import PerformanceTestSuite
from .chaos.test_fault_injection import FaultInjectionSystem
from .chaos.test_resilience import ResilienceTestingFramework
from .security.test_security_vulnerabilities import SecurityVulnerabilityScanner
from .security.test_security_compliance import SecurityComplianceTester
from .data.test_data_manager import TestDataOptimizer


class TestType(Enum):
    """æµ‹è¯•ç±»å‹æšä¸¾"""

    UNIT = "unit"
    INTEGRATION = "integration"
    E2E = "e2e"
    AI_ASSISTED = "ai_assisted"
    CONTRACT = "contract"
    PERFORMANCE = "performance"
    CHAOS = "chaos"
    SECURITY = "security"
    COMPLIANCE = "compliance"
    COMPREHENSIVE = "comprehensive"


class TestStatus(Enum):
    """æµ‹è¯•çŠ¶æ€æšä¸¾"""

    PENDING = "pending"
    RUNNING = "running"
    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"
    CANCELLED = "cancelled"
    TIMEOUT = "timeout"


@dataclass
class TestCase:
    """æµ‹è¯•ç”¨ä¾‹å®šä¹‰"""

    id: str
    name: str
    test_type: TestType
    description: str
    priority: int = 1
    timeout: int = 300
    dependencies: List[str] = field(default_factory=list)
    tags: List[str] = field(default_factory=list)
    setup: Optional[Callable] = None
    teardown: Optional[Callable] = None
    execute: Optional[Callable] = None
    expected_result: Any = None
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)


@dataclass
class TestSuite:
    """æµ‹è¯•å¥—ä»¶å®šä¹‰"""

    id: str
    name: str
    description: str
    test_cases: List[str] = field(default_factory=list)
    test_type: TestType = TestType.COMPREHENSIVE
    execution_order: str = "sequential"  # sequential, parallel, adaptive
    max_parallel: int = 5
    timeout: int = 3600
    setup: Optional[Callable] = None
    teardown: Optional[Callable] = None
    created_at: datetime = field(default_factory=datetime.now)


@dataclass
class TestExecutionResult:
    """æµ‹è¯•æ‰§è¡Œç»“æœ"""

    test_id: str
    test_name: str
    test_type: TestType
    status: TestStatus
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    duration: Optional[float] = None
    error_message: Optional[str] = None
    result_data: Optional[Dict[str, Any]] = None
    metrics: Optional[Dict[str, Any]] = None
    artifacts: List[str] = field(default_factory=list)
    tags: List[str] = field(default_factory=list)
    retry_count: int = 0
    max_retries: int = 0


class _TestComponentInterface(ABC):
    __test__ = False
    """æµ‹è¯•ç»„ä»¶æ¥å£"""

    @abstractmethod
    async def run_test(self, test_case: TestCase) -> TestExecutionResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        pass

    @abstractmethod
    async def get_test_status(self, test_id: str) -> TestStatus:
        """è·å–æµ‹è¯•çŠ¶æ€"""
        pass

    @abstractmethod
    async def cleanup_resources(self):
        """æ¸…ç†èµ„æº"""
        pass


class TestExecutionEngine:
    """æµ‹è¯•æ‰§è¡Œå¼•æ“"""

    def __init__(self, max_workers: int = 10):
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.active_tests: Dict[str, asyncio.Task] = {}
        self.test_results: Dict[str, TestExecutionResult] = {}

    async def execute_tests_sequentially(
        self, test_cases: List[TestCase]
    ) -> List[TestExecutionResult]:
        """é¡ºåºæ‰§è¡Œæµ‹è¯•"""
        results = []

        for test_case in test_cases:
            try:
                result = await self._execute_single_test(test_case)
                results.append(result)
                self.test_results[test_case.id] = result
            except Exception as e:
                error_result = TestExecutionResult(
                    test_id=test_case.id,
                    test_name=test_case.name,
                    test_type=test_case.test_type,
                    status=TestStatus.FAILED,
                    error_message=str(e),
                )
                results.append(error_result)
                self.test_results[test_case.id] = error_result

        return results

    async def execute_tests_parallelly(
        self, test_cases: List[TestCase]
    ) -> List[TestExecutionResult]:
        """å¹¶è¡Œæ‰§è¡Œæµ‹è¯•"""
        semaphore = asyncio.Semaphore(self.max_workers)
        results = []

        async def execute_with_semaphore(test_case):
            async with semaphore:
                try:
                    result = await self._execute_single_test(test_case)
                    self.test_results[test_case.id] = result
                    return result
                except Exception as e:
                    error_result = TestExecutionResult(
                        test_id=test_case.id,
                        test_name=test_case.name,
                        test_type=test_case.test_type,
                        status=TestStatus.FAILED,
                        error_message=str(e),
                    )
                    self.test_results[test_case.id] = error_result
                    return error_result

        tasks = [execute_with_semaphore(test_case) for test_case in test_cases]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†å¼‚å¸¸ç»“æœ
        processed_results = []
        for result in results:
            if isinstance(result, Exception):
                print(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(result)}")
            else:
                processed_results.append(result)

        return processed_results

    async def _execute_single_test(self, test_case: TestCase) -> TestExecutionResult:
        """æ‰§è¡Œå•ä¸ªæµ‹è¯•"""
        start_time = datetime.now()
        result = TestExecutionResult(
            test_id=test_case.id,
            test_name=test_case.name,
            test_type=test_case.test_type,
            status=TestStatus.RUNNING,
            start_time=start_time,
            tags=test_case.tags,
        )

        try:
            # æ‰§è¡Œè®¾ç½®
            if test_case.setup:
                await test_case.setup()

            # æ‰§è¡Œæµ‹è¯•
            if test_case.execute:
                test_result = await test_case.execute()
                result.result_data = test_result
            else:
                raise Exception("æµ‹è¯•æ‰§è¡Œå‡½æ•°æœªæä¾›")

            # æ‰§è¡Œæ¸…ç†
            if test_case.teardown:
                await test_case.teardown()

            result.status = TestStatus.PASSED
            result.end_time = datetime.now()
            result.duration = (result.end_time - result.start_time).total_seconds()

        except asyncio.TimeoutError:
            result.status = TestStatus.TIMEOUT
            result.error_message = f"æµ‹è¯•è¶…æ—¶ï¼ˆ{test_case.timeout}ç§’ï¼‰"
        except Exception as e:
            result.status = TestStatus.FAILED
            result.error_message = str(e)
        finally:
            result.end_time = datetime.now()
            if result.duration is None:
                result.duration = (result.end_time - result.start_time).total_seconds()

        return result


class ComprehensiveTestManager:
    """ç»¼åˆæµ‹è¯•ç®¡ç†å™¨"""

    def __init__(self, config_path: Optional[str] = None):
        self.config_path = (
            config_path
            or "/opt/claude/mystocks_spec/tests/integration/test_config.json"
        )
        self.config = self._load_config()

        # åˆå§‹åŒ–æµ‹è¯•ç»„ä»¶
        self.ai_generator = AITestDataGenerator()
        self.contract_validator = ContractTestValidator()
        self.contract_generator = APIContractGenerator()
        self.contract_executor = ContractTestExecutor()
        self.performance_suite = PerformanceTestSuite()
        self.fault_injection = FaultInjectionSystem()
        self.resilience_framework = ResilienceTestingFramework()
        self.security_scanner = SecurityVulnerabilityScanner()
        self.compliance_tester = SecurityComplianceTester()
        self.data_optimizer = TestDataOptimizer()

        # åˆå§‹åŒ–æ‰§è¡Œå¼•æ“
        self.execution_engine = TestExecutionEngine(
            max_workers=self.config.get("max_workers", 10)
        )

        # æµ‹è¯•ç”¨ä¾‹å’Œå¥—ä»¶å­˜å‚¨
        self.test_cases: Dict[str, TestCase] = {}
        self.test_suites: Dict[str, TestSuite] = {}

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 0,
            "skipped_tests": 0,
            "total_duration": 0,
            "average_duration": 0,
            "last_execution_time": None,
        }

        # æ—¥å¿—é…ç½®
        self.logger = logging.getLogger(__name__)
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        )

    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except FileNotFoundError:
            return self._get_default_config()

    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "max_workers": 10,
            "timeout": 300,
            "retry_count": 3,
            "parallel_execution": True,
            "enable_performance_monitoring": True,
            "enable_compliance_reporting": True,
            "artifact_storage_path": "/tmp/test_artifacts",
        }

    def register_test_case(self, test_case: TestCase):
        """æ³¨å†Œæµ‹è¯•ç”¨ä¾‹"""
        self.test_cases[test_case.id] = test_case
        self.stats["total_tests"] += 1
        print(f"âœ“ æ³¨å†Œæµ‹è¯•ç”¨ä¾‹: {test_case.name} ({test_case.id})")

    def register_test_suite(self, test_suite: TestSuite):
        """æ³¨å†Œæµ‹è¯•å¥—ä»¶"""
        self.test_suites[test_suite.id] = test_suite
        print(f"âœ“ æ³¨å†Œæµ‹è¯•å¥—ä»¶: {test_suite.name} ({test_suite.id})")

    async def run_test_by_id(
        self, test_id: str, retry_count: int = 0
    ) -> TestExecutionResult:
        """è¿è¡ŒæŒ‡å®šIDçš„æµ‹è¯•"""
        if test_id not in self.test_cases:
            raise ValueError(f"æµ‹è¯•ç”¨ä¾‹ä¸å­˜åœ¨: {test_id}")

        test_case = self.test_cases[test_id]

        # æ‰§è¡Œæµ‹è¯•
        result = await self.execution_engine._execute_single_test(test_case)

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        if result.status == TestStatus.PASSED:
            self.stats["passed_tests"] += 1
        elif result.status == TestStatus.FAILED:
            self.stats["failed_tests"] += 1
        elif result.status == TestStatus.SKIPPED:
            self.stats["skipped_tests"] += 1

        return result

    async def run_test_suite(
        self, suite_id: str, execution_mode: str = "adaptive"
    ) -> List[TestExecutionResult]:
        """è¿è¡Œæµ‹è¯•å¥—ä»¶"""
        if suite_id not in self.test_suites:
            raise ValueError(f"æµ‹è¯•å¥—ä»¶ä¸å­˜åœ¨: {suite_id}")

        suite = self.test_suites[suite_id]
        test_cases = [
            self.test_cases[case_id]
            for case_id in suite.test_cases
            if case_id in self.test_cases
        ]

        print(f"\nğŸš€ è¿è¡Œæµ‹è¯•å¥—ä»¶: {suite.name}")
        print(f"ğŸ“Š æµ‹è¯•æ•°é‡: {len(test_cases)}")
        print(f"âš¡ æ‰§è¡Œæ¨¡å¼: {execution_mode}")

        start_time = datetime.now()

        # æ ¹æ®æ‰§è¡Œæ¨¡å¼è¿è¡Œæµ‹è¯•
        if execution_mode == "sequential":
            results = await self.execution_engine.execute_tests_sequentially(test_cases)
        elif execution_mode == "parallel":
            results = await self.execution_engine.execute_tests_parallelly(test_cases)
        else:  # adaptive
            # è‡ªé€‚åº”æ‰§è¡Œï¼šæ ¹æ®æµ‹è¯•ç±»å‹å†³å®šæ‰§è¡Œç­–ç•¥
            results = await self._execute_tests_adaptively(test_cases)

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        end_time = datetime.now()
        suite_duration = (end_time - start_time).total_seconds()
        self.stats["last_execution_time"] = end_time

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r.status == TestStatus.PASSED)
        failed_tests = sum(1 for r in results if r.status == TestStatus.FAILED)

        print("\nâœ… æµ‹è¯•å¥—ä»¶æ‰§è¡Œå®Œæˆ")
        print(f"ğŸ“ˆ æ€»è€—æ—¶: {suite_duration:.2f}ç§’")
        print(f"âœ… é€šè¿‡: {passed_tests}/{total_tests}")
        print(f"âŒ å¤±è´¥: {failed_tests}/{total_tests}")
        print(f"ğŸ“Š æˆåŠŸç‡: {passed_tests / total_tests * 100:.1f}%")

        # ç”Ÿæˆå¥—ä»¶ç»“æœæŠ¥å‘Š
        suite_result = self._generate_suite_report(suite, results, suite_duration)

        return results

    async def _execute_tests_adaptively(
        self, test_cases: List[TestCase]
    ) -> List[TestExecutionResult]:
        """è‡ªé€‚åº”æ‰§è¡Œæµ‹è¯•"""
        # æŒ‰æµ‹è¯•ç±»å‹åˆ†ç»„
        by_type = {}
        for test_case in test_cases:
            test_type = test_case.test_type
            if test_type not in by_type:
                by_type[test_type] = []
            by_type[test_type].append(test_case)

        results = []

        # æ ¹æ®æµ‹è¯•ç±»å‹é€‰æ‹©æ‰§è¡Œç­–ç•¥
        for test_type, cases in by_type.items():
            if test_type in [
                TestType.PERFORMANCE,
                TestType.SECURITY,
                TestType.COMPLIANCE,
            ]:
                # æ€§èƒ½ã€å®‰å…¨ã€åˆè§„æµ‹è¯•ï¼šé¡ºåºæ‰§è¡Œ
                case_results = await self.execution_engine.execute_tests_sequentially(
                    cases
                )
            else:
                # å…¶ä»–æµ‹è¯•ï¼šå¹¶è¡Œæ‰§è¡Œ
                case_results = await self.execution_engine.execute_tests_parallelly(
                    cases
                )
            results.extend(case_results)

        return results

    def _generate_suite_report(
        self, suite: TestSuite, results: List[TestExecutionResult], duration: float
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå¥—ä»¶æŠ¥å‘Š"""
        passed_count = sum(1 for r in results if r.status == TestStatus.PASSED)
        failed_count = sum(1 for r in results if r.status == TestStatus.FAILED)
        skipped_count = sum(1 for r in results if r.status == TestStatus.SKIPPED)

        by_type = {}
        for result in results:
            test_type = result.test_type.value
            if test_type not in by_type:
                by_type[test_type] = {"passed": 0, "failed": 0, "total": 0}
            by_type[test_type]["total"] += 1
            if result.status == TestStatus.PASSED:
                by_type[test_type]["passed"] += 1
            elif result.status == TestStatus.FAILED:
                by_type[test_type]["failed"] += 1

        # è®¡ç®—å¹³å‡æ‰§è¡Œæ—¶é—´
        valid_durations = [r.duration for r in results if r.duration is not None]
        avg_duration = mean(valid_durations) if valid_durations else 0

        report = {
            "suite_id": suite.id,
            "suite_name": suite.name,
            "execution_time": datetime.now().isoformat(),
            "duration_seconds": round(duration, 2),
            "summary": {
                "total_tests": len(results),
                "passed": passed_count,
                "failed": failed_count,
                "skipped": skipped_count,
                "success_rate": round(passed_count / len(results) * 100, 1)
                if results
                else 0,
            },
            "by_type": by_type,
            "performance": {
                "average_duration_seconds": round(avg_duration, 2),
                "longest_test": max(valid_durations) if valid_durations else 0,
                "shortest_test": min(valid_durations) if valid_durations else 0,
            },
            "failed_tests": [
                {"id": r.test_id, "name": r.test_name, "error": r.error_message}
                for r in results
                if r.status == TestStatus.FAILED
            ],
        }

        # ä¿å­˜æŠ¥å‘Š
        report_path = f"/tmp/suite_report_{suite.id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“„ å¥—ä»¶æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report

    async def run_comprehensive_test_session(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆæµ‹è¯•ä¼šè¯"""
        print("\nğŸ¯ å¼€å§‹ç»¼åˆæµ‹è¯•ä¼šè¯")
        print(f"â° å¼€å§‹æ—¶é—´: {datetime.now()}")
        print(f"ğŸ“Š å·²æ³¨å†Œæµ‹è¯•ç”¨ä¾‹: {len(self.test_cases)}")
        print(f"ğŸ“Š å·²æ³¨å†Œæµ‹è¯•å¥—ä»¶: {len(self.test_suites)}")

        session_start = datetime.now()
        session_results = {}

        # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•å¥—ä»¶
        for suite_id in self.test_suites:
            try:
                suite_results = await self.run_test_suite(
                    suite_id, execution_mode="adaptive"
                )
                session_results[suite_id] = suite_results
            except Exception as e:
                print(f"âŒ æµ‹è¯•å¥—ä»¶ {suite_id} æ‰§è¡Œå¤±è´¥: {str(e)}")
                session_results[suite_id] = {"error": str(e)}

        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        session_duration = (datetime.now() - session_start).total_seconds()
        comprehensive_report = self._generate_comprehensive_report(
            session_results, session_duration
        )

        print("\nğŸ‰ ç»¼åˆæµ‹è¯•ä¼šè¯å®Œæˆ")
        print(f"â±ï¸  æ€»è€—æ—¶: {session_duration:.2f}ç§’")
        print(f"ğŸ“Š æ‰§è¡Œäº† {len(session_results)} ä¸ªæµ‹è¯•å¥—ä»¶")

        return comprehensive_report

    def _generate_comprehensive_report(
        self, session_results: Dict, duration: float
    ) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        total_suites = len(session_results)
        successful_suites = sum(1 for r in session_results.values() if "error" not in r)

        total_tests = 0
        total_passed = 0
        total_failed = 0
        total_skipped = 0

        for suite_results in session_results.values():
            if isinstance(suite_results, list):
                total_tests += len(suite_results)
                total_passed += sum(
                    1 for r in suite_results if r.status == TestStatus.PASSED
                )
                total_failed += sum(
                    1 for r in suite_results if r.status == TestStatus.FAILED
                )
                total_skipped += sum(
                    1 for r in suite_results if r.status == TestStatus.SKIPPED
                )

        overall_success_rate = (
            (total_passed / total_tests * 100) if total_tests > 0 else 0
        )

        report = {
            "session_type": "comprehensive",
            "execution_time": datetime.now().isoformat(),
            "duration_seconds": round(duration, 2),
            "summary": {
                "total_suites": total_suites,
                "successful_suites": successful_suites,
                "failed_suites": total_suites - successful_suites,
                "total_tests": total_tests,
                "passed_tests": total_passed,
                "failed_tests": total_failed,
                "skipped_tests": total_skipped,
                "overall_success_rate": round(overall_success_rate, 1),
            },
            "suite_results": session_results,
            "statistics": self.stats,
            "recommendations": self._generate_recommendations(session_results),
        }

        # ä¿å­˜ç»¼åˆæŠ¥å‘Š
        report_path = f"/tmp/comprehensive_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“Š ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report

    def _generate_recommendations(self, session_results: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        # åˆ†æå¤±è´¥æ¨¡å¼
        failed_suites = [s for s, r in session_results.items() if "error" in r]
        if failed_suites:
            recommendations.append(f"æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•å¥—ä»¶: {', '.join(failed_suites)}")

        # åˆ†ææ€§èƒ½é—®é¢˜
        all_durations = []
        for suite_results in session_results.values():
            if isinstance(suite_results, list):
                all_durations.extend(
                    [r.duration for r in suite_results if r.duration is not None]
                )

        if all_durations:
            avg_duration = mean(all_durations)
            max_duration = max(all_durations)

            if avg_duration > 60:  # è¶…è¿‡1åˆ†é’Ÿ
                recommendations.append(
                    f"è€ƒè™‘ä¼˜åŒ–æµ‹è¯•æ€§èƒ½ï¼Œå¹³å‡æ‰§è¡Œæ—¶é—´ {avg_duration:.2f}ç§’"
                )

            if max_duration > 300:  # è¶…è¿‡5åˆ†é’Ÿ
                recommendations.append(
                    f"æœ‰æµ‹è¯•æ‰§è¡Œæ—¶é—´è¿‡é•¿ï¼ˆ{max_duration:.2f}ç§’ï¼‰ï¼Œéœ€è¦æ£€æŸ¥"
                )

        # åŸºäºæˆåŠŸç‡æä¾›å»ºè®®
        if session_results:
            success_rate = sum(
                1 for r in session_results.values() if "error" not in r
            ) / len(session_results)
            if success_rate < 0.8:
                recommendations.append("æµ‹è¯•æˆåŠŸç‡åä½ï¼Œå»ºè®®æ£€æŸ¥æµ‹è¯•ç¯å¢ƒé…ç½®")

        if not recommendations:
            recommendations.append("æµ‹è¯•æ‰§è¡Œæ­£å¸¸ï¼Œç»§ç»­ä¿æŒ")

        return recommendations

    async def get_test_status_dashboard(self) -> Dict[str, Any]:
        """è·å–æµ‹è¯•çŠ¶æ€ä»ªè¡¨ç›˜æ•°æ®"""
        # è®¡ç®—å„çŠ¶æ€æµ‹è¯•æ•°é‡
        status_counts = {status.value: 0 for status in TestStatus}

        for result in self.execution_engine.test_results.values():
            status_counts[result.status.value] += 1

        # ç”Ÿæˆä»ªè¡¨ç›˜æ•°æ®
        dashboard = {
            "timestamp": datetime.now().isoformat(),
            "total_test_cases": len(self.test_cases),
            "total_test_suites": len(self.test_suites),
            "execution_stats": self.stats,
            "status_counts": status_counts,
            "recent_executions": list(self.execution_engine.test_results.keys())[-10:],
            "performance_metrics": {
                "average_test_duration": mean(
                    [
                        r.duration
                        for r in self.execution_engine.test_results.values()
                        if r.duration
                    ]
                )
                or 0,
                "success_rate": (
                    self.stats["passed_tests"] / max(self.stats["total_tests"], 1)
                )
                * 100,
            },
        }

        return dashboard

    def export_test_configuration(self, output_path: str):
        """å¯¼å‡ºæµ‹è¯•é…ç½®"""
        config = {
            "test_cases": {
                test_id: {
                    "name": case.name,
                    "type": case.test_type.value,
                    "priority": case.priority,
                    "timeout": case.timeout,
                    "tags": case.tags,
                }
                for test_id, case in self.test_cases.items()
            },
            "test_suites": {
                suite_id: {
                    "name": suite.name,
                    "test_cases": suite.test_cases,
                    "execution_order": suite.execution_order,
                }
                for suite_id, suite in self.test_suites.items()
            },
            "config": self.config,
        }

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(config, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“ æµ‹è¯•é…ç½®å·²å¯¼å‡º: {output_path}")


# ç¤ºä¾‹ä½¿ç”¨
async def demo_comprehensive_manager():
    """æ¼”ç¤ºç»¼åˆæµ‹è¯•ç®¡ç†å™¨åŠŸèƒ½"""
    print("ğŸš€ æ¼”ç¤ºç»¼åˆæµ‹è¯•ç®¡ç†å™¨")

    # åˆ›å»ºç®¡ç†å™¨
    manager = ComprehensiveTestManager()

    # åˆ›å»ºä¸€äº›æµ‹è¯•ç”¨ä¾‹
    ai_test = TestCase(
        id="ai_test_001",
        name="AIæµ‹è¯•ç”Ÿæˆå™¨éªŒè¯",
        test_type=TestType.AI_ASSISTED,
        description="æµ‹è¯•AIè¾…åŠ©æµ‹è¯•ç”Ÿæˆå™¨åŠŸèƒ½",
        execute=lambda: manager.ai_generator.generate_test_cases("sample_function()"),
        tags=["ai", "validation"],
    )

    performance_test = TestCase(
        id="perf_test_001",
        name="æ€§èƒ½åŸºå‡†æµ‹è¯•",
        test_type=TestType.PERFORMANCE,
        description="æ‰§è¡ŒAPIæ€§èƒ½åŸºå‡†æµ‹è¯•",
        timeout=600,
        execute=lambda: manager.performance_suite.run_performance_benchmark(),
        tags=["performance", "api"],
    )

    security_test = TestCase(
        id="sec_test_001",
        name="å®‰å…¨æ¼æ´æ‰«æ",
        test_type=TestType.SECURITY,
        description="è¿è¡Œå®‰å…¨æ¼æ´æ‰«æ",
        timeout=300,
        execute=lambda: manager.security_scanner.run_comprehensive_security_scan(),
        tags=["security", "vulnerability"],
    )

    # æ³¨å†Œæµ‹è¯•ç”¨ä¾‹
    manager.register_test_case(ai_test)
    manager.register_test_case(performance_test)
    manager.register_test_case(security_test)

    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    comprehensive_suite = TestSuite(
        id="comprehensive_suite_001",
        name="ç»¼åˆæµ‹è¯•å¥—ä»¶",
        description="åŒ…å«æ‰€æœ‰æµ‹è¯•ç±»å‹çš„ç»¼åˆå¥—ä»¶",
        test_cases=[ai_test.id, performance_test.id, security_test.id],
        execution_order="adaptive",
        max_parallel=3,
    )

    manager.register_test_suite(comprehensive_suite)

    # è¿è¡Œæµ‹è¯•å¥—ä»¶
    results = await manager.run_test_suite("comprehensive_suite_001", "adaptive")

    # ç”Ÿæˆä»ªè¡¨ç›˜
    dashboard = await manager.get_test_status_dashboard()
    print("\nğŸ“Š æµ‹è¯•çŠ¶æ€ä»ªè¡¨ç›˜:")
    print(f"- æ€»æµ‹è¯•ç”¨ä¾‹: {dashboard['total_test_cases']}")
    print(f"- æ€»æµ‹è¯•å¥—ä»¶: {dashboard['total_test_suites']}")
    print(f"- æˆåŠŸç‡: {dashboard['performance_metrics']['success_rate']:.1f}%")

    # å¯¼å‡ºé…ç½®
    manager.export_test_configuration("/tmp/test_config_export.json")

    print("\nâœ… ç»¼åˆæµ‹è¯•ç®¡ç†å™¨æ¼”ç¤ºå®Œæˆ")


if __name__ == "__main__":
    asyncio.run(demo_comprehensive_manager())
