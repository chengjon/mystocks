#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MyStocks æµ‹è¯•æ€§èƒ½é›†æˆç³»ç»Ÿ

æä¾›å®Œæ•´çš„æµ‹è¯•æ€§èƒ½ä¼˜åŒ–è§£å†³æ–¹æ¡ˆï¼Œé›†æˆæ‰€æœ‰æ€§èƒ½ç›¸å…³ç»„ä»¶ã€‚
"""

import asyncio
import json
import statistics
import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional

import pandas as pd
import psutil


@dataclass
class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†"""

    name: str
    description: str
    category: str
    baseline_metrics: Dict[str, float]
    current_metrics: Dict[str, float]
    improvement_targets: Dict[str, float]
    status: str = "pending"  # pending/in_progress/completed/failed
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None


@dataclass
class OptimizationProfile:
    """ä¼˜åŒ–é…ç½®æ–‡ä»¶"""

    name: str
    description: str
    enabled_optimizations: List[str]
    custom_thresholds: Dict[str, float]
    scheduling_config: Dict[str, Any]
    resource_limits: Dict[str, Any]
    priority_rules: List[str]


class PerformanceIntegrationSystem:
    """æ€§èƒ½é›†æˆç³»ç»Ÿä¸»ç±»"""

    def __init__(self, config_path: Optional[str] = None):
        self.config_path = config_path
        self.config = self._load_config()

        # åˆå§‹åŒ–ç»„ä»¶
        self.benchmark_registry: Dict[str, PerformanceBenchmark] = {}
        self.optimization_profiles: Dict[str, OptimizationProfile] = {}
        self.performance_history: List[Dict[str, Any]] = []

        # æ ¸å¿ƒç»„ä»¶
        self.optimizer = None
        self.monitor = None
        self.analyzer = None
        self.integration_manager = None

        # çŠ¶æ€ç®¡ç†
        self.is_running = False
        self.current_session_id: Optional[str] = None

    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®"""
        default_config = {
            "monitoring": {
                "check_interval": 1.0,
                "history_size": 1000,
                "alert_thresholds": {
                    "cpu": {"warning": 70, "error": 85, "critical": 95},
                    "memory": {"warning": 70, "error": 85, "critical": 95},
                    "test_execution": {"warning": 5, "error": 10, "critical": 30},
                },
            },
            "optimization": {
                "auto_optimize": True,
                "max_concurrent_optimizations": 3,
                "optimization_timeout": 300,
            },
            "reporting": {
                "auto_generate_reports": True,
                "report_interval": 3600,
                "report_format": "html",
            },
            "integration": {
                "enable_chaos_testing": False,
                "enable_ai_analysis": True,
                "enable_real_time_optimization": True,
            },
        }

        if self.config_path and Path(self.config_path).exists():
            with open(self.config_path, "r", encoding="utf-8") as f:
                user_config = json.load(f)
                # åˆå¹¶é…ç½®
                for key, value in user_config.items():
                    if key in default_config:
                        if isinstance(value, dict) and isinstance(default_config[key], dict):
                            default_config[key].update(value)
                        else:
                            default_config[key] = value
                    else:
                        default_config[key] = value

        return default_config

    def initialize_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        from .test_advanced_performance_monitor import DynamicPerformanceOptimizer
        from .test_performance_optimizer import PerformanceOptimizer

        # åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–å™¨
        self.optimizer = PerformanceOptimizer()

        # åˆå§‹åŒ–åŠ¨æ€æ€§èƒ½ä¼˜åŒ–å™¨
        self.integration_manager = DynamicPerformanceOptimizer()
        self.integration_manager.start_system_monitoring()

        # åˆ›å»ºé»˜è®¤ä¼˜åŒ–é…ç½®æ–‡ä»¶
        self._create_default_profiles()

    def _create_default_profiles(self):
        """åˆ›å»ºé»˜è®¤ä¼˜åŒ–é…ç½®æ–‡ä»¶"""
        profiles = [
            OptimizationProfile(
                name="production",
                description="ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–é…ç½®",
                enabled_optimizations=[
                    "parallel_execution",
                    "memory_optimization",
                    "caching",
                    "database_optimization",
                ],
                custom_thresholds={
                    "cpu_warning": 60,
                    "memory_warning": 65,
                    "test_execution_warning": 3,
                },
                scheduling_config={
                    "optimization_schedule": "off_hours",
                    "window_start": "22:00",
                    "window_end": "06:00",
                },
                resource_limits={"max_memory_mb": 2048, "max_cpu_percent": 80},
                priority_rules=[
                    "stability_first",
                    "performance_second",
                    "resource_efficiency",
                ],
            ),
            OptimizationProfile(
                name="development",
                description="å¼€å‘ç¯å¢ƒä¼˜åŒ–é…ç½®",
                enabled_optimizations=[
                    "fast_feedback",
                    "memory_optimization",
                    "code_analysis",
                ],
                custom_thresholds={
                    "cpu_warning": 80,
                    "memory_warning": 85,
                    "test_execution_warning": 10,
                },
                scheduling_config={
                    "optimization_schedule": "continuous",
                    "immediate_feedback": True,
                },
                resource_limits={"max_memory_mb": 4096, "max_cpu_percent": 90},
                priority_rules=[
                    "fast_feedback",
                    "developer_experience",
                    "resource_efficiency",
                ],
            ),
            OptimizationProfile(
                name="testing",
                description="æµ‹è¯•ç¯å¢ƒä¼˜åŒ–é…ç½®",
                enabled_optimizations=[
                    "parallel_execution",
                    "concurrency_optimization",
                    "test_data_optimization",
                ],
                custom_thresholds={
                    "cpu_warning": 75,
                    "memory_warning": 80,
                    "test_execution_warning": 5,
                },
                scheduling_config={"optimization_schedule": "batch", "batch_size": 10},
                resource_limits={"max_memory_mb": 3072, "max_cpu_percent": 85},
                priority_rules=["test_speed", "reliability", "resource_efficiency"],
            ),
        ]

        for profile in profiles:
            self.optimization_profiles[profile.name] = profile

    async def run_performance_benchmark(self, benchmark_name: str, test_functions: List[Callable]) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print(f"\nğŸš€ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•: {benchmark_name}")

        session_id = f"benchmark_{int(time.time())}"
        self.current_session_id = session_id

        # åˆ›å»ºåŸºå‡†è®°å½•
        benchmark = PerformanceBenchmark(
            name=benchmark_name,
            description=f"æ€§èƒ½åŸºå‡†æµ‹è¯•: {benchmark_name}",
            category="comprehensive",
            baseline_metrics={},
            current_metrics={},
            improvement_targets={},
            status="in_progress",
            started_at=datetime.now(),
        )

        try:
            # è¿è¡ŒåŸºå‡†æµ‹è¯•
            baseline_results = await self._run_baseline_tests(test_functions)
            benchmark.baseline_metrics = baseline_results

            # åº”ç”¨ä¼˜åŒ–
            optimized_results = await self._apply_optimizations(test_functions)
            benchmark.current_metrics = optimized_results

            # è®¡ç®—æ”¹è¿›
            improvement = self._calculate_improvement(baseline_results, optimized_results)
            benchmark.improvement_targets = improvement

            benchmark.status = "completed"
            benchmark.completed_at = datetime.now()

        except Exception as e:
            benchmark.status = "failed"
            benchmark.completed_at = datetime.now()
            print(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")

        self.benchmark_registry[session_id] = benchmark
        self.performance_history.append(
            {
                "session_id": session_id,
                "benchmark": benchmark,
                "timestamp": datetime.now().isoformat(),
            }
        )

        # ç”ŸæˆæŠ¥å‘Š
        report = self._generate_benchmark_report(benchmark)

        print("âœ… åŸºå‡†æµ‹è¯•å®Œæˆ")
        print(f"ğŸ“Š æ”¹è¿›æ•ˆæœ: {improved}")

        return {
            "session_id": session_id,
            "benchmark": benchmark,
            "report": report,
            "improvement": improvement,
        }

    async def _run_baseline_tests(self, test_functions: List[Callable]) -> Dict[str, float]:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        results = {}

        for test_func in test_functions:
            test_name = test_func.__name__

            try:
                import time

                start_time = time.time()
                await test_func()
                execution_time = time.time() - start_time

                process = psutil.Process()
                memory_info = process.memory_info()
                cpu_percent = process.cpu_percent()

                results[test_name] = {
                    "execution_time": execution_time,
                    "memory_usage_mb": memory_info.rss / 1024 / 1024,
                    "cpu_usage_percent": cpu_percent,
                }

            except Exception as e:
                print(f"âŒ æµ‹è¯• {test_name} å¤±è´¥: {e}")
                results[test_name] = {
                    "execution_time": float("inf"),
                    "memory_usage_mb": float("inf"),
                    "cpu_usage_percent": 100.0,
                }

        return results

    async def _apply_optimizations(self, test_functions: List[Callable]) -> Dict[str, float]:
        """åº”ç”¨ä¼˜åŒ–"""
        results = {}

        for test_func in test_functions:
            test_name = test_func.__name__

            try:
                # ä½¿ç”¨ä¼˜åŒ–å™¨æ‰§è¡Œæµ‹è¯•
                optimization_result = await self.optimizer.optimize_test_performance(test_name, test_func)

                results[test_name] = {
                    "execution_time": optimization_result["final_performance"]["execution_time"],
                    "memory_usage_mb": optimization_result["final_performance"]["memory_usage_mb"],
                    "cpu_usage_percent": optimization_result["final_performance"]["cpu_usage_percent"],
                }

            except Exception as e:
                print(f"âŒ ä¼˜åŒ–æµ‹è¯• {test_name} å¤±è´¥: {e}")
                results[test_name] = {
                    "execution_time": float("inf"),
                    "memory_usage_mb": float("inf"),
                    "cpu_usage_percent": 100.0,
                }

        return results

    def _calculate_improvement(self, baseline: Dict[str, Dict], optimized: Dict[str, Dict]) -> Dict[str, float]:
        """è®¡ç®—æ”¹è¿›ç¨‹åº¦"""
        improvements = {}

        for test_name in baseline:
            if test_name in optimized:
                baseline_time = baseline[test_name]["execution_time"]
                optimized_time = optimized[test_name]["execution_time"]

                if baseline_time > 0:
                    time_improvement = (baseline_time - optimized_time) / baseline_time
                else:
                    time_improvement = 0.0

                improvements[test_name] = {
                    "time_improvement": time_improvement,
                    "execution_time_improvement_ms": baseline_time - optimized_time,
                    "status": "improved" if time_improvement > 0 else "degraded",
                }

        return improvements

    def _generate_benchmark_report(self, benchmark: PerformanceBenchmark) -> str:
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        report = "# æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š\n\n"
        report += f"**æµ‹è¯•åç§°**: {benchmark.name}\n"
        report += f"**æè¿°**: {benchmark.description}\n"
        report += f"**å¼€å§‹æ—¶é—´**: {benchmark.started_at}\n"
        report += f"**ç»“æŸæ—¶é—´**: {benchmark.completed_at}\n"
        report += f"**çŠ¶æ€**: {benchmark.status}\n\n"

        if benchmark.status == "completed":
            report += "## åŸºå‡†å¯¹æ¯”\n\n"
            report += "| æµ‹è¯•é¡¹ | åŸºå‡†å€¼ | ä¼˜åŒ–å | æ”¹è¿› | çŠ¶æ€ |\n"
            report += "|--------|--------|--------|------|------|\n"

            for test_name, metrics in benchmark.baseline_metrics.items():
                if test_name in benchmark.current_metrics:
                    baseline = metrics
                    optimized = benchmark.current_metrics[test_name]

                    time_improvement = (
                        (baseline["execution_time"] - optimized["execution_time"]) / baseline["execution_time"]
                        if baseline["execution_time"] > 0
                        else 0
                    )

                    status = "âœ… æ”¹è¿›" if time_improvement > 0 else "âŒ é€€åŒ–"
                    report += f"| {test_name} | {baseline['execution_time']:.2f}s | {optimized['execution_time']:.2f}s | {time_improvement * 100:.1f}% | {status} |\n"

            # æ€»ä½“æ”¹è¿›
            overall_improvement = self._calculate_overall_improvement(benchmark)
            report += "\n## æ€»ä½“è¯„ä¼°\n\n"
            report += f"**æ€»ä½“æ”¹è¿›åˆ†æ•°**: {overall_improvement:.2f}\n"
            report += f"**æ€§èƒ½æå‡**: {((overall_improvement - 0.5) * 200):+.1f}%\n\n"

            # å»ºè®®
            report += "## ä¼˜åŒ–å»ºè®®\n\n"
            if overall_improvement < 0.7:
                report += "- å»ºè®®å¯ç”¨æ›´å¤šä¼˜åŒ–ç­–ç•¥\n"
                report += "- è€ƒè™‘è°ƒæ•´ç³»ç»Ÿèµ„æºåˆ†é…\n"
                report += "- æ£€æŸ¥æ˜¯å¦å­˜åœ¨æœªè¯†åˆ«çš„æ€§èƒ½ç“¶é¢ˆ\n"
            else:
                report += "- æ€§èƒ½ä¼˜åŒ–æ•ˆæœè‰¯å¥½\n"
                report += "- å»ºè®®æŒç»­ç›‘æ§æ€§èƒ½æŒ‡æ ‡\n"
                report += "- å®šæœŸé‡æ–°è¿è¡ŒåŸºå‡†æµ‹è¯•\n"

        return report

    def _calculate_overall_improvement(self, benchmark: PerformanceBenchmark) -> float:
        """è®¡ç®—æ€»ä½“æ”¹è¿›åˆ†æ•°"""
        if not benchmark.baseline_metrics or not benchmark.current_metrics:
            return 0.0

        improvements = []
        for test_name in benchmark.baseline_metrics:
            if test_name in benchmark.current_metrics:
                baseline_time = benchmark.baseline_metrics[test_name]["execution_time"]
                optimized_time = benchmark.current_metrics[test_name]["execution_time"]

                if baseline_time > 0:
                    improvement = (baseline_time - optimized_time) / baseline_time
                    improvements.append(max(0, improvement))

        return statistics.mean(improvements) if improvements else 0.0

    async def run_continuous_optimization(self, duration_hours: int = 24):
        """è¿è¡ŒæŒç»­ä¼˜åŒ–"""
        print(f"ğŸ”„ å¼€å§‹æŒç»­ä¼˜åŒ–ï¼Œå°†æŒç»­ {duration_hours} å°æ—¶...")

        end_time = datetime.now() + timedelta(hours=duration_hours)
        optimization_sessions = []

        while datetime.now() < end_time:
            try:
                # è¿è¡Œæ€§èƒ½åˆ†æ
                analysis_results = self.integration_manager.run_performance_analysis(duration=60)

                # è·å–ä¼˜åŒ–å»ºè®®
                suggestions = self.analyzer.get_optimization_recommendations()

                # åº”ç”¨å…³é”®ä¼˜åŒ–
                if suggestions:
                    top_suggestions = sorted(suggestions, key=lambda x: x.priority)[:3]
                    for suggestion in top_suggestions:
                        print(f"ğŸ”§ åº”ç”¨ä¼˜åŒ–å»ºè®®: {suggestion.title}")
                        # è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„ä¼˜åŒ–é€»è¾‘

                # è®°å½•ä¼šè¯
                session = {
                    "timestamp": datetime.now().isoformat(),
                    "analysis_results": analysis_results,
                    "suggestions_applied": len(top_suggestions),
                    "system_health": analysis_results.get("performance_summary", {}).get("system_health", "unknown"),
                }
                optimization_sessions.append(session)

                # ç­‰å¾…ä¸‹ä¸€ä¸ªå‘¨æœŸ
                await asyncio.sleep(300)  # 5åˆ†é’Ÿé—´éš”

            except Exception as e:
                print(f"âŒ æŒç»­ä¼˜åŒ–é”™è¯¯: {e}")
                await asyncio.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿ

        # ç”ŸæˆæŒç»­ä¼˜åŒ–æŠ¥å‘Š
        report = self._generate_continuous_optimization_report(optimization_sessions)
        print("âœ… æŒç»­ä¼˜åŒ–å®Œæˆ")
        print(f"ğŸ“Š å®ŒæˆæŠ¥å‘Š: {report}")

        return report

    def _generate_continuous_optimization_report(self, sessions: List[Dict]) -> str:
        """ç”ŸæˆæŒç»­ä¼˜åŒ–æŠ¥å‘Š"""
        report = "# æŒç»­ä¼˜åŒ–æŠ¥å‘Š\n\n"
        report += f"å¼€å§‹æ—¶é—´: {sessions[0]['timestamp']}\n"
        report += f"ç»“æŸæ—¶é—´: {sessions[-1]['timestamp']}\n"
        report += f"ä¼˜åŒ–ä¼šè¯æ•°: {len(sessions)}\n\n"

        # ç»Ÿè®¡åˆ†æ
        health_improvements = []
        suggestion_counts = []

        for session in sessions:
            health = session.get("system_health", "unknown")
            suggestions = session.get("suggestions_applied", 0)

            if health == "excellent":
                health_improvements.append(1.0)
            elif health == "good":
                health_improvements.append(0.8)
            elif health == "warning":
                health_improvements.append(0.5)
            else:
                health_improvements.append(0.0)

            suggestion_counts.append(suggestions)

        report += "## ä¼˜åŒ–ç»Ÿè®¡\n\n"
        report += f"- å¹³å‡å¥åº·åˆ†æ•°: {statistics.mean(health_improvements):.2f}\n"
        report += f"- æ€»åº”ç”¨å»ºè®®æ•°: {sum(suggestion_counts)}\n"
        report += f"- å¹³å‡æ¯ä¼šè¯å»ºè®®æ•°: {statistics.mean(suggestion_counts):.1f}\n\n"

        # å¥åº·è¶‹åŠ¿
        if len(health_improvements) > 1:
            trend = "improving" if health_improvements[-1] > health_improvements[0] else "stable"
            report += f"## å¥åº·è¶‹åŠ¿: {trend}\n\n"

        # å»ºè®®
        report += "## å»ºè®®\n\n"
        if statistics.mean(health_improvements) < 0.7:
            report += "- å»ºè®®å¢åŠ ä¼˜åŒ–é¢‘ç‡\n"
            report += "- æ£€æŸ¥ç³»ç»Ÿé…ç½®å’Œèµ„æºåˆ†é…\n"
            report += "- è€ƒè™‘å¯ç”¨æ›´å¤šä¼˜åŒ–ç­–ç•¥\n"
        else:
            report += "- æŒç»­ä¼˜åŒ–è¿è¡Œè‰¯å¥½\n"
            report += "- å»ºè®®ç»´æŒå½“å‰ä¼˜åŒ–ç­–ç•¥\n"
            report += "- å®šæœŸè¯„ä¼°ä¼˜åŒ–æ•ˆæœ\n"

        return report

    def export_performance_data(self, output_path: str, format: str = "json"):
        """å¯¼å‡ºæ€§èƒ½æ•°æ®"""
        export_data = {
            "config": self.config,
            "benchmark_registry": self.benchmark_registry,
            "optimization_profiles": self.optimization_profiles,
            "performance_history": self.performance_history,
            "export_timestamp": datetime.now().isoformat(),
        }

        if format == "json":
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False, default=str)
        elif format == "csv":
            # è½¬æ¢ä¸ºCSVæ ¼å¼
            df = pd.DataFrame(
                [
                    {
                        "timestamp": h["timestamp"],
                        "benchmark_name": h["benchmark"].name,
                        "status": h["benchmark"].status,
                        "baseline_time": h["benchmark"]
                        .baseline_metrics.get("test_execution", {})
                        .get("execution_time", 0),
                        "optimized_time": h["benchmark"]
                        .current_metrics.get("test_execution", {})
                        .get("execution_time", 0),
                    }
                    for h in self.performance_history
                ]
            )
            df.to_csv(output_path, index=False)

        print(f"âœ… æ€§èƒ½æ•°æ®å·²å¯¼å‡ºåˆ°: {output_path}")

    def get_system_health_summary(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿå¥åº·æ‘˜è¦"""
        if self.integration_manager:
            return self.integration_manager.monitor.get_performance_summary()
        return {"status": "unknown", "timestamp": datetime.now().isoformat()}

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.integration_manager:
            self.integration_manager.stop_system_monitoring()

        print("ğŸ§¹ æ€§èƒ½é›†æˆç³»ç»Ÿå·²æ¸…ç†")


# ä½¿ç”¨ç¤ºä¾‹
async def demo_performance_integration():
    """æ¼”ç¤ºæ€§èƒ½é›†æˆç³»ç»ŸåŠŸèƒ½"""
    print("ğŸš€ æ¼”ç¤ºæ€§èƒ½é›†æˆç³»ç»ŸåŠŸèƒ½")

    # åˆ›å»ºé›†æˆç³»ç»Ÿ
    system = PerformanceIntegrationSystem()
    system.initialize_components()

    # æ¨¡æ‹Ÿæµ‹è¯•å‡½æ•°
    async def test_database_operations():
        await asyncio.sleep(1)
        # æ¨¡æ‹Ÿæ•°æ®åº“æ“ä½œ
        return "database_complete"

    async def test_api_calls():
        await asyncio.sleep(2)
        # æ¨¡æ‹ŸAPIè°ƒç”¨
        return "api_complete"

    async def test_file_operations():
        await asyncio.sleep(0.5)
        # æ¨¡æ‹Ÿæ–‡ä»¶æ“ä½œ
        return "file_complete"

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    test_functions = [test_database_operations, test_api_calls, test_file_operations]
    benchmark_result = await system.run_performance_benchmark("comprehensive_performance_test", test_functions)

    print(f"ğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ: {benchmark_result}")

    # è¿è¡ŒæŒç»­ä¼˜åŒ–ï¼ˆçŸ­æ—¶é—´æ¼”ç¤ºï¼‰
    continuous_report = await system.run_continuous_optimization(duration_hours=0.1)  # 6åˆ†é’Ÿ

    # è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€
    health_summary = system.get_system_health_summary()
    print(f"ğŸ¥ ç³»ç»Ÿå¥åº·çŠ¶æ€: {health_summary}")

    # æ¸…ç†
    system.cleanup()


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(demo_performance_integration())
