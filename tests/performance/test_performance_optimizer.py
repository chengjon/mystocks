#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MyStocks æµ‹è¯•æ€§èƒ½ä¼˜åŒ–å™¨

æä¾›æµ‹è¯•æ€§èƒ½åˆ†æã€ä¼˜åŒ–ç­–ç•¥å’Œæ”¹è¿›å»ºè®®åŠŸèƒ½ã€‚
"""

import asyncio
import time
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any, Callable, Dict, List, Optional

import psutil


class OptimizationType(Enum):
    """ä¼˜åŒ–ç±»å‹æšä¸¾"""

    SPEED = "speed"
    MEMORY = "memory"
    CONCURRENCY = "concurrency"
    CACHE = "cache"
    DATABASE = "database"
    NETWORK = "network"
    CODE = "code"


@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ ‡"""

    name: str
    category: str
    value: float
    unit: str
    threshold: float
    improvement_target: float
    status: str = "good"  # good/warning/critical
    trend: str = "stable"  # improving/stable/deteriorating


@dataclass
class TestExecution:
    """æµ‹è¯•æ‰§è¡Œè®°å½•"""

    test_name: str
    execution_time: float
    memory_usage_mb: float
    cpu_usage_percent: float
    timestamp: datetime
    status: str
    error: Optional[str] = None
    optimization_score: float = 0.0


@dataclass
class OptimizationStrategy:
    """ä¼˜åŒ–ç­–ç•¥"""

    name: str
    type: OptimizationType
    description: str
    impact_score: float  # 0-1
    complexity_score: float  # 0-1
    estimated_improvement: float
    implementation_cost: str
    priority: int
    test_impact: bool
    implementation_steps: List[str]


class PerformanceAnalyzer:
    """æ€§èƒ½åˆ†æå™¨"""

    def __init__(self):
        self.metrics_history: Dict[str, List[PerformanceMetric]] = {}
        self.test_executions: List[TestExecution] = []
        self.bottlenecks: List[str] = []
        self.optimization_opportunities: List[Dict[str, Any]] = []

    def analyze_test_execution(self, execution: TestExecution) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•æ‰§è¡Œæ€§èƒ½"""
        analysis = {
            "performance_score": self._calculate_performance_score(execution),
            "bottlenecks": self._identify_bottlenecks(execution),
            "recommendations": self._generate_recommendations(execution),
            "optimization_potential": self._assess_optimization_potential(execution),
        }

        # è®°å½•åˆ°å†å²æ•°æ®
        self.test_executions.append(execution)
        self._update_metrics_history(execution)

        return analysis

    def _calculate_performance_score(self, execution: TestExecution) -> float:
        """è®¡ç®—æ€§èƒ½åˆ†æ•°"""
        score = 100.0

        # æ‰§è¡Œæ—¶é—´å½±å“
        if execution.execution_time > 10.0:
            score -= min(30.0, execution.execution_time - 10.0)

        # å†…å­˜ä½¿ç”¨å½±å“
        if execution.memory_usage_mb > 1000:  # 1GB
            score -= min(20.0, (execution.memory_usage_mb - 1000) / 50)

        # CPUä½¿ç”¨å½±å“
        if execution.cpu_usage_percent > 80:
            score -= min(20.0, (execution.cpu_usage_percent - 80) / 5)

        # é”™è¯¯å½±å“
        if execution.status != "passed":
            score -= 30.0

        return max(0.0, score)

    def _identify_bottlenecks(self, execution: TestExecution) -> List[str]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []

        # æ—¶é—´ç“¶é¢ˆ
        if execution.execution_time > 5.0:
            bottlenecks.append(f"æµ‹è¯•æ‰§è¡Œæ—¶é—´è¿‡é•¿ ({execution.execution_time:.2f}s)")

        # å†…å­˜ç“¶é¢ˆ
        if execution.memory_usage_mb > 500:
            bottlenecks.append(f"å†…å­˜ä½¿ç”¨è¿‡é«˜ ({execution.memory_usage_mb:.2f}MB)")

        # CPUç“¶é¢ˆ
        if execution.cpu_usage_percent > 70:
            bottlenecks.append(f"CPUä½¿ç”¨ç‡é«˜ ({execution.cpu_usage_percent:.2f}%)")

        # ç½‘ç»œç“¶é¢ˆï¼ˆå¦‚æœé€‚ç”¨ï¼‰
        if hasattr(execution, "network_latency") and execution.network_latency > 1.0:
            bottlenecks.append(f"ç½‘ç»œå»¶è¿Ÿé«˜ ({execution.network_latency:.2f}s)")

        return bottlenecks

    def _generate_recommendations(self, execution: TestExecution) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        if execution.execution_time > 10.0:
            recommendations.append("è€ƒè™‘ä½¿ç”¨å¹¶è¡Œæµ‹è¯•æ‰§è¡Œå‡å°‘æ€»æ—¶é—´")

        if execution.memory_usage_mb > 1000:
            recommendations.append("ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œè€ƒè™‘æ•°æ®æ¸…ç†å’Œæ‡’åŠ è½½")

        if execution.cpu_usage_percent > 80:
            recommendations.append("ä¼˜åŒ–CPUå¯†é›†å‹æ“ä½œï¼Œè€ƒè™‘å¼‚æ­¥å¤„ç†")

        if execution.status != "passed":
            recommendations.append("ä¿®å¤æµ‹è¯•å¤±è´¥é—®é¢˜ï¼Œæé«˜æµ‹è¯•ç¨³å®šæ€§")

        return recommendations

    def _assess_optimization_potential(self, execution: TestExecution) -> float:
        """è¯„ä¼°ä¼˜åŒ–æ½œåŠ›"""
        potential = 0.0

        if execution.execution_time > 5.0:
            potential += 0.4

        if execution.memory_usage_mb > 500:
            potential += 0.3

        if execution.cpu_usage_percent > 70:
            potential += 0.3

        return min(1.0, potential)

    def _update_metrics_history(self, execution: TestExecution):
        """æ›´æ–°æŒ‡æ ‡å†å²"""
        metrics = [
            PerformanceMetric(
                name=f"execution_time_{execution.test_name}",
                category="speed",
                value=execution.execution_time,
                unit="s",
                threshold=5.0,
                improvement_target=1.0,
            ),
            PerformanceMetric(
                name=f"memory_usage_{execution.test_name}",
                category="memory",
                value=execution.memory_usage_mb,
                unit="MB",
                threshold=500.0,
                improvement_target=200.0,
            ),
            PerformanceMetric(
                name=f"cpu_usage_{execution.test_name}",
                category="cpu",
                value=execution.cpu_usage_percent,
                unit="%",
                threshold=70.0,
                improvement_target=30.0,
            ),
        ]

        for metric in metrics:
            if metric.name not in self.metrics_history:
                self.metrics_history[metric.name] = []
            self.metrics_history[metric.name].append(metric)

    def analyze_historical_trends(self) -> Dict[str, Any]:
        """åˆ†æå†å²è¶‹åŠ¿"""
        trends = {}

        for metric_name, history in self.metrics_history.items():
            if len(history) >= 3:
                recent_values = [m.value for m in history[-3:]]
                trend = self._calculate_trend(recent_values)
                trends[metric_name] = {
                    "current_value": recent_values[-1],
                    "trend": trend,
                    "improvement_needed": self._assess_improvement_needed(history),
                }

        return trends

    def _calculate_trend(self, values: List[float]) -> str:
        """è®¡ç®—è¶‹åŠ¿"""
        if len(values) < 2:
            return "stable"

        if values[-1] > values[0] * 1.1:
            return "deteriorating"
        elif values[-1] < values[0] * 0.9:
            return "improving"
        else:
            return "stable"

    def _assess_improvement_needed(self, history: List[PerformanceMetric]) -> bool:
        """è¯„ä¼°æ˜¯å¦éœ€è¦æ”¹è¿›"""
        if not history:
            return False

        latest = history[-1]
        return latest.value > latest.threshold


class PerformanceOptimizer:
    """æ€§èƒ½ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.strategies: List[OptimizationStrategy] = self._initialize_optimization_strategies()
        self.analyzer = PerformanceAnalyzer()
        self.optimization_history: List[Dict[str, Any]] = []

    def _initialize_optimization_strategies(self) -> List[OptimizationStrategy]:
        """åˆå§‹åŒ–ä¼˜åŒ–ç­–ç•¥"""
        return [
            OptimizationStrategy(
                name="å¹¶è¡Œæµ‹è¯•æ‰§è¡Œ",
                type=OptimizationType.CONCURRENCY,
                description="ä½¿ç”¨pytest-xdistå¹¶è¡Œè¿è¡Œæµ‹è¯•ï¼Œå……åˆ†åˆ©ç”¨å¤šæ ¸CPU",
                impact_score=0.8,
                complexity_score=0.3,
                estimated_improvement=0.6,
                implementation_cost="low",
                priority=1,
                test_impact=True,
                implementation_steps=[
                    "å®‰è£…pytest-xdist: pip install pytest-xdist",
                    "è¿è¡Œ: pytest -n auto",
                    "é…ç½®å¹¶è¡Œæ•°é‡: pytest -n 4",
                ],
            ),
            OptimizationStrategy(
                name="æµ‹è¯•æ•°æ®ç¼“å­˜",
                type=OptimizationType.CACHE,
                description="ç¼“å­˜æµ‹è¯•æ•°æ®é¿å…é‡å¤åˆ›å»ºï¼Œå‡å°‘I/Oæ“ä½œ",
                impact_score=0.7,
                complexity_score=0.5,
                estimated_improvement=0.5,
                implementation_cost="medium",
                priority=2,
                test_impact=False,
                implementation_steps=[
                    "åˆ›å»ºæµ‹è¯•æ•°æ®ç¼“å­˜æœºåˆ¶",
                    "ä½¿ç”¨å†…å­˜ç¼“å­˜å¸¸ç”¨æ•°æ®",
                    "è®¾ç½®åˆç†çš„ç¼“å­˜è¿‡æœŸç­–ç•¥",
                ],
            ),
            OptimizationStrategy(
                name="æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–",
                type=OptimizationType.DATABASE,
                description="ä¼˜åŒ–æµ‹è¯•ä¸­çš„æ•°æ®åº“æŸ¥è¯¢ï¼Œæ·»åŠ ç´¢å¼•å’Œæ‰¹å¤„ç†",
                impact_score=0.9,
                complexity_score=0.7,
                estimated_improvement=0.7,
                implementation_cost="high",
                priority=3,
                test_impact=True,
                implementation_steps=[
                    "åˆ†ææ…¢æŸ¥è¯¢",
                    "æ·»åŠ åˆé€‚çš„ç´¢å¼•",
                    "ä½¿ç”¨æ‰¹é‡æ“ä½œæ›¿ä»£å•æ¡æ“ä½œ",
                ],
            ),
            OptimizationStrategy(
                name="å¼‚æ­¥æµ‹è¯•æ‰§è¡Œ",
                type=OptimizationType.SPEED,
                description="å°†åŒæ­¥æµ‹è¯•è½¬æ¢ä¸ºå¼‚æ­¥ï¼Œæé«˜å¹¶å‘æ€§èƒ½",
                impact_score=0.6,
                complexity_score=0.8,
                estimated_improvement=0.4,
                implementation_cost="high",
                priority=4,
                test_impact=True,
                implementation_steps=[
                    "ä½¿ç”¨pytest-asyncio",
                    "é‡æ„æµ‹è¯•ä¸ºå¼‚æ­¥æ¨¡å¼",
                    "ä¼˜åŒ–å¼‚æ­¥æµ‹è¯•çš„å¹¶å‘æ§åˆ¶",
                ],
            ),
            OptimizationStrategy(
                name="å†…å­˜ä½¿ç”¨ä¼˜åŒ–",
                type=OptimizationType.MEMORY,
                description="ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œå‡å°‘å†…å­˜æ³„æ¼å’Œè¿‡åº¦åˆ†é…",
                impact_score=0.5,
                complexity_score=0.6,
                estimated_improvement=0.3,
                implementation_cost="medium",
                priority=5,
                test_impact=False,
                implementation_steps=[
                    "ä½¿ç”¨å†…å­˜åˆ†æå·¥å…·æ£€æµ‹æ³„æ¼",
                    "åŠæ—¶é‡Šæ”¾å¤§å¯¹è±¡",
                    "ä½¿ç”¨ç”Ÿæˆå™¨è€Œéåˆ—è¡¨",
                ],
            ),
            OptimizationStrategy(
                name="ç½‘ç»œè¯·æ±‚ä¼˜åŒ–",
                type=OptimizationType.NETWORK,
                description="ä¼˜åŒ–æµ‹è¯•ä¸­çš„ç½‘ç»œè¯·æ±‚ï¼Œå‡å°‘å»¶è¿Ÿå’Œè¿æ¥æ•°",
                impact_score=0.7,
                complexity_score=0.4,
                estimated_improvement=0.5,
                implementation_cost="low",
                priority=2,
                test_impact=False,
                implementation_steps=[
                    "è¿æ¥æ± ç®¡ç†",
                    "æ‰¹é‡è¯·æ±‚æ›¿ä»£å•æ¡è¯·æ±‚",
                    "ä½¿ç”¨CDNå’Œç¼“å­˜",
                ],
            ),
        ]

    async def optimize_test_performance(self, test_name: str, test_function: Callable) -> Dict[str, Any]:
        """ä¼˜åŒ–æµ‹è¯•æ€§èƒ½"""
        print(f"\nğŸ”§ å¼€å§‹ä¼˜åŒ–æµ‹è¯•: {test_name}")

        # åŸºå‡†æµ‹è¯•
        baseline_result = await self._run_benchmark(test_name, test_function)
        print(f"ğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ: {baseline_result}")

        # åˆ†ææ€§èƒ½
        analysis = self.analyzer.analyze_test_execution(baseline_result)
        print(f"ğŸ” æ€§èƒ½åˆ†æ: {analysis}")

        # é€‰æ‹©ä¼˜åŒ–ç­–ç•¥
        strategies = self._select_optimization_strategies(analysis)
        print(f"ğŸ¯ é€‰ä¸­ {len(strategies)} ä¸ªä¼˜åŒ–ç­–ç•¥")

        # åº”ç”¨ä¼˜åŒ–
        optimization_results = []
        for strategy in strategies:
            print(f"\nğŸ“‹ åº”ç”¨ç­–ç•¥: {strategy.name}")
            result = await self._apply_optimization_strategy(test_name, test_function, strategy)
            optimization_results.append(result)

        # è¯„ä¼°ä¼˜åŒ–æ•ˆæœ
        final_result = await self._run_benchmark(test_name, test_function)
        improvement = self._calculate_improvement(baseline_result, final_result)

        optimization_report = {
            "test_name": test_name,
            "baseline_performance": self._execution_to_dict(baseline_result),
            "optimization_strategies": [self._strategy_to_dict(s) for s in strategies],
            "optimization_results": optimization_results,
            "final_performance": self._execution_to_dict(final_result),
            "improvement_metrics": improvement,
            "overall_improvement_score": self._calculate_overall_improvement(improvement),
            "optimization_timestamp": datetime.now().isoformat(),
        }

        self.optimization_history.append(optimization_report)
        print(f"\nâœ… ä¼˜åŒ–å®Œæˆï¼Œæ•´ä½“æ”¹è¿›åˆ†æ•°: {optimization_report['overall_improvement_score']:.2f}")

        return optimization_report

    async def _run_benchmark(self, test_name: str, test_function: Callable) -> TestExecution:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        process = psutil.Process()
        start_time = time.time()
        start_memory = process.memory_info().rss / 1024 / 1024
        start_cpu = psutil.cpu_percent()

        try:
            await test_function()
            status = "passed"
            error = None
        except Exception as e:
            status = "failed"
            error = str(e)

        end_time = time.time()
        end_memory = process.memory_info().rss / 1024 / 1024
        end_cpu = psutil.cpu_percent()

        return TestExecution(
            test_name=test_name,
            execution_time=end_time - start_time,
            memory_usage_mb=end_memory - start_memory,
            cpu_usage_percent=end_cpu,
            timestamp=datetime.now(),
            status=status,
            error=error,
        )

    def _select_optimization_strategies(self, analysis: Dict[str, Any]) -> List[OptimizationStrategy]:
        """é€‰æ‹©ä¼˜åŒ–ç­–ç•¥"""
        selected = []
        bottlenecks = analysis.get("bottlenecks", [])

        # æ ¹æ®ç“¶é¢ˆé€‰æ‹©ç­–ç•¥
        for bottleneck in bottlenecks:
            if "æ—¶é—´é•¿" in bottleneck:
                selected.extend(
                    s
                    for s in self.strategies
                    if s.type in [OptimizationType.SPEED, OptimizationType.CONCURRENCY] and s not in selected
                )

            if "å†…å­˜" in bottleneck:
                selected.extend(s for s in self.strategies if s.type == OptimizationType.MEMORY and s not in selected)

            if "CPU" in bottleneck:
                selected.extend(s for s in self.strategies if s.type == OptimizationType.CODE and s not in selected)

        # æŒ‰ä¼˜å…ˆçº§å’Œå½±å“æ’åº
        selected.sort(key=lambda x: (x.priority, -x.impact_score))

        return selected[:3]  # æœ€å¤šé€‰æ‹©3ä¸ªç­–ç•¥

    async def _apply_optimization_strategy(
        self, test_name: str, test_function: Callable, strategy: OptimizationStrategy
    ) -> Dict[str, Any]:
        """åº”ç”¨ä¼˜åŒ–ç­–ç•¥"""
        result = {
            "strategy_name": strategy.name,
            "type": strategy.type.value,
            "applied": False,
            "improvement": 0.0,
            "details": [],
        }

        try:
            # æ ¹æ®ç­–ç•¥ç±»å‹åº”ç”¨ä¼˜åŒ–
            if strategy.type == OptimizationType.CONCURRENCY:
                optimized_function = self._apply_parallel_execution(test_function)
            elif strategy.type == OptimizationType.CACHE:
                optimized_function = self._apply_caching(test_function)
            elif strategy.type == OptimizationType.MEMORY:
                optimized_function = self._apply_memory_optimization(test_function)
            else:
                optimized_function = test_function

            # æµ‹è¯•ä¼˜åŒ–æ•ˆæœ
            execution = await self._run_benchmark(f"{test_name}_{strategy.name}", optimized_function)
            improvement = execution.execution_time / (len(self.test_executions) + 1)

            result.update(
                {
                    "applied": True,
                    "execution_time": execution.execution_time,
                    "memory_usage_mb": execution.memory_usage_mb,
                    "cpu_usage_percent": execution.cpu_usage_percent,
                    "status": execution.status,
                    "improvement": improvement,
                    "details": strategy.implementation_steps,
                }
            )

        except Exception as e:
            result.update({"applied": False, "error": str(e), "details": ["ä¼˜åŒ–ç­–ç•¥åº”ç”¨å¤±è´¥"]})

        return result

    def _apply_parallel_execution(self, test_function: Callable) -> Callable:
        """åº”ç”¨å¹¶è¡Œæ‰§è¡Œä¼˜åŒ–"""

        async def parallel_function():
            with ThreadPoolExecutor(max_workers=4) as executor:
                await asyncio.gather(
                    *[asyncio.get_event_loop().run_in_executor(executor, test_function) for _ in range(4)]
                )

        return parallel_function

    def _apply_caching(self, test_function: Callable) -> Callable:
        """åº”ç”¨ç¼“å­˜ä¼˜åŒ–"""
        cache = {}

        async def cached_function():
            cache_key = str(id(test_function))
            if cache_key not in cache:
                cache[cache_key] = await test_function()
            return cache[cache_key]

        return cached_function

    def _apply_memory_optimization(self, test_function: Callable) -> Callable:
        """åº”ç”¨å†…å­˜ä¼˜åŒ–"""

        async def memory_optimized_function():
            # ä½¿ç”¨ç”Ÿæˆå™¨è€Œéåˆ—è¡¨
            for item in range(1000):
                yield item
            await test_function()

        return memory_optimized_function

    def _calculate_improvement(self, baseline: TestExecution, final: TestExecution) -> Dict[str, float]:
        """è®¡ç®—æ”¹è¿›ç¨‹åº¦"""
        time_improvement = (baseline.execution_time - final.execution_time) / baseline.execution_time
        memory_improvement = (baseline.memory_usage_mb - final.memory_usage_mb) / baseline.memory_usage_mb
        cpu_improvement = (baseline.cpu_usage_percent - final.cpu_usage_percent) / baseline.cpu_usage_percent

        return {
            "time_improvement": time_improvement,
            "memory_improvement": memory_improvement,
            "cpu_improvement": cpu_improvement,
            "overall_improvement": (time_improvement + memory_improvement + cpu_improvement) / 3,
        }

    def _calculate_overall_improvement(self, improvement: Dict[str, float]) -> float:
        """è®¡ç®—æ€»ä½“æ”¹è¿›åˆ†æ•°"""
        overall = improvement.get("overall_improvement", 0.0)
        time_imp = improvement.get("time_improvement", 0.0) * 0.4
        memory_imp = improvement.get("memory_improvement", 0.0) * 0.3
        cpu_imp = improvement.get("cpu_improvement", 0.0) * 0.3

        return max(0.0, min(1.0, overall + time_imp + memory_imp + cpu_imp))

    def _execution_to_dict(self, execution: TestExecution) -> Dict[str, Any]:
        """å°†æ‰§è¡Œç»“æœè½¬æ¢ä¸ºå­—å…¸"""
        return {
            "execution_time": execution.execution_time,
            "memory_usage_mb": execution.memory_usage_mb,
            "cpu_usage_percent": execution.cpu_usage_percent,
            "status": execution.status,
            "optimization_score": execution.optimization_score,
        }

    def _strategy_to_dict(self, strategy: OptimizationStrategy) -> Dict[str, Any]:
        """å°†ç­–ç•¥è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "name": strategy.name,
            "type": strategy.type.value,
            "impact_score": strategy.impact_score,
            "complexity_score": strategy.complexity_score,
            "estimated_improvement": strategy.estimated_improvement,
            "priority": strategy.priority,
        }

    def generate_optimization_report(self) -> str:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        if not self.optimization_history:
            return "æš‚æ— ä¼˜åŒ–å†å²è®°å½•"

        report = "# æµ‹è¯•æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š\n\n"
        report += f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"

        for i, optimization in enumerate(self.optimization_history, 1):
            report += f"## ä¼˜åŒ– #{i}: {optimization['test_name']}\n\n"
            report += "**åŸºå‡†æ€§èƒ½:**\n"
            report += f"- æ‰§è¡Œæ—¶é—´: {optimization['baseline_performance']['execution_time']:.2f}s\n"
            report += f"- å†…å­˜ä½¿ç”¨: {optimization['baseline_performance']['memory_usage_mb']:.2f}MB\n"
            report += f"- CPUä½¿ç”¨: {optimization['baseline_performance']['cpu_usage_percent']:.2f}%\n\n"

            report += "**ä¼˜åŒ–ç­–ç•¥:**\n"
            for strategy in optimization["optimization_strategies"]:
                report += f"- {strategy['name']} (å½±å“åˆ†æ•°: {strategy['impact_score']:.2f})\n"

            report += "\n**ä¼˜åŒ–åæ€§èƒ½:**\n"
            report += f"- æ‰§è¡Œæ—¶é—´: {optimization['final_performance']['execution_time']:.2f}s\n"
            report += f"- å†…å­˜ä½¿ç”¨: {optimization['final_performance']['memory_usage_mb']:.2f}MB\n"
            report += f"- CPUä½¿ç”¨: {optimization['final_performance']['cpu_usage_percent']:.2f}%\n\n"

            improvement = optimization["improvement_metrics"]
            report += "**æ”¹è¿›æ•ˆæœ:**\n"
            report += f"- æ—¶é—´æ”¹è¿›: {(improvement['time_improvement'] * 100):+.1f}%\n"
            report += f"- å†…å­˜æ”¹è¿›: {(improvement['memory_improvement'] * 100):+.1f}%\n"
            report += f"- CPUæ”¹è¿›: {(improvement['cpu_improvement'] * 100):+.1f}%\n"
            report += f"- æ€»ä½“æ”¹è¿›åˆ†æ•°: {optimization['overall_improvement_score']:.2f}\n\n"

        return report


# ä½¿ç”¨ç¤ºä¾‹
async def demo_performance_optimizer():
    """æ¼”ç¤ºæ€§èƒ½ä¼˜åŒ–å™¨åŠŸèƒ½"""
    print("ğŸš€ æ¼”ç¤ºæµ‹è¯•æ€§èƒ½ä¼˜åŒ–å™¨åŠŸèƒ½")

    optimizer = PerformanceOptimizer()

    # æ¨¡æ‹Ÿæµ‹è¯•å‡½æ•°
    async def slow_test():
        await asyncio.sleep(2)  # æ¨¡æ‹Ÿè€—æ—¶æ“ä½œ
        data = [i * i for i in range(100000)]  # æ¨¡æ‹Ÿå†…å­˜æ“ä½œ
        return sum(data)

    async def memory_intensive_test():
        # æ¨¡æ‹Ÿå†…å­˜å¯†é›†å‹æµ‹è¯•
        large_data = ["test_data"] * 1000000
        return len(large_data)

    # ä¼˜åŒ–ç¬¬ä¸€ä¸ªæµ‹è¯•
    await optimizer.optimize_test_performance("slow_test", slow_test)

    # ä¼˜åŒ–ç¬¬äºŒä¸ªæµ‹è¯•
    await optimizer.optimize_test_performance("memory_intensive_test", memory_intensive_test)

    # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
    report = optimizer.generate_optimization_report()
    print(f"\nğŸ“‹ ä¼˜åŒ–æŠ¥å‘Š:\n{report}")


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(demo_performance_optimizer())
