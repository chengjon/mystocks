#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MyStocks æ€§èƒ½æµ‹è¯•å¥—ä»¶
æä¾›å…¨é¢çš„æ€§èƒ½æµ‹è¯•ã€åŸºå‡†æµ‹è¯•å’Œå›å½’æµ‹è¯•
"""

import pytest
import asyncio
import time
import psutil
import statistics
from datetime import datetime
from typing import Dict, List, Any
import aiohttp
import json

from tests.config.test_config import test_env, performance_baseline


class PerformanceTestSuite:
    """æ€§èƒ½æµ‹è¯•å¥—ä»¶"""

    def __init__(self):
        self.base_url = test_env.API_BASE_URL
        self.results = {}
        self.start_time = None
        self.end_time = None

    async def run_performance_benchmark(self):
        """è¿è¡Œå®Œæ•´æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        self.start_time = datetime.now()

        print("\nğŸš€ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print(f"â° æµ‹è¯•å¼€å§‹æ—¶é—´: {self.start_time}")
        print(f"ğŸ¯ æµ‹è¯•ç›®æ ‡: {performance_baseline.API_RESPONSE_TIME_THRESHOLD}")

        # è¿è¡Œå„é¡¹æ€§èƒ½æµ‹è¯•
        test_methods = [
            self.test_api_response_times,
            self.test_database_query_performance,
            self.test_concurrent_users,
            self.test_memory_usage,
            self.test_cpu_usage,
            self.test_disk_io,
        ]

        results = {}
        for test_method in test_methods:
            try:
                method_name = test_method.__name__
                print(f"\nğŸ“Š è¿è¡Œæ€§èƒ½æµ‹è¯•: {method_name}")

                result = await test_method()
                results[method_name] = result

                self._print_test_summary(method_name, result)

            except Exception as e:
                print(f"âŒ æ€§èƒ½æµ‹è¯• {test_method.__name__} å¤±è´¥: {str(e)}")
                results[test_method.__name__] = {"status": "failed", "error": str(e)}

        self.end_time = datetime.now()
        duration = (self.end_time - self.start_time).total_seconds()

        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        self.results = results
        report = self._generate_performance_report(duration)

        print("\nâœ… æ€§èƒ½æµ‹è¯•å®Œæˆ")
        print(f"â±ï¸  æ€»è€—æ—¶: {duration:.2f}ç§’")
        print(f"ğŸ“ˆ å®Œæ•´æŠ¥å‘Š: {report}")

        return report

    async def test_api_response_times(self) -> Dict[str, Any]:
        """APIå“åº”æ—¶é—´æµ‹è¯•"""
        print("  ğŸ”„ æµ‹è¯•APIå“åº”æ—¶é—´...")

        # æµ‹è¯•ç«¯ç‚¹é…ç½®
        test_endpoints = [
            ("market_data", "/api/market/market-data/fetch", {"symbol": "600519"}),
            (
                "kline_data",
                "/api/market/kline/fetch",
                {"symbol": "600519", "period": "daily"},
            ),
            ("stock_quote", "/api/market/quote/fetch", {"symbols": ["600519"]}),
            ("index_data", "/api/market/index/fetch", {"index_code": "399300"}),
        ]

        results = {}
        session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30))

        for endpoint_name, path, params in test_endpoints:
            print(f"    ğŸ” æµ‹è¯• {endpoint_name}: {path}")

            # å¤šæ¬¡è¯·æ±‚å–å¹³å‡å€¼
            response_times = []
            for i in range(10):
                start_time = time.time()
                try:
                    async with session.get(
                        f"{self.base_url}{path}", params=params
                    ) as response:
                        await response.text()  # è¯»å–å“åº”
                        end_time = time.time()
                        response_times.append(
                            (end_time - start_time) * 1000
                        )  # è½¬æ¢ä¸ºæ¯«ç§’
                except Exception as e:
                    print(f"    âš ï¸  è¯·æ±‚å¤±è´¥: {str(e)}")
                    response_times.append(-1)

            # è®¡ç®—ç»Ÿè®¡æ•°æ®
            valid_times = [t for t in response_times if t > 0]
            if valid_times:
                avg_time = statistics.mean(valid_times)
                max_time = max(valid_times)
                min_time = min(valid_times)
                median_time = statistics.median(valid_times)

                results[endpoint_name] = {
                    "avg_time_ms": round(avg_time, 2),
                    "max_time_ms": round(max_time, 2),
                    "min_time_ms": round(min_time, 2),
                    "median_time_ms": round(median_time, 2),
                    "requests": len(valid_times),
                    "threshold": performance_baseline.API_RESPONSE_TIME_THRESHOLD.get(
                        endpoint_name, 5000
                    ),
                    "passed": avg_time
                    <= performance_baseline.API_RESPONSE_TIME_THRESHOLD.get(
                        endpoint_name, 5000
                    ),
                }
            else:
                results[endpoint_name] = {
                    "status": "failed",
                    "error": "All requests failed",
                }

        await session.close()
        return results

    async def test_database_query_performance(self) -> Dict[str, Any]:
        """æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½æµ‹è¯•"""
        print("  ğŸ”„ æµ‹è¯•æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½...")

        from src.data_access.postgresql_access import PostgreSQLAccess

        results = {}

        try:
            # PostgreSQLè¿æ¥
            pg_access = PostgreSQLAccess()

            # æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„æŸ¥è¯¢
            test_queries = [
                (
                    "simple_lookup",
                    """
                    SELECT * FROM stock_basic
                    WHERE symbol = '600519'
                    LIMIT 1
                """,
                ),
                (
                    "complex_analysis",
                    """
                    SELECT
                        s.symbol,
                        s.name,
                        COUNT(*) as trading_days,
                        AVG(c.close) as avg_price,
                        MAX(c.high) as highest_price,
                        MIN(c.low) as lowest_price
                    FROM stock_basic s
                    LEFT JOIN kline_daily c ON s.symbol = c.symbol
                    WHERE s.sector = 'é‡‘è'
                    GROUP BY s.symbol, s.name
                    ORDER BY trading_days DESC
                    LIMIT 100
                """,
                ),
                (
                    "batch_insert",
                    """
                    INSERT INTO kline_daily
                    (symbol, date, open, high, low, close, volume)
                    VALUES %s
                    ON CONFLICT (symbol, date) DO UPDATE
                    SET open = EXCLUDED.open,
                        high = EXCLUDED.high,
                        low = EXCLUDED.low,
                        close = EXCLUDED.close,
                        volume = EXCLUDED.volume
                """,
                ),
            ]

            for query_name, query in test_queries:
                print(f"    ğŸ” æµ‹è¯• {query_name}...")

                # æ‰§è¡Œå¤šæ¬¡å–å¹³å‡å€¼
                execution_times = []
                for i in range(5):
                    try:
                        start_time = time.time()

                        if query_name == "batch_insert":
                            # æ‰¹é‡æ’å…¥æµ‹è¯•æ•°æ®
                            test_data = [
                                (
                                    "600519",
                                    "2024-12-12",
                                    100.0,
                                    105.0,
                                    98.0,
                                    102.0,
                                    1000000,
                                )
                                for _ in range(100)
                            ]
                            pg_access.execute_batch(query, test_data)
                        else:
                            pg_access.execute_query(query)

                        end_time = time.time()
                        execution_times.append((end_time - start_time) * 1000)

                    except Exception as e:
                        print(f"    âš ï¸  æŸ¥è¯¢å¤±è´¥: {str(e)}")
                        execution_times.append(-1)

                # è®¡ç®—ç»Ÿè®¡æ•°æ®
                valid_times = [t for t in execution_times if t > 0]
                if valid_times:
                    avg_time = statistics.mean(valid_times)
                    results[query_name] = {
                        "avg_time_ms": round(avg_time, 2),
                        "max_time_ms": round(max(valid_times), 2),
                        "min_time_ms": round(min(valid_times), 2),
                        "requests": len(valid_times),
                        "threshold": performance_baseline.DB_QUERY_TIME_THRESHOLD.get(
                            query_name, 1000
                        ),
                        "passed": avg_time
                        <= performance_baseline.DB_QUERY_TIME_THRESHOLD.get(
                            query_name, 1000
                        ),
                    }
                else:
                    results[query_name] = {
                        "status": "failed",
                        "error": "All queries failed",
                    }

        except Exception as e:
            return {
                "status": "failed",
                "error": f"Database connection failed: {str(e)}",
            }

        return results

    async def test_concurrent_users(self) -> Dict[str, Any]:
        """å¹¶å‘ç”¨æˆ·æ€§èƒ½æµ‹è¯•"""
        print("  ğŸ”„ æµ‹è¯•å¹¶å‘ç”¨æˆ·æ€§èƒ½...")

        user_counts = [10, 50, 100, 200]
        results = {}

        async def simulate_user(user_id: int, tasks: List) -> float:
            """æ¨¡æ‹Ÿå•ä¸ªç”¨æˆ·è¡Œä¸º"""
            session = aiohttp.ClientSession()
            start_time = time.time()

            try:
                # ç”¨æˆ·è¡Œä¸ºæ¨¡æ‹Ÿ
                actions = [
                    ("get_quote", "/api/market/quote/fetch", {"symbols": ["600519"]}),
                    (
                        "get_kline",
                        "/api/market/kline/fetch",
                        {"symbol": "600519", "period": "daily"},
                    ),
                    ("get_index", "/api/market/index/fetch", {"index_code": "399300"}),
                ]

                for action_name, path, params in actions:
                    try:
                        async with session.get(
                            f"{self.base_url}{path}", params=params
                        ) as response:
                            await response.text()
                    except:
                        pass  # å¿½ç•¥å•ä¸ªè¯·æ±‚å¤±è´¥

                end_time = time.time()
                return (end_time - start_time) * 1000

            finally:
                await session.close()

        for user_count in user_counts:
            print(f"    ğŸ” æµ‹è¯• {user_count} å¹¶å‘ç”¨æˆ·...")

            # åˆ›å»ºå¹¶å‘ä»»åŠ¡
            tasks = [simulate_user(i, tasks) for i in range(user_count)]
            start_time = time.time()

            # æ‰§è¡Œå¹¶å‘ä»»åŠ¡
            await asyncio.gather(*tasks, return_exceptions=True)

            end_time = time.time()
            total_time = (end_time - start_time) * 1000

            results[user_count] = {
                "total_time_ms": round(total_time, 2),
                "avg_user_time_ms": round(total_time / user_count, 2),
                "requests_per_second": round((user_count * 3) / (total_time / 1000), 2),
                "user_count": user_count,
            }

        return results

    async def test_memory_usage(self) -> Dict[str, Any]:
        """å†…å­˜ä½¿ç”¨æµ‹è¯•"""
        print("  ğŸ”„ æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ...")

        process = psutil.Process()
        memory_info = process.memory_info()

        results = {
            "rss_memory_mb": round(memory_info.rss / 1024 / 1024, 2),
            "vms_memory_mb": round(memory_info.vms / 1024 / 1024, 2),
            "memory_percent": process.memory_percent(),
            "available_memory_mb": round(
                psutil.virtual_memory().available / 1024 / 1024, 2
            ),
            "total_memory_mb": round(psutil.virtual_memory().total / 1024 / 1024, 2),
        }

        return results

    async def test_cpu_usage(self) -> Dict[str, Any]:
        """CPUä½¿ç”¨æµ‹è¯•"""
        print("  ğŸ”„ æµ‹è¯•CPUä½¿ç”¨æƒ…å†µ...")

        # ç›‘æ§CPUä½¿ç”¨ç‡
        cpu_percentages = []
        for i in range(10):
            cpu_percentages.append(psutil.cpu_percent(interval=0.1))

        results = {
            "avg_cpu_percent": round(statistics.mean(cpu_percentages), 2),
            "max_cpu_percent": round(max(cpu_percentages), 2),
            "min_cpu_percent": round(min(cpu_percentages), 2),
            "cpu_count": psutil.cpu_count(),
            "cpu_count_logical": psutil.cpu_count(logical=True),
        }

        return results

    async def test_disk_io(self) -> Dict[str, Any]:
        """ç£ç›˜I/Oæ€§èƒ½æµ‹è¯•"""
        print("  ğŸ”„ æµ‹è¯•ç£ç›˜I/Oæ€§èƒ½...")

        # å†™å…¥æµ‹è¯•æ–‡ä»¶
        test_file = "/tmp/performance_test.tmp"
        test_data = "x" * (1024 * 1024)  # 1MBæ•°æ®

        # å†™å…¥æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        with open(test_file, "w") as f:
            for i in range(10):  # å†™å…¥10MB
                f.write(test_data)
        write_time = time.time() - start_time

        # è¯»å–æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        with open(test_file, "r") as f:
            data = f.read()
        read_time = time.time() - start_time

        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        try:
            os.remove(test_file)
        except:
            pass

        results = {
            "write_speed_mb_s": round(10 / write_time, 2),
            "read_speed_mb_s": round(10 / read_time, 2),
            "write_time_s": round(write_time, 3),
            "read_time_s": round(read_time, 3),
        }

        return results

    def _print_test_summary(self, test_name: str, result: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        if (
            isinstance(result, dict)
            and "status" in result
            and result["status"] == "failed"
        ):
            print(f"    âŒ {test_name} æµ‹è¯•å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        elif isinstance(result, dict) and any(
            key in result for key in ["passed", "avg_time_ms", "avg_cpu_percent"]
        ):
            if "avg_time_ms" in result:
                avg_time = result["avg_time_ms"]
                threshold = result.get("threshold", 5000)
                status = "âœ…" if result.get("passed", False) else "âŒ"
                print(
                    f"    {status} {test_name}: {avg_time:.2f}ms (é˜ˆå€¼: {threshold}ms)"
                )
            elif "avg_cpu_percent" in result:
                cpu_percent = result["avg_cpu_percent"]
                print(f"    âœ… {test_name}: {cpu_percent:.1f}%")
            else:
                print(f"    âœ… {test_name}: æµ‹è¯•é€šè¿‡")
        else:
            print(f"    âš ï¸  {test_name}: å¤æ‚ç»“æœ")

    def _generate_performance_report(self, total_duration: float) -> str:
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        report = {
            "test_summary": {
                "total_duration_seconds": round(total_duration, 2),
                "start_time": self.start_time.isoformat(),
                "end_time": self.end_time.isoformat(),
                "test_count": len(self.results),
            },
            "detailed_results": self.results,
            "performance_metrics": {
                "overall_score": self._calculate_overall_score(),
                "worst_performing": self._identify_worst_performing(),
                "recommendations": self._generate_recommendations(),
            },
        }

        # ä¿å­˜æŠ¥å‘Š
        report_path = (
            f"/tmp/performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        return report_path

    def _calculate_overall_score(self) -> int:
        """è®¡ç®—æ€»ä½“æ€§èƒ½è¯„åˆ†"""
        scores = []

        if "test_api_response_times" in self.results:
            api_results = self.results["test_api_response_times"]
            if isinstance(api_results, dict):
                for endpoint, result in api_results.items():
                    if isinstance(result, dict) and "passed" in result:
                        scores.append(100 if result["passed"] else 0)

        if "test_database_query_performance" in self.results:
            db_results = self.results["test_database_query_performance"]
            if isinstance(db_results, dict):
                for query, result in db_results.items():
                    if isinstance(result, dict) and "passed" in result:
                        scores.append(100 if result["passed"] else 0)

        return round(sum(scores) / len(scores)) if scores else 0

    def _identify_worst_performing(self) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ€§èƒ½æœ€å·®çš„ç»„ä»¶"""
        worst = []

        # åˆ†æAPIå“åº”æ—¶é—´
        if "test_api_response_times" in self.results:
            api_results = self.results["test_api_response_times"]
            if isinstance(api_results, dict):
                for endpoint, result in api_results.items():
                    if isinstance(result, dict) and "avg_time_ms" in result:
                        worst.append(
                            {
                                "component": f"API:{endpoint}",
                                "response_time_ms": result["avg_time_ms"],
                                "threshold": result.get("threshold", 5000),
                            }
                        )

        # æŒ‰å“åº”æ—¶é—´æ’åº
        worst.sort(key=lambda x: x["response_time_ms"], reverse=True)
        return worst[:3]  # è¿”å›å‰3ä¸ªæ€§èƒ½æœ€å·®çš„

    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # APIæ€§èƒ½å»ºè®®
        if "test_api_response_times" in self.results:
            api_results = self.results["test_api_response_times"]
            if isinstance(api_results, dict):
                for endpoint, result in api_results.items():
                    if isinstance(result, dict) and not result.get("passed", True):
                        recommendations.append(
                            f"ä¼˜åŒ–APIç«¯ç‚¹ {endpoint} çš„æ€§èƒ½ï¼Œå½“å‰å“åº”æ—¶é—´ {result.get('avg_time_ms', 0)}ms"
                        )

        # æ•°æ®åº“å»ºè®®
        if "test_database_query_performance" in self.results:
            db_results = self.results["test_database_query_performance"]
            if isinstance(db_results, dict):
                for query, result in db_results.items():
                    if isinstance(result, dict) and not result.get("passed", True):
                        recommendations.append(
                            f"ä¼˜åŒ–æŸ¥è¯¢ {query}ï¼Œè€ƒè™‘æ·»åŠ ç´¢å¼•æˆ–ä¼˜åŒ–SQLè¯­å¥"
                        )

        # ç³»ç»Ÿå»ºè®®
        if "test_memory_usage" in self.results:
            mem_results = self.results["test_memory_usage"]
            if (
                isinstance(mem_results, dict)
                and mem_results.get("memory_percent", 0) > 80
            ):
                recommendations.append("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œè€ƒè™‘å¢åŠ å†…å­˜æˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨")

        return recommendations


# æ€§èƒ½æµ‹è¯•è£…é¥°å™¨
def performance_benchmark(test_func):
    """æ€§èƒ½æµ‹è¯•è£…é¥°å™¨"""

    async def wrapper(*args, **kwargs):
        suite = PerformanceTestSuite()
        return await suite.run_performance_benchmark()

    return wrapper


# Pytestæµ‹è¯•ç”¨ä¾‹
@pytest.mark.performance
async def test_api_performance():
    """APIæ€§èƒ½æµ‹è¯•"""
    suite = PerformanceTestSuite()
    report = await suite.run_performance_benchmark()

    # éªŒè¯æµ‹è¯•ç»“æœ
    assert "test_api_response_times" in suite.results
    assert len(suite.results) >= 3  # è‡³å°‘è¿è¡Œäº†3é¡¹æµ‹è¯•

    print(f"\nğŸ“Š æ€§èƒ½æµ‹è¯•æŠ¥å‘Š: {report}")


@pytest.mark.performance
async def test_database_performance():
    """æ•°æ®åº“æ€§èƒ½æµ‹è¯•"""
    suite = PerformanceTestSuite()

    # åªè¿è¡Œæ•°æ®åº“æµ‹è¯•
    db_result = await suite.test_database_query_performance()

    assert isinstance(db_result, dict)
    assert len(db_result) >= 1  # è‡³å°‘æœ‰ä¸€ä¸ªæŸ¥è¯¢æµ‹è¯•

    # éªŒè¯åŸºæœ¬æŒ‡æ ‡
    for query_name, result in db_result.items():
        if isinstance(result, dict) and "avg_time_ms" in result:
            assert result["avg_time_ms"] >= 0  # æ‰§è¡Œæ—¶é—´åº”è¯¥ä¸ºæ­£æ•°


@pytest.mark.performance
async def test_concurrent_performance():
    """å¹¶å‘æ€§èƒ½æµ‹è¯•"""
    suite = PerformanceTestSuite()

    # åªè¿è¡Œå¹¶å‘æµ‹è¯•
    concurrent_result = await suite.test_concurrent_users()

    assert isinstance(concurrent_result, dict)
    assert len(concurrent_result) >= 2  # æµ‹è¯•äº†è‡³å°‘2ä¸ªå¹¶å‘çº§åˆ«

    # éªŒè¯å¹¶å‘çº§åˆ«
    for user_count, result in concurrent_result.items():
        assert isinstance(result, dict)
        assert result["requests_per_second"] >= 0


@pytest.mark.performance
async def test_system_resources():
    """ç³»ç»Ÿèµ„æºæµ‹è¯•"""
    suite = PerformanceTestSuite()

    # æµ‹è¯•å†…å­˜å’ŒCPU
    memory_result = await suite.test_memory_usage()
    cpu_result = await suite.test_cpu_usage()

    assert isinstance(memory_result, dict)
    assert isinstance(cpu_result, dict)

    # éªŒè¯åŸºæœ¬æŒ‡æ ‡
    assert memory_result["rss_memory_mb"] >= 0
    assert memory_result["memory_percent"] >= 0
    assert cpu_result["avg_cpu_percent"] >= 0
    assert cpu_result["cpu_count"] > 0


if __name__ == "__main__":
    # è¿è¡Œå®Œæ•´æ€§èƒ½æµ‹è¯•
    async def main():
        suite = PerformanceTestSuite()
        report = await suite.run_performance_benchmark()
        print(f"\nğŸ¯ æ€§èƒ½æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report}")

    # è¿è¡Œæµ‹è¯•
    import asyncio

    asyncio.run(main())
