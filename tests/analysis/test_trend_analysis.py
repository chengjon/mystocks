#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•è¶‹åŠ¿åˆ†ææ¨¡å—

æä¾›æµ‹è¯•æ•°æ®çš„æ—¶é—´åºåˆ—åˆ†æã€è¶‹åŠ¿é¢„æµ‹å’Œæ¨¡å¼è¯†åˆ«åŠŸèƒ½
"""

from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from enum import Enum
import numpy as np
import pandas as pd
from scipy import stats
from sklearn.linear_model import LinearRegression
import plotly.graph_objects as go
from plotly.subplots import make_subplots


class TrendDirection(Enum):
    """è¶‹åŠ¿æ–¹å‘"""

    INCREASING = "increasing"
    DECREASING = "decreasing"
    STABLE = "stable"
    VOLATILE = "volatile"


class SeasonalityType(Enum):
    """å­£èŠ‚æ€§ç±»å‹"""

    NONE = "none"
    DAILY = "daily"
    WEEKLY = "weekly"
    MONTHLY = "monthly"
    QUARTERLY = "quarterly"
    YEARLY = "yearly"


class TrendConfidence(Enum):
    """è¶‹åŠ¿ç½®ä¿¡åº¦"""

    VERY_LOW = 0.0
    LOW = 0.33
    MEDIUM = 0.66
    HIGH = 1.0


@dataclass
class TimeSeriesPoint:
    """æ—¶é—´åºåˆ—æ•°æ®ç‚¹"""

    timestamp: datetime
    value: float
    category: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class TrendAnalysisResult:
    """è¶‹åŠ¿åˆ†æç»“æœ"""

    metric_name: str
    direction: TrendDirection
    confidence: TrendConfidence
    slope: float
    r_squared: float
    p_value: float
    prediction: Optional[float] = None
    forecast: Optional[List[float]] = None
    seasonality: SeasonalityType = SeasonalityType.NONE
    anomalies: List[Dict[str, Any]] = field(default_factory=list)
    patterns: List[Dict[str, Any]] = field(default_factory=list)
    insights: List[str] = field(default_factory=list)


@dataclass
class ForecastingConfig:
    """é¢„æµ‹é…ç½®"""

    forecast_periods: int = 24
    confidence_interval: float = 0.95
    include_seasonality: bool = True
    use_machine_learning: bool = True
    smoothing_factor: float = 0.3


class TrendAnalyzer:
    """è¶‹åŠ¿åˆ†æå™¨"""

    def __init__(self, config: Optional[ForecastingConfig] = None):
        self.config = config or ForecastingConfig()
        self.historical_data: Dict[str, List[TimeSeriesPoint]] = {}
        self.analysis_cache: Dict[str, TrendAnalysisResult] = {}
        self.patterns_cache: Dict[str, List[Dict[str, Any]]] = {}

    def add_historical_data(self, metric_name: str, data: List[TimeSeriesPoint]):
        """æ·»åŠ å†å²æ•°æ®"""
        self.historical_data[metric_name] = sorted(data, key=lambda x: x.timestamp)
        self.analysis_cache.pop(metric_name, None)  # æ¸…é™¤ç¼“å­˜
        self.patterns_cache.pop(metric_name, None)  # æ¸…é™¤ç¼“å­˜

    def analyze_trend(self, metric_name: str) -> TrendAnalysisResult:
        """åˆ†æè¶‹åŠ¿"""
        if metric_name in self.analysis_cache:
            return self.analysis_cache[metric_name]

        if metric_name not in self.historical_data:
            raise ValueError(f"No historical data for metric: {metric_name}")

        data = self.historical_data[metric_name]
        values = [point.value for point in data]
        timestamps = [point.timestamp for point in data]

        # å‡†å¤‡å›å½’åˆ†ææ•°æ®
        x_values = np.array(
            [(t - timestamps[0]).total_seconds() / 3600 for t in timestamps]
        ).reshape(-1, 1)
        y_values = np.array(values)

        # çº¿æ€§å›å½’åˆ†æ
        model = LinearRegression()
        model.fit(x_values, y_values)

        # é¢„æµ‹å€¼
        y_pred = model.predict(x_values)

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        slope = model.coef_[0]
        r_squared = model.score(x_values, y_values)
        p_value = self._calculate_p_value(x_values.flatten(), y_values, y_pred)

        # ç¡®å®šè¶‹åŠ¿æ–¹å‘
        direction = self._determine_direction(slope, r_squared, p_value)
        confidence = self._determine_confidence(r_squared, p_value)

        # æ£€æµ‹å­£èŠ‚æ€§
        seasonality = self._detect_seasonality(data, timestamps, values)

        # æ£€æµ‹å¼‚å¸¸å€¼
        anomalies = self._detect_anomalies(values, timestamps)

        # è¯†åˆ«æ¨¡å¼
        patterns = self._identify_patterns(data, values)

        # ç”Ÿæˆæ´å¯Ÿ
        insights = self._generate_insights(
            metric_name,
            direction,
            confidence,
            slope,
            r_squared,
            anomalies,
            patterns,
            seasonality,
        )

        # ç”Ÿæˆé¢„æµ‹
        prediction = None
        forecast = None
        if len(data) >= 10:  # è‡³å°‘éœ€è¦10ä¸ªæ•°æ®ç‚¹è¿›è¡Œé¢„æµ‹
            last_timestamp = timestamps[-1]
            prediction = self._predict_next_value(
                model, last_timestamp, x_values[-1][0]
            )
            forecast = self._generate_forecast(model, last_timestamp, x_values[-1][0])

        # åˆ›å»ºåˆ†æç»“æœ
        result = TrendAnalysisResult(
            metric_name=metric_name,
            direction=direction,
            confidence=confidence,
            slope=slope,
            r_squared=r_squared,
            p_value=p_value,
            prediction=prediction,
            forecast=forecast,
            seasonality=seasonality,
            anomalies=anomalies,
            patterns=patterns,
            insights=insights,
        )

        # ç¼“å­˜ç»“æœ
        self.analysis_cache[metric_name] = result

        return result

    def _calculate_p_value(
        self, x: np.ndarray, y: np.ndarray, y_pred: np.ndarray
    ) -> float:
        """è®¡ç®—på€¼"""
        n = len(x)
        if n <= 2:
            return 1.0

        # è®¡ç®—æ®‹å·®å¹³æ–¹å’Œ
        residuals = y - y_pred
        rss = np.sum(residuals**2)

        # è®¡ç®—æ€»å¹³æ–¹å’Œ
        y_mean = np.mean(y)
        tss = np.sum((y - y_mean) ** 2)

        # è®¡ç®—Fç»Ÿè®¡é‡
        if tss == 0:
            return 0.0

        f_statistic = (tss - rss) / 1 / (rss / (n - 2))

        # è®¡ç®—på€¼
        p_value = 1 - stats.f.cdf(f_statistic, 1, n - 2)
        return p_value

    def _determine_direction(
        self, slope: float, r_squared: float, p_value: float
    ) -> TrendDirection:
        """ç¡®å®šè¶‹åŠ¿æ–¹å‘"""
        if p_value > 0.05:  # ä¸æ˜¾è‘—
            return TrendDirection.STABLE

        if abs(slope) < 0.01:  # å˜åŒ–éå¸¸å°
            return TrendDirection.STABLE

        if r_squared < 0.1:  # æ‹Ÿåˆåº¦å¾ˆä½
            return TrendDirection.VOLATILE

        if slope > 0:
            return TrendDirection.INCREASING
        else:
            return TrendDirection.DECREASING

    def _determine_confidence(
        self, r_squared: float, p_value: float
    ) -> TrendConfidence:
        """ç¡®å®šè¶‹åŠ¿ç½®ä¿¡åº¦"""
        if p_value > 0.05 or r_squared < 0.3:
            return TrendConfidence.VERY_LOW
        elif p_value > 0.01 or r_squared < 0.6:
            return TrendConfidence.LOW
        elif p_value > 0.001 or r_squared < 0.8:
            return TrendConfidence.MEDIUM
        else:
            return TrendConfidence.HIGH

    def _detect_seasonality(
        self,
        data: List[TimeSeriesPoint],
        timestamps: List[datetime],
        values: List[float],
    ) -> SeasonalityType:
        """æ£€æµ‹å­£èŠ‚æ€§"""
        if len(data) < 24:  # æ•°æ®å¤ªå°‘ï¼Œæ— æ³•æ£€æµ‹å­£èŠ‚æ€§
            return SeasonalityType.NONE

        # è½¬æ¢ä¸ºpandas DataFrame
        df = pd.DataFrame(
            {
                "timestamp": timestamps,
                "value": values,
                "hour": [t.hour for t in timestamps],
                "day_of_week": [t.weekday() for t in timestamps],
                "day_of_month": [t.day for t in timestamps],
                "month": [t.month for t in timestamps],
            }
        )

        # æ£€æµ‹ä¸åŒçš„å­£èŠ‚æ€§æ¨¡å¼
        patterns = {
            SeasonalityType.DAILY: self._test_seasonality(df, "hour"),
            SeasonalityType.WEEKLY: self._test_seasonality(df, "day_of_week"),
            SeasonalityType.MONTHLY: self._test_seasonality(df, "month"),
        }

        # é€‰æ‹©æœ€å¼ºçš„å­£èŠ‚æ€§æ¨¡å¼
        strongest_season = max(patterns.keys(), key=lambda k: patterns[k])

        if patterns[strongest_season] > 0.3:  # é˜ˆå€¼
            return strongest_season

        return SeasonalityType.NONE

    def _test_seasonality(self, df: pd.DataFrame, column: str) -> float:
        """æµ‹è¯•ç‰¹å®šåˆ—çš„å­£èŠ‚æ€§"""
        from scipy.stats import f_oneway

        # æŒ‰å­£èŠ‚åˆ†ç»„
        groups = [df[df[column] == val]["value"] for val in df[column].unique()]

        # å¦‚æœåªæœ‰ä¸€ä¸ªç»„ï¼Œæ— æ³•è¿›è¡ŒANOVA
        if len(groups) < 2:
            return 0.0

        # ANOVAæ£€éªŒ
        try:
            f_stat, p_value = f_oneway(*groups)
            return 1.0 - p_value  # è¿”å›æ˜¾è‘—æ€§ç¨‹åº¦
        except:
            return 0.0

    def _detect_anomalies(
        self, values: List[float], timestamps: List[datetime]
    ) -> List[Dict[str, Any]]:
        """æ£€æµ‹å¼‚å¸¸å€¼"""
        if len(values) < 10:
            return []

        # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
        q1 = np.percentile(values, 25)
        q3 = np.percentile(values, 75)
        iqr = q3 - q1

        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr

        anomalies = []
        for i, (value, timestamp) in enumerate(zip(values, timestamps)):
            if value < lower_bound or value > upper_bound:
                anomaly_type = "low" if value < lower_bound else "high"
                anomalies.append(
                    {
                        "index": i,
                        "timestamp": timestamp.isoformat(),
                        "value": value,
                        "type": anomaly_type,
                        "severity": "high"
                        if abs(value - (q1 + q3) / 2) > 2 * iqr
                        else "medium",
                    }
                )

        return anomalies

    def _identify_patterns(
        self, data: List[TimeSeriesPoint], values: List[float]
    ) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ•°æ®æ¨¡å¼"""
        patterns = []

        # æ£€æµ‹è¿ç»­ä¸Šå‡/ä¸‹é™æ¨¡å¼
        patterns.extend(self._detect_trend_patterns(values))

        # æ£€æµ‹å‘¨æœŸæ€§æ¨¡å¼
        if len(values) >= 20:
            patterns.extend(self._detect_cyclic_patterns(values))

        # æ£€æµ‹çªå˜æ¨¡å¼
        patterns.extend(self._detect_change_points(values))

        return patterns

    def _detect_trend_patterns(self, values: List[float]) -> List[Dict[str, Any]]:
        """æ£€æµ‹è¶‹åŠ¿æ¨¡å¼"""
        patterns = []

        # æ»‘åŠ¨çª—å£æ£€æµ‹
        window_size = min(5, len(values) // 4)

        for i in range(len(values) - window_size + 1):
            window = values[i : i + window_size]

            # è®¡ç®—çª—å£å†…çš„è¶‹åŠ¿
            if len(window) >= 2:
                trend = np.polyfit(range(len(window)), window, 1)[0]

                if abs(trend) > 1.0:  # é˜ˆå€¼
                    direction = "increasing" if trend > 0 else "decreasing"
                    patterns.append(
                        {
                            "type": f"{direction}_trend",
                            "start_index": i,
                            "end_index": i + window_size - 1,
                            "strength": abs(trend),
                            "direction": direction,
                        }
                    )

        return patterns

    def _detect_cyclic_patterns(self, values: List[float]) -> List[Dict[str, Any]]:
        """æ£€æµ‹å‘¨æœŸæ€§æ¨¡å¼"""
        patterns = []

        # ä½¿ç”¨FFTæ£€æµ‹å‘¨æœŸæ€§
        fft_values = np.fft.fft(values)
        frequencies = np.fft.fftfreq(len(values))

        # æ‰¾åˆ°ä¸»è¦é¢‘ç‡
        power_spectrum = np.abs(fft_values) ** 2
        peak_indices = np.argsort(power_spectrum)[-3:]  # å‰3ä¸ªå³°å€¼

        for idx in peak_indices:
            if frequencies[idx] > 0:  # å¿½ç•¥0é¢‘ç‡
                period = 1.0 / frequencies[idx]
                if period < len(values) / 2:  # åˆç†çš„å‘¨æœŸ
                    patterns.append(
                        {
                            "type": "cyclic",
                            "period": period,
                            "frequency": frequencies[idx],
                            "power": power_spectrum[idx],
                            "confidence": min(
                                power_spectrum[idx] / np.max(power_spectrum), 1.0
                            ),
                        }
                    )

        return patterns

    def _detect_change_points(self, values: List[float]) -> List[Dict[str, Any]]:
        """æ£€æµ‹çªå˜ç‚¹"""
        change_points = []

        # ä½¿ç”¨CUSUMæ–¹æ³•æ£€æµ‹çªå˜
        if len(values) < 10:
            return change_points

        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        baseline_mean = np.mean(values[: len(values) // 2])
        baseline_std = np.std(values[: len(values) // 2])

        cusum = 0
        threshold = 5 * baseline_std  # CUSUMé˜ˆå€¼

        for i in range(len(values) // 2, len(values)):
            cusum += values[i] - baseline_mean
            if abs(cusum) > threshold:
                change_points.append(
                    {
                        "type": "change_point",
                        "index": i,
                        "timestamp": None,  # éœ€è¦å¤–éƒ¨ä¼ å…¥
                        "magnitude": abs(cusum) / threshold,
                        "direction": "increase" if cusum > 0 else "decrease",
                    }
                )
                # é‡ç½®CUSUM
                cusum = 0

        return change_points

    def _generate_insights(
        self,
        metric_name: str,
        direction: TrendDirection,
        confidence: TrendConfidence,
        slope: float,
        r_squared: float,
        anomalies: List[Dict[str, Any]],
        patterns: List[Dict[str, Any]],
        seasonality: SeasonalityType,
    ) -> List[str]:
        """ç”Ÿæˆæ´å¯Ÿ"""
        insights = []

        # åŸºæœ¬è¶‹åŠ¿æ´å¯Ÿ
        if confidence == TrendConfidence.HIGH:
            if direction == TrendDirection.INCREASING:
                insights.append(f"{metric_name}å‘ˆæ˜¾è‘—ä¸Šå‡è¶‹åŠ¿ï¼Œæ–œç‡: {slope:.4f}")
            elif direction == TrendDirection.DECREASING:
                insights.append(f"{metric_name}å‘ˆæ˜¾è‘—ä¸‹é™è¶‹åŠ¿ï¼Œæ–œç‡: {slope:.4f}")
            else:
                insights.append(f"{metric_name}ä¿æŒç¨³å®š")

        # å¼‚å¸¸æ´å¯Ÿ
        if anomalies:
            high_anomalies = [a for a in anomalies if a["severity"] == "high"]
            if high_anomalies:
                insights.append(f"æ£€æµ‹åˆ°{len(high_anomalies)}ä¸ªé«˜é£é™©å¼‚å¸¸å€¼")

        # æ¨¡å¼æ´å¯Ÿ
        cyclic_patterns = [p for p in patterns if p["type"] == "cyclic"]
        if cyclic_patterns:
            main_pattern = max(cyclic_patterns, key=lambda p: p["confidence"])
            insights.append(
                f"å‘ç°å‘¨æœŸæ€§æ¨¡å¼ï¼Œå‘¨æœŸ: {main_pattern['period']:.1f}ä¸ªæ•°æ®ç‚¹"
            )

        # å­£èŠ‚æ€§æ´å¯Ÿ
        if seasonality != SeasonalityType.NONE:
            insights.append(f"å­˜åœ¨{seasonality.value}å­£èŠ‚æ€§æ¨¡å¼")

        return insights

    def _predict_next_value(
        self, model, last_timestamp: datetime, last_x: float
    ) -> float:
        """é¢„æµ‹ä¸‹ä¸€ä¸ªå€¼"""
        # é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹
        next_x = last_x + 1  # 1å°æ—¶å
        prediction = model.predict([[next_x]])[0]
        return float(prediction)

    def _generate_forecast(
        self, model, last_timestamp: datetime, last_x: float
    ) -> List[float]:
        """ç”Ÿæˆæœªæ¥é¢„æµ‹"""
        forecast = []

        for i in range(1, self.config.forecast_periods + 1):
            next_x = last_x + i
            predicted_value = model.predict([[next_x]])[0]
            forecast.append(float(predicted_value))

        return forecast

    def create_visualization(self, metric_name: str) -> Dict[str, Any]:
        """åˆ›å»ºè¶‹åŠ¿å¯è§†åŒ–å›¾è¡¨"""
        if metric_name not in self.historical_data:
            raise ValueError(f"No historical data for metric: {metric_name}")

        data = self.historical_data[metric_name]
        result = self.analyze_trend(metric_name)

        # åˆ›å»ºå­å›¾
        fig = make_subplots(
            rows=3,
            cols=1,
            subplot_titles=("åŸå§‹æ•°æ®è¶‹åŠ¿", "å­£èŠ‚æ€§åˆ†æ", "é¢„æµ‹å’Œç½®ä¿¡åŒºé—´"),
            vertical_spacing=0.1,
        )

        # åŸå§‹æ•°æ®è¶‹åŠ¿
        timestamps = [point.timestamp for point in data]
        values = [point.value for point in data]

        fig.add_trace(
            go.Scatter(
                x=timestamps,
                y=values,
                mode="lines+markers",
                name="åŸå§‹æ•°æ®",
                line=dict(color="blue"),
            ),
            row=1,
            col=1,
        )

        # æ·»åŠ è¶‹åŠ¿çº¿
        if len(data) >= 2:
            x_numeric = [(t - timestamps[0]).total_seconds() / 3600 for t in timestamps]
            trend_line = result.slope * np.array(x_numeric) + np.mean(values)
            fig.add_trace(
                go.Scatter(
                    x=timestamps,
                    y=trend_line,
                    mode="lines",
                    name="è¶‹åŠ¿çº¿",
                    line=dict(color="red", dash="dash"),
                ),
                row=1,
                col=1,
            )

        # å­£èŠ‚æ€§åˆ†æ
        if result.seasonality != SeasonalityType.NONE:
            # æŒ‰å­£èŠ‚æ€§åˆ†ç»„å±•ç¤º
            if result.seasonality == SeasonalityType.DAILY:
                df = pd.DataFrame(
                    {
                        "timestamp": timestamps,
                        "value": values,
                        "hour": [t.hour for t in timestamps],
                    }
                )
                seasonal_avg = df.groupby("hour")["value"].mean()
                fig.add_trace(
                    go.Scatter(
                        x=seasonal_avg.index,
                        y=seasonal_avg.values,
                        mode="lines+markers",
                        name="å°æ—¶å‡å€¼",
                        line=dict(color="green"),
                    ),
                    row=2,
                    col=1,
                )

        # é¢„æµ‹å›¾
        if result.forecast:
            forecast_timestamps = []
            last_time = timestamps[-1]
            for i in range(len(result.forecast)):
                forecast_timestamps.append(last_time + timedelta(hours=i + 1))

            # é¢„æµ‹çº¿
            fig.add_trace(
                go.Scatter(
                    x=forecast_timestamps,
                    y=result.forecast,
                    mode="lines+markers",
                    name="é¢„æµ‹å€¼",
                    line=dict(color="orange"),
                ),
                row=3,
                col=1,
            )

            # ç½®ä¿¡åŒºé—´ï¼ˆç®€åŒ–ç‰ˆï¼‰
            if result.confidence == TrendConfidence.HIGH:
                std_dev = np.std(values[-10:]) if len(values) >= 10 else np.std(values)
                upper_bound = [f + std_dev for f in result.forecast]
                lower_bound = [f - std_dev for f in result.forecast]

                fig.add_trace(
                    go.Scatter(
                        x=forecast_timestamps,
                        y=upper_bound,
                        mode="lines",
                        name="ç½®ä¿¡åŒºé—´ä¸Šé™",
                        line=dict(color="rgba(255,165,0,0.3)", showlegend=False),
                    ),
                    row=3,
                    col=1,
                )

                fig.add_trace(
                    go.Scatter(
                        x=forecast_timestamps,
                        y=lower_bound,
                        mode="lines",
                        name="ç½®ä¿¡åŒºé—´ä¸‹é™",
                        line=dict(
                            color="rgba(255,165,0,0.3)",
                            fill="tonexty",
                            showlegend=False,
                        ),
                    ),
                    row=3,
                    col=1,
                )

        # å¼‚å¸¸å€¼æ ‡è®°
        for anomaly in result.anomalies:
            anomaly_time = datetime.fromisoformat(anomaly["timestamp"])
            fig.add_trace(
                go.Scatter(
                    x=[anomaly_time],
                    y=[anomaly["value"]],
                    mode="markers",
                    name="å¼‚å¸¸å€¼",
                    marker=dict(color="red", size=10),
                ),
                row=1,
                col=1,
            )

        # æ›´æ–°å¸ƒå±€
        fig.update_layout(title=f"{metric_name} è¶‹åŠ¿åˆ†æ", height=800, showlegend=True)

        return fig.to_dict()

    def get_trend_summary(self, metrics: List[str]) -> Dict[str, Any]:
        """è·å–å¤šä¸ªæŒ‡æ ‡çš„æ€»ä½“è¶‹åŠ¿æ‘˜è¦"""
        summary = {"metrics_count": len(metrics), "trends": {}, "overall_insights": []}

        trend_directions = []
        confidences = []

        for metric in metrics:
            try:
                result = self.analyze_trend(metric)
                summary["trends"][metric] = {
                    "direction": result.direction.value,
                    "confidence": result.confidence.value,
                    "slope": result.slope,
                    "r_squared": result.r_squared,
                    "anomalies_count": len(result.anomalies),
                    "patterns_count": len(result.patterns),
                }

                trend_directions.append(result.direction)
                confidences.append(result.confidence)

            except Exception as e:
                summary["trends"][metric] = {"error": str(e)}

        # æ€»ä½“è¶‹åŠ¿åˆ†æ
        from collections import Counter

        direction_counter = Counter(trend_directions)
        main_direction = direction_counter.most_common(1)[0][0]

        if main_direction == TrendDirection.INCREASING:
            summary["overall_insights"].append("å¤šæ•°æŒ‡æ ‡å‘ˆä¸Šå‡è¶‹åŠ¿")
        elif main_direction == TrendDirection.DECREASING:
            summary["overall_insights"].append("å¤šæ•°æŒ‡æ ‡å‘ˆä¸‹é™è¶‹åŠ¿")
        else:
            summary["overall_insights"].append("å¤šæ•°æŒ‡æ ‡ä¿æŒç¨³å®š")

        # é«˜ç½®ä¿¡åº¦æŒ‡æ ‡æ¯”ä¾‹
        high_confidence_count = sum(1 for c in confidences if c.value >= 0.66)
        confidence_ratio = (
            high_confidence_count / len(confidences) if confidences else 0
        )
        summary["confidence_ratio"] = confidence_ratio

        if confidence_ratio > 0.7:
            summary["overall_insights"].append("å¤§éƒ¨åˆ†è¶‹åŠ¿åˆ†æç»“æœç½®ä¿¡åº¦è¾ƒé«˜")

        return summary


# ä½¿ç”¨ç¤ºä¾‹
def demo_trend_analysis():
    """æ¼”ç¤ºè¶‹åŠ¿åˆ†æåŠŸèƒ½"""
    print("ğŸ“ˆ æ¼”ç¤ºè¶‹åŠ¿åˆ†æåŠŸèƒ½")

    # åˆ›å»ºè¶‹åŠ¿åˆ†æå™¨
    analyzer = TrendAnalyzer()

    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ® - æµ‹è¯•æˆåŠŸç‡
    success_rates = []
    timestamps = []
    base_rate = 95.0

    for i in range(100):
        timestamp = datetime.now() - timedelta(hours=100 - i)
        # æ·»åŠ ä¸€äº›è¶‹åŠ¿å’Œå™ªå£°
        trend = -0.05 * i  # è½»å¾®ä¸‹é™è¶‹åŠ¿
        noise = np.random.normal(0, 2)
        rate = base_rate + trend + noise
        rate = max(80, min(100, rate))  # é™åˆ¶åœ¨80-100ä¹‹é—´

        success_rates.append(rate)
        timestamps.append(timestamp)

    # åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®
    test_success_data = [
        TimeSeriesPoint(timestamp=t, value=v, category="test_success")
        for t, v in zip(timestamps, success_rates)
    ]

    # æ·»åŠ æ•°æ®åˆ°åˆ†æå™¨
    analyzer.add_historical_data("test_success_rate", test_success_data)

    # æ·»åŠ APIå“åº”æ—¶é—´æ•°æ®ï¼ˆä¸Šå‡è¶‹åŠ¿ï¼‰
    response_times = []
    for i in range(80):
        timestamp = datetime.now() - timedelta(hours=100 - i)
        # ä¸Šå‡è¶‹åŠ¿
        trend = 2.0 * i
        noise = np.random.normal(0, 10)
        response_time = 100 + trend + noise
        response_time = max(50, response_time)  # æœ€å°50ms

        response_times.append(response_time)
        timestamps.append(timestamp)

    api_response_data = [
        TimeSeriesPoint(timestamp=t, value=v, category="api_response")
        for t, v in zip(timestamps[:80], response_times)
    ]
    analyzer.add_historical_data("api_response_time", api_response_data)

    # åˆ†æè¶‹åŠ¿
    print("\nğŸ“Š æµ‹è¯•æˆåŠŸç‡è¶‹åŠ¿åˆ†æ:")
    success_result = analyzer.analyze_trend("test_success_rate")
    print(f"  è¶‹åŠ¿æ–¹å‘: {success_result.direction.value}")
    print(f"  ç½®ä¿¡åº¦: {success_result.confidence.value}")
    print(f"  æ–œç‡: {success_result.slope:.4f}")
    print(f"  RÂ²: {success_result.r_squared:.4f}")
    print(f"  på€¼: {success_result.p_value:.4f}")
    print(f"  é¢„æµ‹ä¸‹ä¸€å€¼: {success_result.prediction:.2f}")
    print(f"  å¼‚å¸¸å€¼æ•°é‡: {len(success_result.anomalies)}")
    print(f"  æ¨¡å¼æ•°é‡: {len(success_result.patterns)}")
    print("  æ´å¯Ÿ:")
    for insight in success_result.insights:
        print(f"    - {insight}")

    print("\nâš¡ APIå“åº”æ—¶é—´è¶‹åŠ¿åˆ†æ:")
    api_result = analyzer.analyze_trend("api_response_time")
    print(f"  è¶‹åŠ¿æ–¹å‘: {api_result.direction.value}")
    print(f"  ç½®ä¿¡åº¦: {api_result.confidence.value}")
    print(f"  æ–œç‡: {api_result.slope:.4f}")
    print(f"  RÂ²: {api_result.r_squared:.4f}")
    print(f"  på€¼: {api_result.p_value:.4f}")
    print(f"  é¢„æµ‹ä¸‹ä¸€å€¼: {api_result.prediction:.2f}")
    print("  æ´å¯Ÿ:")
    for insight in api_result.insights:
        print(f"    - {insight}")

    # åˆ›å»ºå¯è§†åŒ–
    print("\nğŸ¨ åˆ›å»ºå¯è§†åŒ–å›¾è¡¨:")
    try:
        success_chart = analyzer.create_visualization("test_success_rate")
        print(
            f"  âœ… æµ‹è¯•æˆåŠŸç‡å›¾è¡¨å·²ç”Ÿæˆ (åŒ…å« {len(success_chart.get('data', []))} ä¸ªæ•°æ®ç³»åˆ—)"
        )

        api_chart = analyzer.create_visualization("api_response_time")
        print(
            f"  âœ… APIå“åº”æ—¶é—´å›¾è¡¨å·²ç”Ÿæˆ (åŒ…å« {len(api_chart.get('data', []))} ä¸ªæ•°æ®ç³»åˆ—)"
        )
    except Exception as e:
        print(f"  âŒ å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")

    # è·å–æ€»ä½“æ‘˜è¦
    print("\nğŸ“‹ æ€»ä½“è¶‹åŠ¿æ‘˜è¦:")
    summary = analyzer.get_trend_summary(["test_success_rate", "api_response_time"])
    print(f"  æŒ‡æ ‡æ•°é‡: {summary['metrics_count']}")
    print(f"  é«˜ç½®ä¿¡åº¦æŒ‡æ ‡æ¯”ä¾‹: {summary['confidence_ratio']:.2f}")
    print("  æ€»ä½“æ´å¯Ÿ:")
    for insight in summary["overall_insights"]:
        print(f"    - {insight}")

    return analyzer


if __name__ == "__main__":
    analyzer = demo_trend_analysis()
    print("\nâœ… è¶‹åŠ¿åˆ†ææ¼”ç¤ºå®Œæˆ")
