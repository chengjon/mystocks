#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºæµ‹è¯•è¿è¡Œå™¨

æä¾›ç»Ÿä¸€çš„æµ‹è¯•æ‰§è¡Œæ¥å£ï¼Œæ”¯æŒï¼š
- å¤šç§æµ‹è¯•ç±»å‹è¿è¡Œ
- å¹¶å‘æµ‹è¯•æ‰§è¡Œ
- æµ‹è¯•ç»“æœåˆ†æ
- æŠ¥å‘Šç”Ÿæˆ
- æ€§èƒ½ç›‘æ§
"""

import asyncio
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field

from .ai import create_ai_testing_session, run_ai_test_suite
from .data import create_data_optimization_session
from .contract import ContractTestExecutor, ContractTestConfig, ContractTestSuite

logger = logging.getLogger(__name__)


@dataclass
class TestRunConfig:
    """æµ‹è¯•è¿è¡Œé…ç½®"""

    test_types: List[str] = field(default_factory=lambda: ["unit", "integration", "e2e"])
    max_workers: int = 4
    timeout_seconds: int = 300
    enable_ai_enhancement: bool = True
    enable_data_optimization: bool = True
    enable_contract_testing: bool = True
    output_format: str = "json"
    report_dir: str = "test_reports"


@dataclass
class TestExecutionResult:
    """æµ‹è¯•æ‰§è¡Œç»“æœ"""

    test_type: str
    status: str  # passed, failed, error, skipped
    duration: float
    test_count: int = 0
    passed_count: int = 0
    failed_count: int = 0
    skipped_count: int = 0
    error_count: int = 0
    details: Dict[str, Any] = field(default_factory=dict)
    error_message: Optional[str] = None
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())


class EnhancedTestRunner:
    """å¢å¼ºæµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self, config: TestRunConfig):
        self.config = config
        self.results: List[TestExecutionResult] = []
        self.start_time: Optional[datetime] = None
        self.end_time: Optional[datetime] = None

        # åˆå§‹åŒ–ç»„ä»¶
        self.ai_testing_system = None
        self.data_optimizer = None
        self.contract_executor = None

        if self.config.enable_ai_enhancement:
            self.ai_testing_system = create_ai_testing_session()

        if self.config.enable_data_optimization:
            self.data_optimizer = create_data_optimization_session()

        if self.config.enable_contract_testing:
            self.contract_config = ContractTestConfig(
                api_base_url="http://localhost:8000",
                test_timeout=30,
                max_retries=2,
                retry_delay=1,
                enable_security_tests=True,
                enable_auth_tests=True,
                performance_threshold={"response_time_ms": 1000},
            )
            self.contract_executor = ContractTestExecutor(self.contract_config)

    async def run_all_tests(self, project_context: Dict[str, Any] = None) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¯åŠ¨å¢å¼ºæµ‹è¯•è¿è¡Œå™¨...")
        self.start_time = datetime.now()

        try:
            # 1. æ•°æ®ä¼˜åŒ–å‡†å¤‡
            if self.config.enable_data_optimization and self.data_optimizer:
                print("ğŸ“Š ä¼˜åŒ–æµ‹è¯•æ•°æ®...")
                await self._optimize_test_data()

            # 2. AIè¾…åŠ©æµ‹è¯•
            if self.config.enable_ai_enhancement and self.ai_testing_system:
                print("ğŸ¤– æ‰§è¡ŒAIè¾…åŠ©æµ‹è¯•...")
                ai_result = await self._run_ai_tests(project_context)
                self.results.append(ai_result)

            # 3. å¥‘çº¦æµ‹è¯•
            if self.config.enable_contract_testing and self.contract_executor:
                print("ğŸ“‹ æ‰§è¡Œå¥‘çº¦æµ‹è¯•...")
                contract_result = await self._run_contract_tests()
                self.results.append(contract_result)

            # 4. æ ‡å‡†æµ‹è¯•
            print("ğŸ§ª æ‰§è¡Œæ ‡å‡†æµ‹è¯•...")
            standard_results = await self._run_standard_tests()
            self.results.extend(standard_results)

            # 5. æ±‡æ€»ç»“æœ
            self.end_time = datetime.now()
            summary = self._generate_test_summary()

            # 6. ç”ŸæˆæŠ¥å‘Š
            await self._generate_report(summary)

            return summary

        except Exception as e:
            logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e),
                "results": [r.__dict__ for r in self.results],
            }

    async def _optimize_test_data(self):
        """ä¼˜åŒ–æµ‹è¯•æ•°æ®"""
        # è¿™é‡Œå¯ä»¥ä¼˜åŒ–ç‰¹å®šçš„æµ‹è¯•æ•°æ®æ¡£æ¡ˆ
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬åªæ˜¯è¿è¡Œä¸€ä¸ªé€šç”¨çš„ä¼˜åŒ–
        try:
            optimizer = self.data_optimizer
            if optimizer:
                # è·å–æ‰€æœ‰æ•°æ®æ¡£æ¡ˆè¿›è¡Œä¼˜åŒ–
                statistics = await optimizer.get_optimization_statistics()
                print(f"æ•°æ®ç»Ÿè®¡: {statistics}")
        except Exception as e:
            logger.warning(f"æ•°æ®ä¼˜åŒ–è·³è¿‡: {e}")

    async def _run_ai_tests(self, project_context: Dict[str, Any]) -> TestExecutionResult:
        """è¿è¡ŒAIè¾…åŠ©æµ‹è¯•"""
        try:
            # ä½¿ç”¨MyStocksé»˜è®¤ä¸Šä¸‹æ–‡
            if project_context is None:
                from .ai import create_my_stocks_test_context

                project_context = create_my_stocks_test_context()

            # åˆ›å»ºæµ‹è¯•æ‰§è¡Œå™¨
            test_executors = {
                "unit_tests": self._run_unit_tests_with_ai,
                "integration_tests": self._run_integration_tests_with_ai,
                "performance_tests": self._run_performance_tests_with_ai,
            }

            # è¿è¡ŒAIæµ‹è¯•å¥—ä»¶
            results = await run_ai_test_suite(project_context, test_executors)

            return TestExecutionResult(
                test_type="ai_assisted",
                status="passed",
                duration=results.get("execution_duration", 0),
                test_count=results.get("total_tests", 0),
                passed_count=results.get("passed_tests", 0),
                failed_count=results.get("failed_tests", 0),
                skipped_count=results.get("skipped_tests", 0),
                details=results,
            )

        except Exception as e:
            return TestExecutionResult(
                test_type="ai_assisted",
                status="error",
                duration=0,
                error_message=str(e),
            )

    async def _run_contract_tests(self) -> TestExecutionResult:
        """è¿è¡Œå¥‘çº¦æµ‹è¯•"""
        try:
            # åˆ›å»ºæµ‹è¯•å¥—ä»¶ï¼ˆç®€åŒ–ç‰ˆï¼‰
            test_suite = ContractTestSuite(
                name="APIå¥‘çº¦æµ‹è¯•",
                test_cases=[
                    self._create_basic_test_case("GET", "/api/health"),
                    self._create_basic_test_case("GET", "/api/market/status"),
                    self._create_basic_test_case("POST", "/api/auth/login"),
                ],
                parallel_execution=False,
                max_workers=2,
            )

            # æ‰§è¡Œæµ‹è¯•
            async with self.contract_executor:
                execution_results = await self.contract_executor.execute_suite(test_suite)

            # ç»Ÿè®¡ç»“æœ
            total = len(execution_results)
            passed = sum(1 for r in execution_results if r.status.value == "PASSED")
            failed = sum(1 for r in execution_results if r.status.value == "FAILED")
            error = sum(1 for r in execution_results if r.status.value == "ERROR")
            skipped = sum(1 for r in execution_results if r.status.value == "SKIPPED")

            return TestExecutionResult(
                test_type="contract",
                status="passed" if failed == 0 and error == 0 else "completed",
                duration=10,  # å‡è®¾çš„æŒç»­æ—¶é—´
                test_count=total,
                passed_count=passed,
                failed_count=failed,
                error_count=error,
                skipped_count=skipped,
                details={
                    "test_cases": total,
                    "execution_results": [r.__dict__ for r in execution_results],
                },
            )

        except Exception as e:
            return TestExecutionResult(test_type="contract", status="error", duration=0, error_message=str(e))

    def _create_basic_test_case(self, method: str, endpoint: str):
        """åˆ›å»ºåŸºç¡€æµ‹è¯•ç”¨ä¾‹"""
        from .contract.models import ContractTestCase, TestCategory

        return ContractTestCase(
            name=f"{method}_{endpoint}",
            method=method,
            endpoint=endpoint,
            category=TestCategory.API,
            enabled=True,
            priority=5,
        )

    async def _run_standard_tests(self) -> List[TestExecutionResult]:
        """è¿è¡Œæ ‡å‡†æµ‹è¯•"""
        results = []

        # å•å…ƒæµ‹è¯•
        unit_result = await self._run_unit_tests()
        results.append(unit_result)

        # é›†æˆæµ‹è¯•
        integration_result = await self._run_integration_tests()
        results.append(integration_result)

        # E2Eæµ‹è¯•
        e2e_result = await self._run_e2e_tests()
        results.append(e2e_result)

        return results

    async def _run_unit_tests(self) -> TestExecutionResult:
        """è¿è¡Œå•å…ƒæµ‹è¯•"""
        try:
            # æ¨¡æ‹Ÿå•å…ƒæµ‹è¯•æ‰§è¡Œ
            await asyncio.sleep(2)
            return TestExecutionResult(
                test_type="unit",
                status="passed",
                duration=2.0,
                test_count=45,
                passed_count=42,
                failed_count=3,
                skipped_count=0,
                details={"modules_tested": 8, "coverage": 85.5},
            )
        except Exception as e:
            return TestExecutionResult(test_type="unit", status="error", duration=0, error_message=str(e))

    async def _run_integration_tests(self) -> TestExecutionResult:
        """è¿è¡Œé›†æˆæµ‹è¯•"""
        try:
            # æ¨¡æ‹Ÿé›†æˆæµ‹è¯•æ‰§è¡Œ
            await asyncio.sleep(5)
            return TestExecutionResult(
                test_type="integration",
                status="passed",
                duration=5.0,
                test_count=15,
                passed_count=13,
                failed_count=2,
                skipped_count=0,
                details={"services_tested": 5, "api_endpoints": 12},
            )
        except Exception as e:
            return TestExecutionResult(
                test_type="integration",
                status="error",
                duration=0,
                error_message=str(e),
            )

    async def _run_e2e_tests(self) -> TestExecutionResult:
        """è¿è¡ŒE2Eæµ‹è¯•"""
        try:
            # æ¨¡æ‹ŸE2Eæµ‹è¯•æ‰§è¡Œ
            await asyncio.sleep(15)
            return TestExecutionResult(
                test_type="e2e",
                status="passed",
                duration=15.0,
                test_count=8,
                passed_count=7,
                failed_count=1,
                skipped_count=0,
                details={"scenarios": 4, "browsers": ["chrome"]},
            )
        except Exception as e:
            return TestExecutionResult(test_type="e2e", status="error", duration=0, error_message=str(e))

    async def _run_unit_tests_with_ai(self) -> Dict[str, Any]:
        """AIè¾…åŠ©å•å…ƒæµ‹è¯•"""
        await asyncio.sleep(1)
        return {"passed": 25, "failed": 1, "skipped": 0}

    async def _run_integration_tests_with_ai(self) -> Dict[str, Any]:
        """AIè¾…åŠ©é›†æˆæµ‹è¯•"""
        await asyncio.sleep(3)
        return {"passed": 18, "failed": 2, "skipped": 0}

    async def _run_performance_tests_with_ai(self) -> Dict[str, Any]:
        """AIè¾…åŠ©æ€§èƒ½æµ‹è¯•"""
        await asyncio.sleep(8)
        return {"passed": 5, "failed": 0, "skipped": 0}

    def _generate_test_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
        if not self.results:
            return {"status": "no_results"}

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_tests = sum(r.test_count for r in self.results)
        total_passed = sum(r.passed_count for r in self.results)
        total_failed = sum(r.failed_count for r in self.results)
        total_errors = sum(r.error_count for r in self.results)
        total_skipped = sum(r.skipped_count for r in self.results)

        # è®¡ç®—æ€»è€—æ—¶
        total_duration = sum(r.duration for r in self.results)
        if self.start_time and self.end_time:
            actual_duration = (self.end_time - self.start_time).total_seconds()
        else:
            actual_duration = total_duration

        # æŒ‰ç±»å‹ç»Ÿè®¡
        by_type = {}
        for result in self.results:
            by_type[result.test_type] = {
                "status": result.status,
                "test_count": result.test_count,
                "passed": result.passed_count,
                "failed": result.failed_count,
                "errors": result.error_count,
                "skipped": result.skipped_count,
                "duration": result.duration,
            }

        # è®¡ç®—æˆåŠŸç‡
        success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0

        return {
            "status": "completed",
            "execution_start": self.start_time.isoformat() if self.start_time else None,
            "execution_end": self.end_time.isoformat() if self.end_time else None,
            "total_duration_seconds": actual_duration,
            "summary": {
                "total_tests": total_tests,
                "passed": total_passed,
                "failed": total_failed,
                "errors": total_errors,
                "skipped": total_skipped,
                "success_rate": round(success_rate, 2),
            },
            "by_type": by_type,
            "detailed_results": [r.__dict__ for r in self.results],
            "recommendations": self._generate_recommendations(),
        }

    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæµ‹è¯•å»ºè®®"""
        recommendations = []

        # åˆ†æç»“æœ
        has_errors = any(r.status == "error" for r in self.results)
        has_failures = any(r.failed_count > 0 for r in self.results)
        low_coverage = any(r.details.get("coverage", 100) < 80 for r in self.results)

        if has_errors:
            recommendations.append("æ£€æµ‹åˆ°æµ‹è¯•é”™è¯¯ï¼Œè¯·æ£€æŸ¥æµ‹è¯•ç¯å¢ƒå’Œä¾èµ–")

        if has_failures:
            recommendations.append("éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨å¤±è´¥çš„ç”¨ä¾‹")

        if low_coverage:
            recommendations.append("æµ‹è¯•è¦†ç›–ç‡åä½ï¼Œå»ºè®®å¢åŠ æµ‹è¯•ç”¨ä¾‹")

        if self.config.enable_ai_enhancement:
            recommendations.append("AIè¾…åŠ©æµ‹è¯•å·²å¯ç”¨ï¼Œå¯ä»¥åˆ©ç”¨AIä¼˜åŒ–æµ‹è¯•ç­–ç•¥")

        if not recommendations:
            recommendations.append("æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œç³»ç»Ÿè¿è¡Œè‰¯å¥½")

        return recommendations

    async def _generate_report(self, summary: Dict[str, Any]):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report_dir = Path(self.config.report_dir)
        report_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"test_report_{timestamp}.{self.config.output_format}"

        if self.config.output_format == "json":
            report_file = report_dir / filename
            with open(report_file, "w", encoding="utf-8") as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)

        elif self.config.output_format == "html":
            html_content = self._generate_html_report(summary)
            report_file = report_dir / f"{filename}.html"
            with open(report_file, "w", encoding="utf-8") as f:
                f.write(html_content)

        print(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

    def _generate_html_report(self, summary: Dict[str, Any]) -> str:
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        html_template = """
        <!DOCTYPE html>
        <html>
        <head>
            <title>æµ‹è¯•æŠ¥å‘Š</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 20px; }
                .header { background-color: #f0f0f0; padding: 20px; border-radius: 5px; }
                .summary { margin: 20px 0; }
                .by-type { margin: 20px 0; }
                .test-result { border: 1px solid #ddd; padding: 10px; margin: 5px 0; border-radius: 3px; }
                .passed { background-color: #d4edda; }
                .failed { background-color: #f8d7da; }
                .error { background-color: #f8d7da; }
                .skipped { background-color: #fff3cd; }
            </style>
        </head>
        <body>
            <div class="header">
                <h1>æµ‹è¯•æŠ¥å‘Š</h1>
                <p>æ‰§è¡Œæ—¶é—´: {execution_start}</p>
                <p>æ€»è€—æ—¶: {total_duration_seconds:.2f} ç§’</p>
            </div>

            <div class="summary">
                <h2>æµ‹è¯•æ‘˜è¦</h2>
                <p>æ€»æµ‹è¯•æ•°: {summary[total_tests]}</p>
                <p>é€šè¿‡: {summary[passed]}</p>
                <p>å¤±è´¥: {summary[failed]}</p>
                <p>é”™è¯¯: {summary[errors]}</p>
                <p>è·³è¿‡: {summary[skipped]}</p>
                <p>æˆåŠŸç‡: {summary[success_rate]}%</p>
            </div>

            <div class="by-type">
                <h2>æŒ‰ç±»å‹ç»Ÿè®¡</h2>
                {by_type_html}
            </div>

            <div class="recommendations">
                <h2>å»ºè®®</h2>
                <ul>
                    {recommendations_html}
                </ul>
            </div>
        </body>
        </html>
        """

        # ç”Ÿæˆå„ç±»å‹HTML
        by_type_html = ""
        for test_type, data in summary.get("by_type", {}).items():
            status_class = data.get("status", "")
            by_type_html += f"""
            <div class="test-result {status_class}">
                <h3>{test_type}</h3>
                <p>çŠ¶æ€: {data["status"]}</p>
                <p>æµ‹è¯•æ•°: {data["test_count"]}</p>
                <p>é€šè¿‡: {data["passed"]}</p>
                <p>å¤±è´¥: {data["failed"]}</p>
                <p>é”™è¯¯: {data["errors"]}</p>
                <p>è€—æ—¶: {data["duration"]:.2f}ç§’</p>
            </div>
            """

        # ç”Ÿæˆå»ºè®®HTML
        recommendations_html = ""
        for rec in summary.get("recommendations", []):
            recommendations_html += f"<li>{rec}</li>"

        return html_template.format(
            execution_start=summary.get("execution_start", ""),
            total_duration_seconds=summary.get("total_duration_seconds", 0),
            summary=summary.get("summary", {}),
            by_type_html=by_type_html,
            recommendations_html=recommendations_html,
        )


async def run_comprehensive_test_run(
    config: TestRunConfig = None, project_context: Dict[str, Any] = None
) -> Dict[str, Any]:
    """è¿è¡Œç»¼åˆæµ‹è¯•å¥—ä»¶

    Args:
        config: æµ‹è¯•è¿è¡Œé…ç½®
        project_context: é¡¹ç›®ä¸Šä¸‹æ–‡

    Returns:
        æµ‹è¯•ç»“æœå­—å…¸
    """
    if config is None:
        config = TestRunConfig()

    runner = EnhancedTestRunner(config)
    return await runner.run_all_tests(project_context)


# ä½¿ç”¨ç¤ºä¾‹
async def demo_comprehensive_testing():
    """æ¼”ç¤ºç»¼åˆæµ‹è¯•åŠŸèƒ½"""
    print("ğŸ¯ ç»¼åˆæµ‹è¯•æ¼”ç¤º")

    # 1. åˆ›å»ºæµ‹è¯•é…ç½®
    config = TestRunConfig(
        test_types=["unit", "integration", "e2e", "ai", "contract"],
        max_workers=4,
        enable_ai_enhancement=True,
        enable_data_optimization=True,
        enable_contract_testing=True,
        output_format="html",
    )

    # 2. åˆ›å»ºé¡¹ç›®ä¸Šä¸‹æ–‡
    from .ai import create_my_stocks_test_context

    project_context = create_my_stocks_test_context()

    # 3. è¿è¡Œæµ‹è¯•
    results = await run_comprehensive_test_run(config, project_context)

    # 4. æ˜¾ç¤ºç»“æœ
    print("\n=== æµ‹è¯•ç»“æœæ‘˜è¦ ===")
    summary = results.get("summary", {})
    print(f"æ€»æµ‹è¯•æ•°: {summary.get('total_tests', 0)}")
    print(f"é€šè¿‡æ•°: {summary.get('passed', 0)}")
    print(f"å¤±è´¥æ•°: {summary.get('failed', 0)}")
    print(f"æˆåŠŸç‡: {summary.get('success_rate', 0)}%")

    print("\n=== æµ‹è¯•å»ºè®® ===")
    for i, rec in enumerate(results.get("recommendations", []), 1):
        print(f"{i}. {rec}")


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    import asyncio

    asyncio.run(demo_comprehensive_testing())
