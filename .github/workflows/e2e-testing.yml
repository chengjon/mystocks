name: End-to-End Testing Pipeline

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'web/frontend/**'
      - 'web/backend/**'
      - 'tests/e2e/**'
      - '.github/workflows/e2e-testing.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'web/frontend/**'
      - 'web/backend/**'
      - 'tests/e2e/**'
  schedule:
    # æ¯å¤©å‡Œæ™¨2ç‚¹è¿è¡Œå®Œæ•´E2Eæµ‹è¯•
    - cron: '0 2 * * *'

env:
  # å…¨å±€ç¯å¢ƒå˜é‡
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'
  FRONTEND_PORT: 5173
  BACKEND_PORT: 8000
  PLAYWRIGHT_BASE_URL: 'http://localhost:5173'
  PLAYWRIGHT_API_URL: 'http://localhost:8000'

jobs:
  # ç¬¬ä¸€é˜¶æ®µï¼šç¯å¢ƒå‡†å¤‡å’Œä¾èµ–å®‰è£…
  setup-and-install:
    name: Environment Setup & Dependencies
    runs-on: ubuntu-latest
    timeout-minutes: 15

    outputs:
      cache_key_frontend: ${{ steps.cache_key_frontend.outputs.cache_key }}
      cache_key_backend: ${{ steps.cache_key_backend.outputs.cache_key }}

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Generate Frontend Cache Key
        id: cache_key_frontend
        run: |
          echo "cache_key=frontend-${{ hashFiles('web/frontend/package-lock.json', 'web/frontend/package.json') }}" >> $GITHUB_OUTPUT
          echo "Frontend cache key: frontend-${{ hashFiles('web/frontend/package-lock.json', 'web/frontend/package.json') }}"

      - name: Generate Backend Cache Key
        id: cache_key_backend
        run: |
          echo "cache_key=backend-${{ hashFiles('web/backend/requirements.txt', 'web/backend/pyproject.toml') }}" >> $GITHUB_OUTPUT
          echo "Backend cache key: backend-${{ hashFiles('web/backend/requirements.txt', 'web/backend/pyproject.toml') }}"

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: 'web/frontend/package-lock.json'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'web/backend/requirements.txt'

      - name: Cache Frontend Dependencies
        uses: actions/cache@v3
        with:
          path: |
            web/frontend/node_modules
            ~/.npm
          key: ${{ steps.cache_key_frontend.outputs.cache_key }}
          restore-keys: |
            frontend-

      - name: Cache Backend Dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.cache/pip
            web/backend/venv
          key: ${{ steps.cache_key_backend.outputs.cache_key }}
          restore-keys: |
            backend-

      - name: Install Frontend Dependencies
        run: |
          cd web/frontend
          npm ci --only=production
          npm install @playwright/test@1.48.1
          npx playwright install --with-deps

      - name: Install Backend Dependencies
        run: |
          cd web/backend
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt
          pip install pytest-playwright==0.5.1

      - name: Setup Mock Data System
        run: |
          # å¯ç”¨Mockæ•°æ®ç³»ç»Ÿç”¨äºæµ‹è¯•
          export USE_MOCK_DATA=true
          export DATA_SOURCE=mock
          echo "USE_MOCK_DATA=true" >> $GITHUB_ENV
          echo "DATA_SOURCE=mock" >> $GITHUB_ENV
          echo "Mock data system configured for testing"

      - name: Verify Setup
        run: |
          echo "âœ… Node.js version: $(node --version)"
          echo "âœ… Python version: $(python --version)"
          echo "âœ… npm packages installed: $(cd web/frontend && npm list --depth=0 | wc -l)"
          echo "âœ… Python packages installed: $(pip list | wc -l)"
          echo "âœ… Playwright browsers installed: $(npx playwright --version)"

  # ç¬¬äºŒé˜¶æ®µï¼šåç«¯æœåŠ¡æ„å»ºå’Œå¯åŠ¨
  backend-build:
    name: Backend Build & Test
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: setup-and-install

    services:
      # PostgreSQLæœåŠ¡ï¼ˆå¦‚æœéœ€è¦çœŸå®æ•°æ®åº“ï¼‰
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: mystocks_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      # RedisæœåŠ¡ï¼ˆå¦‚æœéœ€è¦ï¼‰
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'web/backend/requirements.txt'

      - name: Install Backend Dependencies
        run: |
          cd web/backend
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt

      - name: Run Backend Tests
        run: |
          cd web/backend
          # è¿è¡Œåç«¯å•å…ƒæµ‹è¯•
          python -m pytest tests/ -v --tb=short

      - name: Start Backend Server
        run: |
          cd web/backend
          export USE_MOCK_DATA=true
          export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/mystocks_test
          python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload &
          echo "Backend server started on port 8000"
          
          # ç­‰å¾…åç«¯æœåŠ¡å¯åŠ¨
          timeout 30 bash -c 'until curl -f http://localhost:8000/health; do sleep 1; done'

      - name: Test Backend API
        run: |
          # æµ‹è¯•å…³é”®APIç«¯ç‚¹
          curl -f http://localhost:8000/health
          curl -f http://localhost:8000/api/auth/csrf-token
          echo "âœ… Backend API endpoints tested successfully"

  # ç¬¬ä¸‰é˜¶æ®µï¼šå‰ç«¯æ„å»º
  frontend-build:
    name: Frontend Build
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: setup-and-install

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: 'web/frontend/package-lock.json'

      - name: Install Dependencies
        run: |
          cd web/frontend
          npm ci

      - name: Run Frontend Tests
        run: |
          cd web/frontend
          # è¿è¡Œå‰ç«¯å•å…ƒæµ‹è¯•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
          if [ -f "tests/unit/vitest.config.ts" ]; then
            npm run test
          fi

      - name: Build Frontend
        run: |
          cd web/frontend
          npm run build
          echo "Frontend build completed"

      - name: Verify Build
        run: |
          ls -la web/frontend/dist/
          echo "âœ… Frontend build artifacts verified"

  # ç¬¬å››é˜¶æ®µï¼šç«¯åˆ°ç«¯æµ‹è¯•æ‰§è¡Œ
  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    timeout-minutes: 60
    needs: [setup-and-install, backend-build, frontend-build]
    strategy:
      fail-fast: false
      matrix:
        browser: [chromium, firefox, webkit]
        shard: [1/4, 2/4, 3/4, 4/4]

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: 'web/frontend/package-lock.json'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'web/backend/requirements.txt'

      - name: Install Dependencies
        run: |
          cd web/frontend
          npm ci
          npm install @playwright/test@1.48.1
          npx playwright install ${{ matrix.browser }} --with-deps
          
          cd ../../web/backend
          pip install -r requirements.txt
          pip install pytest-playwright==0.5.1

      - name: Start Services
        run: |
          # å¯åŠ¨åç«¯æœåŠ¡
          cd web/backend
          export USE_MOCK_DATA=true
          export DATA_SOURCE=mock
          python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 &
          
          # å¯åŠ¨å‰ç«¯å¼€å‘æœåŠ¡å™¨
          cd ../../web/frontend
          export VITE_API_BASE_URL=http://localhost:8000
          npm run dev &
          
          echo "Services started - Backend:8000, Frontend:5173"
          
          # ç­‰å¾…æœåŠ¡å¯åŠ¨
          timeout 60 bash -c 'until curl -f http://localhost:8000/health; do sleep 2; done'
          timeout 60 bash -c 'until curl -f http://localhost:5173; do sleep 2; done'

      - name: Run E2E Tests
        run: |
          cd web/frontend
          
          # è®¾ç½®ç¯å¢ƒå˜é‡
          export PLAYWRIGHT_BASE_URL=http://localhost:5173
          export PLAYWRIGHT_API_URL=http://localhost:8000
          
          # è¿è¡ŒæŒ‡å®šåˆ†ç‰‡çš„E2Eæµ‹è¯•
          npx playwright test \
            --project=${{ matrix.browser }} \
            --shard=${{ matrix.shard }} \
            --reporter=html,json,junit \
            --output=test-results/ \
            --trace=on \
            --video=retain-on-failure \
            --screenshot=only-on-failure

      - name: Upload E2E Test Results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: e2e-test-results-${{ matrix.browser }}-${{ matrix.shard }}
          path: |
            web/frontend/test-results/
            !web/frontend/test-results/**/*.zip
          retention-days: 30

      - name: Upload Playwright Report
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: playwright-report-${{ matrix.browser }}-${{ matrix.shard }}
          path: web/frontend/playwright-report/
          retention-days: 30

      - name: Publish Test Results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: web/frontend/test-results/junit.xml

  # ç¬¬äº”é˜¶æ®µï¼šæµ‹è¯•ç»“æœæ±‡æ€»å’ŒæŠ¥å‘Š
  test-results:
    name: Test Results Summary
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: e2e-tests
    if: always()

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Download All Artifacts
        uses: actions/download-artifact@v3
        with:
          path: test-results/

      - name: Collect Test Results
        run: |
          echo "ğŸ“Š Collecting E2E Test Results..."
          
          # å®‰è£…jqç”¨äºJSONå¤„ç†
          sudo apt-get update && sudo apt-get install -y jq
          
          # ç»Ÿè®¡æµ‹è¯•ç»“æœ
          total_tests=0
          passed_tests=0
          failed_tests=0
          
          for result_dir in test-results/*/; do
            if [ -f "${result_dir}results.json" ]; then
              local_tests=$(jq '.suites[].tests | length' "${result_dir}results.json" 2>/dev/null || echo "0")
              local_passed=$(jq '[.suites[].tests[] | select(.status == "passed")] | length' "${result_dir}results.json" 2>/dev/null || echo "0")
              local_failed=$(jq '[.suites[].tests[] | select(.status == "failed")] | length' "${result_dir}results.json" 2>/dev/null || echo "0")
              
              total_tests=$((total_tests + local_tests))
              passed_tests=$((passed_tests + local_passed))
              failed_tests=$((failed_tests + local_failed))
            fi
          done
          
          echo "ğŸ“ˆ E2E Test Summary:"
          echo "   Total Tests: $total_tests"
          echo "   Passed: $passed_tests"
          echo "   Failed: $failed_tests"
          if [ $total_tests -gt 0 ]; then
            success_rate=$(echo "scale=2; $passed_tests * 100 / $total_tests" | bc)
            echo "   Success Rate: ${success_rate}%"
          fi
          
          # ä¿å­˜æ‘˜è¦åˆ°æ–‡ä»¶
          cat > test-summary.json << EOF
{
  "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
  "total_tests": $total_tests,
  "passed_tests": $passed_tests,
  "failed_tests": $failed_tests,
  "success_rate": "$(echo "scale=2; $passed_tests * 100 / $total_tests" | bc 2>/dev/null || echo "0")",
  "commit": "${{ github.sha }}",
  "branch": "${{ github.ref_name }}",
  "workflow": "${{ github.workflow }}",
  "run_id": "${{ github.run_id }}"
}
EOF
          
          echo "âœ… Test summary saved to test-summary.json"

      - name: Generate Test Report
        run: |
          cat > test-report.md << 'EOF'
# E2E Test Report

**Run ID:** ${{ github.run_id }}  
**Commit:** ${{ github.sha }}  
**Branch:** ${{ github.ref_name }}  
**Timestamp:** $(date -u +%Y-%m-%dT%H:%M:%SZ)

## Test Summary

| Browser | Tests | Passed | Failed | Success Rate |
|---------|-------|---------|---------|-------------|
$(for browser in chromium firefox webkit; do
  echo "| $browser | - | - | - | - |"
done)

## Artifacts

- [E2E Test Results](./artifacts)
- [Playwright Reports](./artifacts)
- [Test Summary](./test-summary.json)

## Next Steps

- Review failed tests in the Playwright report
- Check console logs for any runtime errors
- Monitor test flakiness across runs
EOF
          
          echo "ğŸ“‹ Test report generated"

      - name: Upload Test Summary
        uses: actions/upload-artifact@v3
        with:
          name: test-summary
          path: |
            test-summary.json
            test-report.md

      - name: Comment PR with Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            let summary = 'E2E Tests completed. Check the workflow run for detailed results.';
            
            if (fs.existsSync('test-summary.json')) {
              const data = JSON.parse(fs.readFileSync('test-summary.json', 'utf8'));
              summary = `ğŸ“Š E2E Test Results Summary:
              
              - **Total Tests:** ${data.total_tests}
              - **Passed:** ${data.passed_tests}
              - **Failed:** ${data.failed_tests}
              - **Success Rate:** ${data.success_rate}%
              
              ğŸ“‹ [View Detailed Report](${context.payload.repository.html_url}/actions/runs/${context.runId})`;
            }
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });

  # ç¬¬å…­é˜¶æ®µï¼šæ€§èƒ½æµ‹è¯•
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [setup-and-install, backend-build, frontend-build]
    if: github.event_name == 'push' || github.event_name == 'schedule'

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: 'web/frontend/package-lock.json'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: 'web/backend/requirements.txt'

      - name: Install Dependencies
        run: |
          cd web/frontend
          npm ci
          npm install @playwright/test@1.48.1 lighthouse @lhci/cli
          npx playwright install chromium --with-deps
          
          cd ../../web/backend
          pip install -r requirements.txt

      - name: Start Services
        run: |
          # å¯åŠ¨åç«¯æœåŠ¡
          cd web/backend
          export USE_MOCK_DATA=true
          python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 &
          
          # å¯åŠ¨å‰ç«¯æœåŠ¡
          cd ../../web/frontend
          export VITE_API_BASE_URL=http://localhost:8000
          npm run preview &
          
          echo "Services started for performance testing"
          timeout 60 bash -c 'until curl -f http://localhost:8000/health; do sleep 2; done'
          timeout 60 bash -c 'until curl -f http://localhost:4173; do sleep 2; done'

      - name: Run Lighthouse Performance Tests
        run: |
          cd web/frontend
          
          # è¿è¡ŒLighthouseæ€§èƒ½æµ‹è¯•
          npx lhci autorun \
            --config=.lighthouserc.json \
            --upload.target=temporary-public-storage \
            --collect.numberOfRuns=3

      - name: Run Playwright Performance Tests
        run: |
          cd web/frontend
          
          # è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
          npx playwright test --project=chromium --grep="performance" --reporter=dot

      - name: Upload Performance Results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-results
          path: |
            web/frontend/.lighthouseci/
            web/frontend/test-results/performance-*

  # ç¬¬ä¸ƒé˜¶æ®µï¼šéƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ
  deploy-to-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [test-results, performance-tests]
    if: github.ref == 'refs/heads/develop' && github.event_name == 'push'

    environment: staging

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Deploy to Staging Environment
        run: |
          echo "ğŸš€ Deploying to staging environment..."
          # è¿™é‡Œæ·»åŠ å®é™…çš„éƒ¨ç½²é€»è¾‘
          # ä¾‹å¦‚ï¼šä½¿ç”¨Dockerã€AWSã€Azureç­‰éƒ¨ç½²å·¥å…·
          echo "âœ… Deployment to staging completed"

      - name: Run Smoke Tests on Staging
        run: |
          echo "ğŸ§ª Running smoke tests on staging environment..."
          # è¿è¡Œå…³é”®åŠŸèƒ½çš„å†’çƒŸæµ‹è¯•
          echo "âœ… Smoke tests passed"

  # ç¬¬å…«é˜¶æ®µï¼šéƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
  deploy-to-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [test-results, deploy-to-staging]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    environment: production

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Deploy to Production Environment
        run: |
          echo "ğŸš€ Deploying to production environment..."
          # è¿™é‡Œæ·»åŠ å®é™…çš„ç”Ÿäº§éƒ¨ç½²é€»è¾‘
          # éœ€è¦è€ƒè™‘è“ç»¿éƒ¨ç½²ã€æ»šåŠ¨æ›´æ–°ç­‰ç­–ç•¥
          echo "âœ… Deployment to production completed"

      - name: Run Production Health Checks
        run: |
          echo "ğŸ¥ Running production health checks..."
          # æ£€æŸ¥ç”Ÿäº§ç¯å¢ƒçš„å¥åº·çŠ¶æ€
          echo "âœ… All health checks passed"

      - name: Notify Deployment
        uses: 8398a7/action-slack@v3
        if: always()
        with:
          status: ${{ job.status }}
          channel: '#deployments'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
          fields: repo,message,commit,author,action,eventName,ref,workflow