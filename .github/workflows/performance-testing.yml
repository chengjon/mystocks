name: Performance Testing

on:
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test to run'
        required: true
        default: 'load'
        type: choice
        options:
        - 'load'
        - 'stress'
        - 'spike'
        - 'volume'
  schedule:
    # Run performance tests weekly on Sundays at 3 AM UTC
    - cron: '0 3 * * 0'

jobs:
  performance-test:
    runs-on: ubuntu-latest
    services:
      tdengine:
        image: tdengine/tdengine:3.3.2.0
        ports:
          - 6030:6030
        options: >-
          --health-cmd "taos -s 'show databases;'"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      postgres:
        image: postgres:17
        env:
          POSTGRES_PASSWORD: mystocks_perf
          POSTGRES_DB: mystocks_perf
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e ".[dev]"
        pip install locust pytest-benchmark

    - name: Set up performance test environment
      run: |
        cat > .env << EOF
        TESTING=True
        USE_MOCK_DATA=False
        TDENGINE_HOST=localhost
        TDENGINE_PORT=6030
        TDENGINE_USER=root
        TDENGINE_PASSWORD=taosdata
        TDENGINE_DATABASE=mystocks_perf
        POSTGRESQL_HOST=localhost
        POSTGRESQL_PORT=5432
        POSTGRESQL_USER=postgres
        POSTGRESQL_PASSWORD=mystocks_perf
        POSTGRESQL_DATABASE=mystocks_perf
        JWT_SECRET_KEY=perf_test_jwt_secret_key_for_performance_testing_only
        EOF

    - name: Initialize databases
      run: |
        python scripts/setup_performance_test_db.py

    - name: Run microbenchmarks
      run: |
        python -m pytest tests/performance/ -v --benchmark-only --benchmark-autosave --benchmark-save-data

    - name: Run API performance tests
      run: |
        python scripts/run_api_performance_tests.py --test-type ${{ inputs.test_type || 'load' }}

    - name: Run database performance tests
      run: |
        python scripts/run_db_performance_tests.py

    - name: Generate performance report
      run: |
        python scripts/generate_performance_report.py \
          --benchmark-data .benchmarks/ \
          --api-results api_perf_results.json \
          --db-results db_perf_results.json \
          --output performance-report.html

    - name: Upload performance results
      uses: actions/upload-artifact@v4
      with:
        name: performance-results
        path: |
          performance-report.html
          .benchmarks/
          api_perf_results.json
          db_perf_results.json

    - name: Comment performance results on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const report = fs.readFileSync('performance-report.html', 'utf8');

          await github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: `## ðŸ“Š Performance Test Results

### Test Type: ${{ inputs.test_type || 'load' }}

**Performance Report:** [View Full Report](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})

Key Metrics:
- API Response Time (p95): Check report
- Database Query Time: Check report
- Memory Usage: Check report
- CPU Usage: Check report

${report.substring(0, 1000)}...`
          });

  regression-check:
    needs: performance-test
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Download performance results
      uses: actions/download-artifact@v4
      with:
        name: performance-results

    - name: Check for performance regressions
      run: |
        python scripts/check_performance_regression.py \
          --current-results performance-report.html \
          --baseline-results baseline-performance.json \
          --threshold 10  # 10% degradation threshold

    - name: Update performance baseline
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        cp performance-report.html baseline-performance.json