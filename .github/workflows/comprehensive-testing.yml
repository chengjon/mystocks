name: Comprehensive Testing Solution

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 2 * * 0'  # Weekly Sunday at 2 AM

jobs:
  # AI-Assisted Tests
  ai-tests:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Run AI-Assisted Tests
      run: |
        echo "ğŸ§ª Running AI-Assisted Tests..."
        python -m pytest tests/ai/ -v --tb=short -x

    - name: Upload AI Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: ai-test-results
        path: |
          reports/ai/
          *.log

  # Contract Tests
  contract-tests:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Run Contract Tests
      run: |
        echo "ğŸ“‹ Running Contract Tests..."
        python -m pytest tests/contract/ -v --tb=short -x

    - name: Upload Contract Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: contract-test-results
        path: |
          reports/contract/
          *.log

  # Performance Tests
  performance-tests:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install psutil

    - name: Run Performance Tests
      run: |
        echo "âš¡ Running Performance Tests..."
        python -m pytest tests/performance/ -v --tb=short -x

    - name: Run Benchmark
      run: |
        echo "ğŸ¯ Running Benchmarks..."
        python tests/performance/benchmark.py

    - name: Run Profiling Analysis
      run: |
        echo "ğŸ“Š Running Profiling Analysis..."
        python tests/performance/profiling.py

    - name: Upload Performance Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-test-results
        path: |
          reports/performance/
          benchmark_*.json
          profile_*.json
          *.log

  # Security Tests
  security-tests:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-security.txt

    - name: Run Security Tests
      run: |
        echo "ğŸ” Running Security Tests..."
        python -m pytest tests/security/ -v --tb=short -x

    - name: Security Scan with Bandit
      run: |
        bandit -r tests/ -f json -o reports/security/bandit_report.json || true

    - name: Security Scan with Safety
      run: |
        safety check -r requirements.txt --output=json -o reports/security/safety_report.json || true

    - name: Upload Security Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-test-results
        path: |
          reports/security/
          *.log

  # Chaos Engineering Tests
  chaos-tests:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Run Chaos Tests
      run: |
        echo "ğŸŒªï¸ Running Chaos Engineering Tests..."
        python -m pytest tests/chaos/ -v --tb=short -x

    - name: Upload Chaos Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: chaos-test-results
        path: |
          reports/chaos/
          *.log

  # Data Quality Tests
  data-quality-tests:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Run Data Quality Tests
      run: |
        echo "ğŸ“Š Running Data Quality Tests..."
        python -m pytest tests/data/ -v --tb=short -x

    - name: Generate Quality Report
      run: |
        echo "ğŸ“ˆ Generating Quality Report..."
        python tests/data/quality_metrics.py > reports/data/quality_report.json

    - name: Upload Data Quality Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: data-quality-results
        path: |
          reports/data/
          *.log

  # Quality Gate
  quality-gate:
    needs: [ai-tests, contract-tests, performance-tests, security-tests, chaos-tests, data-quality-tests]
    runs-on: ubuntu-latest
    if: always()

    steps:
    - name: Check All Tests Passed
      run: |
        echo "ğŸ” Quality Gate Check..."

        JOBS=("ai-tests" "contract-tests" "performance-tests" "security-tests" "chaos-tests" "data-quality-tests")
        ALL_PASSED=true

        for job in "${JOBS[@]}"; do
          status=$(gh api repos/${{ github.repository }}/actions/runs --jq '.workflow_runs[0].conclusion' 2>/dev/null || echo "success")
          echo "  $job: $status"
          if [ "$status" != "success" ] && [ "$status" != "skipped" ]; then
            ALL_PASSED=false
          fi
        done

        if [ "$ALL_PASSED" = true ]; then
          echo "âœ… All tests passed quality gate!"
          exit 0
        else
          echo "âŒ Quality gate failed!"
          exit 1
        fi

    - name: Send Notification on Failure
      if: failure()
      run: |
        echo "âš ï¸  Tests failed! Sending notification..."
        # Add notification logic here (Slack, Discord, Email, etc.)

    - name: Generate Summary Report
      run: |
        echo "ğŸ“Š Generating Summary Report..."

        cat > test-summary.md << EOF
# Test Summary Report

**Date**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")
**Branch**: ${{ github.ref_name }}
**Commit**: ${{ github.sha }}

## Test Results

| Test Category | Status |
|---------------|--------|
| AI Tests | ${{ needs.ai-tests.result }} |
| Contract Tests | ${{ needs.contract-tests.result }} |
| Performance Tests | ${{ needs.performance-tests.result }} |
| Security Tests | ${{ needs.security-tests.result }} |
| Chaos Tests | ${{ needs.chaos-tests.result }} |
| Data Quality Tests | ${{ needs.data-quality-tests.result }} |

## Overall Status: ${{ needs.quality-gate.result }}

---
Generated by Comprehensive Testing Solution
EOF

        echo "âœ… Summary report generated"

  # Full Test Suite (Optional - for scheduled runs)
  full-test-suite:
    needs: [ai-tests, contract-tests, performance-tests, security-tests, chaos-tests, data-quality-tests]
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'

    - name: Install all dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -r requirements-security.txt

    - name: Run Full Test Suite
      run: |
        echo "ğŸ§ª Running Full Test Suite..."
        python -m pytest tests/ -v --tb=short --cov=src --cov-report=xml --cov-report=html --cov-fail-under=50

    - name: Generate Full Coverage Report
      run: |
        coverage combine
        coverage xml
        coverage html
        coverage report --show-missing

    - name: Upload Full Coverage
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage.xml
        flags: full-tests
        fail_ci_if_error: false

    - name: Archive Full Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: full-test-results
        path: |
          coverage.xml
          htmlcov/
          reports/
          test-summary.md
